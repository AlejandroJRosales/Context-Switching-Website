<!DOCTYPE html>
<html>

<head>
	<!-- Style sheet for Bootstrap -->
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
		integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous" />

	<!-- Style sheets for "font awesome" icons -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

	<link rel="stylesheet" href="/assets/css/style.css">

	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>Quantum Convolutional Neural Network - Context Switching</title>
</head>

<body>
	<div class="nav-placeholder" id="nav-placeholder"></div>
	<div class="title-section"></div>

	<div class="information">

		<div class="table-of-contents"></div>

		<div class="category-header-div">
			<p class="category-header" id="quantum-cnn">Quantum Convolutional Neural Network</p>
			<div class="section-header-div">
				<p class="section-header" id="research-relation">Introduction</p>
				<p>
					We will look at how Quantum Convolutional Neural Networks QCNN can be used for deep neural networks,
					potentially paving the way for groundbreaking advancements in machine learning image recognition. <a
						href="/tcs/quantumcomputingtheory/">Quantum computing</a> can improve the <a
						href="/tcs/algorithmicanalysis/">time complexity</a> of
					performing
					larger operations, such as convolution matrix multiplication, therefore, allowing
					for an increased number or size of convolution kernel, as well as exploring larger
					or more complex input structures.
				</p>
				<p>
					To continue, we define and analyze each procedure of the QCNN, such as <a class="sliding-link"
						href="#forward-pass">quantum forward propagation</a>, <a class="sliding-link"
						href="#backward-pass">quantum back propagation</a>, and more, and see how quantum computing
					techniques can be applied to improve the operation. Let's begin!
				</p>
			</div>
			<div class="section-header-div">
				<p class="section-header" id="forward-pass">Quantum Forward Propagation</p>
				<p id="section-reference-1">
					Section Reference:
					<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/1911.01117.pdf">
						<i>
							Quantum Algorithms for Deep Convolutional Neural Networks by: Iordanis Kerenidis, Jonas
							Landman, Anupam Prakash. CNRS, IRIF, Universite Paris Diderot, Paris, France (Dated:
							November 5, 2019)
						</i>
						\(^{[1]}\)
					</a>
				</p>
				<div class="subsection-header-div">
					<p class="subsection-header" id="quantum-convolutional-layer">Quantum Convolutional Layer
					</p>

					<p>
						The input for the quantum convolution layer is a \(3\)D tensor input \(X^{\ell} \in
						\mathbb{R}^{H^{\ell} \times W^{\ell} \times D^{\ell}}\) and the weights layer, filter layer, or
						kernel layer is a \(4\)D tensor \(K^{\ell} \in \mathbb{R}^{H \times W \times D^{\ell}
						\times D^{\ell+1}}\), where the input and kernel layer are both stored in QRAM. Given precision
						parameters \(\epsilon\),
						\(\Delta>0\), there exists a quantum algorithm that computes a quantum state that is \(\Delta\)
						close to \(|f(\bar{X}^{\ell+1})\rangle\) where \(X^{\ell+1}=X^{\ell} * K^{\ell}\), and \(f:
						\mathbb{R} \mapsto[0, C]\) is a non-linear activation function. Note, this function could be a
						sigmoidal, hyperbolic tangent, ReLU, softmax, or any other activation function and is rather
						dependent mainly
						on the task the activation function needs to complete. To continue, a classical appoximation for
						the computer quantum state exists where:
					</p>

					<p>
						\(\left\|f\big(\bar{X}^{\ell+1}\big)-f\big(X^{\ell+1}\big)\right\|_{\infty} \leq
						\epsilon\)
					</p>

					<p>
						The time complexity of the quantum algorithm that computes the output quantum state
						\(|f(\bar{X}^{\ell+1})\rangle\)
						is \(\widetilde{O}(M / \epsilon)\). In this equation, \(M\) denotes the maximum norm of
						the product state between one of the kernels and one of the tensor input regions in
						\(X^{\ell}\), where the size is \(HW D^{\ell}\). It is important to note, that
						\(\widetilde{O}\) is made to hide factors poly-logarithmic in \(\Delta\) with respect to the
						size of \(X^{\ell}\) and \(K^{\ell}\).
					</p>

					<p>
						Given that a convolution product is equivalant to a matrix-matrix multiplication and the
						convolutional product between \(X^{\ell}\) and \(K^{\ell}\) is:
					</p>

					<p>
						\(X_{i^{\ell+1}, j^{\ell+1}, d^{\ell+1}}^{\ell+1}=\) \(\sum_{i=0}^H \sum_{j=0}^W
						\sum_{d=0}^{D^{\ell}} K_{i, j, d, d^{\ell+1}}^{\ell} X_{i^{\ell+1}+i, j^{\ell+1}+j, d}^{\ell}\)
					</p>

					<p>
						it is possible to reformulate this convolution product equation as a matrix product. A diagram
						of this reshaping of the input and kernel is given by the authors and depicited below:
					</p>

					<img src="/assets/images/quantum-convolution-matrix-expression.png"
						alt="Quantum Convolution Matrix Reshaping" class="figure-ignore"
						style="width:55%;height:auto;padding:0%0%;">

					<p class="figure-source-txt" id="section-reference-2">
						Image Source:
						<a class="sliding-link" href="#section-reference-1">
							\([1]\)
						</a>
					</p>

					<p> To express this reformulation mathematically, we must do the following. First, the original
						\(3\)D tensor input input for the quantum convolution layer \(X^{\ell} \in
						\mathbb{R}^{H^{\ell} \times W^{\ell} \times D^{\ell}}\) and the \(4\)D tensor kernel layer
						\(K^{\ell} \in \mathbb{R}^{H \times W \times D^{\ell}
						\times D^{\ell+1}}\) needs to be reshaped to matrices. First, \(X^{\ell}\) needs to be expanded
						into a matrix \(A^{\ell} \in\) \(\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times\left(H W
						D^{\ell}\right)}\), such that each row of \(A^{\ell}\) is a vectorized version of a subregion of
						\(X^{\ell}\). Next, the original kernel tensor \(K^{\ell}\) is reshaped into a matrix \(F^{\ell}
						\in\) \(\mathbb{R}^{\left(H W D^{\ell}\right) \times D^{\ell+1}}\), such that each column of
						\(F^{\ell}\) is a vectrozied version of one of the \(D^{\ell+1}\) kernels.
					</p>

					<p>
						This matrix expression of the tensor input and kernel is needed for the
						convolution product \(X^{\ell} * K^{\ell}=X^{\ell+1}\) to be written as a matrix multiplication, such that each column of the output matrix \(Y^{\ell+1} \in
						\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times D^{\ell+1}}\) is a first vectorized form
						of one of the \(D^{\ell+1}\) channels of \(X^{\ell+1}\). Later, we will dicuss how quantum
						computing, specifically quantum parallelism can not only be applied, but improve the time
						complexity of this step.
					</p>

					<p>
						Lastly, quantum states proportional to the rows of input \(A^{\ell}\) and \(F^{\ell}\) are
						used,
						denoted \(|A_p^{\ell}\rangle\) and \(|F_q^{\ell}\rangle\) respectively. These quantum states
						are
						defined as:
					</p>

					<p>
						\( \left|A_p^{\ell}\right\rangle=\) \(\frac{1}{\left\|A_p^{\ell}\right\|} \sum_{r=0}^{H W
						D^{\ell}-1} A_{p r}^{\ell}|r\rangle\)
					</p>

					<p>
						\(\left|F_q^{\ell}\right\rangle=\) \(\frac{1}{\left\|F_q^{\ell}\right\|}
						\sum_{s=0}^{D^{\ell+1}-1}
						F_{s q}^{\ell}|s\rangle\)
					</p>

					<p>
						and will continue to be used throughout this section.
					</p>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="quantum-convolution">Quantum Convolution</p>

						<p>
							The authors describe procedure for the quantum convolutional in the following four
							sequential steps: <a class="sliding-link" href="#step1">inner product estimation</a>, <a
							class="sliding-link" href="#step2">non linearity</a>, <a class="sliding-link"
							href="#step3">quantum sampling</a>, and then <a class="sliding-link"
							href="#qram-update-pooling">quantum random access memory update and pooling</a>.
						</p>

						<p id="step1">
							<b>Inner Product Estimation</b>
						</p>

						<p>
							First we load input row vector \(A_p^{\ell}\) and kernel vector \(F_q^{\ell}\) into quantum
							states by quering QRAM in the following manner:
						</p>

						<p>
							\(\left\{\begin{aligned}
							|p\rangle|0\rangle & \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\
							|q\rangle|0\rangle & \mapsto|q\rangle\left|F_q^{\ell}\right\rangle
							\end{aligned}\right.\)
						</p>

						<p>
							This is done so that following mapping can be perfromed with the two vectors:
						</p>

						<p>
							\(\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto \) \(\frac{1}{K} \sum_{p,
							q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle\)
						</p>

						<p>
							Here \(\bar{P}_{p q}\) is the inner product estimation of
							\(A_p^{\ell}\) and \(F_q^{\ell}\). The "true" value of the inner product is
							calculated as \(P_{p q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}\),
							such
							that the inner product estimation differs by a chosen constant \(\epsilon\). Here, the
							normalization factor is \(K=\sqrt{H^{\ell+1} W^{\ell+1} D^{\ell+1}}\) and \(|g_{pq}\rangle\)
							is
							some garbage state.
						</p>

						<p id="step2">
							<b>Non Linearity</b>
						</p>

						<p>
							Our obtained approximated convolution output \(\bar{P}_{p q}\) or
							\(\bar{Y}^{\ell+1}\) differs from the "true" convolution output \(Y_{p,
							q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)\) by \(\epsilon\). Now, we apply a
							non-linear
							function \(f\) as a boolean circut giving the quantum state:
						</p>

						<p>
							\(\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
							q}\right\rangle\)
						</p>
					</div>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="quantum-sampling">Quantum Sampling</p>
						<p id="step3">
							<b>Conditional Rotation and Amplitude Amplification</b>
						</p>

						<p>
							To procure the state below, states are conditionally rotated and the probabilistic
							amplitudes
							are
							amplified, such that we arrive at the state:
						</p>

						<p>
							\(\frac{1}{K} \sum_{p, q} \alpha_{p
							q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p q}\right\rangle\)
						</p>

						<p>
							Quantum tomography is performed with precision \(\eta\) so that all values and positions
							\((p,
							q, f(\bar{Y}_{p q}^{\ell+1}))\) are obtained with a high probability. Values above \(\eta\)
							are
							known exactly, while values that are less than or equal to \(\eta\) are set to \(0\).
						</p>
					</div>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="qram-update-pooling">QRAM Update and Pooling</p>
						<p>
							Next, QRAM needs to updated with the value for the next layer, which is \(A^{\ell+1}\),
							while sampling. Pooling needs to be implemented in this step as well, either through a
							specific update or by using a QRAM data data structure. We will review <a
								class="sliding-link" href="#quantum-pooling">quantum pooling</a> more in depth later.
						</p>
					</div>
				</div>
				<div class="subection-header-div">
					<p class="subsection-header" id="quantum-pooling">Quantum Pooling</p>

					<p>
						At the end of layer \(\ell\), the pooling operation of size \(P\) is performed on the
						convolution layer output \(f(X^{\ell+1})\), yielding the tensor after pooling
						\(\tilde{X}^{\ell+1}\). Below, thee authors provide a figure shows a \(2\times 2\) tensor
						pooling such that different pooling regions having seperatre colors:
					</p>

					<img src="/assets/images/qcnn-pooling.png" alt="QCNN Pooling Representation" class="figure-ignore"
						style="width:35%;height:auto;padding:1%1%;">

					<p class="figure-source-txt" id="section-reference-2">
						Image Source:
						<a class="sliding-link" href="#section-reference-1">
							\([1]\)
						</a>
					</p>

					<p>
						Here, for some point at position \((i^{\ell+1}, j^{\ell+1}, d^{\ell+1})\) in
						\(f(X^{\ell+1})\), the pooling region it corresponds to is at postion
						\((\tilde{i}^{\ell+1}, \tilde{j}^{\ell+1}, \tilde{d}^{\ell+1})\) in
						\(\tilde{X}^{\ell+1}\), such that:
					</p>

					<p>
						\(\left\{\begin{array}{l}
						\tilde{d}^{\ell+1}=d^{\ell+1} \\
						\tilde{j}^{\ell+1}=\left\lfloor\frac{j^{\ell+1}}{P}\right\rfloor \\
						\tilde{i}^{\ell+1}=\left\lfloor\frac{i^{\ell+1}}{P}\right\rfloor
						\end{array}\right.\)
					</p>

					<p>
						To continue.
					</p>
				</div>
			</div>

			<div class="section-header-div">
				<p class="section-header" id="backward-pass">Quantum Back Propagation</p>
				<p>
					To continue.
				</p>
			</div>

			<div class="section-header-div">
				<p class="section-header" id="quantum-computing-prelimanries">Quantum Computing Preliminaries</p>
				<p>
					You can read about quantum computing preliminaries on my website <a
						href="/tcs/quantumcomputingtheory/">here</a>.
				</p>
			</div>
		</div>
	</div>

	<!--Collapsible table of contents-->
	<div class="table-of-contents-collapsible-div"></div>
	<!-- end of collapsible table of contents -->

	<!-- Footer bar -->
	<div class="footer-placeholder" id="footer-placeholder"></div>
	<!-- end of Footer bar -->

	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
		integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
		crossorigin="anonymous"></script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
		integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
		crossorigin="anonymous"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="/assets/js/preset-divs.js"></script>
	<script src="/assets/js/helper-functions.js"></script>
	<script src="/assets/js/script.js"></script>
</body>

</html>