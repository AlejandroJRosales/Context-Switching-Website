<!DOCTYPE html>
<html>

<head>
	<!-- Style sheet for Bootstrap -->
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
		integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous" />

	<!-- Style sheets for "font awesome" icons -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

	<link rel="stylesheet" href="/assets/css/style.css">

	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>Information Theory - Context Switching</title>
</head>

<body>
	<div class="nav-placeholder" id="nav-placeholder"></div>
	<div class="title-section"></div>

	<div class="information">

		<div class="table-of-contents"></div>

		<div class="category-header-div">
			<p class="category-header" id="information-theory">Information Theory</p>

			<div class="section-header-div">
				<p class="section-header" id="quantties-of-information">Quantities of Information</p>

				<div class="subsection-header-div">
					<p class="subsection-header" id="shannon-information">Shannon Information</p>

					<p>
						Three properties were required by Shannon:
					</p>

					<ol>
						<li>\(I(p) \geq 0\), i.e. information is a real non-negative measure.</li>
						<li>\(I(p_{1},p_{2})=I(p_{1})+I(p_{2})\) for independent events.</li>
						<li>\(I(p)\) is a continous function of \(p\).</li>
					</ol>

					<p>
						The mathematical function that satisfies these requirements is:
					</p>

					<p class="math-def">
						\(I(p)=k\;log(p)\)
					</p>

					<p>
						In the equation, the value of \(k\) is arbitrary, so we choose \(k=-1\) to
						make the math more convient.
					</p>

					<p class="math-def">
						\(I(p)=-\log(p)=\log(\frac{1}{p})\)
					</p>

					<p>
						The base of the logarithm is representative of the units of measure for the
						given information, but can also be chosen arbitarily. However, today
						units of information are exclusively use base 2 logarithms, in units of bits.
					</p>

					<p class="math-def">
						\(I(p)=-\log_{2}(p)=\log_{2}(\frac{1}{p})\) bits of information.
					</p>

				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="shannon-entropy-and-average-code-length">Shannon Entropy and
						Average Code
						Length</p>

					<p>
						Let \(S=\left\{s_{1}, s_{2},\ldots s_{n}\right\}\) some information source with \(n\) symbols.
						Let
						\(P=\left\{p_{1}, p_{2}, \ldots p_{n}\right\}\) be the corresponding probability distribution.
					</p>

					<p>
						Entropy of the source is the defined as:
					</p>

					<p class="math-def">
						\(H(S)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})\)
						<br>
						<br>
						\(=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}\)
					</p>

					<p>
						Another important number is the average code length \(L_{avg}\), defined as:
					</p>

					<p class="math-def">
						\(L_{avg}=\sum_{i=1}^{n}p_{i} \left(\lceil\log_{2}\frac{1}{p_{i}}\rceil+1\right)\)
					</p>

					<p>
						Where, \(c_{i}\) is some code string of the code set \(C\) and \(|\;|\) is the <a
							class="sliding-link" href="#cardinality">cardinality</a> of \(c_{i}\).
					</p>

					<p> Note, the entropy of the source is a lower bound on the average code length that can be
						achieved, given by
						\(H(S) \leq
						L_{avg}\). Meaning, that the source entropy can be reach the compression limit, but can
						never
						exceed it.
					</p>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="kraft-inequality">Kraft Inequality</p>

					<p class="math-def">
						\(K=\sum_{i=1}^{q}\frac{1}{r^{l^{i}}}\leq 1\)
					</p>

					<p>
						Where:
					<ul>
						<li>\(K\) is the Kraft sum</li>
						<li>\(q\) is the number of source symbols</li>
						<li>\(r\) is the radix of the channel alphabet</li>
						<li>\(l_{i}\) is the lengths of the coded symbols.</li>
					</ul>
					</p>

					<p>
						Note that the craft inequality must be satisfied for codes that called <a class="sliding-link"
							href="#uniquely-decodable">uniquely decodable</a>, instantaneous
						codes, or prefix-free codes.
					</p>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="proof-of-achieving-entropy-bound">Proof of Achieving Entropy Bound
					</p>

					<p>
						We can prove the entropy bound that can be achieved using a sufficiently large extension. We can
						first
						define that:
					</p>

					<p class="math-def">
						\(H(S)\leq L_{avg} < H(S)+1\) </p>

							<p>
								We take the extension for any code to the \(n^{th}\) term, resulting in:
							</p>

							<p class="math-def">
						\(H(S^{n})\leq L_{n} < H(S^{n})+1\) </p>

									<p>
										We sequence \(n\) symbols from \(S\), taken from the extended source \(S^{n}\),
										which now has
										\(q^{n}\) symbols.
									</p>

									<p>
										Lastly, we divide our previous equation by \(n\), giving us:
									</p>

									<p class="math-def">
						\(H(S)\leq \frac{L_{n}}{n} < H(S)+\frac{1}{n}\) </p>

											<p>
												What this shows, is that by choosing \(n\) sufficently large, the
												average code length can come
												arbitrarily closer to source entropy, \(H(S)\).
											</p>

											<p>
												This being said, there are practical limitations to choosing a large
												extension on
												our source symbols. As the extensions on our souce become larger, the
												returns on efficecny
												deminish and
												have a higher cost on delay, stroage, and computation.
											</p>
				</div>

				<!-- <div class="subsection-header-div">
					<p class="subsection-header" id="markov-source-modeling">Markov Source Modeling</p>

					<p>
						To continue.
					</p>

				</div> -->

				<div class="subsection-header-div">
					<p class="subsection-header" id="system-entropies">System Entropies</p>
					<p>
						The system entropies represent the input and output entropies. The source entropy can be defined
						as:
					</p>

					<p class="math-def">
						\(H_{r}(A)=\sum_{i=1}^{q}p(a_{i})\log_{r}\left(\frac{1}{p(a_{i})}\right)\)
					</p>

					<p>
						Recall that properties of the entropy function are:
					</p>

					<p>
					<ol>
						<li>\(H_{r}(A)\geq 0\)</li>
						<li>\(H_{r}(A)\leq \log_{r}q\), where \(q\) is the number of input symbols</li>
						<li>\(H_{r}(A)=\log_{r}q\) when all source symbols are equally likely</li>
					</ol>
					</p>

					<p>
						The output entropy can be defined as:
					</p>

					<p class="math-def">
						\(H_{r}(B)=\sum_{j=1}^{s}p(b_{j})\log_{r}\left[\frac{1}{p(b_{j})}\right]\)
					</p>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="conditional-entropy">Conditional Entropy</p>
					<p>
						To continue.
					</p>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="mutual-information">Mutual Information</p>
					<p>
						To continue.
					</p>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="channel-capacity">Channel Capacity</p>
					<p>
						To continue.
					</p>
				</div>
			</div>

			<div class="section-header-div">
				<p class="section-header" id="hamming-code-and-error-correcting">Hamming Code and Error Correcting</p>

				<p>
					We will look at the encoding of a binary source for transmission on a noisy channel and then
					build a Hamming Code to transmit
					one bit of information. The Hamman Coding Model illustrated below shows two valid codewords spheres
					in
					\(3\)-dimensional Hamming
					Space:
				</p>

				<img src="/assets/images/hamming-code-to-transmit-one-bit-of-information-k1-n3.png"
					alt="Hamming Code to Trasmit One Bit Figure" class="figure"
					style="width:70%;height:auto;padding:3%3%;">

				<p class="figure-source-txt">
					Image Source: <a target="_blank" rel="noopener noreferrer"
						href="https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226">https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226</a>
				</p>

				<div class="subsection-header-div">
					<p class="subsection-header" id="hamming-code-and-error-correction-mathematical-tools">Mathematical
						Tools</p>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="hamming-distance">Hamming Distance</p>
						<p>
							Given two \(N\)-bit binary symbols, the Hamming distance between them is the number of
							symbols that
							differ.
							For example, if the symbols are \(0110\) and \(1010\) is \(2\).
						</p>

						<p>
							Note, that binary numbers can be represented as points in an \(N\)-dimensional Hamming
							Space.
						</p>

						<div class="subsubsection-header-div">
							<p class="subsubsection-header" id="n-dimensional-hamming-space">N-Dimensional Hamming Space
							</p>
							<p>
								The volume of an \(N\)-dimensional Hamming space is based on the number of binary
								numbers or points in
								the
								\(N\)-bit binary number and is defined as \(2^{N}\). The hamming space is represented
								with a set of
								\(N\)
								coordinate axes, where each axis contains the discrete values
								\(\left\{0,1\right\}\).</a>
							</p>

							<p>
								The figure below is an example of \(N=4\) dimensional space cube representing a
								\(4\)-dimensional
								Hamming
								space.
							</p>

							<img src="/assets/images/4-dimmensional-hamming-space.png"
								alt="4-dimmensional Hamming Space" class="figure"
								style="width:35%;height:auto;padding:3%3%;">

							<p>
								Hamming space gets harder to visualize when the number of dimensions gets larger, but we
								must trust the
								math.
							</p>
						</div>


					</div>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="prelimenaries-encoding-source-symbols">Encoding Source
							Symbols</p>
						<p>
							Let's use a \(3\)-dimensional Hamming space for the encoding of a binary source on a noisy
							channel.
						</p>

						<img src="/assets/images/3-dimmensional-hamming-space.png" alt="3-dimmensional Hamming Space"
							class="figure" style="width:35%;height:auto;padding:3%3%;">
						<p class="figure-source-txt">
							Image Source: Methods of Mathematics Applied to Calculus, Probability, and Statistics by
							Richard Wesley
							Hamming
						</p>

						<p>
							Let the source alphabet be \(\left\{s_{0}, s_{1}\right\}\) with triple repetition coding
							\(s_{0}=000,
							s_{1}=111\).
						</p>

						<p>
							Encoding a binary source for transmission over a noisy channel can cause bit error, so the
							decoder needs
							to try to catch and fix this. To do, the decoder will map the recievecd codeword to the
							closest legal
							coderword, which is smallest Hamming distance. By looking at our \(3\)-dimensional Hamming
							space, we can
							see
							that the encoder needs to encode the source symbols by a sequence of channel symbols with a
							"good"
							distance. This allows for the decoder to have an eaiser time catching and correcting bit
							errors. However,
							the decoder can still be foolder by multiple bit errors.
						</p>
					</div>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="prelimenaries-hamming-distance-of-a-code">Hamming Distance
							of a Code</p>
						<p>
							The Hamming distance of a
							code can be defined as the minimum pairwise Hamming distance between all of the
							codeword pairs and can allows us to measure the error correction capability of a code.
						</p>

						<p>
							The Hamming distance for the example \(3\)-dimensional Hamming space below is \(3\).
						</p>

						<img src="/assets/images/3-dimmensional-hamming-space.png" alt="3-dimmensional Hamming Space"
							class="figure" style="width:35%;height:auto;padding:3%3%;">

						<p>
							There exists a subset of points in \(N\)-dimensional Hamming space that is the set of legal
							codewords. To
							optimize error correction capability of a code, we need to spread out the set of legal
							codewords in
							\(N\)-dimensional space such that they all have the maximum possible mututal Hamming
							distances.
						</p>
					</div>
				</div>
				<div class="subsection-header-div">
					<p class="subsection-header" id="hamming-code">Hamming Code</p>
					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="hamming-codes">Hamming Codes</p>



						<p>
							Hamming codes are a family of perfect single error correcting codes that can correct all
							\(1\)-bit
							errors or detect all \(2\)-bit errors and are described by \((n,m)\). Where
							\(n\) is the total number of bits in a codeword and \(n=2^{k}-1\). \(m\) is the number of
							data bits in a
							codeword and then lastly \(k\) is the number of parity or check bits in a codeword and
							\(k=n-m\), so
							\(n=m+k\).
						</p>

						<table class="table info-table">
							<caption>
								Codeword Examples
							</caption>
							<thead>
								<tr>
									<th scope="col">n (# of bits)</th>
									<th scope="col">m (# of data bits)</th>
									<th scope="col">k (# of parity bits)</th>
								</tr>
							</thead>
							<tbody>
								<tr>
									<td>7</th>
									<td>4</td>
									<td>3</td>
								</tr>
								<tr>
									<td>15</th>
									<td>11</td>
									<td>3</td>
								</tr>
								<tr>
									<td>31</th>
									<td>26</td>
									<td>5</td>
								</tr>
							</tbody>
						</table>


						<div class="subsubsubsection-header-div">
							<p class="subsubsubsection-header" id="7-4-hamming-code">\((7,4)\) Hamming Code</p>
							<p>
								The \((7,4)\) Hamming code, as with the rest of the Hamming codes in the family, are
								perfect single
								error
								correcting codes and can correct all \(1\)-bit errors or detect all \(2\)-bit errors.
							</p>

							<p>
								The \((7,4)\) Hamming code has \(7\) total number of bits in the
								codeword and \(4\) number of data bits in the codeword. Thus, given that \(k=n-m\), the
								number of parity
								bits is \(3=7-4\).
							</p>

							<p>
								The \((7,4)\) Hamming code has \(2^{m}=2^{4}=16\) codewords with a Hamming distance
								found to be \(3\).
								Given a \(7\)-dimensional Hamming space there exists \(2^{N}=2^{7}=128\) points. Every
								codeword has a
								subset of neighboring codewords seperated by \(1\) vertex that we will call a sphere
								with radius of
								\(r=1\). Each sphere contains \(\binom{N}{r}+\binom{N}{0}\) which is
								\(\binom{7}{1}+\binom{7}{0}=8\)
								points. Given the \(128\) points
								in \(7\)-dimensional
								Hamming space and that there are \(16\) spheres and \(8\) points per sphere
								(\(6*8=128\)) in the
								\((7,4)\)
								Hammaning code, all points are covered. Thus, the \((7,4)\) Hamming code is a perfect
								code.
							</p>

							<p>
								The sphere has with \(r=1\) has a point called the center and \(\binom{n}{1}=n\) points
								on the surface,
								thus, the volume of the sphere or total points in the sphere is:
							</p>

							<p class="math-def">
						\(center\;+\binom{n}{1}=1+n\)
							</p>

							<p>
								Example of a sphere with \(r=2\):
							</p>

							<p class="math-def">
						\(\binom{n}{0}+\binom{n}{1}+\binom{n}{2}\)
							</p>

							<p>
								volume or total points in the sphere where the center is a codeword.
							</p>
						</div>
					</div>
					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="hamming-decoding">Hamming Decoding</p>
						<p>
							To continue.
						</p>
					</div>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="error-correcting">Error Correcting</p>
					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="error-correcting-capability">Error Correcting Capability</p>
						<p>
							The error correcting capability \(t\) of a given code with some Hamming distance \(d\) is
							defined as:
						</p>

						<p class="math-def">
						\(t=\left(\frac{d-1}{2}\right)\)
						</p>

						<p>
							Meaning, that for the Hamming Code to correct \(t\) errors, the sphere with \(r=t\) of some
							codeword in
							Hamming Space, must not intersect with any other sphere. So, some Hamming Code can correct
							\(t\) errors
							when \(d>t\), but fails otherwise.
						</p>

						<p>
							For example, looking at the \((7,4)\) Hamming Code, where \(t=1\) and \(d=3\), \(d\) can be
							visualized as
							the Hamming distance between two codewords. Thus, the \((7,4)\) Hamming Code can correct
							\(t\) errors when
							\(t < 3\), but fails otherwise.</p>
					</div>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="sphere-packing-bound">Sphere Packing Bound</p>
						<p>
							The important part of sphere packing in Hamming Space, is that valid codewords speheres
							cannot intersect.
							To satisfy this requirment, for any code that can correct \(t\) errors, \(2^{m}\) spheres
							each with radius
							\(t\) is required. However, all spheres must fit in the \(2^{n}\) volume of Hamming space.
							So, we are
							limited by the following:
						</p>

						<p class="math-def">
						\(\frac{v_{HS}}{v_{S_{i}}}\geq max(S)\)
						</p>

						<p>
							Where \(v_{HS}\) is the total volume of the Hamming Space \(HS\), \(v_{S_{i}}\) is the
							volume of the
							given sphere \(S_{i}\) from the set of spheres \(S\), and \(max(S)\) is the maximum number
							of spheres.
							Using this, we can now figure out the minimum number if <a class="sliding-link"
								href="#hamming-codes">check bits or parity bits</a> \(k\) that are acheieve any error
							correction
							capability. Recall, that \(k=n-m\) where \(n\) is the total number of bits in a codeword and
							\(m\) is the
							number of data bits in a codeword. Where \(n\) and \(m\) are described by \((n,m)\) of the
							Hamming Code,
							such as \((7,4)\) Hamming Code.
						</p>
					</div>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="hamming-code-example">Hamming Code Example</p>
					<img src="/assets/images/hamming-code-to-transmit-one-bit-of-information-k1-n3.png"
						alt="Hamming Code to Trasmit One Bit Figure" class="figure"
						style="width:70%;height:auto;padding:3%3%;">

					<p>
						To continue.
					</p>
				</div>
			</div>

			<!-- <div class="section-header-div">
				<p class="section-header" id="markov-source-modeling">Markov Source Modeling</p>

				<p>
					To continue.
				</p>

			</div> -->

			<div class="section-header-div">
				<p class="section-header" id="compression-algorithms">Compression Algorithms</p>
				<div class="subsection-header-div">
					<p class="subsection-header" id="huffman-coding-algorithm">Huffman Coding Algorithm</p>
					<p>
						To continue.
					</p>
				</div>
				<div class="subsection-header-div">
					<p class="subsection-header" id="lempel-ziv-compression">Lempel-Ziv Compression</p>
					<p>
						To continue.
					</p>
				</div>
				<div class="subsection-header-div">
					<p class="subsection-header" id="arithmetic-coding">Arithmetic Coding</p>
					<p>
						To continue.
					</p>
				</div>
			</div>

			<div class="section-header-div">
				<p class="section-header" id="glossary">Glossary</p>
				<div class="subsection-header-div">
					<p class="subsection-header" id="cardinality">Cardinality</p>
					<p>
						The cardinality of a set represents the number of elements in the set.
					</p>
				</div>
			</div>

			<div class="subsection-header-div">
				<p class="subsection-header" id="uniquely-decodable">Uniquely Decodable</p>
				<p>
					Codes that are uniquely decodable, instantaneous codes, or prefix-free codes are called such because
					the
					decoder that scans the code instantly recognizes the end of the codeword.
				</p>
			</div>
		</div>
	</div>
	</div>

	<!--Collapsible table of contents-->
	<div class="table-of-contents-collapsible-div"></div>
	<!-- end of collapsible table of contents -->

	<!-- Footer bar -->
	<div class="footer-placeholder" id="footer-placeholder"></div>
	<!-- end of Footer bar -->

	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
		integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
		crossorigin="anonymous"></script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
		integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
		crossorigin="anonymous"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="/assets/js/preset-divs.js"></script>
	<script src="/assets/js/helper-functions.js"></script>
	<script src="/assets/js/script.js"></script>
</body>

</html>