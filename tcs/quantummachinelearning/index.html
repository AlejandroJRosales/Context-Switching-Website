<!DOCTYPE html>
<html>

<head>
	<!-- Style sheet for Bootstrap -->
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
		integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous" />

	<!-- Style sheets for "font awesome" icons -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

	<link rel="stylesheet" href="/assets/css/style.css">

	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>Quantum Machine Learning - Context Switching</title>
</head>

<body>
	<div class="nav-placeholder" id="nav-placeholder"></div>
	<div class="title-section"></div>

	<div class="information">

		<div class="table-of-contents"></div>

		<div class="category-header-div">
			<p class="category-header" id="quantum-machine-learning">Quantum Machine Learning</p>

			<div class="section-header-div">
				<p class="section-header" id="quantum-support-vector-machine">Quantum Support Vector Machine</p>
				<p id="section-reference-1">
					Section Reference:
					<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/1804.11326.pdf">
						<i>
							Supervised Learning with Quantum Enhanced Feature Spaces by: Vojtech Havlicek,* Antonio D.
							Corcoles,
							Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. IBM T.J.
							Watson Research Center and Center for Theoretical Physics,
							Massachusetts Institute of Technology (Dated: June 7, 2018)
						</i>
						\(^{[1]}\)
					</a>
				</p>
				<p id="code-reference-1">
					View my code for using a Quantum Support Vector Machine for credit card fraud detection: <a target="_blank" rel="noopener noreferrer"
						href="https://github.com/AlejandroJRosales/QuantumSVMFraudDetection/blob/main/qsvm.py">
						<i>
							here
						</i>
					</a>
				</p>
				<p id="code-reference-2">
					View my code for a general Quantum Support Vector Machine: <a target="_blank" rel="noopener noreferrer"
						href="https://github.com/AlejandroJRosales/QuantumSupportVectorMachine/blob/main/qsvm.py">
						<i>
							here
						</i>
					</a>
				</p>



				<div class="subsection-header-div">
					<p class="subsection-header" id="quantum-feature-introduction">Introduction</p>
					<p>
						Using quantum computing, the authors suggest obtaining a quantum advantage for
						Support Vector Machine SVM processing. Here, we will analyze how the authors exploit quantum
						mechanics for the algorithmic complexity optimization of a Support Vector Machine with
						high-dimensional feature space.
					</p>

					<p>
						The mapping of classical feature space to quantum feature space needs to first be defined. Let's
						consider we have data from a training set \(T\) and a
						test set \(S\), where \(T, S\) are subsets \(\Omega\subset\mathbb{R}^{d}\).
						For each training set, we assume the data is labeled by a true map \(m: T \cup S
						\rightarrow\left\{+1,
						-1\right\}\), which is not given to the algorithm. The algorithm is given training data, where
						the goal is for the algorithm to infer an approximate mappping \(\tilde{m}:
						T\rightarrow\{+1,-1\}\) for the data. After training, the algorithm is asked to apply the
						approximate mapping on a test set \(\tilde{m}: S\rightarrow\{+1,-1\}\). The approximate mapping
						needs to match the true
						map \(m(\vec{s})=\tilde{m}(\vec{s})\) with a high probability.
					</p>
					<p>
						The authors map the data non-linearly to higher dimensional space and then construct a
						hyperplane to
						seperate and label the data. By using the quantum state space
						as the feature space, the authors hope to obtain a quantum advantage. This quantum state space
						as feature
						space is constructed by mapping the data non-linearly to a quantum state
						\(\Phi:\vec{x}\in\Omega\rightarrow
						|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|\), where \(|\Phi(\vec{x})\rangle\) is a part of
						complex vector space or Hilbert Space and \(\langle\Phi(\vec{x})|\) is the complex conjugate
						that is a dual correspondance. Looking at the non-linear mapping, we can see that the domain
						of the mapping \(\Phi\) is the data
						\(\vec{x}\in\Omega\subset\mathbb{R}^{d}\) or \(\vec{x}\in
						T\cup S\) is now reperesented in quantum state space.
						The codomain or image of \(\Phi\) is a quantum state
						\(|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|\). To continue, we will look at the procedure
						behind quantum mapping, rather than its domain and codomain, training, and then testing.
					</p>
					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="quantum-feature-mapping">Quantum Feature Mapping</p>

						<p>
							The authors suggest that in order to have an advantage over classical approaches, a map
							based on circuts
							that are clasically hard to simulate needs to be constructed, but where the cicut is not too
							deep to test on during experimentation.
						</p>


						<p>
							The authors provide a circut diagram that represents a unitary function for feature mapping
							\(n\)-qubits. Note: we will examine the feature mapping unitary function later. This circut
							diagram is
							given
							as:
						</p>

						<img src="/assets/images/quantum-feature-space-circut.png" alt="Quantum Feature Space Circut"
							class="figure" style="width:40%;height:auto;padding:3%3%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\(^{[1]}\)
							</a>
						</p>
					</div>
				</div>

				<div class="subsubsection-header-div">
					<p class="subsubsection-header" id="quantum-enhanced-feature-space-data-generation">Data Generation
					</p>
					<p>
						First, we will define the map used for the artifical data. The data is generated of dimension
						\(n=d=2\) for
						a
						\(2\)-qubit system with the mapping
						\(\phi_{\{i\}}(\vec{x})=x_{i}\) and \(\phi_{\{1,2\}}(\vec{x})=(\pi-x_{1})(\pi-x_{2})\).
					</p>
					<p>
						Next, we look at how the data is generated. For the data vector labels \(\vec{x}\in T\cup S
						\subset (0,
						2\pi]^{2}\) the authors mention generating it using the parity function
						\(\mathbf{f}=Z_{1}Z_{2}\) and random
						unitary
						\(V\in SU(4)\).
					</p>
					<p>
						Next, to label the data the authors describe the following mapping. Given \(\Delta=0.3\), if:
					</p>

					<p>
						\(\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\geq\Delta\)
					</p>

					<p>
						then \(m(\vec{x})=+1\). If:
					</p>

					<p>
						\(\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\leq-\Delta\)
					</p>

					<p>
						then \(m(\vec{x})=-1\).
					</p>
				</div>
			</div>

			<div class="subsection-header-div">
				<p class="subsection-header" id="quantum-enhanced-feature-proposed-svm-classifiers">Proposed SVM
					Classifiers</p>
				<div class="subsubsection-header-div">
					<p class="subsubsection-header" id="quantum-enhanced-feature-quantum-variational-classification">
						Quantum
						Variational Classification</p>

					<p>
						In the this section, we will deconstruct and analyze individual components of the circut,
						unitary, and
						steps to better
						understand how this processes works. Then will will define the cost function and analyze the
						authors
						exprimental findings.
					</p>

					<div class="subsubsubsection-header-div">
						<p class="subsubsubsection-header" id="svm-classifier-type1-procedure">Procedure</p>

						<p>
							The authors have provided a figure of the quantum variational classification QVC circut and
							it is shown
							below:
						</p>

						<img src="/assets/images/quantum-variational-classification-circut.png"
							alt="Quantum Variational Classification Circut" class="figure"
							style="width:40%;height:auto;padding:1%1%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\(^{[1]}\)
							</a>
						</p>

						<p>
							where \(C=\{+1,-1\}\) and the varitional unitary for QVC as a whole is defined as:
						</p>

						<p>
							\(p_{y}(\vec{x})=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle\)
						</p>

						<p>
							The authors describe the procedure for the QVC in the following four sequential steps: <a
								class="sliding-link" href="#qvc-step1">feature map encoding</a>, <a class="sliding-link"
								href="#qvc-step2">variational optimization</a>, <a class="sliding-link"
								href="#qvc-step3">
								measurement</a>, and then <a class="sliding-link" href="#qvc-step4">
								error probability</a>. The training and classification phase consist of these four main
							parts and is
							what we
							will analyze in the following.
						</p>

						<p id="qvc-step1"><i>Feature Map Encoding</i></p>
						<p>
							Recall that the circut looks like the following:
						</p>

						<img src="/assets/images/quantum-feature-space-circut.png" alt="Quantum Feature Space Circut"
							class="figure" style="width:35%;height:auto;padding:1%1%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\(^{[1]}\)
							</a>
						</p>

						<p>
							The data
							\(\vec{x}\in\Omega\) is mapped to a reference state \(|0\rangle^{n}\) using the feature map
							circut
							\(\mathcal{U}_{\Phi(\vec{x})}\).
							This feature map is an injective encoding of classical information
							\(\vec{x}\in\mathbb{R}^{d}\) to a
							quantum
							state \(|\Phi|(\vec{x})\rangle\langle(\vec{x})\Phi|\) that is on a \(n\)-qubit register such
							that \(d=n\). A quibit is a
							two-level system of Hilbert space \(\mathcal{H}\) and can be represented as
							\(\mathcal{H}_{2}=\mathbb{C}^{2}\). To represent \(n\)-qubits we denote it as
							\(\mathcal{S}(\mathcal{H}_{2}^{\otimes n})\).
						</p>

						<p>
							The offered feature mapping portrayed above is a family of feature maps, that the authors
							because they
							conjecture it is hard to estimate overlap \(|\langle\Phi(\vec{x}) \mid
							\Phi(\vec{y})\rangle|^2\) on a
							classical computer. The family of feature map circuts is defined as follows:
						</p>

						<p>
							\(|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})} H^{\otimes n} U_{\Phi(\vec{x})} H^{\otimes
							n}|0\rangle^{\otimes
							n}\)
						</p>

						<p>
							where \(H\) is a conventional Hadmard gate and \(U_{\Phi(\vec{x})}\) is defined as:
						</p>

						<p>
							\(\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in S}Z_{i}\right)\)
						</p>

						<p>
							and is a diagonal gate in the Pauli Z-basis. For a single qubit, the unitary function
							\(U_{\Phi(x)}\) acts
							as a phase-gate \(Z_{x}\) of angle \(x\in\Omega\).
						</p>

						<p>
							Feature mapping allows for \(2^{n}\) possible coefficents
							\(\phi_{S}\left(\vec{x}\right)\) for non-linear function of the inputed data
							\(\vec{x}\in\mathbb{R}^{n}\).
						</p>

						<p id="qvc-step2"><i>Variational Classification</i></p>
						<p>
							A short depth quantum circut \(W(\vec{\theta})\) is applied to the feature state is a
							variational
							circut used for the optimization method. A depiction of this short depth quantum circut is
							given by the
							authors and shown in the figure below:
						</p>


						<img src="/assets/images/short-depth-quantum-circuit.png" alt="Short Depth Quantum Circut"
							class="figure" style="width:35%;height:auto;padding:1%1%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\([1]\)
							</a>
						</p>

						<p>
							For this short depth quantum circut, the authors use an Ansatz variational unitary
							\(W(\vec{\theta})\), defined as:
						</p>

						<p>
							\(U_{loc}^{(l)}(\theta_{l})U_{ent}...U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})\)
						</p>

						<p>
							Where, \(U_{loc}^{(l)}(\theta_{l})\) is full layers single qubit rotations given as:
						</p>

						<p>
							\(U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})\)
						</p>

						<p>
							and \(U(\theta_{i,t})\in SU(2)\).
						</p>

						<p>
							For the variational unitary, \(U_{ent}\) is an alternating layers of entangling gates and is
							defined as:
						</p>

						<p>
							\(U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)\)
						</p>

						<p>
							Here, \(E\) is an edge in the circut defined vetice set \(\{v_{i}, v_{j}\}\in V\).
							\(\mathbf{CZ}\)
							is
							a
							controlled phase gate applied along the
							edges \((i,j)\in E\) which the authors state is present in the connectivity of the
							superconducting chip.
						</p>

						<!-- The \(\mathbf{CZ}\) circut looks like the following:
						</p>

						<img src="/assets/images/cz-gate.png" alt="CZ Circut" class="figure"
							style="width:20%;height:auto;padding:1%1%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\(^{[1]}\)
							</a>
						</p> -->

						<p>
							This variational circut is parametrized by
							\(\vec{\theta
							}\in\mathbb{R}^{2n(l+1)}\) and it is what is optimized during training as this is what
							classifies the
							data.
						</p>

						<p id="qvc-step3"><i>Measurment</i></p>
						<p>
							Given that the problem is a two label classification \(y\in\{+1,-1\}\), the authors apply a
							binary
							measurment \(\{M_{y}\}\) to the state
							\(W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}\).
						<p>
						<p>
							The authors describe the binary measurment in the following way. The measurment is in the
							\(Z\)-basis,
							outputs
							the bit-string
							\(z\in\{0,1\}^{n}\), where the bit-string is then passed to the boolean function
							\(f:\{0,1\}^{n}\rightarrow\{+1,-1\}\). The binary measurment is defined
							as \(M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})\), where
							\(\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle
							z|\).
						</p>

						<p>
							From this third step, the probability for the outcome \(y\) is obtained. This probability
							\(p_{y}(\vec{x})\) is
							defined as:
						</p>

						<p>
							\(p_{y}(\vec{x})=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle\)
						</p>

						<p>
							Where the probability for measuring either label \(y\in\{+1,-1\}\), denoted as \(p_y\), is
							defined as:
						</p>

						<p>
							\(\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta) \mathbf{f}
							W(\theta)\right|
							\Phi(\vec{x})\right\rangle\right)\)
						</p>

						<p>
							Since the expected value of the measured observable is
							\(\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})| W^{\dagger}(\theta,
							\varphi)
							\mathbf{f} W(\theta, \varphi)\right]\), it can be expressed in terms of the inner product:
						</p>

						<p>
							\(\frac{1}{2^n} \sum_\alpha w_\alpha(\theta) \Phi_\alpha(\vec{x})\)
						</p>

						<p>
							Next, we need to incorporate assigning the label \(y\in\{+1,-1\}\) over the label \(-y\)
							with a given
							fixed
							basis \(b\in[-1,+1]\). In this case, it must be that \(p_{-y}-yb<\ p_{y}\). Subsituting the
								probability for measuring either label \(y\in\{+1,-1\}\) in to the inner product of the
								measured observable, we obtain the value for \(\tilde{m}(\vec{x})\) as:</p>

								<p>
									\(\operatorname{sign}\left(\frac{1}{2^n} \sum_\alpha w_\alpha(\theta)
									\Phi_\alpha(\vec{x})+b\right)\)
								</p>

								<p>
									For the final decision ruling of \(y\), \(R\) repeated measurment shots are
									preformed, yielding the
									empircal
									distribution \(\hat{p}(\vec{x})\), where if
									\(\hat{p}_{-y}(\vec{x}-yb)>\hat{p}_{y}(\vec{x})\) the
									label
									assigned is \(\tilde{m}(\vec{x})=y\). The authors introduced a bias parameter
									\(b\in[-1,1]\) that can
									also be optimized during training. </p>

								<p>
									The authors mention that the feature map circut \(\mathcal{U}_{\Phi(\vec{x})}\) and
									the boolean
									function
									\(f\) are fixed choices. The parameters that are being optimized during training are
									\((\vec{\theta},b)\)
									and
									in order to be optimized, a cost-function is need to be defined. To do so, we need
									to define an error
									probability first, which is what we will look at in the next section.
								</p>
								<p id="qvc-step4"><i>Error Probability</i></p>
								<p>
									In order to find the empirical risk \(R_{emp}(\vec{\theta})\) of the empirical
									distribution
									\(\hat{p}(\vec{x})\) we define the error probability of assigning the incorrect
									labels averaged over
									the
									samples in the training set \(T\). The authors give the definition:
								</p>

								<p>
									\(R_{emp}(\vec{\theta})=\frac{1}{|T|}\sum_{\vec{x}\in T}Pr(\tilde{m}(\vec{x})\neq
									m(\vec{x}))\)
								</p>

								<p>
									that defines this error probability. For our binary classification problem, the
									error probability of assigning the wrong label to some
									given
									data can be clacilated using the binomial cimulative density function CDF for the
									empircal
									distribution
									\(\hat{p}(\vec{x})\). For a large number of samples or shots \(R\gg 1\) the CDF is
									approximated by a
									sigmoid function \(\operatorname{sig}(x)=(1+e^{-x})^{-1}\). The probability for label
									\(m(\vec{x}=y)\) being assigned
									incorrectly can be approximated by:
								</p>

								<p>
									\(Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))\)
									<br>
									<br>
									\(\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
									b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
									\hat{p}_y(\vec{x})}}\right)\)
								</p>
					</div>

					<div class="subsubsubsection-header-div">
						<p class="subsubsubsection-header"
							id="svm-classifier-type1-classifier-optimization-and-testing-results">Classifier
							Optimization and Testing Results
						</p>

						<p>
							The first step is to train the classifier and optimize it. The authors found that they found
							Spall's SPSA stochastic gradient descent algorithm performs the best in the inherent noisy
							experimental setting of quantum computing. They mentioned after the parameters converged
							onto \(\big(\vec{\Theta^{*}},b^{*}\big)\). The second step is then to classify data and
							assign them labels according to the decision rule given by \(\tilde{m}(\vec{s})\), where the
							binary measurement is obtained from the parity function \(\mathbf{f}=Z_1 Z_2\), and ensure
							that for the testing data, \(m(\vec{s})=\tilde{m}(\vec{s})\) with high probability for the
							given data set \(\vec{s}\in S\).
						</p>

						<p>
							The authors note that what is obeserved for the empirical risk, or cost value used for the
							optimizer, is that it converges to a lower depth when the number of layers in the short
							depth quantum circuit for optimization is \(l=4\) than \(l=0\). Interesting enough, error
							mitigation does not appreciably improve the empirical risk results when depth is at \(0\),
							however, does substantially help for larger depths. An important note, is that although
							\(\operatorname{Pr}(\tilde{m}(\vec{x}) \neq m(\vec{x}))\) includes the number of shots \(R\)
							taken in its calculation, during experimentation the authors fixed \(R=200\). The authors
							state the reason for doing this, even though \(R=2000\) in actual experiment, was to avoid
							gradient problems.
						</p>

						<p>
							To continue, after training, comes testing. The yielded trained set of parameters
							\(\big(\vec{\theta}^{*},b^*=0\big)\) where used to classify 20 different test sets randomly
							drawn each time per data set. After analyzing the results, the authors an increasing
							classification success with increasing circut depth \(l\). Noting, that the success rate
							very nearl reaches \(100\)% for circut depths larger than \(1\) and remains up to depth
							\(4\), despite decoherence associated with \(8\) CNOT gates during training and
							classification circuts.
						</p>
					</div>

					<div class="subsubsubsection-header-div">
						<p class="subsubsubsection-header" id="quantum-variational-classification-analysis">Quantum
							Variational Classification Analysis
						</p>
						<p>
							An important aspect of SVM's, is the classification of the data, which is done here based
							off of the decision rule \(p_y(\vec{x})>p_{-y}(\vec{x})-y b\). This decision rule
							\(\tilde{m}(\vec{x})\) can can be restated as
							\(\operatorname{sign}\left(\left\langle\Phi(\vec{x})\left|W^{\dagger}(\vec{\theta})
							\mathbf{f} W(\vec{\theta})\right| \Phi(\vec{x})\right\rangle+b\right)\). The step that
							structures the data in such a way for the decision ruling for classification, is the
							variational circut \(W\), which can be understood as a seperating hyperplane in quantum
							state space.
						</p>

						<p>
							We start by decomposing the quantum variational circut. First, we look to define the
							properties for our group of matrices used. For us, we need a group that has an orthogonal,
							hermatian, matrix basis \(\left\{P_\alpha\right\} \subset \mathbb{C}^{2^n \times 2^n}\).
							Such that \(\alpha=1,...,4^{n}\) with \(\operatorname{tr}\left[P_\alpha^{\dagger}
							P_\beta\right]=2^n \delta_{\alpha, \beta}\). A group of matrices adhereing to this criteria
							is the Pauli-group on \(n\)-qubits. Next, we expand the quantum state and the measurment in
							our choosen matrix basis.
						</p>

						<p>
							The expanded quantum state, or expectation value, can be expressed in terms of
							\(|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|\) \(\rightarrow\)
							\(\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
							\Phi(\vec{x})\right\rangle\). The decision rule expressed in our choosen matrix basis as
							\(w_\alpha(\vec{\theta})\) can be defined as \(W^{\dagger}(\vec{\theta}) \mathbf{f}
							W(\vec{\theta})\) \(\rightarrow\) \(\operatorname{tr}\left[W^{\dagger}(\vec{\theta})
							\mathbf{f} W(\vec{\theta}) P_\alpha\right]\). Lastly, any classification rule or mapping
							\(\tilde{m}(x)\) from a variational unitary can be restated in the SVM form:
						</p>

						<p>
							\(\operatorname{sign}\left(2^{-n} \sum_\alpha w_\alpha(\vec{\theta})
							\Phi_\alpha(\vec{x})+b\right)\)
						</p>

						<p>
							What we can see is that the behavior of the classifier is dependent on the larger term
							\(\omega_{\alpha}\), so by improving this term, this constraining term is lifted from the
							variational circut. The authors note as well that the optimal value for \(\omega_{\alpha}\)
							can alternatively be found through implementing kernel methods and the Wolfe dual approach
							of the SVM. The important idea we can gain from decomposing of variational circut, is that
							we can think of the feature space as the quantum state space that has feature vectors
							\(|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|\) and inner products \(K(\vec{x},
							\vec{z})=|\langle\Phi(\vec{x}) \mid \Phi(\vec{z})\rangle|^2\).
						</p>

						<p>
							With this more clear image of how feature space is being represented as quantum state space,
							we can see that the direct use of Hilbert space
							\(\mathcal{H}=\left(\mathbb{C}^2\right)^{\otimes n}\) for this can lead to conceptual
							errors. Such that, a vector in Hilbert space \(|\Phi(\vec{x})\rangle \in \mathcal{H}\) is
							only defined up to a global phase physically.
						</p>

						<p>
							What was seen was that a quantum advantage is mainly obtained from feature maps that have a
							classicaly hard to estimate kernel.
						</p>
					</div>
				</div>
				<div class="subsubsection-header-div">
					<p class="subsubsection-header" id="quantum-enhanced-feature-quantum-kernel-estimiation">Quantum
						Kernel
						Estimation</p>
					<div class="subsubsubsection-header-div">
						<p class="subsubsubsection-header" id="svm-classifier-type2-procedure">Procedure</p>

						<p>
							Unlike in a variational quantum circut to generate a seperating hyperplane for the
							high-dimesnional feature space, Quantum Kernel Estimation a classical SVM classification
							instead. Meaning, this type of classification does not make a direct use of Hilbert space
							\(\mathcal{H}=\left(\mathbb{C}^2\right)^{\otimes n}\), allowing for it to side step the
							inherent conceptual errors of that quantum variational classification feature map
							representation.
						</p>

						<p>
							To continue.
						</p>
					</div>
				</div>

				<div class="subsubsection-header-div">
					<p class="subsubsection-header" id="quantum-feature-mapping-implementations-for-svm">
						Quantum Feature Mapping Implementations for Support Vector Machines</p>
						<p>
							A feature map can be denoted as \(\Phi: \Omega \subset \mathbb{R}^d \rightarrow \mathcal{S}\left(\mathcal{H}_2^{\otimes n}\right)\) such that \(\Phi: \vec{x} \mapsto|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|\). The map acting on data is input is a unitary circut family \(\mathcal{U}_{\Phi(\vec{x})}\) applied to \(|0\rangle^n\). The result can be denoted as \(|\Phi(\vec{x})\rangle=\mathcal{U}_{\Phi(\vec{x})}|0\rangle^n\), such that the state in the feature space is linearly independent.
							<!-- Consider an injective encoding of classical information \(\vec{x}\in\mathbb{R}^{d}\) to a quantum state \(|\Phi\rangle\langle\Phi|\) on an \(n\)-qubit register. One quibit in Hilbert space is \(\mathcal{H}_2=\mathbb{C}^2\), where \(\mathcal{S}\left(\mathcal{H}_2^{\otimes n}\right)\) is the positive semidefinitie density matrices, apart of \(4^{n}\) dimensional Hilbert space of complex matrices \(\mathcal{M}_{2^n \times 2^n}(\mathbb{C})\). Where, for \(A, B \in \mathcal{M}_{2^n \times 2^n}(\mathbb{C})\), the fitted inner product is defined as \(\left[A^{\dagger} B\right]\). Now we can continue with mapping. -->
						</p>

						<p>
							Let's analyze a feature map corresponding to a product state. First, assume that a feature map is comprised of single qubit rotations \(U(\varphi) \in \mathrm{SU}(2)\) aranged in a quantum circuit. These angles correspond to a non-linear function \(\varphi: \vec{x} \rightarrow(0,2 \pi]^2 \times[0, \pi]\) mapped to the space of Euler angles. This means that the feature mapping action for an individual qubit is:
						</p>

						<p>
							\(\vec{x} \mapsto\left|\phi_i(\vec{x})\right\rangle=U\left(\varphi_i(\vec{x})\right)|0\rangle\)
						</p>

						<p>
							and the feature mapping action for the full quibit state is:
						</p>

						<p>
							\(\Phi: \vec{x} \mapsto|\Phi(\vec{x})\rangle\left\langle\Phi(\vec{x})|\right.\)\(\left.=\bigotimes_{i=1}^n| \phi_i(x)\right\rangle\left\langle\phi_i(x)\right|\)
						</p>

						<p>
							Stoudenmire and Schwab using tensor networks is an example of unitary implementation of feature mapping of classical classifiers. In this implementation, each qubit encodes a single component \(x_{i}\) of \(\vec{x}\in[0,1]^{n}\), thus, using \(n\) qubits. The prepared state of this full qubit feature mapping encoding when expanded to the Pauli-matrix basis:
						</p>

						<p>
							\(\bigotimes_{i=1}^n\left|\phi_i(x)\right\rangle\left\langle\phi_i(x)\right|\)\(=\frac{1}{2^n} \bigotimes_{j=1}^n\left(\sum_{\alpha_j} \Phi_j^{\alpha_j}\left(\theta_j(\vec{x})\right) P_{\alpha_j}\right)\)
						</p>

						<p>
							Such that, with respect to the Pauli-matrix basis, \(\Phi_i^\alpha\left(\theta_i(\vec{x})\right)\)\(=\left\langle\phi_i(x)\left|P_{\alpha_i}\right| \phi_i(x)\right\rangle\) for every \(i=1...,n\) and where \(P_{\alpha_i} \in\left\{\mathbb{1}, X_i, Z_i, Y_i\right\}\).
						</p>

						<p>
							To continue.
						</p>
				</div>
			</div>

			<div class="section-header-div">
				<p class="section-header" id="quantum-nueral-networks">Quantum Neural Networks</p>

				<p id="section-reference-1">
					Section Reference:
					<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/1808.10601.pdf">
						<i>
							Quantum Neural Network States: A Brief Review of Methods and Applications by Zhih-Ahn Jia,
							Biao Yi, Rui
							Zhai, Yu-Chun Wu, Guang-Can Guo, and Guo-Ping Guo from the Key Laboratory of Quantum
							Information, Chinese Academy of Sciences, School of Physics, University of Science and
							Technology of
							China, the CAS Center For Excellence in Quantum Information and Quantum Physics,
							University of Science and Technology of China, Microsoft Station Q and Department of
							Mathematics,
							University of California, et al.
						</i>
						\(^{[2]}\)
					</a>
				</p>

				<p>
					Quantum Neural Networks QNN are models, systems, or devices that combine the features of quantum
					theory with neural networks, where the current goal of QNN model can be to fully exploit both the
					advantages of quantum mechanics and computing in neural networks.
				</p>

				<div class="subsection-header-div">
					<p class="subsection-header" id="quantum-nueral-network-preliminaries">Preliminaries</p>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="hopfield-neural-network">Hopfield Neural Network</p>

						<div class="categories" style="overflow: hidden;">
							<p>McCulloch-Pitts Neurons</p>
						</div>

						<p>
							There are two terms we will introduce for a simple architecture for our neural network.
							First, we will talk about McCulloch-Pitts neurons, which is where a neuron is in an
							'active' state if it is firing with a sufficient rate above some threshold,
							otherwise, the neuron is in a 'resting' state. Mathematically, we denote this relationship
							for a neuron with variables \(x, y = \left\{−1,1\right\}\), where \(1\) is a neuron is
							firing and
							\(−1\) is a neuron resting.
						</p>

						<p>
							The two important features of this neuron are (I) the incoming or pre-synaptic signal coming
							in
							and (II) the outgoing or post-synaptic signal going out.
						</p>

						<p>
							(I) The incoming signal \(\left\{-1,
							1\right\}\)
							from each neuron from \(x_{1},...,x_{m}\) is transformed into a outgoing signal by the
							respective
							synapse
							connecting it to neuron \(y\). The modulation of each synapse is denoted by the parameter
							\(w_{ij}\in[-1,1]\), where \(i=1,...,m\) where \(w_{ij}\) reperesnts the synaptic strength.
						</p>

						<p>
							(II) The outgoing signal of neuron \(y\) is activated in an integrate-and-fire mechanism.
							Meaning,
							that if the sum of the outgoing signals from the neuron is greater than some threshold
							\(\theta_{y}\)
							of neuron \(y\) then the neuron fires \(1\). Otherwise, if the signal strength does not
							exceed some
							threshold \(\theta_{y}\), \(y\) does not fire \(-1\). We can formally denote this as:
						</p>

						<p>
							\(y
							= \begin{cases}
							& 1,\;\;\;\;\;\text{if } \sum_{j=1}^{m}w_{jy}x_{j}\leq\theta_{y}, \\
							& -1,\;\;\text{else. }
							\end{cases}\)
						</p>

						<p>
							Using McCulloch-Pitts neurons, a neural network can be formally defined as a set of neurons
							\(x_{1},...,x_{N}\in\left\{-1,1\right\}\) with thresholds \(\theta_{1},...,\theta_{N}\),
							connected by synapses with synaptic stengths \(w_{ij}\in[-1,1]\), where \(i,j=1,...,N\).
						</p>

						<div class="categories" style="overflow: hidden;">
							<p>Hopfield Neural Network</p>
						</div>

						<p>
							A neural network made of McCulloch-Pitts neurons and obeys the connectivity architecture:
						</p>

						<p>
							\(w_{ij}=w_{ji}, w_{ii}=0\)
						</p>

						<p>
							is called Hopfield Neural Network HNN. We will use HNN to continue, but it is important to
							note
							that the later information can be transferred to other classes of neural networks.
						</p>

					</div>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="qnn-requirements">Requirements</p>

					<p>
						The idea for adding quantum properties into a classical NN is to replace our McCulloch-Pitts
						neuron
						\(x=\left\{-1,1\right\}\) with a quantum neuron QN \(|x\rangle\). Where \(|x\rangle\) is apart
						of
						two-dimensional Hilbert space \(H^{2}\) with a set of basis states
						\(\left\{|0\rangle,|1\rangle\right\}\). The QN is a qubit where the two-levels represent active
						and
						resting neural firing states. We can denote this as:
					</p>

					<p>
						\(|QN\rangle=rest|0\rangle+active|1\rangle\)
					</p>

					<p>
						where \(|rest|^2+|active|^2=1\). By using a qubit we allow for the NN to be in a superposition
						of
						firing patterns. This feature is the central property that we look to exploit in the QNN.
					</p>

					<p>
						To continue.
					</p>

				</div>

			</div>
		</div>
	</div>

	<!--Collapsible table of contents-->
	<div class="table-of-contents-collapsible-div"></div>
	<!-- end of collapsible table of contents -->

	<!-- Footer bar -->
	<div class="footer-placeholder" id="footer-placeholder"></div>
	<!-- end of Footer bar -->

	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
		integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
		crossorigin="anonymous"></script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
		integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
		crossorigin="anonymous"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="/assets/js/preset-divs.js"></script>
	<script src="/assets/js/helper-functions.js"></script>
	<script src="/assets/js/script.js"></script>
</body>

</html>