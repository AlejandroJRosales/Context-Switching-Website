<!DOCTYPE html>
<html>

<head>
	<!-- Style sheet for Bootstrap -->
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
		integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous" />

	<!-- Style sheets for "font awesome" icons -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

	<link rel="stylesheet" href="/assets/css/style.css">

	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>Quantum Enhanced Machine Learning - Context Switching</title>
</head>

<body>
	<div class="nav-placeholder" id="nav-placeholder"></div>
	<div class="title-section"></div>

	<div class="information">

		<div class="table-of-contents"></div>

		<div class="category-header-div">
			<p class="category-header" id="quantum-enhanced-machine-learning">Quantum Enhanced Machine Learning</p>

			<div class="section-header-div">
				<p class="section-header" id="quantum-support-vector-machine">Quantum Support Vector Machine</p>
				<p id="section-reference-1">
					Section Reference:
					<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/1804.11326.pdf">
						<i>
							Supervised Learning with Quantum Enhanced Feature Spaces by: Vojtech Havlicek,∗ Antonio D. Corcoles,
							Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. IBM T.J.
							Watson Research Center and Center for Theoretical Physics,
							Massachusetts Institute of Technology (Dated: June 7, 2018)
						</i>
						\(^{[1]}\)
					</a>
				</p>

				<div class="subsection-header-div">
					<p class="subsection-header" id="quantum-feature-introduction">Introduction</p>
					<p>
						Using quantum computing, the authors suggest obtaining a quantum advantage for
						Support Vector Machine SVM processing. In order to do this, the authors suggest mapping the SVM feature
						space provided in the purely classical data to quantum state space.
					</p>

					<p>
						For the problem, we have data from a training set \(T\) and a
						test set \(S\), where \(T, S\) are a subset \(\Omega\subset\mathbb{R}^{d}\).
						For each training set, we assume the data is labled by a true map \(m: T \cup S \rightarrow\left\{+1,
						-1\right\}\) which is not given to the algorithm. The algorithm is given the training data labels of the
						training data and is asked to infer an approximate mapping \(\tilde{m}\). The goal of the training
						algorithm
						is to infer an approximate map on test set \(\tilde{m}: S\rightarrow\{+1,-1\}\) such that it matches the
						true
						map \(m(\vec{s})=\tilde{m}(\vec{s})\) with high probability for some given test data \(\vec{s}\in S\).
					</p>
					<p>
						The authors map the data non-linearly to a high dimensional space, which is the feature
						space, and then seperate the labeled sample by constructing a hyperplane. By using the quantum state space
						as the feature space, the authors hope to obtain a quantum advantage. This quantum state space as feature
						space is constructed by mapping the data non-linearly to a quantum state \(\Phi:\vec{x}\in\Omega\rightarrow
						|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|\). Looking at the non-linear mapping, we can see that the domain
						of the mapping \(\Phi\) is the data
						\(\vec{x}\in\Omega\subset\mathbb{R}^{d}\) or \(\vec{x}\in
						T\cup S\).
						The codomain or image of \(\Phi\) is a quantum state \(|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|\).
						\(|\Phi(\vec{x})\rangle\) is a part of infinite complex vector space or Hilbert Space. Note, that since
						\(|\Phi(\vec{x})\rangle\) is a part of complex vector space there exists a complex conjugate
						\(\langle\Phi(\vec{x})|\) that is a dual correspondance. We will look more at how this quantum feature
						mapping is done in the next section.
					</p>
				</div>
				<div class="subsection-header-div">
					<p class="subsection-header" id="quantum-feature-mapping">Quantum Feature Mapping</p>

					<p>
						The authors suggest that in order to test an advantage over classical approaches, a map based on circuts
						that are clasically hard to simulate needs to be constructed. The authors propose a cicut that works well
						in
						their expirements and is not too deep to test.
					</p>

					<p>
						First, a feature map for \(n\)-qubits is generated using the following unitary function:
					</p>

					<p>
						\(\mathcal{U}_{\Phi(\vec{x})}=U_{\Phi{(\vec{x})}}H^{\otimes n}U_{\Phi{(\vec{x})}}H^{\otimes n}\)
					</p>

					<p>
						where \(H\) is a conventional Hadmard gate and \(U_{\Phi(\vec{x})}\) is defined as:
					</p>

					<p>
						\(U_{\Phi(\vec{x})}=\exp(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in S}Z_{i})\)
					</p>

					<p>
						and is a diagonal gate in the Pauli Z-basis. For a single qubit, the unitary function \(U_{\Phi(x)}\) acts
						as a phase-gate \(Z_{x}\) of angle \(x\in\Omega\).
					</p>

					<p>
						The authors provide a circut diagram that represents this unitary function for feature mapping
						\(n\)-qubits. This circut diagram is given as:
					</p>

					<img src="/assets/images/quantum-feature-space-circut.png" alt="Quantum Feature Space Circut" class="figure"
						style="width:40%;height:auto;padding:3%3%;">

					<p class="figure-source-txt" id="section-reference-2">
						Image Source:
						<a class="sliding-link" href="#section-reference-1">
							\(^{[1]}\)
						</a>
					</p>
				</div>
			</div>

			<div class="subsection-header-div">
				<p class="subsection-header" id="quantum-enhanced-feature-space-data-generation">Data Generation</p>
				<p>
					First, we will define the map used for the artifical data. The data is generated of dimension \(n=d=2\) for a
					\(2\)-qubit system with the mapping
					\(\phi_{\{i\}}(\vec{x})=x_{i}\) and \(\phi_{\{1,2\}}(\vec{x})=(\pi-x_{1})(\pi-x_{2})\).
				</p>
				<p>
					Next, we look at how the data is generated. For the data vector labels \(\vec{x}\in T\cup S \subset (0,
					2\pi]^{2}\) the authors mention generating it using the parity function \(\mathbf{f}=Z_{1}Z_{2}\) and random
					unitary
					\(V\in SU(4)\).
				</p>
				<p>
					Next, to label the data the authors describe the following mapping. Given \(\Delta=0.3\), if:
				</p>

				<p>
					\(\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\geq\Delta\)
				</p>

				<p>
					then \(m(\vec{x})=+1\). If:
				</p>

				<p>
					\(\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\leq-\Delta\)
				</p>

				<p>
					then \(m(\vec{x})=-1\).
				</p>
			</div>

			<div class="subsection-header-div">
				<p class="subsection-header" id="quantum-enhanced-feature-proposed-svm-classifiers">Proposed SVM
					Classifiers</p>
				<div class="subsubsection-header-div">
					<p class="subsubsection-header" id="quantum-enhanced-feature-quantum-variational-classification">Quantum
						Variational Classification</p>

					<p>
						In the this section, we will deconstruct and analyze individual components of the circut, unitary, and
						steps to better
						understand how this processes works. Then will will define the cost function and analyze the authors
						exprimental findings.
					</p>

					<div class="subsubsubsection-header-div">
						<p class="subsubsubsection-header" id="svm-classifier-type1-protocol">Protocol</p>

						<p>
							The authors have provided a figure of the quantum variational classification QVC circut and it is shown
							below:
						</p>

						<img src="/assets/images/quantum-variational-classification-circut.png"
							alt="Quantum Variational Classification Circut" class="figure"
							style="width:40%;height:auto;padding:1%1%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\(^{[1]}\)
							</a>
						</p>

						<p>
							where \(C=\{+1,-1\}\) and the varitional unitary for QVC as a whole is defined as:
						</p>

						<p>
							\(p_{y}(\vec{x})=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle\)
						</p>

						<p>
							The authors describe the process for the QVC protocol in the following three sequential steps: <a
								class="sliding-link" href="#qvc-step1">feature map encoding</a>, <a class="sliding-link"
								href="#qvc-step2">variational optimization</a>, and then <a class="sliding-link" href="#qvc-step3">
								measurement</a>. The training and classification phase consist of these three main parts and is what we
							will analyze in the following.
						</p>

						<p id="qvc-step1"><i>Feature Map Encoding</i></p>
						<p>
							The data
							\(\vec{x}\in\Omega\) is mapped to a reference state \(|0\rangle^{n}\) using the feature map circut
							\(\mathcal{U}_{\Phi(\vec{x})}\). Recall that the circut looks like the following:
						</p>

						<img src="/assets/images/quantum-feature-space-circut.png" alt="Quantum Feature Space Circut" class="figure"
							style="width:35%;height:auto;padding:1%1%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\(^{[1]}\)
							</a>
						</p>

						<p>
							This feature map is an injective encoding of classical information \(\vec{x}\in\mathbb{R}^{d}\) to a
							quantum
							state \(|\Phi\rangle\langle\Phi|\) that is on a \(n\)-qubit register such that \(d=n\). A quibit is a
							two-level system of Hilbert space \(\mathcal{H}\) and can be represented as
							\(\mathcal{H}_{2}=\mathbb{C}^{2}\). To represent \(n\)-qubits we denote it as
							\(\mathcal{S}(\mathcal{H}_{2}^{\otimes n})\).
						</p>

						<p id="qvc-step2"><i>Variational Classification</i></p>
						<p>
							A short depth quantum circut \(W(\vec{\theta})\) is applied to the feature state is a variational
							circut used for the optimization method. A depiction of this short depth quantum circut is given by the
							authors and shown in the figure below:
						</p>


						<img src="/assets/images/short-depth-quantum-circuit.png" alt="Short Depth Quantum Circut" class="figure"
							style="width:35%;height:auto;padding:1%1%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\(^{[1]}\)
							</a>
						</p>

						<p>
							For this short depth quantum circut, the authors use an Ansatz for the
							variational unitary and define \(W(\vec{\theta})\) as:
						</p>

						<p>
							\(U_{loc}^{(l)}(\theta_{l})U_{ent}...U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})\)
						</p>

						<p>
							Where, \(U_{loc}^{(l)}(\theta_{l})\) is full layers single qubit rotations defined as:
						</p>

						<p>
							\(U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})\)
						</p>

						<p>
							and \(U(\theta_{i,t})\in SU(2)\).
						</p>

						<p>
							For the variational unitary, \(U_{ent}\) is an alternating layers of entangling gates and is defined as:
						</p>

						<p>
							\(U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)\)
						</p>

						<p>
							Here, \(E\) is the an edge in the circut defined vetice set \(\{v_{i}, v_{j}\}\in V\). \(\mathbf{CZ}\)
							is
							a
							controlled phase gate applied along the
							edges \((i,j)\in E\) which the authors state is present in the connectivity of the superconducting chip.
						</p>

						<!-- The \(\mathbf{CZ}\) circut looks like the following:
						</p>

						<img src="/assets/images/cz-gate.png" alt="CZ Circut" class="figure"
							style="width:20%;height:auto;padding:1%1%;">

						<p class="figure-source-txt" id="section-reference-2">
							Image Source:
							<a class="sliding-link" href="#section-reference-1">
								\(^{[1]}\)
							</a>
						</p> -->

						<p>
							This variational circut is parametrized by
							\(\vec{\theta
							}\in\mathbb{R}^{2n(l+1)}\) and it is what is optimized during training as this is what classifies the
							data.
						</p>

						<p id="qvc-step3"><i>Measurment</i></p>
						<p>
							Given that the problem is a two label classification \(y\in\{+1,-1\}\), the authors apply a binary
							measurment \(\{M_{y}\}\) to the state \(W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}\).
						<p>
						<p>
							The authors describe the binary measurment in the following way. The measurment is in the \(Z\)-basis,
							outputs
							the bit-string
							\(z\in\{0,1\}^{n}\), where the bit-string is then passed to the boolean function
							\(f:\{0,1\}^{n}\rightarrow\{+1,-1\}\). The binary measurment is defined
							as \(M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})\), where \(\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle
							z|\).
						</p>

						<p>
							From this third step, the probability for the outcome \(y\) is obtained. This probability
							\(p_{y}(\vec{x})\) is
							defined as:
						</p>

						<p>
							\(p_{y}(\vec{x})=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle\)
						</p>

						<p>
							Where the probability for measuring either lable \(y\in\{+1,-1\}\) is defined as:
						</p>

						<p>
							\(p_y=\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta) \mathbf{f} W(\theta)\right|
							\Phi(\vec{x})\right\rangle\right)\)
						</p>

						<p>
							Since the expected value of the measured observable is
							\(\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})| W^{\dagger}(\theta, \varphi)
							\mathbf{f} W(\theta, \varphi)\right]\), it can be expressed in terms of the inner proudct:
						</p>

						<p>
							\(\frac{1}{2^n} \sum_\alpha w_\alpha(\theta) \Phi_\alpha(\vec{x})\)
						</p>

						<p>
							Next, we need to incorporate assigning the label \(y\in\{+1,-1\}\) over the label \(-y\) with a given
							fixed
							basis \(b\in[-1,+1]\). In this case, it must be that \(p_{-y}-yb<\ p_{y}\). Subsituting the probability
								for measuring either lable \(y\in\{+1,-1\}\)in to the inner product of the measured observable, we
								get:</p>

								<p>
									\(\tilde{m}(\vec{x})=\operatorname{sign}\left(\frac{1}{2^n} \sum_\alpha w_\alpha(\theta)
									\Phi_\alpha(\vec{x})+b\right)\)
								</p>

								<p>
									For the final decision ruling of \(y\), \(R\) repeated measurment shots are preformed, yielding the
									empircal
									distribution \(\hat{p}(\vec{x})\), where if \(\hat{p}_{-y}(\vec{x}-yb)>\hat{p}_{y}(\vec{x})\) the
									label
									assigned is \(\tilde{m}(\vec{x})=y\). The authors introduced a bias parameter \(b\in[-1,1]\) that can
									also be optimized during training. </p>

								<p>
									The authors mention that the feature map circut \(\mathcal{U}_{\Phi(\vec{x})}\) and the boolean
									function
									\(f\) are fixed choices. The parameters that are being optimized during training are
									\((\vec{\theta},b)\)
									and
									in order to be optimized, a cost-function is need to be defined. To do so, we need to define an error
									probability first, which is what we will look at in the next section.
								</p>
					</div>

					<div class="subsubsubsection-header-div">
						<p class="subsubsubsection-header" id="svm-classifier-type1-error-probability">Error Probability</p>

						<p>
							In order to find the empirical risk \(R_{emp}(\vec{\theta})\) of the empirical distribution
							\(\hat{p}(\vec{x})\) we define the error probability of assigning the incorrect labels averaged over the
							samples in the training set \(T\). The authors define this error probability as:
						</p>

						<p>
							\(R_{emp}(\vec{\theta})=\frac{1}{|T|}\sum_{\vec{x}\in T}Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))\)
						</p>

						<p>
							For our binary classification problem, the error probability of assigning the wrong label to some givemn
							data can be clacilated using the binomial cimulative density function CDF for the empircal distribution
							\(\hat{p}(\vec{x})\). For a large number of samples or shots \(R\gg 1\) the CDF is approximated by a
							sigmoid function \(sig(x)=(1+e^{-x})^{-1}\). The probability for label \(m(\vec{x}=y)\) being assigned
							incorrectily can be approximated by:
						</p>

						<p>
							\(Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))\)
							<br>
							<br>
							\(\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
							b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right) \hat{p}_y(\vec{x})}}\right)\)
						</p>
					</div>
				</div>
				<div class="subsubsection-header-div">
					<p class="subsubsection-header" id="quantum-enhanced-feature-quantum-kernel-estimiation">Quantum Kernel
						Estimation</p>
					<p>
						To continue.
					</p>
				</div>
			</div>

			<div class="section-header-div">
				<p class="section-header" id="quantum-nueral-networks">Quantum Neural Networks</p>

				<p id="section-reference-1">
					Section Reference:
					<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/pdf/1808.10601.pdf">
						<i>
							Quantum Neural Network States: A Brief Review of Methods and Applications
							Zhih-Ahn Jia, Biao Yi, Rui Zhai, Yu-Chun Wu, Guang-Can Guo, and Guo-Ping Guo1, Key Laboratory of Quantum
							Information, Chinese Academy of Sciences, School of Physics, University of Science and Technology of
							China, Hefei, Anhui, 230026, P.R. China, et al.
						</i>
						\(^{[2]}\)
					</a>
				</p>

				<p>
					Quantum Neural Networks QNN are models, systems, or devices that combine the features of quantum
					theory with neural networks, where the current goal of QNN model can be to fully exploit both the
					advantages of quantum mechanics and computing in neural networks.
				</p>

				<div class="subsection-header-div">
					<p class="subsection-header" id="quantum-nueral-network-preliminaries">Preliminaries</p>

					<div class="subsubsection-header-div">
						<p class="subsubsection-header" id="hopfield-neural-network">Hopfield Neural Network</p>

						<div class="categories" style="overflow: hidden;">
							<p>McCulloch-Pitts Neurons</p>
						</div>

						<p>
							There are two terms we will introduce for a simple architecture for our neural network.
							First, we will talk about McCulloch-Pitts neurons, which is where a neuron is in an
							'active' state if it is firing with a sufficient rate above some threshold,
							otherwise, the neuron is in a 'resting' state. Mathematically, we denote this relationship
							for a neuron with variables \(x, y = \left\{−1,1\right\}\), where \(1\) is a neuron is firing and
							\(−1\) is a neuron resting.
						</p>

						<p>
							The two important features of this neuron are (I) the incoming or pre-synaptic signal coming in
							and (II) the outgoing or post-synaptic signal going out.
						</p>

						<p>
							(I) The incoming signal \(\left\{-1,
							1\right\}\)
							from each neuron from \(x_{1},...,x_{m}\) is transformed into a outgoing signal by the respective
							synapse
							connecting it to neuron \(y\). The modulation of each synapse is denoted by the parameter
							\(w_{ij}\in[-1,1]\), where \(i=1,...,m\) where \(w_{ij}\) reperesnts the synaptic strength.
						</p>

						<p>
							(II) The outgoing signal of neuron \(y\) is activated in an integrate-and-fire mechanism. Meaning,
							that if the sum of the outgoing signals from the neuron is greater than some threshold \(\theta_{y}\)
							of neuron \(y\) then the neuron fires \(1\). Otherwise, if the signal strength does not exceed some
							threshold \(\theta_{y}\), \(y\) does not fire \(-1\). We can formally denote this as:
						</p>

						<p>
							\(y
							= \begin{cases}
							& 1,\;\;\;\;\;\text{if } \sum_{j=1}^{m}w_{jy}x_{j}\leq\theta_{y}, \\
							& -1,\;\;\text{else. }
							\end{cases}\)
						</p>

						<p>
							Using McCulloch-Pitts neurons, a neural network can be formally defined as a set of neurons
							\(x_{1},...,x_{N}\in\left\{-1,1\right\}\) with thresholds \(\theta_{1},...,\theta_{N}\),
							connected by synapses with synaptic stengths \(w_{ij}\in[-1,1]\), where \(i,j=1,...,N\).
						</p>

						<div class="categories" style="overflow: hidden;">
							<p>Hopfield Neural Network</p>
						</div>

						<p>
							A neural network made of McCulloch-Pitts neurons and obeys the connectivity architecture:
						</p>

						<p>
							\(w_{ij}=w_{ji}, w_{ii}=0\)
						</p>

						<p>
							is called Hopfield Neural Network HNN. We will use HNN to continue, but it is important to note
							that the later information can be transferred to other classes of neural networks.
						</p>

					</div>
				</div>

				<div class="subsection-header-div">
					<p class="subsection-header" id="qnn-requirements">Requirements</p>

					<p>
						The idea for adding quantum properties into a classical NN is to replace our McCulloch-Pitts neuron
						\(x=\left\{-1,1\right\}\) with a quantum neuron QN \(|x\rangle\). Where \(|x\rangle\) is apart of
						two-dimensional Hilbert space \(H^{2}\) with a set of basis states
						\(\left\{|0\rangle,|1\rangle\right\}\). The QN is a qubit where the two-levels represent active and
						resting neural firing states. We can denote this as:
					</p>

					<p>
						\(|QN\rangle=rest|0\rangle+active|1\rangle\)
					</p>

					<p>
						where \(|rest|^2+|active|^2=1\). By using a qubit we allow for the NN to be in a superposition of
						firing patterns. This feature is the central property that we look to exploit in the QNN.
					</p>

					<p>
						To continue.
					</p>

				</div>

			</div>
		</div>
	</div>

	<!--Collapsible table of contents-->
	<div class="table-of-contents-collapsible-div"></div>
	<!-- end of collapsible table of contents -->

	<!-- Footer bar -->
	<div class="footer-placeholder" id="footer-placeholder"></div>
	<!-- end of Footer bar -->

	<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
		integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
		crossorigin="anonymous"></script>
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
	<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
		integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
		crossorigin="anonymous"></script>
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script src="/assets/js/preset-divs.js"></script>
	<script src="/assets/js/helper-functions.js"></script>
	<script src="/assets/js/script.js"></script>
</body>

</html>