<!DOCTYPE html>
<html>

<head>
	<!-- Style sheet for Bootstrap -->
	<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
		integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous" />

	<!-- Style sheets for "font awesome" icons -->
	<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

	<link rel="stylesheet" href="/assets/css/style.css">

	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>Eveything Easter Egg - Context Switching</title>
</head>
</body>
[https://www.contextswitching.org/misc/deontologyconsequentialismvirtueethics]
Deontological, Consequentialist, Virtue Ethics - Context Switching Deontology, Consequentialism, Virtue Ethics
Introduction Virtue Ethics is a branch of one of three major approaches to normative ethics, where normative ethics, at
the risk of oversimplification, is concerned with criteria for what is right and wrong. The three main philosophical
ideologies concerning normative ethics are the following. Virtue ethics, which can be identified from the other
approaches to normative ethics, is that which emphasizes virtues or moral character. Whereas deontology, emphasizes
duties or rules. Lastly, consequentialism, emphasizes the consequences of actions. We will examine the ethical
perspectives, consequences, and implications for each of these notions of what is right and wrong. Let's begin.
Consequentialism Hedonistic vs. Pluralistic Consequentialism Hedonists count both pleasures and pains, where their
ethical theory is that where satisfaction of desire is the highest good and proper aim of human life. A notable
philosopher who espoused hedonistic notions on that which is right and wrong, was Epicurus, such that Epicurus' ethics
is a form of egoistic hedonism. It is important we note, that pleasure is not the absence of pain because there are
times when people experience neither pleasure nor pain and times when we feel both. We can read opposing view points of
hedonism in classic utilitarianism from contemporaries such as Bentham and Mill. They argued that hedonism lowers the
value of human life to the level of animals. Bentham stated that a simple game such as push-pin is renderd as equally
valuable as something highly intellectual, such as poetry, if it produces an equal amount of pleasure. Quantitative
hedonists assert that poetry produced with significance can generate more pleasure than "trivial" games. This is due
impart to the fact that the pleasures derived from poetry are certain (or probable), durable (or lasting), fecund
(likely to lead to other pleasures), pure (unlikely to lead to pains), and so on. Mill employed a different approach in
order to avoid equating push-pin with poetry. Where he made a distinction between higher and lower qualities of
pleasures based on the preferences of individuals who have encountered both types. This hedonistic notion is defined as
qualitative hedonism and has also been a subject of much critism; as with any philsophy worth debating. To continue.
Virtue Ethics Eudaimonist Virtue Ethics Eudaimonist Virtue Ethics, or eudaemonism, defines virtues respective to
eudaimonia. Where Eudaemonists claim we should develop virtues that contribute to or are a constituent of eudaimonia, a
term in Ancient Greek philosophy. Its standard translation is "happiness" or "flourishing" and sometimes translated as
"well-being." Let's consider the disadvantages of each translation. To continue. Deontology To continue. Glossary
Normative Ethics Normative ethics explores ethical behavior and constitutes a subfield of philosophical ethics that
delves into questions about the appropriate moral actions one should take. Deontological, Consequentialist, Virtue
Ethics - Context Switching Deontological, Consequentialist, Virtue Ethics - Context SwitchingDeontology,
Consequentialism, Virtue Ethics Introduction Virtue Ethics is a branch of one of three major approaches to normative
ethics, where normative ethics, at the risk of oversimplification, is concerned with criteria for what is right and
wrong. The three main philosophical ideologies concerning normative ethics are the following. Virtue ethics, which can
be identified from the other approaches to normative ethics, is that which emphasizes virtues or moral character.
Whereas deontology, emphasizes duties or rules. Lastly, consequentialism, emphasizes the consequences of actions. We
will examine the ethical perspectives, consequences, and implications for each of these notions of what is right and
wrong. Let's begin. Consequentialism Hedonistic vs. Pluralistic Consequentialism Hedonists count both pleasures and
pains, where their ethical theory is that where satisfaction of desire is the highest good and proper aim of human life.
A notable philosopher who espoused hedonistic notions on that which is right and wrong, was Epicurus, such that
Epicurus' ethics is a form of egoistic hedonism. It is important we note, that pleasure is not the absence of pain
because there are times when people experience neither pleasure nor pain and times when we feel both. We can read
opposing view points of hedonism in classic utilitarianism from contemporaries such as Bentham and Mill. They argued
that hedonism lowers the value of human life to the level of animals. Bentham stated that a simple game such as push-pin
is renderd as equally valuable as something highly intellectual, such as poetry, if it produces an equal amount of
pleasure. Quantitative hedonists assert that poetry produced with significance can generate more pleasure than "trivial"
games. This is due impart to the fact that the pleasures derived from poetry are certain (or probable), durable (or
lasting), fecund (likely to lead to other pleasures), pure (unlikely to lead to pains), and so on. Mill employed a
different approach in order to avoid equating push-pin with poetry. Where he made a distinction between higher and lower
qualities of pleasures based on the preferences of individuals who have encountered both types. This hedonistic notion
is defined as qualitative hedonism and has also been a subject of much critism; as with any philsophy worth debating. To
continue. Virtue Ethics Eudaimonist Virtue Ethics Eudaimonist Virtue Ethics, or eudaemonism, defines virtues respective
to eudaimonia. Where Eudaemonists claim we should develop virtues that contribute to or are a constituent of eudaimonia,
a term in Ancient Greek philosophy. Its standard translation is "happiness" or "flourishing" and sometimes translated as
"well-being." Let's consider the disadvantages of each translation. To continue. Deontology To continue. Glossary
Normative Ethics Normative ethics explores ethical behavior and constitutes a subfield of philosophical ethics that
delves into questions about the appropriate moral actions one should take. Deontology, Consequentialism, Virtue Ethics
Introduction Virtue Ethics is a branch of one of three major approaches to normative ethics, where normative ethics, at
the risk of oversimplification, is concerned with criteria for what is right and wrong. The three main philosophical
ideologies concerning normative ethics are the following. Virtue ethics, which can be identified from the other
approaches to normative ethics, is that which emphasizes virtues or moral character. Whereas deontology, emphasizes
duties or rules. Lastly, consequentialism, emphasizes the consequences of actions. We will examine the ethical
perspectives, consequences, and implications for each of these notions of what is right and wrong. Let's begin.
Consequentialism Hedonistic vs. Pluralistic Consequentialism Hedonists count both pleasures and pains, where their
ethical theory is that where satisfaction of desire is the highest good and proper aim of human life. A notable
philosopher who espoused hedonistic notions on that which is right and wrong, was Epicurus, such that Epicurus' ethics
is a form of egoistic hedonism. It is important we note, that pleasure is not the absence of pain because there are
times when people experience neither pleasure nor pain and times when we feel both. We can read opposing view points of
hedonism in classic utilitarianism from contemporaries such as Bentham and Mill. They argued that hedonism lowers the
value of human life to the level of animals. Bentham stated that a simple game such as push-pin is renderd as equally
valuable as something highly intellectual, such as poetry, if it produces an equal amount of pleasure. Quantitative
hedonists assert that poetry produced with significance can generate more pleasure than "trivial" games. This is due
impart to the fact that the pleasures derived from poetry are certain (or probable), durable (or lasting), fecund
(likely to lead to other pleasures), pure (unlikely to lead to pains), and so on. Mill employed a different approach in
order to avoid equating push-pin with poetry. Where he made a distinction between higher and lower qualities of
pleasures based on the preferences of individuals who have encountered both types. This hedonistic notion is defined as
qualitative hedonism and has also been a subject of much critism; as with any philsophy worth debating. To continue.
Virtue Ethics Eudaimonist Virtue Ethics Eudaimonist Virtue Ethics, or eudaemonism, defines virtues respective to
eudaimonia. Where Eudaemonists claim we should develop virtues that contribute to or are a constituent of eudaimonia, a
term in Ancient Greek philosophy. Its standard translation is "happiness" or "flourishing" and sometimes translated as
"well-being." Let's consider the disadvantages of each translation. To continue. Deontology To continue. Glossary
Normative Ethics Normative ethics explores ethical behavior and constitutes a subfield of philosophical ethics that
delves into questions about the appropriate moral actions one should take. Deontology, Consequentialism, Virtue Ethics
Introduction Virtue Ethics is a branch of one of three major approaches to normative ethics, where normative ethics, at
the risk of oversimplification, is concerned with criteria for what is right and wrong. The three main philosophical
ideologies concerning normative ethics are the following. Virtue ethics, which can be identified from the other
approaches to normative ethics, is that which emphasizes virtues or moral character. Whereas deontology, emphasizes
duties or rules. Lastly, consequentialism, emphasizes the consequences of actions. We will examine the ethical
perspectives, consequences, and implications for each of these notions of what is right and wrong. Let's begin.
Consequentialism Hedonistic vs. Pluralistic Consequentialism Hedonists count both pleasures and pains, where their
ethical theory is that where satisfaction of desire is the highest good and proper aim of human life. A notable
philosopher who espoused hedonistic notions on that which is right and wrong, was Epicurus, such that Epicurus' ethics
is a form of egoistic hedonism. It is important we note, that pleasure is not the absence of pain because there are
times when people experience neither pleasure nor pain and times when we feel both. We can read opposing view points of
hedonism in classic utilitarianism from contemporaries such as Bentham and Mill. They argued that hedonism lowers the
value of human life to the level of animals. Bentham stated that a simple game such as push-pin is renderd as equally
valuable as something highly intellectual, such as poetry, if it produces an equal amount of pleasure. Quantitative
hedonists assert that poetry produced with significance can generate more pleasure than "trivial" games. This is due
impart to the fact that the pleasures derived from poetry are certain (or probable), durable (or lasting), fecund
(likely to lead to other pleasures), pure (unlikely to lead to pains), and so on. Mill employed a different approach in
order to avoid equating push-pin with poetry. Where he made a distinction between higher and lower qualities of
pleasures based on the preferences of individuals who have encountered both types. This hedonistic notion is defined as
qualitative hedonism and has also been a subject of much critism; as with any philsophy worth debating. To continue.
Virtue Ethics Eudaimonist Virtue Ethics Eudaimonist Virtue Ethics, or eudaemonism, defines virtues respective to
eudaimonia. Where Eudaemonists claim we should develop virtues that contribute to or are a constituent of eudaimonia, a
term in Ancient Greek philosophy. Its standard translation is "happiness" or "flourishing" and sometimes translated as
"well-being." Let's consider the disadvantages of each translation. To continue. Deontology To continue. Deontology,
Consequentialism, Virtue EthicsIntroduction Virtue Ethics is a branch of one of three major approaches to normative
ethics, where normative ethics, at the risk of oversimplification, is concerned with criteria for what is right and
wrong. The three main philosophical ideologies concerning normative ethics are the following. Virtue ethics, which can
be identified from the other approaches to normative ethics, is that which emphasizes virtues or moral character.
Whereas deontology, emphasizes duties or rules. Lastly, consequentialism, emphasizes the consequences of actions. We
will examine the ethical perspectives, consequences, and implications for each of these notions of what is right and
wrong. Let's begin. IntroductionVirtue Ethics is a branch of one of three major approaches to normative ethics, where
normative ethics, at the risk of oversimplification, is concerned with criteria for what is right and wrong. The three
main philosophical ideologies concerning normative ethics are the following. Virtue ethics, which can be identified from
the other approaches to normative ethics, is that which emphasizes virtues or moral character. Whereas deontology,
emphasizes duties or rules. Lastly, consequentialism, emphasizes the consequences of actions. We will examine the
ethical perspectives, consequences, and implications for each of these notions of what is right and wrong. Let's begin.
normative ethicsConsequentialism Hedonistic vs. Pluralistic Consequentialism Hedonists count both pleasures and pains,
where their ethical theory is that where satisfaction of desire is the highest good and proper aim of human life. A
notable philosopher who espoused hedonistic notions on that which is right and wrong, was Epicurus, such that Epicurus'
ethics is a form of egoistic hedonism. It is important we note, that pleasure is not the absence of pain because there
are times when people experience neither pleasure nor pain and times when we feel both. We can read opposing view points
of hedonism in classic utilitarianism from contemporaries such as Bentham and Mill. They argued that hedonism lowers the
value of human life to the level of animals. Bentham stated that a simple game such as push-pin is renderd as equally
valuable as something highly intellectual, such as poetry, if it produces an equal amount of pleasure. Quantitative
hedonists assert that poetry produced with significance can generate more pleasure than "trivial" games. This is due
impart to the fact that the pleasures derived from poetry are certain (or probable), durable (or lasting), fecund
(likely to lead to other pleasures), pure (unlikely to lead to pains), and so on. Mill employed a different approach in
order to avoid equating push-pin with poetry. Where he made a distinction between higher and lower qualities of
pleasures based on the preferences of individuals who have encountered both types. This hedonistic notion is defined as
qualitative hedonism and has also been a subject of much critism; as with any philsophy worth debating. To continue.
ConsequentialismHedonistic vs. Pluralistic Consequentialism Hedonists count both pleasures and pains, where their
ethical theory is that where satisfaction of desire is the highest good and proper aim of human life. A notable
philosopher who espoused hedonistic notions on that which is right and wrong, was Epicurus, such that Epicurus' ethics
is a form of egoistic hedonism. It is important we note, that pleasure is not the absence of pain because there are
times when people experience neither pleasure nor pain and times when we feel both. We can read opposing view points of
hedonism in classic utilitarianism from contemporaries such as Bentham and Mill. They argued that hedonism lowers the
value of human life to the level of animals. Bentham stated that a simple game such as push-pin is renderd as equally
valuable as something highly intellectual, such as poetry, if it produces an equal amount of pleasure. Quantitative
hedonists assert that poetry produced with significance can generate more pleasure than "trivial" games. This is due
impart to the fact that the pleasures derived from poetry are certain (or probable), durable (or lasting), fecund
(likely to lead to other pleasures), pure (unlikely to lead to pains), and so on. Mill employed a different approach in
order to avoid equating push-pin with poetry. Where he made a distinction between higher and lower qualities of
pleasures based on the preferences of individuals who have encountered both types. This hedonistic notion is defined as
qualitative hedonism and has also been a subject of much critism; as with any philsophy worth debating. Hedonistic vs.
Pluralistic ConsequentialismHedonists count both pleasures and pains, where their ethical theory is that where
satisfaction of desire is the highest good and proper aim of human life. A notable philosopher who espoused hedonistic
notions on that which is right and wrong, was Epicurus, such that Epicurus' ethics is a form of egoistic hedonism. It is
important we note, that pleasure is not the absence of pain because there are times when people experience neither
pleasure nor pain and times when we feel both. We can read opposing view points of hedonism in classic utilitarianism
from contemporaries such as Bentham and Mill. They argued that hedonism lowers the value of human life to the level of
animals. Bentham stated that a simple game such as push-pin is renderd as equally valuable as something highly
intellectual, such as poetry, if it produces an equal amount of pleasure. Quantitative hedonists assert that poetry
produced with significance can generate more pleasure than "trivial" games. This is due impart to the fact that the
pleasures derived from poetry are certain (or probable), durable (or lasting), fecund (likely to lead to other
pleasures), pure (unlikely to lead to pains), and so on. Mill employed a different approach in order to avoid equating
push-pin with poetry. Where he made a distinction between higher and lower qualities of pleasures based on the
preferences of individuals who have encountered both types. This hedonistic notion is defined as qualitative hedonism
and has also been a subject of much critism; as with any philsophy worth debating. To continue. Virtue Ethics
Eudaimonist Virtue Ethics Eudaimonist Virtue Ethics, or eudaemonism, defines virtues respective to eudaimonia. Where
Eudaemonists claim we should develop virtues that contribute to or are a constituent of eudaimonia, a term in Ancient
Greek philosophy. Its standard translation is "happiness" or "flourishing" and sometimes translated as "well-being."
Let's consider the disadvantages of each translation. To continue. Virtue EthicsEudaimonist Virtue Ethics Eudaimonist
Virtue Ethics, or eudaemonism, defines virtues respective to eudaimonia. Where Eudaemonists claim we should develop
virtues that contribute to or are a constituent of eudaimonia, a term in Ancient Greek philosophy. Its standard
translation is "happiness" or "flourishing" and sometimes translated as "well-being." Let's consider the disadvantages
of each translation. To continue. Eudaimonist Virtue EthicsEudaimonist Virtue Ethics, or eudaemonism, defines virtues
respective to eudaimonia. Where Eudaemonists claim we should develop virtues that contribute to or are a constituent of
eudaimonia, a term in Ancient Greek philosophy. Its standard translation is "happiness" or "flourishing" and sometimes
translated as "well-being." Let's consider the disadvantages of each translation. To continue. Deontology To continue.
DeontologyTo continue. Glossary Normative Ethics Normative ethics explores ethical behavior and constitutes a subfield
of philosophical ethics that delves into questions about the appropriate moral actions one should take.
GlossaryNormative Ethics Normative ethics explores ethical behavior and constitutes a subfield of philosophical ethics
that delves into questions about the appropriate moral actions one should take. Normative EthicsNormative ethics
explores ethical behavior and constitutes a subfield of philosophical ethics that delves into questions about the
appropriate moral actions one should take.
[https://www.contextswitching.org/phys/qfandadsmaldacenaconjecture]
Quantum Fields in Anti-de Sitter Space and the Maldacena Conjecture - Context Switching Quantum Fields in Anti-de Sitter
Space and the Maldacena Conjecture Holography and the Maldacena Conjecture Preliminaries In theoretical physics, the
Maldacena Conjecture states supergravity and string theory on the product of $(n+1)$-dimensional Anti-de Sitter space
AdS with a compact manifold capable of describing large $N$ limits of conformal field theories CFT in $d$-dimensions.
Correlation functions in CFT are dependent on the supergravity action of asymptotic behavior at infinity. The
mathematical properties of AdS space and string theory also offer solutions for the empty space Einstein Equations,
which we will review as well. Let's begin! 10-dimensional String Theory and the Einstein Frame We will look at how to
obtain the Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime. Let's look at applying the effective low-energy
string action for type II (A or B) strings for the string frame $S_{s}$: $S_{s}= -s \frac{1}{16 \pi G_{D}}
{\displaystyle \int } d^{D} x \sqrt{|g|} $ $ \left( e^{-2 \phi}\left(R+4 g^{\mu \nu} \partial_\mu \phi \partial_\nu
\phi\right) \right. $ $ -\frac{1}{2} \sum_n \frac{1}{n !} F_n^2 +\ldots\big)$ where the Newton constant is in
$D$-dimensions. The $\ldots$ represent fermionic terms and NS-NS 3-form field strength term, $\phi$ is the dialation,
and $n$-form field strengths $F_{n}$ that are apart of the sector $RR$. For the Newton constant in $D$-dimensions $16
\pi G_D=2 \kappa_D^2$. When the Minkowski Signature $s=-1(+1)$ flips to the "mostly minus" signature, an additional
$(-)^{n}$ is added in front of $F^{2}_{n}$. For IIA strings (IIB strings) only odd frames exist for $n$ and for IIB
string $n=5$ in Minkowski space the field strength tensor is self-dual. Where a self-dual tensor satisfies
$*F=F\Rightarrow F=**F$. Now, we will show that by adopting the low energy string action action above, we are able to
derive the equations of motion while imposing self-duality and ensuring that the normalization of $F^{2}_{5}$ is
unchanged. It is convient to represent the actions for fields in the Einstein frame by first rescaling the strings,
which can be accomplished using a specific type of Wely rescaling. To validate it is sufficient to adopt the effective
low energy action for type II strings in order to derive the equations of motions while imposing self-duality for
$D$-spacetime dimensions, we consider the following implication: $ g_{\mu \nu} \rightarrow e^{2\ \sigma \phi}\Rightarrow
$ $ \sqrt{|g|} e^{-2 \phi} R \rightarrow$ $ \sqrt{|g|} e^{-\phi(\sigma(D-2)+2)} \left\{ R+2 \sigma(D-1)\right. $ $
\frac{1}{\sqrt{|g|}} \partial_\mu\big(\sqrt{|g|} \partial^{\mu} \phi\big) -\sigma^{2} (D-1) $ $ \left. (D-2)(\partial
\phi)^{2} \right\} $ Using this implication, we can see it is sufficient to adopt the string action to derive the
equations of motions and imposing self-duality. Thus, we can now continue in good conscience. We will let
$\sigma=-\frac{2}{D-2}$. By choosing this value for $\sigma$, we can remove a total derivative when applied in
$10$-dimensions, which gives: $g_{\mu \nu}(\text { Einstein })=e^{-\frac{1}{2} \phi} g_{\mu \nu}(\text { string })$ From
the equation above, we can see that the result for a string action is equivalent to result of the Einstein-Hilbert
action in AdS spacetime. Continuing in $10$-dimensional spacetime, we obtain our Einstein frame as: $ S_{E}= -s
\frac{1}{16 \pi G_{10}} {\displaystyle \int } d^{10} x \sqrt{|g|} $ $ \left( R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi
\partial_\nu \phi\right. $ $ \left.-\frac{1}{2} \sum_n \frac{1}{n !} e^{a_{n}\phi} F_n^2+\ldots\right) $ where
$a_n=-\frac{1}{2}(n-5)$. From this equation, we can recognize the Minkowksi metric $g_{\mu \nu}$ and the Minkowski
Euclidean signature $s=-1(+1)$ paired with the Newton constant in $D$-dimensions $16 \pi G_D$. We also see the
familiarity of this equation to the Einstein-Hilbert action, where, if you recall, the Einstein-Hilbert action is: $S=-s
\frac{1}{16 \pi G_D} {\displaystyle \int } d^D x \sqrt{|g|}(R+\Lambda)$ All-in-all, in this section we validated that
adopting the low-energy string action for the Einstein-Hilbert action is sufficent for calcuating Einsteins equations of
motions and imposing self-duality, specifically, in $10$-dimensional AdS spacetime. Therefore, yay! We have now derived
an Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime implementing string theory! M-theory as 11-dimensional Super
Gravity and the Einstein Frame We will look at low-energy M-theory in the form of $11$-dimensional super gravity, where
the bosonic part of the action in the Einstein frame is: $S_{\text {bosonic }}(11 \text {-dim SUGRA })=$ $-s \frac{1}{2
\kappa_{11}^2} \left({\displaystyle \int } d^{11} x \sqrt{|g|} \right.$ $\left\{R-\frac{1}{48} K^2\right\}-\frac{1}{6}$
$\left.{\displaystyle \int } C \wedge K \wedge K \right)$ such that there is no dilation $\phi$ like we saw with
$10$-dimensional spacetime. The bosonic field is the metric with a $3$-form guage potential $C$ with a $4$-frame field
strength tensor. We can denote this bosonic field as: $K=dC$ Meaning that when moving from $10$-dimensional spacetime to
$11$-dimensional spacetime, we switch from the fermionic field to the bosonic field. Note, that we can still provide
classical based solutions of the above theories. An example of a classic based solution, is by considering static
solutions to flat translationally invariant $p$-branes, isotropic in transverse directions. For these static solutions
and to cover all cases for classical based solutions in $11$-dimensional supergravity, we can use the following generic
action: $S= -s \frac{1}{2 \kappa_D^2} {\displaystyle \int } d^D x \sqrt{g} $ $ \left\{R-\frac{1}{2} g^{\mu \nu}
\partial_\mu \phi \partial_\nu \phi \right. $ $ \left. -\frac{1}{2} \sum_n \frac{1}{n !} e^{a_n \phi} F_n^2+. .\right\}
$ where $a=0$ and $\phi \equiv 0$. To continue. Boundary on (n+1)-dimensional Anti-de Sitter Space AdS space has a
projective boundary to allow for the embedding space $(y^{0},y^{\mu})$ in $AdS_{n+1}$ for some very large $y$. These new
variables are defined as $y^a=R \tilde{y}^a$, $u=R \tilde{u}$, $v=R \tilde{v}$, where $R \rightarrow \infty$. Next,
given the implication $y^2=b^2 \Rightarrow \tilde{u} \tilde{v}-\vec{\tilde{y}}^2=$$b^2 / R^2 \rightarrow 0$, we can
infer that the boundary in $AdS_{d+1}$ space is a manifold, defined as $\tilde{u} \tilde{v}-\vec{\tilde{y}}^2=0$. Since
$t\in\mathbb{R}$ is sufficent for $R$, we then continue by considering the boundary to be the projective equivalence
classes: $\begin{aligned}u v-\vec{y}^2 & =0 \\ (u, v, \vec{y}) & \sim t(u, v, \vec{y})\end{aligned}$ Meaning, that, as
defined above, since our initial state of the manifold boundary is asympotically equivalent to that manifold state on
any $t$ the boundary is $n$-dimensional. Next, we apply equivalence scaling on the boundary, such that the boundary can
be represented with the Minkowski signature as $\left(y^0\right)^2+\left(y^{n+1}\right)^2=1$$=\vec{y}^2$, where
topologically the boundary is $S^{1}\times S^{n+1}$. Sometimes when scaling, the points with $v\neq 0$ scale to $v=1$,
so we define $u=\vec{y}^2$ and can then use $\vec{y}$ as input coordinates for the boundary. In the case, however, that
when scaling the points with $u\neq 0$, we can instead scale $u=1$ and use $\vec{\tilde{y}}$ as input coordinatesfor the
boundary. This gives us $v=\vec{\tilde{y}}^2$, from which we can draw the following connection between the two sets:
$\vec{\tilde{y}}=\frac{\vec{y}}{y^2}$ Note, that the above conditions allow only one of the two set may be used,
dependent on if $v=0$ or $u=0$. Such that when $v=0$ then $\vec{\tilde{y}}=\vec{0}$, where as when $u=0$ then
$\vec{y}=\vec{0}$. In these equations regarding the manifold boundary of $AdS_{n+1}$ space, the one point $v=0$ is the
point at infinity for the coordinates $\vec{y}$ and similiarily for the point $u=0$. Thus, the boundary for the space is
automatically compactified with the application of the given walk through. All-in-all, the important aspect to take away
from this section is that he isometry group $SO(2,n)$, or $SO(2,n+1)$ for the Eucledian Signature, acts on the boundary
as the conformal group acting on Minkowski Euclidean space! Eucledian Representation of Boundary on (n+1)-dimensional
Anti-de Sitter Space Given $\mathbb{R}^{d+1}$ with coordinates $y_{0},...,y_{d}$, let $B_{d+1}$ represent the open unit
ball $\sum_{i=0}^d y_i^2 < 1$. $AdS_{d+1}$ can be represented as $B_{d+1}$ with the metric: ${\displaystyle d
    s^2=\frac{4 \sum_{i=0}^d d y_i^2}{\left(1-|y|^2\right)^2} }$ The compactification of $B_{d+1}$ yields the closed
    unit ball $\bar{B}_{d+1}$, defined as $\sum_{i=0}^d y_i^2\leq1$, with boundary $\mathbf{S}^d$ given as $\sum_{i=0}^d
    y_i^2=1$. Where $\mathbf{S}^d$ is the Eucledian conformal compitification of Minkowski space, meaning the boundy of
    $AdS_{d+1}$ is Minkowski space. To continue. Massless Field Equations Consider a scalar field $\phi$ where the
    massless field equation is a naive Laplace equation $D_i D^i \phi=0$. For any function $\phi(\Omega)$ on the
    boundary $\mathbf{S}^d$ in $AdS_{d+1}$ space, there exists an extension of $\phi$ to $\bar{B}_{d+1}$ that perserves
    the boundary values and obeys the field equation. $\begin{aligned}0 &=-\int_{B_{d+1}} d^{d+1} y \sqrt{g} \phi D_i
    D^i \phi \\ &=\int_{B_{d+1}} d^5 y \sqrt{g}|d \phi|^2\end{aligned}$ Using the square-integrable solution above, we
    can validate the given property of $AdS_{d+1}$ is preserved and that $d\phi=0$ and therefore $\phi=0$. The reason
    this validates the property of a unique extension, is that we can see if we integrate the naive Laplace equation by
    parts respective to the square-intergrable solution, there does not exist a nonzero solution. This validation is
    important, because if a nonzero solution did exist, it would imply that any given solution works. Which tells us
    there is not an unique extenion and, therefore, in the space we can not properly calculate field equations. Since we
    see that for $AdS_{d+1}$ space, there does exist an unique extension from $\phi$ to $\bar{B}_{d+1}$ for any function
    $\phi(\Omega)$ on the boundary $\mathbf{S}^d$, we can continue with calculating the massless field equation in good
    conscious. Next, let's implement the Laplace equation with respect to $B_{d+1}$ which can be written as:
    $\left(-\frac{1}{(\sinh y)^d} \frac{d}{d y}(\sinh y)^d \frac{d}{d y}+\frac{L^2}{\sinh ^2 y}\right)$$\phi=0$ where
    the angular momentum squared $L^{2}$ represents the angular of the Laplacian. If we then set the value for the
    scalar field $\phi=\sum_\alpha \phi_\alpha(y)$, where $f_\alpha$ are spherical harmonics, the equation for any
    $\phi_{\alpha}$ on any large $y$ is denoted as: $\frac{d}{d y} e^{d y} \frac{d}{d y} \phi_\alpha=0$ We can see that
    two solutions for the Laplace equations are $\phi_\alpha \sim 1$ and $\phi_\alpha \sim e^{-d y}$. Therefore, yay! We
    captured two solutions. These two solutions tell us that for every partial wave, we get a unique solution for the
    Lapalace equation with a given infinite value for the constant. To continue Einstein's Equations The equations of
    motion for an Einstein frame are given below and note we will write $a$ for $a_{n}$: $R^\mu{ }_\nu=\frac{1}{2}
    \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n F^{\mu \xi_2 \ldots \xi_n} F_{\nu \xi_2
    \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $ $\begin{aligned} \nabla^2 \phi &=\frac{1}{\sqrt{g}}
    \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right) \\ &=\frac{a}{2 n !} F_n^2 \\ 0
    &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right) \end{aligned}$ Where the $n$-form field
    strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the Ricci tensor. Next, we will briefly
    examine Einstein's equations as they relate to asymptotically flat spacetimes and asymptotic symmetries, the use to
    construct such spacetimes is that the cosmological constant is zero and matter sources are localized. Here, we will
    look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu} R=8 \pi G T_{\mu \nu}$ An important
    property of Einstein's equation that relates to asmptotically flat spacetime is that, for the equation, as one moves
    far away, the matter stress tensor approaches zero and the metric in the asympotic region begins to approach that of
    flat spacetime. To continue, for the coordinate-based reprentation we will write down an expansion of the metric in
    powers of a radial coordinate, such that for a class of diffeomorphisms, it will preserve the falloffs to identify
    the asymptotic symmetry group. This region of expansion exists around a region of spacetime called
    the 'radiation zone' , which is yielded by null geodesics at infinite affine parameter. To continue. The Maldacena
    Conjecture Given $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid in $(n+2)$-dimensional flat
    spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric $\eta_{a b=}$
    $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant: $\Lambda^2=$
    $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let
    $X_0=$ $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$ $\Lambda \sec \rho \sin
    \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time variable
    is $\tau$ is compact, we need an infinite set of copies of $AdS$ space in the $\tau$ direction to unwrap it from the
    $AdS$ covering space. Glossary AdS/CFT Correspondence Anti-de Sitter/conformal theory correspondence AdS/CFT in
    theoretical physics is a conjecture that describes the relationship between two kinds of physical theories. AdS used
    in quantum gravtiy and is formulated in terms of string theory of M-theory, while, CFT are quantum field theories
    that include theories such as Yang-Mills theories describing elementatry particles. Lorentzian Space Lorentzian
    $n$-space is the inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional Lorentzian inner product.
    Where the vector space is a set that is closed under finite vector addition and scalar multiplication and inner
    product is defined as a vector space with an inner product on it. Minkowski Space The Minkowski Space is a
    particular type of Lorentzian space, specifically $4$-dimensional Lorentzian space, with a Minkowski metric or
    Minkowski tensor. Where the Minkowski metric is a type of metric tensor denoted as $d \tau^2$ with the form
    $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the
    basis of the study of spacetime within special relativity. Another relative feature of this space is that it unifies
    $\mathbb{R}^{3}$ plus time (the "fourth dimension" ) in Einstein's theory of special relativity. Minkowski Metric
    The Minkowski metric is defined as: $g_{\mu \nu} \approx \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0
    & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of generally curved
    spacetime. One application of the Minkowski metric is modeling the cosmological constant term in Einstein's field
    equation with stress-energy from a vaccum or not. Metric Signature In theoretical physics, the metric signature
    counts the number of time-like or space-like characters are in the spacetime. For example, in the case Minkowski
    metric signature is $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time direction. If the
    eigenvalue is defined in three spatial directions $x,y,z$, then the metric signature is $(1,3,0)^{-}$ or
    $(-,+,+,+)$. Quantum Fields in Anti-de Sitter Space and the Maldacena Conjecture - Context Switching Quantum Fields
    in Anti-de Sitter Space and the Maldacena Conjecture - Context SwitchingQuantum Fields in Anti-de Sitter Space and
    the Maldacena Conjecture Holography and the Maldacena Conjecture Preliminaries In theoretical physics, the Maldacena
    Conjecture states supergravity and string theory on the product of $(n+1)$-dimensional Anti-de Sitter space AdS with
    a compact manifold capable of describing large $N$ limits of conformal field theories CFT in $d$-dimensions.
    Correlation functions in CFT are dependent on the supergravity action of asymptotic behavior at infinity. The
    mathematical properties of AdS space and string theory also offer solutions for the empty space Einstein Equations,
    which we will review as well. Let's begin! 10-dimensional String Theory and the Einstein Frame We will look at how
    to obtain the Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime. Let's look at applying the effective
    low-energy string action for type II (A or B) strings for the string frame $S_{s}$: $S_{s}=-s \frac{1}{16 \pi G_{D}}
    {\displaystyle \int } d^{D} x \sqrt{|g|} $ $ \left( e^{-2 \phi}\left(R+4 g^{\mu \nu} \partial_\mu \phi \partial_\nu
    \phi\right) \right. $ $ -\frac{1}{2} \sum_n \frac{1}{n !} F_n^2 +\ldots\big)$ where the Newton constant is in
    $D$-dimensions. The $\ldots$ represent fermionic terms and NS-NS 3-form field strength term, $\phi$ is the
    dialation, and $n$-form field strengths $F_{n}$ that are apart of the sector $RR$. For the Newton constant in
    $D$-dimensions $16 \pi G_D=2 \kappa_D^2$. When the Minkowski Signature $s=-1(+1)$ flips to the "mostly minus"
    signature, an additional $(-)^{n}$ is added in front of $F^{2}_{n}$. For IIA strings (IIB strings) only odd frames
    exist for $n$ and for IIB string $n=5$ in Minkowski space the field strength tensor is self-dual. Where a self-dual
    tensor satisfies $*F=F\Rightarrow F=**F$. Now, we will show that by adopting the low energy string action action
    above, we are able to derive the equations of motion while imposing self-duality and ensuring that the normalization
    of $F^{2}_{5}$ is unchanged. It is convient to represent the actions for fields in the Einstein frame by first
    rescaling the strings, which can be accomplished using a specific type of Wely rescaling. To validate it is
    sufficient to adopt the effective low energy action for type II strings in order to derive the equations of motions
    while imposing self-duality for $D$-spacetime dimensions, we consider the following implication: $ g_{\mu \nu}
    \rightarrow e^{2\ \sigma \phi}\Rightarrow $ $ \sqrt{|g|} e^{-2 \phi} R \rightarrow$ $ \sqrt{|g|}
    e^{-\phi(\sigma(D-2)+2)} \left\{ R+2 \sigma(D-1)\right. $ $ \frac{1}{\sqrt{|g|}} \partial_\mu\big(\sqrt{|g|}
    \partial^{\mu} \phi\big) -\sigma^{2} (D-1) $ $ \left. (D-2)(\partial \phi)^{2} \right\} $ Using this implication, we
    can see it is sufficient to adopt the string action to derive the equations of motions and imposing self-duality.
    Thus, we can now continue in good conscience. We will let $\sigma=-\frac{2}{D-2}$. By choosing this value for
    $\sigma$, we can remove a total derivative when applied in $10$-dimensions, which gives: $g_{\mu \nu}(\text {
    Einstein })=e^{-\frac{1}{2} \phi} g_{\mu \nu}(\text { string })$ From the equation above, we can see that the result
    for a string action is equivalent to result of the Einstein-Hilbert action in AdS spacetime. Continuing in
    $10$-dimensional spacetime, we obtain our Einstein frame as: $ S_{E}=-s \frac{1}{16 \pi G_{10}} {\displaystyle \int
    } d^{10} x \sqrt{|g|} $ $ \left( R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi\right. $ $
    \left.-\frac{1}{2} \sum_n \frac{1}{n !} e^{a_{n}\phi} F_n^2+\ldots\right) $ where $a_n=-\frac{1}{2}(n-5)$. From this
    equation, we can recognize the Minkowksi metric $g_{\mu \nu}$ and the Minkowski Euclidean signature $s=-1(+1)$
    paired with the Newton constant in $D$-dimensions $16 \pi G_D$. We also see the familiarity of this equation to the
    Einstein-Hilbert action, where, if you recall, the Einstein-Hilbert action is: $S=-s \frac{1}{16 \pi G_D}
    {\displaystyle \int } d^D x \sqrt{|g|}(R+\Lambda)$ All-in-all, in this section we validated that adopting the
    low-energy string action for the Einstein-Hilbert action is sufficent for calcuating Einsteins equations of motions
    and imposing self-duality, specifically, in $10$-dimensional AdS spacetime. Therefore, yay! We have now derived an
    Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime implementing string theory! M-theory as 11-dimensional
    Super Gravity and the Einstein Frame We will look at low-energy M-theory in the form of $11$-dimensional super
    gravity, where the bosonic part of the action in the Einstein frame is: $S_{\text {bosonic }}(11 \text {-dim SUGRA
    })=$ $-s \frac{1}{2 \kappa_{11}^2} \left({\displaystyle \int } d^{11} x \sqrt{|g|} \right.$ $\left\{R-\frac{1}{48}
    K^2\right\}-\frac{1}{6}$ $\left.{\displaystyle \int } C \wedge K \wedge K \right)$ such that there is no dilation
    $\phi$ like we saw with $10$-dimensional spacetime. The bosonic field is the metric with a $3$-form guage potential
    $C$ with a $4$-frame field strength tensor. We can denote this bosonic field as: $K=dC$ Meaning that when moving
    from $10$-dimensional spacetime to $11$-dimensional spacetime, we switch from the fermionic field to the bosonic
    field. Note, that we can still provide classical based solutions of the above theories. An example of a classic
    based solution, is by considering static solutions to flat translationally invariant $p$-branes, isotropic in
    transverse directions. For these static solutions and to cover all cases for classical based solutions in
    $11$-dimensional supergravity, we can use the following generic action: $S=-s \frac{1}{2 \kappa_D^2} {\displaystyle
    \int } d^D x \sqrt{g} $ $ \left\{R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi \right. $ $ \left.
    -\frac{1}{2} \sum_n \frac{1}{n !} e^{a_n \phi} F_n^2+. .\right\} $ where $a=0$ and $\phi \equiv 0$. To continue.
    Boundary on (n+1)-dimensional Anti-de Sitter Space AdS space has a projective boundary to allow for the embedding
    space $(y^{0},y^{\mu})$ in $AdS_{n+1}$ for some very large $y$. These new variables are defined as $y^a=R
    \tilde{y}^a$, $u=R \tilde{u}$, $v=R \tilde{v}$, where $R \rightarrow \infty$. Next, given the implication $y^2=b^2
    \Rightarrow \tilde{u} \tilde{v}-\vec{\tilde{y}}^2=$$b^2 / R^2 \rightarrow 0$, we can infer that the boundary in
    $AdS_{d+1}$ space is a manifold, defined as $\tilde{u} \tilde{v}-\vec{\tilde{y}}^2=0$. Since $t\in\mathbb{R}$ is
    sufficent for $R$, we then continue by considering the boundary to be the projective equivalence classes:
    $\begin{aligned}u v-\vec{y}^2 &=0 \\ (u, v, \vec{y}) & \sim t(u, v, \vec{y})\end{aligned}$ Meaning, that, as defined
    above, since our initial state of the manifold boundary is asympotically equivalent to that manifold state on any
    $t$ the boundary is $n$-dimensional. Next, we apply equivalence scaling on the boundary, such that the boundary can
    be represented with the Minkowski signature as $\left(y^0\right)^2+\left(y^{n+1}\right)^2=1$$=\vec{y}^2$, where
    topologically the boundary is $S^{1}\times S^{n+1}$. Sometimes when scaling, the points with $v\neq 0$ scale to
    $v=1$, so we define $u=\vec{y}^2$ and can then use $\vec{y}$ as input coordinates for the boundary. In the case,
    however, that when scaling the points with $u\neq 0$, we can instead scale $u=1$ and use $\vec{\tilde{y}}$ as input
    coordinatesfor the boundary. This gives us $v=\vec{\tilde{y}}^2$, from which we can draw the following connection
    between the two sets: $\vec{\tilde{y}}=\frac{\vec{y}}{y^2}$ Note, that the above conditions allow only one of the
    two set may be used, dependent on if $v=0$ or $u=0$. Such that when $v=0$ then $\vec{\tilde{y}}=\vec{0}$, where as
    when $u=0$ then $\vec{y}=\vec{0}$. In these equations regarding the manifold boundary of $AdS_{n+1}$ space, the one
    point $v=0$ is the point at infinity for the coordinates $\vec{y}$ and similiarily for the point $u=0$. Thus, the
    boundary for the space is automatically compactified with the application of the given walk through. All-in-all, the
    important aspect to take away from this section is that he isometry group $SO(2,n)$, or $SO(2,n+1)$ for the
    Eucledian Signature, acts on the boundary as the conformal group acting on Minkowski Euclidean space! Eucledian
    Representation of Boundary on (n+1)-dimensional Anti-de Sitter Space Given $\mathbb{R}^{d+1}$ with coordinates
    $y_{0},...,y_{d}$, let $B_{d+1}$ represent the open unit ball $\sum_{i=0}^d y_i^2 < 1$. $AdS_{d+1}$ can be
    represented as $B_{d+1}$ with the metric: ${\displaystyle d s^2=\frac{4 \sum_{i=0}^d d
    y_i^2}{\left(1-|y|^2\right)^2} }$ The compactification of $B_{d+1}$ yields the closed unit ball $\bar{B}_{d+1}$,
    defined as $\sum_{i=0}^d y_i^2\leq1$, with boundary $\mathbf{S}^d$ given as $\sum_{i=0}^d y_i^2=1$. Where
    $\mathbf{S}^d$ is the Eucledian conformal compitification of Minkowski space, meaning the boundy of $AdS_{d+1}$ is
    Minkowski space. To continue. Massless Field Equations Consider a scalar field $\phi$ where the massless field
    equation is a naive Laplace equation $D_i D^i \phi=0$. For any function $\phi(\Omega)$ on the boundary
    $\mathbf{S}^d$ in $AdS_{d+1}$ space, there exists an extension of $\phi$ to $\bar{B}_{d+1}$ that perserves the
    boundary values and obeys the field equation. $\begin{aligned}0 &=-\int_{B_{d+1}} d^{d+1} y \sqrt{g} \phi D_i D^i
    \phi \\ &=\int_{B_{d+1}} d^5 y \sqrt{g}|d \phi|^2\end{aligned}$ Using the square-integrable solution above, we can
    validate the given property of $AdS_{d+1}$ is preserved and that $d\phi=0$ and therefore $\phi=0$. The reason this
    validates the property of a unique extension, is that we can see if we integrate the naive Laplace equation by parts
    respective to the square-intergrable solution, there does not exist a nonzero solution. This validation is
    important, because if a nonzero solution did exist, it would imply that any given solution works. Which tells us
    there is not an unique extenion and, therefore, in the space we can not properly calculate field equations. Since we
    see that for $AdS_{d+1}$ space, there does exist an unique extension from $\phi$ to $\bar{B}_{d+1}$ for any function
    $\phi(\Omega)$ on the boundary $\mathbf{S}^d$, we can continue with calculating the massless field equation in good
    conscious. Next, let's implement the Laplace equation with respect to $B_{d+1}$ which can be written as:
    $\left(-\frac{1}{(\sinh y)^d} \frac{d}{d y}(\sinh y)^d \frac{d}{d y}+\frac{L^2}{\sinh ^2 y}\right)$$\phi=0$ where
    the angular momentum squared $L^{2}$ represents the angular of the Laplacian. If we then set the value for the
    scalar field $\phi=\sum_\alpha \phi_\alpha(y)$, where $f_\alpha$ are spherical harmonics, the equation for any
    $\phi_{\alpha}$ on any large $y$ is denoted as: $\frac{d}{d y} e^{d y} \frac{d}{d y} \phi_\alpha=0$ We can see that
    two solutions for the Laplace equations are $\phi_\alpha \sim 1$ and $\phi_\alpha \sim e^{-d y}$. Therefore, yay! We
    captured two solutions. These two solutions tell us that for every partial wave, we get a unique solution for the
    Lapalace equation with a given infinite value for the constant. To continue Einstein's Equations The equations of
    motion for an Einstein frame are given below and note we will write $a$ for $a_{n}$: $R^\mu{ }_\nu=\frac{1}{2}
    \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n F^{\mu \xi_2 \ldots \xi_n} F_{\nu \xi_2
    \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $ $\begin{aligned} \nabla^2 \phi &=\frac{1}{\sqrt{g}}
    \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right) \\ &=\frac{a}{2 n !} F_n^2 \\ 0
    &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right) \end{aligned}$ Where the $n$-form field
    strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the Ricci tensor. Next, we will briefly
    examine Einstein's equations as they relate to asymptotically flat spacetimes and asymptotic symmetries, the use to
    construct such spacetimes is that the cosmological constant is zero and matter sources are localized. Here, we will
    look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu} R=8 \pi G T_{\mu \nu}$ An important
    property of Einstein's equation that relates to asmptotically flat spacetime is that, for the equation, as one moves
    far away, the matter stress tensor approaches zero and the metric in the asympotic region begins to approach that of
    flat spacetime. To continue, for the coordinate-based reprentation we will write down an expansion of the metric in
    powers of a radial coordinate, such that for a class of diffeomorphisms, it will preserve the falloffs to identify
    the asymptotic symmetry group. This region of expansion exists around a region of spacetime called
    the 'radiation zone' , which is yielded by null geodesics at infinite affine parameter. To continue. The Maldacena
    Conjecture Given $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid in $(n+2)$-dimensional flat
    spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric $\eta_{a b=}$
    $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant: $\Lambda^2=$
    $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let
    $X_0=$ $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$ $\Lambda \sec \rho \sin
    \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time variable
    is $\tau$ is compact, we need an infinite set of copies of $AdS$ space in the $\tau$ direction to unwrap it from the
    $AdS$ covering space. Glossary AdS/CFT Correspondence Anti-de Sitter/conformal theory correspondence AdS/CFT in
    theoretical physics is a conjecture that describes the relationship between two kinds of physical theories. AdS used
    in quantum gravtiy and is formulated in terms of string theory of M-theory, while, CFT are quantum field theories
    that include theories such as Yang-Mills theories describing elementatry particles. Lorentzian Space Lorentzian
    $n$-space is the inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional Lorentzian inner product.
    Where the vector space is a set that is closed under finite vector addition and scalar multiplication and inner
    product is defined as a vector space with an inner product on it. Minkowski Space The Minkowski Space is a
    particular type of Lorentzian space, specifically $4$-dimensional Lorentzian space, with a Minkowski metric or
    Minkowski tensor. Where the Minkowski metric is a type of metric tensor denoted as $d \tau^2$ with the form
    $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the
    basis of the study of spacetime within special relativity. Another relative feature of this space is that it unifies
    $\mathbb{R}^{3}$ plus time (the "fourth dimension" ) in Einstein's theory of special relativity. Minkowski Metric
    The Minkowski metric is defined as: $g_{\mu \nu} \approx \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0
    & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of generally curved
    spacetime. One application of the Minkowski metric is modeling the cosmological constant term in Einstein's field
    equation with stress-energy from a vaccum or not. Metric Signature In theoretical physics, the metric signature
    counts the number of time-like or space-like characters are in the spacetime. For example, in the case Minkowski
    metric signature is $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time direction. If the
    eigenvalue is defined in three spatial directions $x,y,z$, then the metric signature is $(1,3,0)^{-}$ or
    $(-,+,+,+)$. Quantum Fields in Anti-de Sitter Space and the Maldacena Conjecture Holography and the Maldacena
    Conjecture Preliminaries In theoretical physics, the Maldacena Conjecture states supergravity and string theory on
    the product of $(n+1)$-dimensional Anti-de Sitter space AdS with a compact manifold capable of describing large $N$
    limits of conformal field theories CFT in $d$-dimensions. Correlation functions in CFT are dependent on the
    supergravity action of asymptotic behavior at infinity. The mathematical properties of AdS space and string theory
    also offer solutions for the empty space Einstein Equations, which we will review as well. Let's begin!
    10-dimensional String Theory and the Einstein Frame We will look at how to obtain the Einstein frame $S_{E}$ in
    $10$-dimensional AdS spacetime. Let's look at applying the effective low-energy string action for type II (A or B)
    strings for the string frame $S_{s}$: $S_{s}=-s \frac{1}{16 \pi G_{D}} {\displaystyle \int } d^{D} x \sqrt{|g|} $ $
    \left( e^{-2 \phi}\left(R+4 g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi\right) \right. $ $ -\frac{1}{2} \sum_n
    \frac{1}{n !} F_n^2 +\ldots\big)$ where the Newton constant is in $D$-dimensions. The $\ldots$ represent fermionic
    terms and NS-NS 3-form field strength term, $\phi$ is the dialation, and $n$-form field strengths $F_{n}$ that are
    apart of the sector $RR$. For the Newton constant in $D$-dimensions $16 \pi G_D=2 \kappa_D^2$. When the Minkowski
    Signature $s=-1(+1)$ flips to the "mostly minus" signature, an additional $(-)^{n}$ is added in front of
    $F^{2}_{n}$. For IIA strings (IIB strings) only odd frames exist for $n$ and for IIB string $n=5$ in Minkowski space
    the field strength tensor is self-dual. Where a self-dual tensor satisfies $*F=F\Rightarrow F=**F$. Now, we will
    show that by adopting the low energy string action action above, we are able to derive the equations of motion while
    imposing self-duality and ensuring that the normalization of $F^{2}_{5}$ is unchanged. It is convient to represent
    the actions for fields in the Einstein frame by first rescaling the strings, which can be accomplished using a
    specific type of Wely rescaling. To validate it is sufficient to adopt the effective low energy action for type II
    strings in order to derive the equations of motions while imposing self-duality for $D$-spacetime dimensions, we
    consider the following implication: $ g_{\mu \nu} \rightarrow e^{2\ \sigma \phi}\Rightarrow $ $ \sqrt{|g|} e^{-2
    \phi} R \rightarrow$ $ \sqrt{|g|} e^{-\phi(\sigma(D-2)+2)} \left\{ R+2 \sigma(D-1)\right. $ $ \frac{1}{\sqrt{|g|}}
    \partial_\mu\big(\sqrt{|g|} \partial^{\mu} \phi\big) -\sigma^{2} (D-1) $ $ \left. (D-2)(\partial \phi)^{2} \right\}
    $ Using this implication, we can see it is sufficient to adopt the string action to derive the equations of motions
    and imposing self-duality. Thus, we can now continue in good conscience. We will let $\sigma=-\frac{2}{D-2}$. By
    choosing this value for $\sigma$, we can remove a total derivative when applied in $10$-dimensions, which gives:
    $g_{\mu \nu}(\text { Einstein })=e^{-\frac{1}{2} \phi} g_{\mu \nu}(\text { string })$ From the equation above, we
    can see that the result for a string action is equivalent to result of the Einstein-Hilbert action in AdS spacetime.
    Continuing in $10$-dimensional spacetime, we obtain our Einstein frame as: $ S_{E}=-s \frac{1}{16 \pi G_{10}}
    {\displaystyle \int } d^{10} x \sqrt{|g|} $ $ \left( R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi \partial_\nu
    \phi\right. $ $ \left.-\frac{1}{2} \sum_n \frac{1}{n !} e^{a_{n}\phi} F_n^2+\ldots\right) $ where
    $a_n=-\frac{1}{2}(n-5)$. From this equation, we can recognize the Minkowksi metric $g_{\mu \nu}$ and the Minkowski
    Euclidean signature $s=-1(+1)$ paired with the Newton constant in $D$-dimensions $16 \pi G_D$. We also see the
    familiarity of this equation to the Einstein-Hilbert action, where, if you recall, the Einstein-Hilbert action is:
    $S=-s \frac{1}{16 \pi G_D} {\displaystyle \int } d^D x \sqrt{|g|}(R+\Lambda)$ All-in-all, in this section we
    validated that adopting the low-energy string action for the Einstein-Hilbert action is sufficent for calcuating
    Einsteins equations of motions and imposing self-duality, specifically, in $10$-dimensional AdS spacetime.
    Therefore, yay! We have now derived an Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime implementing string
    theory! M-theory as 11-dimensional Super Gravity and the Einstein Frame We will look at low-energy M-theory in the
    form of $11$-dimensional super gravity, where the bosonic part of the action in the Einstein frame is: $S_{\text
    {bosonic }}(11 \text {-dim SUGRA })=$ $-s \frac{1}{2 \kappa_{11}^2} \left({\displaystyle \int } d^{11} x \sqrt{|g|}
    \right.$ $\left\{R-\frac{1}{48} K^2\right\}-\frac{1}{6}$ $\left.{\displaystyle \int } C \wedge K \wedge K \right)$
    such that there is no dilation $\phi$ like we saw with $10$-dimensional spacetime. The bosonic field is the metric
    with a $3$-form guage potential $C$ with a $4$-frame field strength tensor. We can denote this bosonic field as:
    $K=dC$ Meaning that when moving from $10$-dimensional spacetime to $11$-dimensional spacetime, we switch from the
    fermionic field to the bosonic field. Note, that we can still provide classical based solutions of the above
    theories. An example of a classic based solution, is by considering static solutions to flat translationally
    invariant $p$-branes, isotropic in transverse directions. For these static solutions and to cover all cases for
    classical based solutions in $11$-dimensional supergravity, we can use the following generic action: $S=-s
    \frac{1}{2 \kappa_D^2} {\displaystyle \int } d^D x \sqrt{g} $ $ \left\{R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi
    \partial_\nu \phi \right. $ $ \left. -\frac{1}{2} \sum_n \frac{1}{n !} e^{a_n \phi} F_n^2+. .\right\} $ where $a=0$
    and $\phi \equiv 0$. To continue. Boundary on (n+1)-dimensional Anti-de Sitter Space AdS space has a projective
    boundary to allow for the embedding space $(y^{0},y^{\mu})$ in $AdS_{n+1}$ for some very large $y$. These new
    variables are defined as $y^a=R \tilde{y}^a$, $u=R \tilde{u}$, $v=R \tilde{v}$, where $R \rightarrow \infty$. Next,
    given the implication $y^2=b^2 \Rightarrow \tilde{u} \tilde{v}-\vec{\tilde{y}}^2=$$b^2 / R^2 \rightarrow 0$, we can
    infer that the boundary in $AdS_{d+1}$ space is a manifold, defined as $\tilde{u} \tilde{v}-\vec{\tilde{y}}^2=0$.
    Since $t\in\mathbb{R}$ is sufficent for $R$, we then continue by considering the boundary to be the projective
    equivalence classes: $\begin{aligned}u v-\vec{y}^2 &=0 \\ (u, v, \vec{y}) & \sim t(u, v, \vec{y})\end{aligned}$
    Meaning, that, as defined above, since our initial state of the manifold boundary is asympotically equivalent to
    that manifold state on any $t$ the boundary is $n$-dimensional. Next, we apply equivalence scaling on the boundary,
    such that the boundary can be represented with the Minkowski signature as
    $\left(y^0\right)^2+\left(y^{n+1}\right)^2=1$$=\vec{y}^2$, where topologically the boundary is $S^{1}\times
    S^{n+1}$. Sometimes when scaling, the points with $v\neq 0$ scale to $v=1$, so we define $u=\vec{y}^2$ and can then
    use $\vec{y}$ as input coordinates for the boundary. In the case, however, that when scaling the points with $u\neq
    0$, we can instead scale $u=1$ and use $\vec{\tilde{y}}$ as input coordinatesfor the boundary. This gives us
    $v=\vec{\tilde{y}}^2$, from which we can draw the following connection between the two sets:
    $\vec{\tilde{y}}=\frac{\vec{y}}{y^2}$ Note, that the above conditions allow only one of the two set may be used,
    dependent on if $v=0$ or $u=0$. Such that when $v=0$ then $\vec{\tilde{y}}=\vec{0}$, where as when $u=0$ then
    $\vec{y}=\vec{0}$. In these equations regarding the manifold boundary of $AdS_{n+1}$ space, the one point $v=0$ is
    the point at infinity for the coordinates $\vec{y}$ and similiarily for the point $u=0$. Thus, the boundary for the
    space is automatically compactified with the application of the given walk through. All-in-all, the important aspect
    to take away from this section is that he isometry group $SO(2,n)$, or $SO(2,n+1)$ for the Eucledian Signature, acts
    on the boundary as the conformal group acting on Minkowski Euclidean space! Eucledian Representation of Boundary on
    (n+1)-dimensional Anti-de Sitter Space Given $\mathbb{R}^{d+1}$ with coordinates $y_{0},...,y_{d}$, let $B_{d+1}$
    represent the open unit ball $\sum_{i=0}^d y_i^2 < 1$. $AdS_{d+1}$ can be represented as $B_{d+1}$ with the metric:
    ${\displaystyle d s^2=\frac{4 \sum_{i=0}^d d y_i^2}{\left(1-|y|^2\right)^2} }$ The compactification of $B_{d+1}$
    yields the closed unit ball $\bar{B}_{d+1}$, defined as $\sum_{i=0}^d y_i^2\leq1$, with boundary $\mathbf{S}^d$
    given as $\sum_{i=0}^d y_i^2=1$. Where $\mathbf{S}^d$ is the Eucledian conformal compitification of Minkowski space,
    meaning the boundy of $AdS_{d+1}$ is Minkowski space. To continue. Massless Field Equations Consider a scalar field
    $\phi$ where the massless field equation is a naive Laplace equation $D_i D^i \phi=0$. For any function
    $\phi(\Omega)$ on the boundary $\mathbf{S}^d$ in $AdS_{d+1}$ space, there exists an extension of $\phi$ to
    $\bar{B}_{d+1}$ that perserves the boundary values and obeys the field equation. $\begin{aligned}0 &=-\int_{B_{d+1}}
    d^{d+1} y \sqrt{g} \phi D_i D^i \phi \\ &=\int_{B_{d+1}} d^5 y \sqrt{g}|d \phi|^2\end{aligned}$ Using the
    square-integrable solution above, we can validate the given property of $AdS_{d+1}$ is preserved and that $d\phi=0$
    and therefore $\phi=0$. The reason this validates the property of a unique extension, is that we can see if we
    integrate the naive Laplace equation by parts respective to the square-intergrable solution, there does not exist a
    nonzero solution. This validation is important, because if a nonzero solution did exist, it would imply that any
    given solution works. Which tells us there is not an unique extenion and, therefore, in the space we can not
    properly calculate field equations. Since we see that for $AdS_{d+1}$ space, there does exist an unique extension
    from $\phi$ to $\bar{B}_{d+1}$ for any function $\phi(\Omega)$ on the boundary $\mathbf{S}^d$, we can continue with
    calculating the massless field equation in good conscious. Next, let's implement the Laplace equation with respect
    to $B_{d+1}$ which can be written as: $\left(-\frac{1}{(\sinh y)^d} \frac{d}{d y}(\sinh y)^d \frac{d}{d
    y}+\frac{L^2}{\sinh ^2 y}\right)$$\phi=0$ where the angular momentum squared $L^{2}$ represents the angular of the
    Laplacian. If we then set the value for the scalar field $\phi=\sum_\alpha \phi_\alpha(y)$, where $f_\alpha$ are
    spherical harmonics, the equation for any $\phi_{\alpha}$ on any large $y$ is denoted as: $\frac{d}{d y} e^{d y}
    \frac{d}{d y} \phi_\alpha=0$ We can see that two solutions for the Laplace equations are $\phi_\alpha \sim 1$ and
    $\phi_\alpha \sim e^{-d y}$. Therefore, yay! We captured two solutions. These two solutions tell us that for every
    partial wave, we get a unique solution for the Lapalace equation with a given infinite value for the constant. To
    continue Einstein's Equations The equations of motion for an Einstein frame are given below and note we will write
    $a$ for $a_{n}$: $R^\mu{ }_\nu=\frac{1}{2} \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n
    F^{\mu \xi_2 \ldots \xi_n} F_{\nu \xi_2 \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $
    $\begin{aligned} \nabla^2 \phi &=\frac{1}{\sqrt{g}} \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right)
    \\ &=\frac{a}{2 n !} F_n^2 \\ 0 &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right)
    \end{aligned}$ Where the $n$-form field strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the
    Ricci tensor. Next, we will briefly examine Einstein's equations as they relate to asymptotically flat spacetimes
    and asymptotic symmetries, the use to construct such spacetimes is that the cosmological constant is zero and matter
    sources are localized. Here, we will look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu}
    R=8 \pi G T_{\mu \nu}$ An important property of Einstein's equation that relates to asmptotically flat spacetime is
    that, for the equation, as one moves far away, the matter stress tensor approaches zero and the metric in the
    asympotic region begins to approach that of flat spacetime. To continue, for the coordinate-based reprentation we
    will write down an expansion of the metric in powers of a radial coordinate, such that for a class of
    diffeomorphisms, it will preserve the falloffs to identify the asymptotic symmetry group. This region of expansion
    exists around a region of spacetime called the 'radiation zone' , which is yielded by null geodesics at infinite
    affine parameter. To continue. The Maldacena Conjecture Given $AdS_{n+1}$ space of constant negative curvature, a
    Hyperboloid in $(n+2)$-dimensional flat spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric
    $\eta_{a b=}$ $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant: $\Lambda^2=$
    $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let
    $X_0=$ $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$ $\Lambda \sec \rho \sin
    \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time variable
    is $\tau$ is compact, we need an infinite set of copies of $AdS$ space in the $\tau$ direction to unwrap it from the
    $AdS$ covering space. Glossary AdS/CFT Correspondence Anti-de Sitter/conformal theory correspondence AdS/CFT in
    theoretical physics is a conjecture that describes the relationship between two kinds of physical theories. AdS used
    in quantum gravtiy and is formulated in terms of string theory of M-theory, while, CFT are quantum field theories
    that include theories such as Yang-Mills theories describing elementatry particles. Lorentzian Space Lorentzian
    $n$-space is the inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional Lorentzian inner product.
    Where the vector space is a set that is closed under finite vector addition and scalar multiplication and inner
    product is defined as a vector space with an inner product on it. Minkowski Space The Minkowski Space is a
    particular type of Lorentzian space, specifically $4$-dimensional Lorentzian space, with a Minkowski metric or
    Minkowski tensor. Where the Minkowski metric is a type of metric tensor denoted as $d \tau^2$ with the form
    $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the
    basis of the study of spacetime within special relativity. Another relative feature of this space is that it unifies
    $\mathbb{R}^{3}$ plus time (the "fourth dimension" ) in Einstein's theory of special relativity. Minkowski Metric
    The Minkowski metric is defined as: $g_{\mu \nu} \approx \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0
    & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of generally curved
    spacetime. One application of the Minkowski metric is modeling the cosmological constant term in Einstein's field
    equation with stress-energy from a vaccum or not. Metric Signature In theoretical physics, the metric signature
    counts the number of time-like or space-like characters are in the spacetime. For example, in the case Minkowski
    metric signature is $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time direction. If the
    eigenvalue is defined in three spatial directions $x,y,z$, then the metric signature is $(1,3,0)^{-}$ or
    $(-,+,+,+)$. Quantum Fields in Anti-de Sitter Space and the Maldacena Conjecture Holography and the Maldacena
    Conjecture Preliminaries In theoretical physics, the Maldacena Conjecture states supergravity and string theory on
    the product of $(n+1)$-dimensional Anti-de Sitter space AdS with a compact manifold capable of describing large $N$
    limits of conformal field theories CFT in $d$-dimensions. Correlation functions in CFT are dependent on the
    supergravity action of asymptotic behavior at infinity. The mathematical properties of AdS space and string theory
    also offer solutions for the empty space Einstein Equations, which we will review as well. Let's begin!
    10-dimensional String Theory and the Einstein Frame We will look at how to obtain the Einstein frame $S_{E}$ in
    $10$-dimensional AdS spacetime. Let's look at applying the effective low-energy string action for type II (A or B)
    strings for the string frame $S_{s}$: $S_{s}=-s \frac{1}{16 \pi G_{D}} {\displaystyle \int } d^{D} x \sqrt{|g|} $ $
    \left( e^{-2 \phi}\left(R+4 g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi\right) \right. $ $ -\frac{1}{2} \sum_n
    \frac{1}{n !} F_n^2 +\ldots\big)$ where the Newton constant is in $D$-dimensions. The $\ldots$ represent fermionic
    terms and NS-NS 3-form field strength term, $\phi$ is the dialation, and $n$-form field strengths $F_{n}$ that are
    apart of the sector $RR$. For the Newton constant in $D$-dimensions $16 \pi G_D=2 \kappa_D^2$. When the Minkowski
    Signature $s=-1(+1)$ flips to the "mostly minus" signature, an additional $(-)^{n}$ is added in front of
    $F^{2}_{n}$. For IIA strings (IIB strings) only odd frames exist for $n$ and for IIB string $n=5$ in Minkowski space
    the field strength tensor is self-dual. Where a self-dual tensor satisfies $*F=F\Rightarrow F=**F$. Now, we will
    show that by adopting the low energy string action action above, we are able to derive the equations of motion while
    imposing self-duality and ensuring that the normalization of $F^{2}_{5}$ is unchanged. It is convient to represent
    the actions for fields in the Einstein frame by first rescaling the strings, which can be accomplished using a
    specific type of Wely rescaling. To validate it is sufficient to adopt the effective low energy action for type II
    strings in order to derive the equations of motions while imposing self-duality for $D$-spacetime dimensions, we
    consider the following implication: $ g_{\mu \nu} \rightarrow e^{2\ \sigma \phi}\Rightarrow $ $ \sqrt{|g|} e^{-2
    \phi} R \rightarrow$ $ \sqrt{|g|} e^{-\phi(\sigma(D-2)+2)} \left\{ R+2 \sigma(D-1)\right. $ $ \frac{1}{\sqrt{|g|}}
    \partial_\mu\big(\sqrt{|g|} \partial^{\mu} \phi\big) -\sigma^{2} (D-1) $ $ \left. (D-2)(\partial \phi)^{2} \right\}
    $ Using this implication, we can see it is sufficient to adopt the string action to derive the equations of motions
    and imposing self-duality. Thus, we can now continue in good conscience. We will let $\sigma=-\frac{2}{D-2}$. By
    choosing this value for $\sigma$, we can remove a total derivative when applied in $10$-dimensions, which gives:
    $g_{\mu \nu}(\text { Einstein })=e^{-\frac{1}{2} \phi} g_{\mu \nu}(\text { string })$ From the equation above, we
    can see that the result for a string action is equivalent to result of the Einstein-Hilbert action in AdS spacetime.
    Continuing in $10$-dimensional spacetime, we obtain our Einstein frame as: $ S_{E}=-s \frac{1}{16 \pi G_{10}}
    {\displaystyle \int } d^{10} x \sqrt{|g|} $ $ \left( R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi \partial_\nu
    \phi\right. $ $ \left.-\frac{1}{2} \sum_n \frac{1}{n !} e^{a_{n}\phi} F_n^2+\ldots\right) $ where
    $a_n=-\frac{1}{2}(n-5)$. From this equation, we can recognize the Minkowksi metric $g_{\mu \nu}$ and the Minkowski
    Euclidean signature $s=-1(+1)$ paired with the Newton constant in $D$-dimensions $16 \pi G_D$. We also see the
    familiarity of this equation to the Einstein-Hilbert action, where, if you recall, the Einstein-Hilbert action is:
    $S=-s \frac{1}{16 \pi G_D} {\displaystyle \int } d^D x \sqrt{|g|}(R+\Lambda)$ All-in-all, in this section we
    validated that adopting the low-energy string action for the Einstein-Hilbert action is sufficent for calcuating
    Einsteins equations of motions and imposing self-duality, specifically, in $10$-dimensional AdS spacetime.
    Therefore, yay! We have now derived an Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime implementing string
    theory! M-theory as 11-dimensional Super Gravity and the Einstein Frame We will look at low-energy M-theory in the
    form of $11$-dimensional super gravity, where the bosonic part of the action in the Einstein frame is: $S_{\text
    {bosonic }}(11 \text {-dim SUGRA })=$ $-s \frac{1}{2 \kappa_{11}^2} \left({\displaystyle \int } d^{11} x \sqrt{|g|}
    \right.$ $\left\{R-\frac{1}{48} K^2\right\}-\frac{1}{6}$ $\left.{\displaystyle \int } C \wedge K \wedge K \right)$
    such that there is no dilation $\phi$ like we saw with $10$-dimensional spacetime. The bosonic field is the metric
    with a $3$-form guage potential $C$ with a $4$-frame field strength tensor. We can denote this bosonic field as:
    $K=dC$ Meaning that when moving from $10$-dimensional spacetime to $11$-dimensional spacetime, we switch from the
    fermionic field to the bosonic field. Note, that we can still provide classical based solutions of the above
    theories. An example of a classic based solution, is by considering static solutions to flat translationally
    invariant $p$-branes, isotropic in transverse directions. For these static solutions and to cover all cases for
    classical based solutions in $11$-dimensional supergravity, we can use the following generic action: $S=-s
    \frac{1}{2 \kappa_D^2} {\displaystyle \int } d^D x \sqrt{g} $ $ \left\{R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi
    \partial_\nu \phi \right. $ $ \left. -\frac{1}{2} \sum_n \frac{1}{n !} e^{a_n \phi} F_n^2+. .\right\} $ where $a=0$
    and $\phi \equiv 0$. To continue. Boundary on (n+1)-dimensional Anti-de Sitter Space AdS space has a projective
    boundary to allow for the embedding space $(y^{0},y^{\mu})$ in $AdS_{n+1}$ for some very large $y$. These new
    variables are defined as $y^a=R \tilde{y}^a$, $u=R \tilde{u}$, $v=R \tilde{v}$, where $R \rightarrow \infty$. Next,
    given the implication $y^2=b^2 \Rightarrow \tilde{u} \tilde{v}-\vec{\tilde{y}}^2=$$b^2 / R^2 \rightarrow 0$, we can
    infer that the boundary in $AdS_{d+1}$ space is a manifold, defined as $\tilde{u} \tilde{v}-\vec{\tilde{y}}^2=0$.
    Since $t\in\mathbb{R}$ is sufficent for $R$, we then continue by considering the boundary to be the projective
    equivalence classes: $\begin{aligned}u v-\vec{y}^2 &=0 \\ (u, v, \vec{y}) & \sim t(u, v, \vec{y})\end{aligned}$
    Meaning, that, as defined above, since our initial state of the manifold boundary is asympotically equivalent to
    that manifold state on any $t$ the boundary is $n$-dimensional. Next, we apply equivalence scaling on the boundary,
    such that the boundary can be represented with the Minkowski signature as
    $\left(y^0\right)^2+\left(y^{n+1}\right)^2=1$$=\vec{y}^2$, where topologically the boundary is $S^{1}\times
    S^{n+1}$. Sometimes when scaling, the points with $v\neq 0$ scale to $v=1$, so we define $u=\vec{y}^2$ and can then
    use $\vec{y}$ as input coordinates for the boundary. In the case, however, that when scaling the points with $u\neq
    0$, we can instead scale $u=1$ and use $\vec{\tilde{y}}$ as input coordinatesfor the boundary. This gives us
    $v=\vec{\tilde{y}}^2$, from which we can draw the following connection between the two sets:
    $\vec{\tilde{y}}=\frac{\vec{y}}{y^2}$ Note, that the above conditions allow only one of the two set may be used,
    dependent on if $v=0$ or $u=0$. Such that when $v=0$ then $\vec{\tilde{y}}=\vec{0}$, where as when $u=0$ then
    $\vec{y}=\vec{0}$. In these equations regarding the manifold boundary of $AdS_{n+1}$ space, the one point $v=0$ is
    the point at infinity for the coordinates $\vec{y}$ and similiarily for the point $u=0$. Thus, the boundary for the
    space is automatically compactified with the application of the given walk through. All-in-all, the important aspect
    to take away from this section is that he isometry group $SO(2,n)$, or $SO(2,n+1)$ for the Eucledian Signature, acts
    on the boundary as the conformal group acting on Minkowski Euclidean space! Eucledian Representation of Boundary on
    (n+1)-dimensional Anti-de Sitter Space Given $\mathbb{R}^{d+1}$ with coordinates $y_{0},...,y_{d}$, let $B_{d+1}$
    represent the open unit ball $\sum_{i=0}^d y_i^2 < 1$. $AdS_{d+1}$ can be represented as $B_{d+1}$ with the metric:
    ${\displaystyle d s^2=\frac{4 \sum_{i=0}^d d y_i^2}{\left(1-|y|^2\right)^2} }$ The compactification of $B_{d+1}$
    yields the closed unit ball $\bar{B}_{d+1}$, defined as $\sum_{i=0}^d y_i^2\leq1$, with boundary $\mathbf{S}^d$
    given as $\sum_{i=0}^d y_i^2=1$. Where $\mathbf{S}^d$ is the Eucledian conformal compitification of Minkowski space,
    meaning the boundy of $AdS_{d+1}$ is Minkowski space. To continue. Massless Field Equations Consider a scalar field
    $\phi$ where the massless field equation is a naive Laplace equation $D_i D^i \phi=0$. For any function
    $\phi(\Omega)$ on the boundary $\mathbf{S}^d$ in $AdS_{d+1}$ space, there exists an extension of $\phi$ to
    $\bar{B}_{d+1}$ that perserves the boundary values and obeys the field equation. $\begin{aligned}0 &=-\int_{B_{d+1}}
    d^{d+1} y \sqrt{g} \phi D_i D^i \phi \\ &=\int_{B_{d+1}} d^5 y \sqrt{g}|d \phi|^2\end{aligned}$ Using the
    square-integrable solution above, we can validate the given property of $AdS_{d+1}$ is preserved and that $d\phi=0$
    and therefore $\phi=0$. The reason this validates the property of a unique extension, is that we can see if we
    integrate the naive Laplace equation by parts respective to the square-intergrable solution, there does not exist a
    nonzero solution. This validation is important, because if a nonzero solution did exist, it would imply that any
    given solution works. Which tells us there is not an unique extenion and, therefore, in the space we can not
    properly calculate field equations. Since we see that for $AdS_{d+1}$ space, there does exist an unique extension
    from $\phi$ to $\bar{B}_{d+1}$ for any function $\phi(\Omega)$ on the boundary $\mathbf{S}^d$, we can continue with
    calculating the massless field equation in good conscious. Next, let's implement the Laplace equation with respect
    to $B_{d+1}$ which can be written as: $\left(-\frac{1}{(\sinh y)^d} \frac{d}{d y}(\sinh y)^d \frac{d}{d
    y}+\frac{L^2}{\sinh ^2 y}\right)$$\phi=0$ where the angular momentum squared $L^{2}$ represents the angular of the
    Laplacian. If we then set the value for the scalar field $\phi=\sum_\alpha \phi_\alpha(y)$, where $f_\alpha$ are
    spherical harmonics, the equation for any $\phi_{\alpha}$ on any large $y$ is denoted as: $\frac{d}{d y} e^{d y}
    \frac{d}{d y} \phi_\alpha=0$ We can see that two solutions for the Laplace equations are $\phi_\alpha \sim 1$ and
    $\phi_\alpha \sim e^{-d y}$. Therefore, yay! We captured two solutions. These two solutions tell us that for every
    partial wave, we get a unique solution for the Lapalace equation with a given infinite value for the constant. To
    continue Einstein's Equations The equations of motion for an Einstein frame are given below and note we will write
    $a$ for $a_{n}$: $R^\mu{ }_\nu=\frac{1}{2} \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n
    F^{\mu \xi_2 \ldots \xi_n} F_{\nu \xi_2 \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $
    $\begin{aligned} \nabla^2 \phi &=\frac{1}{\sqrt{g}} \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right)
    \\ &=\frac{a}{2 n !} F_n^2 \\ 0 &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right)
    \end{aligned}$ Where the $n$-form field strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the
    Ricci tensor. Next, we will briefly examine Einstein's equations as they relate to asymptotically flat spacetimes
    and asymptotic symmetries, the use to construct such spacetimes is that the cosmological constant is zero and matter
    sources are localized. Here, we will look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu}
    R=8 \pi G T_{\mu \nu}$ An important property of Einstein's equation that relates to asmptotically flat spacetime is
    that, for the equation, as one moves far away, the matter stress tensor approaches zero and the metric in the
    asympotic region begins to approach that of flat spacetime. To continue, for the coordinate-based reprentation we
    will write down an expansion of the metric in powers of a radial coordinate, such that for a class of
    diffeomorphisms, it will preserve the falloffs to identify the asymptotic symmetry group. This region of expansion
    exists around a region of spacetime called the 'radiation zone' , which is yielded by null geodesics at infinite
    affine parameter. To continue. The Maldacena Conjecture Given $AdS_{n+1}$ space of constant negative curvature, a
    Hyperboloid in $(n+2)$-dimensional flat spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric
    $\eta_{a b=}$ $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant: $\Lambda^2=$
    $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let
    $X_0=$ $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$ $\Lambda \sec \rho \sin
    \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time variable
    is $\tau$ is compact, we need an infinite set of copies of $AdS$ space in the $\tau$ direction to unwrap it from the
    $AdS$ covering space. Glossary AdS/CFT Correspondence Anti-de Sitter/conformal theory correspondence AdS/CFT in
    theoretical physics is a conjecture that describes the relationship between two kinds of physical theories. AdS used
    in quantum gravtiy and is formulated in terms of string theory of M-theory, while, CFT are quantum field theories
    that include theories such as Yang-Mills theories describing elementatry particles. Lorentzian Space Lorentzian
    $n$-space is the inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional Lorentzian inner product.
    Where the vector space is a set that is closed under finite vector addition and scalar multiplication and inner
    product is defined as a vector space with an inner product on it. Minkowski Space The Minkowski Space is a
    particular type of Lorentzian space, specifically $4$-dimensional Lorentzian space, with a Minkowski metric or
    Minkowski tensor. Where the Minkowski metric is a type of metric tensor denoted as $d \tau^2$ with the form
    $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the
    basis of the study of spacetime within special relativity. Another relative feature of this space is that it unifies
    $\mathbb{R}^{3}$ plus time (the "fourth dimension" ) in Einstein's theory of special relativity. Minkowski Metric
    The Minkowski metric is defined as: $g_{\mu \nu} \approx \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0
    & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of generally curved
    spacetime. One application of the Minkowski metric is modeling the cosmological constant term in Einstein's field
    equation with stress-energy from a vaccum or not. Metric Signature In theoretical physics, the metric signature
    counts the number of time-like or space-like characters are in the spacetime. For example, in the case Minkowski
    metric signature is $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time direction. If the
    eigenvalue is defined in three spatial directions $x,y,z$, then the metric signature is $(1,3,0)^{-}$ or
    $(-,+,+,+)$. Quantum Fields in Anti-de Sitter Space and the Maldacena ConjectureHolography and the Maldacena
    Conjecture Preliminaries In theoretical physics, the Maldacena Conjecture states supergravity and string theory on
    the product of $(n+1)$-dimensional Anti-de Sitter space AdS with a compact manifold capable of describing large $N$
    limits of conformal field theories CFT in $d$-dimensions. Correlation functions in CFT are dependent on the
    supergravity action of asymptotic behavior at infinity. The mathematical properties of AdS space and string theory
    also offer solutions for the empty space Einstein Equations, which we will review as well. Let's begin!
    10-dimensional String Theory and the Einstein Frame We will look at how to obtain the Einstein frame $S_{E}$ in
    $10$-dimensional AdS spacetime. Let's look at applying the effective low-energy string action for type II (A or B)
    strings for the string frame $S_{s}$: $S_{s}=-s \frac{1}{16 \pi G_{D}} {\displaystyle \int } d^{D} x \sqrt{|g|} $ $
    \left( e^{-2 \phi}\left(R+4 g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi\right) \right. $ $ -\frac{1}{2} \sum_n
    \frac{1}{n !} F_n^2 +\ldots\big)$ where the Newton constant is in $D$-dimensions. The $\ldots$ represent fermionic
    terms and NS-NS 3-form field strength term, $\phi$ is the dialation, and $n$-form field strengths $F_{n}$ that are
    apart of the sector $RR$. For the Newton constant in $D$-dimensions $16 \pi G_D=2 \kappa_D^2$. When the Minkowski
    Signature $s=-1(+1)$ flips to the "mostly minus" signature, an additional $(-)^{n}$ is added in front of
    $F^{2}_{n}$. For IIA strings (IIB strings) only odd frames exist for $n$ and for IIB string $n=5$ in Minkowski space
    the field strength tensor is self-dual. Where a self-dual tensor satisfies $*F=F\Rightarrow F=**F$. Now, we will
    show that by adopting the low energy string action action above, we are able to derive the equations of motion while
    imposing self-duality and ensuring that the normalization of $F^{2}_{5}$ is unchanged. It is convient to represent
    the actions for fields in the Einstein frame by first rescaling the strings, which can be accomplished using a
    specific type of Wely rescaling. To validate it is sufficient to adopt the effective low energy action for type II
    strings in order to derive the equations of motions while imposing self-duality for $D$-spacetime dimensions, we
    consider the following implication: $ g_{\mu \nu} \rightarrow e^{2\ \sigma \phi}\Rightarrow $ $ \sqrt{|g|} e^{-2
    \phi} R \rightarrow$ $ \sqrt{|g|} e^{-\phi(\sigma(D-2)+2)} \left\{ R+2 \sigma(D-1)\right. $ $ \frac{1}{\sqrt{|g|}}
    \partial_\mu\big(\sqrt{|g|} \partial^{\mu} \phi\big) -\sigma^{2} (D-1) $ $ \left. (D-2)(\partial \phi)^{2} \right\}
    $ Using this implication, we can see it is sufficient to adopt the string action to derive the equations of motions
    and imposing self-duality. Thus, we can now continue in good conscience. We will let $\sigma=-\frac{2}{D-2}$. By
    choosing this value for $\sigma$, we can remove a total derivative when applied in $10$-dimensions, which gives:
    $g_{\mu \nu}(\text { Einstein })=e^{-\frac{1}{2} \phi} g_{\mu \nu}(\text { string })$ From the equation above, we
    can see that the result for a string action is equivalent to result of the Einstein-Hilbert action in AdS spacetime.
    Continuing in $10$-dimensional spacetime, we obtain our Einstein frame as: $ S_{E}=-s \frac{1}{16 \pi G_{10}}
    {\displaystyle \int } d^{10} x \sqrt{|g|} $ $ \left( R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi \partial_\nu
    \phi\right. $ $ \left.-\frac{1}{2} \sum_n \frac{1}{n !} e^{a_{n}\phi} F_n^2+\ldots\right) $ where
    $a_n=-\frac{1}{2}(n-5)$. From this equation, we can recognize the Minkowksi metric $g_{\mu \nu}$ and the Minkowski
    Euclidean signature $s=-1(+1)$ paired with the Newton constant in $D$-dimensions $16 \pi G_D$. We also see the
    familiarity of this equation to the Einstein-Hilbert action, where, if you recall, the Einstein-Hilbert action is:
    $S=-s \frac{1}{16 \pi G_D} {\displaystyle \int } d^D x \sqrt{|g|}(R+\Lambda)$ All-in-all, in this section we
    validated that adopting the low-energy string action for the Einstein-Hilbert action is sufficent for calcuating
    Einsteins equations of motions and imposing self-duality, specifically, in $10$-dimensional AdS spacetime.
    Therefore, yay! We have now derived an Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime implementing string
    theory! M-theory as 11-dimensional Super Gravity and the Einstein Frame We will look at low-energy M-theory in the
    form of $11$-dimensional super gravity, where the bosonic part of the action in the Einstein frame is: $S_{\text
    {bosonic }}(11 \text {-dim SUGRA })=$ $-s \frac{1}{2 \kappa_{11}^2} \left({\displaystyle \int } d^{11} x \sqrt{|g|}
    \right.$ $\left\{R-\frac{1}{48} K^2\right\}-\frac{1}{6}$ $\left.{\displaystyle \int } C \wedge K \wedge K \right)$
    such that there is no dilation $\phi$ like we saw with $10$-dimensional spacetime. The bosonic field is the metric
    with a $3$-form guage potential $C$ with a $4$-frame field strength tensor. We can denote this bosonic field as:
    $K=dC$ Meaning that when moving from $10$-dimensional spacetime to $11$-dimensional spacetime, we switch from the
    fermionic field to the bosonic field. Note, that we can still provide classical based solutions of the above
    theories. An example of a classic based solution, is by considering static solutions to flat translationally
    invariant $p$-branes, isotropic in transverse directions. For these static solutions and to cover all cases for
    classical based solutions in $11$-dimensional supergravity, we can use the following generic action: $S=-s
    \frac{1}{2 \kappa_D^2} {\displaystyle \int } d^D x \sqrt{g} $ $ \left\{R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi
    \partial_\nu \phi \right. $ $ \left. -\frac{1}{2} \sum_n \frac{1}{n !} e^{a_n \phi} F_n^2+. .\right\} $ where $a=0$
    and $\phi \equiv 0$. To continue. Boundary on (n+1)-dimensional Anti-de Sitter Space AdS space has a projective
    boundary to allow for the embedding space $(y^{0},y^{\mu})$ in $AdS_{n+1}$ for some very large $y$. These new
    variables are defined as $y^a=R \tilde{y}^a$, $u=R \tilde{u}$, $v=R \tilde{v}$, where $R \rightarrow \infty$. Next,
    given the implication $y^2=b^2 \Rightarrow \tilde{u} \tilde{v}-\vec{\tilde{y}}^2=$$b^2 / R^2 \rightarrow 0$, we can
    infer that the boundary in $AdS_{d+1}$ space is a manifold, defined as $\tilde{u} \tilde{v}-\vec{\tilde{y}}^2=0$.
    Since $t\in\mathbb{R}$ is sufficent for $R$, we then continue by considering the boundary to be the projective
    equivalence classes: $\begin{aligned}u v-\vec{y}^2 &=0 \\ (u, v, \vec{y}) & \sim t(u, v, \vec{y})\end{aligned}$
    Meaning, that, as defined above, since our initial state of the manifold boundary is asympotically equivalent to
    that manifold state on any $t$ the boundary is $n$-dimensional. Next, we apply equivalence scaling on the boundary,
    such that the boundary can be represented with the Minkowski signature as
    $\left(y^0\right)^2+\left(y^{n+1}\right)^2=1$$=\vec{y}^2$, where topologically the boundary is $S^{1}\times
    S^{n+1}$. Sometimes when scaling, the points with $v\neq 0$ scale to $v=1$, so we define $u=\vec{y}^2$ and can then
    use $\vec{y}$ as input coordinates for the boundary. In the case, however, that when scaling the points with $u\neq
    0$, we can instead scale $u=1$ and use $\vec{\tilde{y}}$ as input coordinatesfor the boundary. This gives us
    $v=\vec{\tilde{y}}^2$, from which we can draw the following connection between the two sets:
    $\vec{\tilde{y}}=\frac{\vec{y}}{y^2}$ Note, that the above conditions allow only one of the two set may be used,
    dependent on if $v=0$ or $u=0$. Such that when $v=0$ then $\vec{\tilde{y}}=\vec{0}$, where as when $u=0$ then
    $\vec{y}=\vec{0}$. In these equations regarding the manifold boundary of $AdS_{n+1}$ space, the one point $v=0$ is
    the point at infinity for the coordinates $\vec{y}$ and similiarily for the point $u=0$. Thus, the boundary for the
    space is automatically compactified with the application of the given walk through. All-in-all, the important aspect
    to take away from this section is that he isometry group $SO(2,n)$, or $SO(2,n+1)$ for the Eucledian Signature, acts
    on the boundary as the conformal group acting on Minkowski Euclidean space! Eucledian Representation of Boundary on
    (n+1)-dimensional Anti-de Sitter Space Given $\mathbb{R}^{d+1}$ with coordinates $y_{0},...,y_{d}$, let $B_{d+1}$
    represent the open unit ball $\sum_{i=0}^d y_i^2 < 1$. $AdS_{d+1}$ can be represented as $B_{d+1}$ with the metric:
    ${\displaystyle d s^2=\frac{4 \sum_{i=0}^d d y_i^2}{\left(1-|y|^2\right)^2} }$ The compactification of $B_{d+1}$
    yields the closed unit ball $\bar{B}_{d+1}$, defined as $\sum_{i=0}^d y_i^2\leq1$, with boundary $\mathbf{S}^d$
    given as $\sum_{i=0}^d y_i^2=1$. Where $\mathbf{S}^d$ is the Eucledian conformal compitification of Minkowski space,
    meaning the boundy of $AdS_{d+1}$ is Minkowski space. To continue. Massless Field Equations Consider a scalar field
    $\phi$ where the massless field equation is a naive Laplace equation $D_i D^i \phi=0$. For any function
    $\phi(\Omega)$ on the boundary $\mathbf{S}^d$ in $AdS_{d+1}$ space, there exists an extension of $\phi$ to
    $\bar{B}_{d+1}$ that perserves the boundary values and obeys the field equation. $\begin{aligned}0 &=-\int_{B_{d+1}}
    d^{d+1} y \sqrt{g} \phi D_i D^i \phi \\ &=\int_{B_{d+1}} d^5 y \sqrt{g}|d \phi|^2\end{aligned}$ Using the
    square-integrable solution above, we can validate the given property of $AdS_{d+1}$ is preserved and that $d\phi=0$
    and therefore $\phi=0$. The reason this validates the property of a unique extension, is that we can see if we
    integrate the naive Laplace equation by parts respective to the square-intergrable solution, there does not exist a
    nonzero solution. This validation is important, because if a nonzero solution did exist, it would imply that any
    given solution works. Which tells us there is not an unique extenion and, therefore, in the space we can not
    properly calculate field equations. Since we see that for $AdS_{d+1}$ space, there does exist an unique extension
    from $\phi$ to $\bar{B}_{d+1}$ for any function $\phi(\Omega)$ on the boundary $\mathbf{S}^d$, we can continue with
    calculating the massless field equation in good conscious. Next, let's implement the Laplace equation with respect
    to $B_{d+1}$ which can be written as: $\left(-\frac{1}{(\sinh y)^d} \frac{d}{d y}(\sinh y)^d \frac{d}{d
    y}+\frac{L^2}{\sinh ^2 y}\right)$$\phi=0$ where the angular momentum squared $L^{2}$ represents the angular of the
    Laplacian. If we then set the value for the scalar field $\phi=\sum_\alpha \phi_\alpha(y)$, where $f_\alpha$ are
    spherical harmonics, the equation for any $\phi_{\alpha}$ on any large $y$ is denoted as: $\frac{d}{d y} e^{d y}
    \frac{d}{d y} \phi_\alpha=0$ We can see that two solutions for the Laplace equations are $\phi_\alpha \sim 1$ and
    $\phi_\alpha \sim e^{-d y}$. Therefore, yay! We captured two solutions. These two solutions tell us that for every
    partial wave, we get a unique solution for the Lapalace equation with a given infinite value for the constant. To
    continue Einstein's Equations The equations of motion for an Einstein frame are given below and note we will write
    $a$ for $a_{n}$: $R^\mu{ }_\nu=\frac{1}{2} \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n
    F^{\mu \xi_2 \ldots \xi_n} F_{\nu \xi_2 \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $
    $\begin{aligned} \nabla^2 \phi &=\frac{1}{\sqrt{g}} \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right)
    \\ &=\frac{a}{2 n !} F_n^2 \\ 0 &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right)
    \end{aligned}$ Where the $n$-form field strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the
    Ricci tensor. Next, we will briefly examine Einstein's equations as they relate to asymptotically flat spacetimes
    and asymptotic symmetries, the use to construct such spacetimes is that the cosmological constant is zero and matter
    sources are localized. Here, we will look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu}
    R=8 \pi G T_{\mu \nu}$ An important property of Einstein's equation that relates to asmptotically flat spacetime is
    that, for the equation, as one moves far away, the matter stress tensor approaches zero and the metric in the
    asympotic region begins to approach that of flat spacetime. To continue, for the coordinate-based reprentation we
    will write down an expansion of the metric in powers of a radial coordinate, such that for a class of
    diffeomorphisms, it will preserve the falloffs to identify the asymptotic symmetry group. This region of expansion
    exists around a region of spacetime called the 'radiation zone' , which is yielded by null geodesics at infinite
    affine parameter. To continue. Holography and the Maldacena Conjecture Preliminaries In theoretical physics, the
    Maldacena Conjecture states supergravity and string theory on the product of $(n+1)$-dimensional Anti-de Sitter
    space AdS with a compact manifold capable of describing large $N$ limits of conformal field theories CFT in
    $d$-dimensions. Correlation functions in CFT are dependent on the supergravity action of asymptotic behavior at
    infinity. The mathematical properties of AdS space and string theory also offer solutions for the empty space
    Einstein Equations, which we will review as well. Let's begin! supergravitystring theoryAnti-de Sitter
    spacemanifoldEinstein Equations10-dimensional String Theory and the Einstein Frame We will look at how to obtain the
    Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime. Let's look at applying the effective low-energy string
    action for type II (A or B) strings for the string frame $S_{s}$: $S_{s}=-s \frac{1}{16 \pi G_{D}} {\displaystyle
    \int } d^{D} x \sqrt{|g|} $ $ \left( e^{-2 \phi}\left(R+4 g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi\right)
    \right. $ $ -\frac{1}{2} \sum_n \frac{1}{n !} F_n^2 +\ldots\big)$ where the Newton constant is in $D$-dimensions.
    The $\ldots$ represent fermionic terms and NS-NS 3-form field strength term, $\phi$ is the dialation, and $n$-form
    field strengths $F_{n}$ that are apart of the sector $RR$. For the Newton constant in $D$-dimensions $16 \pi G_D=2
    \kappa_D^2$. When the Minkowski Signature $s=-1(+1)$ flips to the "mostly minus" signature, an additional $(-)^{n}$
    is added in front of $F^{2}_{n}$. For IIA strings (IIB strings) only odd frames exist for $n$ and for IIB string
    $n=5$ in Minkowski space the field strength tensor is self-dual. Where a self-dual tensor satisfies $*F=F\Rightarrow
    F=**F$. Now, we will show that by adopting the low energy string action action above, we are able to derive the
    equations of motion while imposing self-duality and ensuring that the normalization of $F^{2}_{5}$ is unchanged. It
    is convient to represent the actions for fields in the Einstein frame by first rescaling the strings, which can be
    accomplished using a specific type of Wely rescaling. To validate it is sufficient to adopt the effective low energy
    action for type II strings in order to derive the equations of motions while imposing self-duality for $D$-spacetime
    dimensions, we consider the following implication: $ g_{\mu \nu} \rightarrow e^{2\ \sigma \phi}\Rightarrow $ $
    \sqrt{|g|} e^{-2 \phi} R \rightarrow$ $ \sqrt{|g|} e^{-\phi(\sigma(D-2)+2)} \left\{ R+2 \sigma(D-1)\right. $ $
    \frac{1}{\sqrt{|g|}} \partial_\mu\big(\sqrt{|g|} \partial^{\mu} \phi\big) -\sigma^{2} (D-1) $ $ \left.
    (D-2)(\partial \phi)^{2} \right\} $ Using this implication, we can see it is sufficient to adopt the string action
    to derive the equations of motions and imposing self-duality. Thus, we can now continue in good conscience. We will
    let $\sigma=-\frac{2}{D-2}$. By choosing this value for $\sigma$, we can remove a total derivative when applied in
    $10$-dimensions, which gives: $g_{\mu \nu}(\text { Einstein })=e^{-\frac{1}{2} \phi} g_{\mu \nu}(\text { string })$
    From the equation above, we can see that the result for a string action is equivalent to result of the
    Einstein-Hilbert action in AdS spacetime. Continuing in $10$-dimensional spacetime, we obtain our Einstein frame as:
    $ S_{E}=-s \frac{1}{16 \pi G_{10}} {\displaystyle \int } d^{10} x \sqrt{|g|} $ $ \left( R-\frac{1}{2} g^{\mu \nu}
    \partial_\mu \phi \partial_\nu \phi\right. $ $ \left.-\frac{1}{2} \sum_n \frac{1}{n !} e^{a_{n}\phi}
    F_n^2+\ldots\right) $ where $a_n=-\frac{1}{2}(n-5)$. From this equation, we can recognize the Minkowksi metric
    $g_{\mu \nu}$ and the Minkowski Euclidean signature $s=-1(+1)$ paired with the Newton constant in $D$-dimensions $16
    \pi G_D$. We also see the familiarity of this equation to the Einstein-Hilbert action, where, if you recall, the
    Einstein-Hilbert action is: $S=-s \frac{1}{16 \pi G_D} {\displaystyle \int } d^D x \sqrt{|g|}(R+\Lambda)$
    All-in-all, in this section we validated that adopting the low-energy string action for the Einstein-Hilbert action
    is sufficent for calcuating Einsteins equations of motions and imposing self-duality, specifically, in
    $10$-dimensional AdS spacetime. Therefore, yay! We have now derived an Einstein frame $S_{E}$ in $10$-dimensional
    AdS spacetime implementing string theory! M-theory as 11-dimensional Super Gravity and the Einstein Frame We will
    look at low-energy M-theory in the form of $11$-dimensional super gravity, where the bosonic part of the action in
    the Einstein frame is: $S_{\text {bosonic }}(11 \text {-dim SUGRA })=$ $-s \frac{1}{2 \kappa_{11}^2}
    \left({\displaystyle \int } d^{11} x \sqrt{|g|} \right.$ $\left\{R-\frac{1}{48} K^2\right\}-\frac{1}{6}$
    $\left.{\displaystyle \int } C \wedge K \wedge K \right)$ such that there is no dilation $\phi$ like we saw with
    $10$-dimensional spacetime. The bosonic field is the metric with a $3$-form guage potential $C$ with a $4$-frame
    field strength tensor. We can denote this bosonic field as: $K=dC$ Meaning that when moving from $10$-dimensional
    spacetime to $11$-dimensional spacetime, we switch from the fermionic field to the bosonic field. Note, that we can
    still provide classical based solutions of the above theories. An example of a classic based solution, is by
    considering static solutions to flat translationally invariant $p$-branes, isotropic in transverse directions. For
    these static solutions and to cover all cases for classical based solutions in $11$-dimensional supergravity, we can
    use the following generic action: $S=-s \frac{1}{2 \kappa_D^2} {\displaystyle \int } d^D x \sqrt{g} $ $
    \left\{R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi \right. $ $ \left. -\frac{1}{2} \sum_n
    \frac{1}{n !} e^{a_n \phi} F_n^2+. .\right\} $ where $a=0$ and $\phi \equiv 0$. To continue. Boundary on
    (n+1)-dimensional Anti-de Sitter Space AdS space has a projective boundary to allow for the embedding space
    $(y^{0},y^{\mu})$ in $AdS_{n+1}$ for some very large $y$. These new variables are defined as $y^a=R \tilde{y}^a$,
    $u=R \tilde{u}$, $v=R \tilde{v}$, where $R \rightarrow \infty$. Next, given the implication $y^2=b^2 \Rightarrow
    \tilde{u} \tilde{v}-\vec{\tilde{y}}^2=$$b^2 / R^2 \rightarrow 0$, we can infer that the boundary in $AdS_{d+1}$
    space is a manifold, defined as $\tilde{u} \tilde{v}-\vec{\tilde{y}}^2=0$. Since $t\in\mathbb{R}$ is sufficent for
    $R$, we then continue by considering the boundary to be the projective equivalence classes: $\begin{aligned}u
    v-\vec{y}^2 &=0 \\ (u, v, \vec{y}) & \sim t(u, v, \vec{y})\end{aligned}$ Meaning, that, as defined above, since our
    initial state of the manifold boundary is asympotically equivalent to that manifold state on any $t$ the boundary is
    $n$-dimensional. Next, we apply equivalence scaling on the boundary, such that the boundary can be represented with
    the Minkowski signature as $\left(y^0\right)^2+\left(y^{n+1}\right)^2=1$$=\vec{y}^2$, where topologically the
    boundary is $S^{1}\times S^{n+1}$. Sometimes when scaling, the points with $v\neq 0$ scale to $v=1$, so we define
    $u=\vec{y}^2$ and can then use $\vec{y}$ as input coordinates for the boundary. In the case, however, that when
    scaling the points with $u\neq 0$, we can instead scale $u=1$ and use $\vec{\tilde{y}}$ as input coordinatesfor the
    boundary. This gives us $v=\vec{\tilde{y}}^2$, from which we can draw the following connection between the two sets:
    $\vec{\tilde{y}}=\frac{\vec{y}}{y^2}$ Note, that the above conditions allow only one of the two set may be used,
    dependent on if $v=0$ or $u=0$. Such that when $v=0$ then $\vec{\tilde{y}}=\vec{0}$, where as when $u=0$ then
    $\vec{y}=\vec{0}$. In these equations regarding the manifold boundary of $AdS_{n+1}$ space, the one point $v=0$ is
    the point at infinity for the coordinates $\vec{y}$ and similiarily for the point $u=0$. Thus, the boundary for the
    space is automatically compactified with the application of the given walk through. All-in-all, the important aspect
    to take away from this section is that he isometry group $SO(2,n)$, or $SO(2,n+1)$ for the Eucledian Signature, acts
    on the boundary as the conformal group acting on Minkowski Euclidean space! Eucledian Representation of Boundary on
    (n+1)-dimensional Anti-de Sitter Space Given $\mathbb{R}^{d+1}$ with coordinates $y_{0},...,y_{d}$, let $B_{d+1}$
    represent the open unit ball $\sum_{i=0}^d y_i^2 < 1$. $AdS_{d+1}$ can be represented as $B_{d+1}$ with the metric:
    ${\displaystyle d s^2=\frac{4 \sum_{i=0}^d d y_i^2}{\left(1-|y|^2\right)^2} }$ The compactification of $B_{d+1}$
    yields the closed unit ball $\bar{B}_{d+1}$, defined as $\sum_{i=0}^d y_i^2\leq1$, with boundary $\mathbf{S}^d$
    given as $\sum_{i=0}^d y_i^2=1$. Where $\mathbf{S}^d$ is the Eucledian conformal compitification of Minkowski space,
    meaning the boundy of $AdS_{d+1}$ is Minkowski space. To continue. Massless Field Equations Consider a scalar field
    $\phi$ where the massless field equation is a naive Laplace equation $D_i D^i \phi=0$. For any function
    $\phi(\Omega)$ on the boundary $\mathbf{S}^d$ in $AdS_{d+1}$ space, there exists an extension of $\phi$ to
    $\bar{B}_{d+1}$ that perserves the boundary values and obeys the field equation. $\begin{aligned}0 &=-\int_{B_{d+1}}
    d^{d+1} y \sqrt{g} \phi D_i D^i \phi \\ &=\int_{B_{d+1}} d^5 y \sqrt{g}|d \phi|^2\end{aligned}$ Using the
    square-integrable solution above, we can validate the given property of $AdS_{d+1}$ is preserved and that $d\phi=0$
    and therefore $\phi=0$. The reason this validates the property of a unique extension, is that we can see if we
    integrate the naive Laplace equation by parts respective to the square-intergrable solution, there does not exist a
    nonzero solution. This validation is important, because if a nonzero solution did exist, it would imply that any
    given solution works. Which tells us there is not an unique extenion and, therefore, in the space we can not
    properly calculate field equations. Since we see that for $AdS_{d+1}$ space, there does exist an unique extension
    from $\phi$ to $\bar{B}_{d+1}$ for any function $\phi(\Omega)$ on the boundary $\mathbf{S}^d$, we can continue with
    calculating the massless field equation in good conscious. Next, let's implement the Laplace equation with respect
    to $B_{d+1}$ which can be written as: $\left(-\frac{1}{(\sinh y)^d} \frac{d}{d y}(\sinh y)^d \frac{d}{d
    y}+\frac{L^2}{\sinh ^2 y}\right)$$\phi=0$ where the angular momentum squared $L^{2}$ represents the angular of the
    Laplacian. If we then set the value for the scalar field $\phi=\sum_\alpha \phi_\alpha(y)$, where $f_\alpha$ are
    spherical harmonics, the equation for any $\phi_{\alpha}$ on any large $y$ is denoted as: $\frac{d}{d y} e^{d y}
    \frac{d}{d y} \phi_\alpha=0$ We can see that two solutions for the Laplace equations are $\phi_\alpha \sim 1$ and
    $\phi_\alpha \sim e^{-d y}$. Therefore, yay! We captured two solutions. These two solutions tell us that for every
    partial wave, we get a unique solution for the Lapalace equation with a given infinite value for the constant. To
    continue Einstein's Equations The equations of motion for an Einstein frame are given below and note we will write
    $a$ for $a_{n}$: $R^\mu{ }_\nu=\frac{1}{2} \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n
    F^{\mu \xi_2 \ldots \xi_n} F_{\nu \xi_2 \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $
    $\begin{aligned} \nabla^2 \phi &=\frac{1}{\sqrt{g}} \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right)
    \\ &=\frac{a}{2 n !} F_n^2 \\ 0 &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right)
    \end{aligned}$ Where the $n$-form field strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the
    Ricci tensor. Next, we will briefly examine Einstein's equations as they relate to asymptotically flat spacetimes
    and asymptotic symmetries, the use to construct such spacetimes is that the cosmological constant is zero and matter
    sources are localized. Here, we will look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu}
    R=8 \pi G T_{\mu \nu}$ An important property of Einstein's equation that relates to asmptotically flat spacetime is
    that, for the equation, as one moves far away, the matter stress tensor approaches zero and the metric in the
    asympotic region begins to approach that of flat spacetime. To continue, for the coordinate-based reprentation we
    will write down an expansion of the metric in powers of a radial coordinate, such that for a class of
    diffeomorphisms, it will preserve the falloffs to identify the asymptotic symmetry group. This region of expansion
    exists around a region of spacetime called the 'radiation zone' , which is yielded by null geodesics at infinite
    affine parameter. To continue. 10-dimensional String Theory and the Einstein Frame We will look at how to obtain the
    Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime. Let's look at applying the effective low-energy string
    action for type II (A or B) strings for the string frame $S_{s}$: $S_{s}=-s \frac{1}{16 \pi G_{D}} {\displaystyle
    \int } d^{D} x \sqrt{|g|} $ $ \left( e^{-2 \phi}\left(R+4 g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi\right)
    \right. $ $ -\frac{1}{2} \sum_n \frac{1}{n !} F_n^2 +\ldots\big)$ where the Newton constant is in $D$-dimensions.
    The $\ldots$ represent fermionic terms and NS-NS 3-form field strength term, $\phi$ is the dialation, and $n$-form
    field strengths $F_{n}$ that are apart of the sector $RR$. For the Newton constant in $D$-dimensions $16 \pi G_D=2
    \kappa_D^2$. When the Minkowski Signature $s=-1(+1)$ flips to the "mostly minus" signature, an additional $(-)^{n}$
    is added in front of $F^{2}_{n}$. For IIA strings (IIB strings) only odd frames exist for $n$ and for IIB string
    $n=5$ in Minkowski space the field strength tensor is self-dual. Where a self-dual tensor satisfies $*F=F\Rightarrow
    F=**F$. Now, we will show that by adopting the low energy string action action above, we are able to derive the
    equations of motion while imposing self-duality and ensuring that the normalization of $F^{2}_{5}$ is unchanged. It
    is convient to represent the actions for fields in the Einstein frame by first rescaling the strings, which can be
    accomplished using a specific type of Wely rescaling. To validate it is sufficient to adopt the effective low energy
    action for type II strings in order to derive the equations of motions while imposing self-duality for $D$-spacetime
    dimensions, we consider the following implication: $ g_{\mu \nu} \rightarrow e^{2\ \sigma \phi}\Rightarrow $ $
    \sqrt{|g|} e^{-2 \phi} R \rightarrow$ $ \sqrt{|g|} e^{-\phi(\sigma(D-2)+2)} \left\{ R+2 \sigma(D-1)\right. $ $
    \frac{1}{\sqrt{|g|}} \partial_\mu\big(\sqrt{|g|} \partial^{\mu} \phi\big) -\sigma^{2} (D-1) $ $ \left.
    (D-2)(\partial \phi)^{2} \right\} $ Using this implication, we can see it is sufficient to adopt the string action
    to derive the equations of motions and imposing self-duality. Thus, we can now continue in good conscience. We will
    let $\sigma=-\frac{2}{D-2}$. By choosing this value for $\sigma$, we can remove a total derivative when applied in
    $10$-dimensions, which gives: $g_{\mu \nu}(\text { Einstein })=e^{-\frac{1}{2} \phi} g_{\mu \nu}(\text { string })$
    From the equation above, we can see that the result for a string action is equivalent to result of the
    Einstein-Hilbert action in AdS spacetime. Continuing in $10$-dimensional spacetime, we obtain our Einstein frame as:
    $ S_{E}=-s \frac{1}{16 \pi G_{10}} {\displaystyle \int } d^{10} x \sqrt{|g|} $ $ \left( R-\frac{1}{2} g^{\mu \nu}
    \partial_\mu \phi \partial_\nu \phi\right. $ $ \left.-\frac{1}{2} \sum_n \frac{1}{n !} e^{a_{n}\phi}
    F_n^2+\ldots\right) $ where $a_n=-\frac{1}{2}(n-5)$. From this equation, we can recognize the Minkowksi metric
    $g_{\mu \nu}$ and the Minkowski Euclidean signature $s=-1(+1)$ paired with the Newton constant in $D$-dimensions $16
    \pi G_D$. We also see the familiarity of this equation to the Einstein-Hilbert action, where, if you recall, the
    Einstein-Hilbert action is: $S=-s \frac{1}{16 \pi G_D} {\displaystyle \int } d^D x \sqrt{|g|}(R+\Lambda)$
    All-in-all, in this section we validated that adopting the low-energy string action for the Einstein-Hilbert action
    is sufficent for calcuating Einsteins equations of motions and imposing self-duality, specifically, in
    $10$-dimensional AdS spacetime. Therefore, yay! We have now derived an Einstein frame $S_{E}$ in $10$-dimensional
    AdS spacetime implementing string theory! 10-dimensional String Theory and the Einstein Frame We will look at how to
    obtain the Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime. Let's look at applying the effective low-energy
    string action for type II (A or B) strings for the string frame $S_{s}$: $S_{s}=-s \frac{1}{16 \pi G_{D}}
    {\displaystyle \int } d^{D} x \sqrt{|g|} $ $ \left( e^{-2 \phi}\left(R+4 g^{\mu \nu} \partial_\mu \phi \partial_\nu
    \phi\right) \right. $ $ -\frac{1}{2} \sum_n \frac{1}{n !} F_n^2 +\ldots\big)$ where the Newton constant is in
    $D$-dimensions. The $\ldots$ represent fermionic terms and NS-NS 3-form field strength term, $\phi$ is the
    dialation, and $n$-form field strengths $F_{n}$ that are apart of the sector $RR$. For the Newton constant in
    $D$-dimensions $16 \pi G_D=2 \kappa_D^2$. When the Minkowski Signature $s=-1(+1)$ flips to the "mostly minus"
    signature, an additional $(-)^{n}$ is added in front of $F^{2}_{n}$. For IIA strings (IIB strings) only odd frames
    exist for $n$ and for IIB string $n=5$ in Minkowski space the field strength tensor is self-dual. Where a self-dual
    tensor satisfies $*F=F\Rightarrow F=**F$. Now, we will show that by adopting the low energy string action action
    above, we are able to derive the equations of motion while imposing self-duality and ensuring that the normalization
    of $F^{2}_{5}$ is unchanged. It is convient to represent the actions for fields in the Einstein frame by first
    rescaling the strings, which can be accomplished using a specific type of Wely rescaling. equations of motion To
    validate it is sufficient to adopt the effective low energy action for type II strings in order to derive the
    equations of motions while imposing self-duality for $D$-spacetime dimensions, we consider the following
    implication: $ g_{\mu \nu} \rightarrow e^{2\ \sigma \phi}\Rightarrow $ $ \sqrt{|g|} e^{-2 \phi} R \rightarrow$ $
    \sqrt{|g|} e^{-\phi(\sigma(D-2)+2)} \left\{ R+2 \sigma(D-1)\right. $ $ \frac{1}{\sqrt{|g|}}
    \partial_\mu\big(\sqrt{|g|} \partial^{\mu} \phi\big) -\sigma^{2} (D-1) $ $ \left. (D-2)(\partial \phi)^{2} \right\}
    $ Using this implication, we can see it is sufficient to adopt the string action to derive the equations of motions
    and imposing self-duality. Thus, we can now continue in good conscience. We will let $\sigma=-\frac{2}{D-2}$. By
    choosing this value for $\sigma$, we can remove a total derivative when applied in $10$-dimensions, which gives:
    $g_{\mu \nu}(\text { Einstein })=e^{-\frac{1}{2} \phi} g_{\mu \nu}(\text { string })$ From the equation above, we
    can see that the result for a string action is equivalent to result of the Einstein-Hilbert action in AdS spacetime.
    Continuing in $10$-dimensional spacetime, we obtain our Einstein frame as: $ S_{E}=-s \frac{1}{16 \pi G_{10}}
    {\displaystyle \int } d^{10} x \sqrt{|g|} $ $ \left( R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi \partial_\nu
    \phi\right. $ $ \left.-\frac{1}{2} \sum_n \frac{1}{n !} e^{a_{n}\phi} F_n^2+\ldots\right) $ where
    $a_n=-\frac{1}{2}(n-5)$. From this equation, we can recognize the Minkowksi metric $g_{\mu \nu}$ and the Minkowski
    Euclidean signature $s=-1(+1)$ paired with the Newton constant in $D$-dimensions $16 \pi G_D$. We also see the
    familiarity of this equation to the Einstein-Hilbert action, where, if you recall, the Einstein-Hilbert action is:
    $S=-s \frac{1}{16 \pi G_D} {\displaystyle \int } d^D x \sqrt{|g|}(R+\Lambda)$ All-in-all, in this section we
    validated that adopting the low-energy string action for the Einstein-Hilbert action is sufficent for calcuating
    Einsteins equations of motions and imposing self-duality, specifically, in $10$-dimensional AdS spacetime.
    Therefore, yay! We have now derived an Einstein frame $S_{E}$ in $10$-dimensional AdS spacetime implementing string
    theory! M-theory as 11-dimensional Super Gravity and the Einstein Frame We will look at low-energy M-theory in the
    form of $11$-dimensional super gravity, where the bosonic part of the action in the Einstein frame is: $S_{\text
    {bosonic }}(11 \text {-dim SUGRA })=$ $-s \frac{1}{2 \kappa_{11}^2} \left({\displaystyle \int } d^{11} x \sqrt{|g|}
    \right.$ $\left\{R-\frac{1}{48} K^2\right\}-\frac{1}{6}$ $\left.{\displaystyle \int } C \wedge K \wedge K \right)$
    such that there is no dilation $\phi$ like we saw with $10$-dimensional spacetime. The bosonic field is the metric
    with a $3$-form guage potential $C$ with a $4$-frame field strength tensor. We can denote this bosonic field as:
    $K=dC$ Meaning that when moving from $10$-dimensional spacetime to $11$-dimensional spacetime, we switch from the
    fermionic field to the bosonic field. Note, that we can still provide classical based solutions of the above
    theories. An example of a classic based solution, is by considering static solutions to flat translationally
    invariant $p$-branes, isotropic in transverse directions. For these static solutions and to cover all cases for
    classical based solutions in $11$-dimensional supergravity, we can use the following generic action: $S=-s
    \frac{1}{2 \kappa_D^2} {\displaystyle \int } d^D x \sqrt{g} $ $ \left\{R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi
    \partial_\nu \phi \right. $ $ \left. -\frac{1}{2} \sum_n \frac{1}{n !} e^{a_n \phi} F_n^2+. .\right\} $ where $a=0$
    and $\phi \equiv 0$. To continue. M-theory as 11-dimensional Super Gravity and the Einstein Frame We will look at
    low-energy M-theory in the form of $11$-dimensional super gravity, where the bosonic part of the action in the
    Einstein frame is: $S_{\text {bosonic }}(11 \text {-dim SUGRA })=$ $-s \frac{1}{2 \kappa_{11}^2}
    \left({\displaystyle \int } d^{11} x \sqrt{|g|} \right.$ $\left\{R-\frac{1}{48} K^2\right\}-\frac{1}{6}$
    $\left.{\displaystyle \int } C \wedge K \wedge K \right)$ such that there is no dilation $\phi$ like we saw with
    $10$-dimensional spacetime. The bosonic field is the metric with a $3$-form guage potential $C$ with a $4$-frame
    field strength tensor. We can denote this bosonic field as: $K=dC$ Meaning that when moving from $10$-dimensional
    spacetime to $11$-dimensional spacetime, we switch from the fermionic field to the bosonic field. Note, that we can
    still provide classical based solutions of the above theories. An example of a classic based solution, is by
    considering static solutions to flat translationally invariant $p$-branes, isotropic in transverse directions. For
    these static solutions and to cover all cases for classical based solutions in $11$-dimensional supergravity, we can
    use the following generic action: $S=-s \frac{1}{2 \kappa_D^2} {\displaystyle \int } d^D x \sqrt{g} $ $
    \left\{R-\frac{1}{2} g^{\mu \nu} \partial_\mu \phi \partial_\nu \phi \right. $ $ \left. -\frac{1}{2} \sum_n
    \frac{1}{n !} e^{a_n \phi} F_n^2+. .\right\} $ where $a=0$ and $\phi \equiv 0$. To continue. Boundary on
    (n+1)-dimensional Anti-de Sitter Space AdS space has a projective boundary to allow for the embedding space
    $(y^{0},y^{\mu})$ in $AdS_{n+1}$ for some very large $y$. These new variables are defined as $y^a=R \tilde{y}^a$,
    $u=R \tilde{u}$, $v=R \tilde{v}$, where $R \rightarrow \infty$. Next, given the implication $y^2=b^2 \Rightarrow
    \tilde{u} \tilde{v}-\vec{\tilde{y}}^2=$$b^2 / R^2 \rightarrow 0$, we can infer that the boundary in $AdS_{d+1}$
    space is a manifold, defined as $\tilde{u} \tilde{v}-\vec{\tilde{y}}^2=0$. Since $t\in\mathbb{R}$ is sufficent for
    $R$, we then continue by considering the boundary to be the projective equivalence classes: $\begin{aligned}u
    v-\vec{y}^2 &=0 \\ (u, v, \vec{y}) & \sim t(u, v, \vec{y})\end{aligned}$ Meaning, that, as defined above, since our
    initial state of the manifold boundary is asympotically equivalent to that manifold state on any $t$ the boundary is
    $n$-dimensional. Next, we apply equivalence scaling on the boundary, such that the boundary can be represented with
    the Minkowski signature as $\left(y^0\right)^2+\left(y^{n+1}\right)^2=1$$=\vec{y}^2$, where topologically the
    boundary is $S^{1}\times S^{n+1}$. Sometimes when scaling, the points with $v\neq 0$ scale to $v=1$, so we define
    $u=\vec{y}^2$ and can then use $\vec{y}$ as input coordinates for the boundary. In the case, however, that when
    scaling the points with $u\neq 0$, we can instead scale $u=1$ and use $\vec{\tilde{y}}$ as input coordinatesfor the
    boundary. This gives us $v=\vec{\tilde{y}}^2$, from which we can draw the following connection between the two sets:
    $\vec{\tilde{y}}=\frac{\vec{y}}{y^2}$ Note, that the above conditions allow only one of the two set may be used,
    dependent on if $v=0$ or $u=0$. Such that when $v=0$ then $\vec{\tilde{y}}=\vec{0}$, where as when $u=0$ then
    $\vec{y}=\vec{0}$. In these equations regarding the manifold boundary of $AdS_{n+1}$ space, the one point $v=0$ is
    the point at infinity for the coordinates $\vec{y}$ and similiarily for the point $u=0$. Thus, the boundary for the
    space is automatically compactified with the application of the given walk through. All-in-all, the important aspect
    to take away from this section is that he isometry group $SO(2,n)$, or $SO(2,n+1)$ for the Eucledian Signature, acts
    on the boundary as the conformal group acting on Minkowski Euclidean space! Boundary on (n+1)-dimensional Anti-de
    Sitter Space AdS space has a projective boundary to allow for the embedding space $(y^{0},y^{\mu})$ in $AdS_{n+1}$
    for some very large $y$. These new variables are defined as $y^a=R \tilde{y}^a$, $u=R \tilde{u}$, $v=R \tilde{v}$,
    where $R \rightarrow \infty$. Next, given the implication $y^2=b^2 \Rightarrow \tilde{u}
    \tilde{v}-\vec{\tilde{y}}^2=$$b^2 / R^2 \rightarrow 0$, we can infer that the boundary in $AdS_{d+1}$ space is a
    manifold, defined as $\tilde{u} \tilde{v}-\vec{\tilde{y}}^2=0$. Since $t\in\mathbb{R}$ is sufficent for $R$, we then
    continue by considering the boundary to be the projective equivalence classes: $\begin{aligned}u v-\vec{y}^2 &=0 \\
    (u, v, \vec{y}) & \sim t(u, v, \vec{y})\end{aligned}$ Meaning, that, as defined above, since our initial state of
    the manifold boundary is asympotically equivalent to that manifold state on any $t$ the boundary is $n$-dimensional.
    Next, we apply equivalence scaling on the boundary, such that the boundary can be represented with the Minkowski
    signature as $\left(y^0\right)^2+\left(y^{n+1}\right)^2=1$$=\vec{y}^2$, where topologically the boundary is
    $S^{1}\times S^{n+1}$. Sometimes when scaling, the points with $v\neq 0$ scale to $v=1$, so we define $u=\vec{y}^2$
    and can then use $\vec{y}$ as input coordinates for the boundary. In the case, however, that when scaling the points
    with $u\neq 0$, we can instead scale $u=1$ and use $\vec{\tilde{y}}$ as input coordinatesfor the boundary. This
    gives us $v=\vec{\tilde{y}}^2$, from which we can draw the following connection between the two sets:
    $\vec{\tilde{y}}=\frac{\vec{y}}{y^2}$ Note, that the above conditions allow only one of the two set may be used,
    dependent on if $v=0$ or $u=0$. Such that when $v=0$ then $\vec{\tilde{y}}=\vec{0}$, where as when $u=0$ then
    $\vec{y}=\vec{0}$. In these equations regarding the manifold boundary of $AdS_{n+1}$ space, the one point $v=0$ is
    the point at infinity for the coordinates $\vec{y}$ and similiarily for the point $u=0$. Thus, the boundary for the
    space is automatically compactified with the application of the given walk through. All-in-all, the important aspect
    to take away from this section is that he isometry group $SO(2,n)$, or $SO(2,n+1)$ for the Eucledian Signature, acts
    on the boundary as the conformal group acting on Minkowski Euclidean space! Eucledian Representation of Boundary on
    (n+1)-dimensional Anti-de Sitter Space Given $\mathbb{R}^{d+1}$ with coordinates $y_{0},...,y_{d}$, let $B_{d+1}$
    represent the open unit ball $\sum_{i=0}^d y_i^2 < 1$. $AdS_{d+1}$ can be represented as $B_{d+1}$ with the metric:
    ${\displaystyle d s^2=\frac{4 \sum_{i=0}^d d y_i^2}{\left(1-|y|^2\right)^2} }$ The compactification of $B_{d+1}$
    yields the closed unit ball $\bar{B}_{d+1}$, defined as $\sum_{i=0}^d y_i^2\leq1$, with boundary $\mathbf{S}^d$
    given as $\sum_{i=0}^d y_i^2=1$. Where $\mathbf{S}^d$ is the Eucledian conformal compitification of Minkowski space,
    meaning the boundy of $AdS_{d+1}$ is Minkowski space. To continue. Eucledian Representation of Boundary on
    (n+1)-dimensional Anti-de Sitter Space Given $\mathbb{R}^{d+1}$ with coordinates $y_{0},...,y_{d}$, let $B_{d+1}$
    represent the open unit ball $\sum_{i=0}^d y_i^2 < 1$. $AdS_{d+1}$ can be represented as $B_{d+1}$ with the metric:
    ${\displaystyle d s^2=\frac{4 \sum_{i=0}^d d y_i^2}{\left(1-|y|^2\right)^2} }$ The compactification of $B_{d+1}$
    yields the closed unit ball $\bar{B}_{d+1}$, defined as $\sum_{i=0}^d y_i^2\leq1$, with boundary $\mathbf{S}^d$
    given as $\sum_{i=0}^d y_i^2=1$. Where $\mathbf{S}^d$ is the Eucledian conformal compitification of Minkowski space,
    meaning the boundy of $AdS_{d+1}$ is Minkowski space. To continue. Massless Field Equations Consider a scalar field
    $\phi$ where the massless field equation is a naive Laplace equation $D_i D^i \phi=0$. For any function
    $\phi(\Omega)$ on the boundary $\mathbf{S}^d$ in $AdS_{d+1}$ space, there exists an extension of $\phi$ to
    $\bar{B}_{d+1}$ that perserves the boundary values and obeys the field equation. $\begin{aligned}0 &=-\int_{B_{d+1}}
    d^{d+1} y \sqrt{g} \phi D_i D^i \phi \\ &=\int_{B_{d+1}} d^5 y \sqrt{g}|d \phi|^2\end{aligned}$ Using the
    square-integrable solution above, we can validate the given property of $AdS_{d+1}$ is preserved and that $d\phi=0$
    and therefore $\phi=0$. The reason this validates the property of a unique extension, is that we can see if we
    integrate the naive Laplace equation by parts respective to the square-intergrable solution, there does not exist a
    nonzero solution. This validation is important, because if a nonzero solution did exist, it would imply that any
    given solution works. Which tells us there is not an unique extenion and, therefore, in the space we can not
    properly calculate field equations. Since we see that for $AdS_{d+1}$ space, there does exist an unique extension
    from $\phi$ to $\bar{B}_{d+1}$ for any function $\phi(\Omega)$ on the boundary $\mathbf{S}^d$, we can continue with
    calculating the massless field equation in good conscious. Next, let's implement the Laplace equation with respect
    to $B_{d+1}$ which can be written as: $\left(-\frac{1}{(\sinh y)^d} \frac{d}{d y}(\sinh y)^d \frac{d}{d
    y}+\frac{L^2}{\sinh ^2 y}\right)$$\phi=0$ where the angular momentum squared $L^{2}$ represents the angular of the
    Laplacian. If we then set the value for the scalar field $\phi=\sum_\alpha \phi_\alpha(y)$, where $f_\alpha$ are
    spherical harmonics, the equation for any $\phi_{\alpha}$ on any large $y$ is denoted as: $\frac{d}{d y} e^{d y}
    \frac{d}{d y} \phi_\alpha=0$ We can see that two solutions for the Laplace equations are $\phi_\alpha \sim 1$ and
    $\phi_\alpha \sim e^{-d y}$. Therefore, yay! We captured two solutions. These two solutions tell us that for every
    partial wave, we get a unique solution for the Lapalace equation with a given infinite value for the constant. To
    continue Einstein's Equations The equations of motion for an Einstein frame are given below and note we will write
    $a$ for $a_{n}$: $R^\mu{ }_\nu=\frac{1}{2} \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n
    F^{\mu \xi_2 \ldots \xi_n} F_{\nu \xi_2 \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $
    $\begin{aligned} \nabla^2 \phi &=\frac{1}{\sqrt{g}} \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right)
    \\ &=\frac{a}{2 n !} F_n^2 \\ 0 &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right)
    \end{aligned}$ Where the $n$-form field strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the
    Ricci tensor. Next, we will briefly examine Einstein's equations as they relate to asymptotically flat spacetimes
    and asymptotic symmetries, the use to construct such spacetimes is that the cosmological constant is zero and matter
    sources are localized. Here, we will look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu}
    R=8 \pi G T_{\mu \nu}$ An important property of Einstein's equation that relates to asmptotically flat spacetime is
    that, for the equation, as one moves far away, the matter stress tensor approaches zero and the metric in the
    asympotic region begins to approach that of flat spacetime. To continue, for the coordinate-based reprentation we
    will write down an expansion of the metric in powers of a radial coordinate, such that for a class of
    diffeomorphisms, it will preserve the falloffs to identify the asymptotic symmetry group. This region of expansion
    exists around a region of spacetime called the 'radiation zone' , which is yielded by null geodesics at infinite
    affine parameter. To continue. Massless Field Equations Consider a scalar field $\phi$ where the massless field
    equation is a naive Laplace equation $D_i D^i \phi=0$. For any function $\phi(\Omega)$ on the boundary
    $\mathbf{S}^d$ in $AdS_{d+1}$ space, there exists an extension of $\phi$ to $\bar{B}_{d+1}$ that perserves the
    boundary values and obeys the field equation. $\begin{aligned}0 &=-\int_{B_{d+1}} d^{d+1} y \sqrt{g} \phi D_i D^i
    \phi \\ &=\int_{B_{d+1}} d^5 y \sqrt{g}|d \phi|^2\end{aligned}$ Using the square-integrable solution above, we can
    validate the given property of $AdS_{d+1}$ is preserved and that $d\phi=0$ and therefore $\phi=0$. The reason this
    validates the property of a unique extension, is that we can see if we integrate the naive Laplace equation by parts
    respective to the square-intergrable solution, there does not exist a nonzero solution. This validation is
    important, because if a nonzero solution did exist, it would imply that any given solution works. Which tells us
    there is not an unique extenion and, therefore, in the space we can not properly calculate field equations. Since we
    see that for $AdS_{d+1}$ space, there does exist an unique extension from $\phi$ to $\bar{B}_{d+1}$ for any function
    $\phi(\Omega)$ on the boundary $\mathbf{S}^d$, we can continue with calculating the massless field equation in good
    conscious. Next, let's implement the Laplace equation with respect to $B_{d+1}$ which can be written as:
    $\left(-\frac{1}{(\sinh y)^d} \frac{d}{d y}(\sinh y)^d \frac{d}{d y}+\frac{L^2}{\sinh ^2 y}\right)$$\phi=0$ where
    the angular momentum squared $L^{2}$ represents the angular of the Laplacian. If we then set the value for the
    scalar field $\phi=\sum_\alpha \phi_\alpha(y)$, where $f_\alpha$ are spherical harmonics, the equation for any
    $\phi_{\alpha}$ on any large $y$ is denoted as: $\frac{d}{d y} e^{d y} \frac{d}{d y} \phi_\alpha=0$ We can see that
    two solutions for the Laplace equations are $\phi_\alpha \sim 1$ and $\phi_\alpha \sim e^{-d y}$. Therefore, yay! We
    captured two solutions. These two solutions tell us that for every partial wave, we get a unique solution for the
    Lapalace equation with a given infinite value for the constant. To continue Einstein's Equations The equations of
    motion for an Einstein frame are given below and note we will write $a$ for $a_{n}$: $R^\mu{ }_\nu=\frac{1}{2}
    \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n F^{\mu \xi_2 \ldots \xi_n} F_{\nu \xi_2
    \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $ $\begin{aligned} \nabla^2 \phi &=\frac{1}{\sqrt{g}}
    \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right) \\ &=\frac{a}{2 n !} F_n^2 \\ 0
    &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right) \end{aligned}$ Where the $n$-form field
    strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the Ricci tensor. Next, we will briefly
    examine Einstein's equations as they relate to asymptotically flat spacetimes and asymptotic symmetries, the use to
    construct such spacetimes is that the cosmological constant is zero and matter sources are localized. Here, we will
    look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu} R=8 \pi G T_{\mu \nu}$ An important
    property of Einstein's equation that relates to asmptotically flat spacetime is that, for the equation, as one moves
    far away, the matter stress tensor approaches zero and the metric in the asympotic region begins to approach that of
    flat spacetime. To continue, for the coordinate-based reprentation we will write down an expansion of the metric in
    powers of a radial coordinate, such that for a class of diffeomorphisms, it will preserve the falloffs to identify
    the asymptotic symmetry group. This region of expansion exists around a region of spacetime called
    the 'radiation zone' , which is yielded by null geodesics at infinite affine parameter. To continue. Einstein's
    Equations The equations of motion for an Einstein frame are given below and note we will write $a$ for $a_{n}$:
    $R^\mu{ }_\nu=\frac{1}{2} \partial^\mu \phi \partial_\nu \phi+\frac{1}{2 n !} e^{a \phi} $ $ \Big(n F^{\mu \xi_2
    \ldots \xi_n} F_{\nu \xi_2 \ldots \xi_n} $ $ -\frac{n-1}{D-2} \delta_\nu^\mu F_n^2\Big) $ $\begin{aligned} \nabla^2
    \phi &=\frac{1}{\sqrt{g}} \partial_\mu\left(\sqrt{g} \partial_\nu \phi g^{\mu \nu}\right) \\ &=\frac{a}{2 n !} F_n^2
    \\ 0 &=\partial_\mu\left(\sqrt{g} e^{a \phi} F^{\mu \nu_2 \ldots \nu_n}\right) \end{aligned}$ Where the $n$-form
    field strength $F_{n}\neq 0$ for only one value of $n$ and $R^\mu{ }_\nu$ is the Ricci tensor. Next, we will briefly
    examine Einstein's equations as they relate to asymptotically flat spacetimes and asymptotic symmetries, the use to
    construct such spacetimes is that the cosmological constant is zero and matter sources are localized. Here, we will
    look at solutions to Einstein's equation: $R_{\mu \nu}-\frac{1}{2} g_{\mu \nu} R=8 \pi G T_{\mu \nu}$ An important
    property of Einstein's equation that relates to asmptotically flat spacetime is that, for the equation, as one moves
    far away, the matter stress tensor approaches zero and the metric in the asympotic region begins to approach that of
    flat spacetime. To continue, for the coordinate-based reprentation we will write down an expansion of the metric in
    powers of a radial coordinate, such that for a class of diffeomorphisms, it will preserve the falloffs to identify
    the asymptotic symmetry group. This region of expansion exists around a region of spacetime called
    the 'radiation zone' , which is yielded by null geodesics at infinite affine parameter. To continue. The Maldacena
    Conjecture Given $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid in $(n+2)$-dimensional flat
    spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric $\eta_{a b=}$
    $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant: $\Lambda^2=$
    $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let
    $X_0=$ $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$ $\Lambda \sec \rho \sin
    \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time variable
    is $\tau$ is compact, we need an infinite set of copies of $AdS$ space in the $\tau$ direction to unwrap it from the
    $AdS$ covering space. The Maldacena Conjecture Given $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid
    in $(n+2)$-dimensional flat spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric $\eta_{a
    b=}$ $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant: $\Lambda^2=$
    $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let
    $X_0=$ $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$ $\Lambda \sec \rho \sin
    \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time variable
    is $\tau$ is compact, we need an infinite set of copies of $AdS$ space in the $\tau$ direction to unwrap it from the
    $AdS$ covering space. Glossary AdS/CFT Correspondence Anti-de Sitter/conformal theory correspondence AdS/CFT in
    theoretical physics is a conjecture that describes the relationship between two kinds of physical theories. AdS used
    in quantum gravtiy and is formulated in terms of string theory of M-theory, while, CFT are quantum field theories
    that include theories such as Yang-Mills theories describing elementatry particles. Lorentzian Space Lorentzian
    $n$-space is the inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional Lorentzian inner product.
    Where the vector space is a set that is closed under finite vector addition and scalar multiplication and inner
    product is defined as a vector space with an inner product on it. Minkowski Space The Minkowski Space is a
    particular type of Lorentzian space, specifically $4$-dimensional Lorentzian space, with a Minkowski metric or
    Minkowski tensor. Where the Minkowski metric is a type of metric tensor denoted as $d \tau^2$ with the form
    $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the
    basis of the study of spacetime within special relativity. Another relative feature of this space is that it unifies
    $\mathbb{R}^{3}$ plus time (the "fourth dimension" ) in Einstein's theory of special relativity. Minkowski Metric
    The Minkowski metric is defined as: $g_{\mu \nu} \approx \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0
    & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of generally curved
    spacetime. One application of the Minkowski metric is modeling the cosmological constant term in Einstein's field
    equation with stress-energy from a vaccum or not. Metric Signature In theoretical physics, the metric signature
    counts the number of time-like or space-like characters are in the spacetime. For example, in the case Minkowski
    metric signature is $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time direction. If the
    eigenvalue is defined in three spatial directions $x,y,z$, then the metric signature is $(1,3,0)^{-}$ or
    $(-,+,+,+)$. GlossaryAdS/CFT Correspondence Anti-de Sitter/conformal theory correspondence AdS/CFT in theoretical
    physics is a conjecture that describes the relationship between two kinds of physical theories. AdS used in quantum
    gravtiy and is formulated in terms of string theory of M-theory, while, CFT are quantum field theories that include
    theories such as Yang-Mills theories describing elementatry particles. AdS/CFT Correspondence Anti-de
    Sitter/conformal theory correspondence AdS/CFT in theoretical physics is a conjecture that describes the
    relationship between two kinds of physical theories. AdS used in quantum gravtiy and is formulated in terms of
    string theory of M-theory, while, CFT are quantum field theories that include theories such as Yang-Mills theories
    describing elementatry particles. Lorentzian Space Lorentzian $n$-space is the inner product space of
    $\mathbb{R}^{n}$ vector space with $n$-dimensional Lorentzian inner product. Where the vector space is a set that is
    closed under finite vector addition and scalar multiplication and inner product is defined as a vector space with an
    inner product on it. Lorentzian Space Lorentzian $n$-space is the inner product space of $\mathbb{R}^{n}$ vector
    space with $n$-dimensional Lorentzian inner product. Where the vector space is a set that is closed under finite
    vector addition and scalar multiplication and inner product is defined as a vector space with an inner product on
    it. Minkowski Space The Minkowski Space is a particular type of Lorentzian space, specifically $4$-dimensional
    Lorentzian space, with a Minkowski metric or Minkowski tensor. Where the Minkowski metric is a type of metric tensor
    denoted as $d \tau^2$ with the form $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d x^2\right)^2+\left(d
    x^3\right)^2$. Minkowski space forms the basis of the study of spacetime within special relativity. Another relative
    feature of this space is that it unifies $\mathbb{R}^{3}$ plus time (the "fourth dimension" ) in Einstein's theory
    of special relativity. Minkowski Space The Minkowski Space is a particular type of Lorentzian space, specifically
    $4$-dimensional Lorentzian space, with a Minkowski metric or Minkowski tensor. Where the Minkowski metric is a type
    of metric tensor denoted as $d \tau^2$ with the form $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d
    x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of the study of spacetime within special
    relativity. Another relative feature of this space is that it unifies $\mathbb{R}^{3}$ plus time
    (the "fourth dimension" ) in Einstein's theory of special relativity. Minkowski Metric The Minkowski metric is
    defined as: $g_{\mu \nu} \approx \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 &
    1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of generally curved spacetime. One application of
    the Minkowski metric is modeling the cosmological constant term in Einstein's field equation with stress-energy from
    a vaccum or not. Minkowski Metric The Minkowski metric is defined as: $g_{\mu \nu} \approx \eta_{\mu
    \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$
    Such that it is a metric of generally curved spacetime. One application of the Minkowski metric is modeling the
    cosmological constant term in Einstein's field equation with stress-energy from a vaccum or not. Metric Signature In
    theoretical physics, the metric signature counts the number of time-like or space-like characters are in the
    spacetime. For example, in the case Minkowski metric signature is $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction
    is defined in the time direction. If the eigenvalue is defined in three spatial directions $x,y,z$, then the metric
    signature is $(1,3,0)^{-}$ or $(-,+,+,+)$. Metric Signature In theoretical physics, the metric signature counts the
    number of time-like or space-like characters are in the spacetime. For example, in the case Minkowski metric
    signature is $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time direction. If the eigenvalue
    is defined in three spatial directions $x,y,z$, then the metric signature is $(1,3,0)^{-}$ or $(-,+,+,+)$.
    [https://www.contextswitching.org/math/chernclasses] Chern Classes - Context Switching Chern Classes Introduction
    Chern classes are part of algebraic topology, as well as other math groups, and are characteristic classes related
    to complex vector bundles. The first Chern class is the only nontrivial Chern class and is an element of the second
    cohomology group of $X$, where $X$ is a topological space of closure-finite weak CW complex. Chern Classes Defined
    The Chern classes $c_{i}(\omega)\in H^{2i}(B;\mathbb{Z})$, where $B$ is a paracompact base space admitting a
    Hermation metric, are found through induction on the complex dimension $n$ of the complex real vector bundle
    $\omega$, defined with in following conditionals. The top Chern class $c_{n}(\omega)$ is equivalent to the Euler
    class $e(\omega\mathbb{R})$. For any given Chern class less than the top Chern classes, where $n> i$, it is defined
    as: $c_{i}(\omega)=\pi_{0}^{*-1}c_{i}(\omega_{0})$, s.t. $\pi_{0}^{*-1}:H^{2i}(B)\rightarrow H^{2i}(E_{0})$ where
    $E_{0}$ denotes the deleted total space in the real case of $E_{0}(\omega)$ and $\pi_{0}^{*-1}$ is an isomorphism
    for $n > i$. Lastly, for all $c_{i}(\omega)$ where $i > n$, the Chern classes are defined as zero. The formal sum of
    $c_{\omega}=1+c_{1}(\omega)+c_{2}(\omega)...+c_{n}(\omega)$ for the given ring $H^{\Pi}(B;\mathbb{Z})$ is termed the
    total Chern class of the complex $n$-plane bundle $\omega$. Construction of Chern Classes For the construction of
    chern classes, let's give an inductive definition for the characterstics of classes for a complex $n$-plane bundle
    $\omega$. Note, that there exists an underlying real vector bundle $\omega_{\mathbb{R}}$ which has a prefered
    canonical orientation for the complex vector bundle $\omega$. Where the oriented bundle to $(\omega\oplus
    w')_{\mathbb{R}}$ is an isomoprhic bundle, but only if $\omega'$ is a complex $m$-plane bundle over the same basis
    as $\omega$. Consider an inductive definition of characterstics classes for a complex $(n-1)$-plane bundle. To start
    the construction, first, we need to build a canonical $(n-1)$-plane bundle $\omega_{0}$ over $E_{0}$, which denotes
    the deleted total space. The deleted total space in the real case $E_{0}=E_{0}(\omega)$, dentoes the set of all of
    the non-zero vectors in the total space $E_{0}=E_{0}(\omega_{\mathbb{R}})$. There is a point on the set of all
    non-zero vectors $E_{0}$ is specified with a fiber $F$ of $\omega$ and a non-zero vector $v$ in that fiber $F$.
    Given a Hermatian metric defined on $\omega$, the fiber of $\omega_{0}$ over the non-zero vector $v$ in the total
    space of non-zero vectors, by defintion, is orthogonal complement of the vector $v$ in the vector space $F$, by
    definition. This new ($n-1$) dimensional complex vector space is where all vector spaces can be considered as new
    vector bundle $w_{0}$ over $E_{0}$. Product Theorem for Chern Classes The following formula shows that the total
    Chern class of Whiteny sum of $\omega\oplus\phi$ is equivalant to the total Chern clesses of $\omega$ and $\phi$:
    $c(\omega\oplus\phi)=c(\omega)c(\phi)$ where $\omega$ and $\phi$ are two complex vector bundles over a shared
    paracompact base space $B$. Glossary Eucledian Vector Bundles A Eucledian vector bundles are such that each fiber
    has the structure of Eucledian vector space. Meaning that a Euclidean vector bundle is defined as a real vector
    bundle $\xi$ combined with a continuous function: $\mu:E(\xi)\rightarrow\mathbb{R}$ where the $\mu$ restriction for
    each fiber of $\xi$ is positive definite and quadratic. Such that the Eucledian vector space is a real vector space
    $V$ with a postive definite quadratic function: $\mu:V\rightarrow\mathbb{R}$ Hermitian Metric Hermitian metrics play
    a role in complex vector bundles, just as Euclidean metrics play a role in real or eucledian vector bundles. A
    Hermitian metric on a complex vector bundle $\omega$ is a Euclidean metric: $v\mapsto|v|^{2}\geq0$ where the
    underlying real vector bundle satisifes the identity: $|iv|=|v|$ Group Cohomology Group cohomology is a set of
    mathematical tools that are used to study groups using cohomology. Cohomology Cohomology is apart of homology theory
    and algebraic topology. Cohomology refers to a sequence of albenian groups that are apart of topological space or
    cochain complex. Topolgical Space You can read about topological space here. Chern Classes - Context Switching Chern
    Classes - Context SwitchingChern Classes Introduction Chern classes are part of algebraic topology, as well as other
    math groups, and are characteristic classes related to complex vector bundles. The first Chern class is the only
    nontrivial Chern class and is an element of the second cohomology group of $X$, where $X$ is a topological space of
    closure-finite weak CW complex. Chern Classes Defined The Chern classes $c_{i}(\omega)\in H^{2i}(B;\mathbb{Z})$,
    where $B$ is a paracompact base space admitting a Hermation metric, are found through induction on the complex
    dimension $n$ of the complex real vector bundle $\omega$, defined with in following conditionals. The top Chern
    class $c_{n}(\omega)$ is equivalent to the Euler class $e(\omega\mathbb{R})$. For any given Chern class less than
    the top Chern classes, where $n > i$, it is defined as: $c_{i}(\omega)=\pi_{0}^{*-1}c_{i}(\omega_{0})$, s.t.
    $\pi_{0}^{*-1}:H^{2i}(B)\rightarrow H^{2i}(E_{0})$ where $E_{0}$ denotes the deleted total space in the real case of
    $E_{0}(\omega)$ and $\pi_{0}^{*-1}$ is an isomorphism for $n > i$. Lastly, for all $c_{i}(\omega)$ where $i > n$,
    the Chern classes are defined as zero. The formal sum of $c_{\omega}=1+c_{1}(\omega)+c_{2}(\omega)...+c_{n}(\omega)$
    for the given ring $H^{\Pi}(B;\mathbb{Z})$ is termed the total Chern class of the complex $n$-plane bundle $\omega$.
    Construction of Chern Classes For the construction of chern classes, let's give an inductive definition for the
    characterstics of classes for a complex $n$-plane bundle $\omega$. Note, that there exists an underlying real vector
    bundle $\omega_{\mathbb{R}}$ which has a prefered canonical orientation for the complex vector bundle $\omega$.
    Where the oriented bundle to $(\omega\oplus w')_{\mathbb{R}}$ is an isomoprhic bundle, but only if $\omega'$ is a
    complex $m$-plane bundle over the same basis as $\omega$. Consider an inductive definition of characterstics classes
    for a complex $(n-1)$-plane bundle. To start the construction, first, we need to build a canonical $(n-1)$-plane
    bundle $\omega_{0}$ over $E_{0}$, which denotes the deleted total space. The deleted total space in the real case
    $E_{0}=E_{0}(\omega)$, dentoes the set of all of the non-zero vectors in the total space
    $E_{0}=E_{0}(\omega_{\mathbb{R}})$. There is a point on the set of all non-zero vectors $E_{0}$ is specified with a
    fiber $F$ of $\omega$ and a non-zero vector $v$ in that fiber $F$. Given a Hermatian metric defined on $\omega$, the
    fiber of $\omega_{0}$ over the non-zero vector $v$ in the total space of non-zero vectors, by defintion, is
    orthogonal complement of the vector $v$ in the vector space $F$, by definition. This new ($n-1$) dimensional complex
    vector space is where all vector spaces can be considered as new vector bundle $w_{0}$ over $E_{0}$. Product Theorem
    for Chern Classes The following formula shows that the total Chern class of Whiteny sum of $\omega\oplus\phi$ is
    equivalant to the total Chern clesses of $\omega$ and $\phi$: $c(\omega\oplus\phi)=c(\omega)c(\phi)$ where $\omega$
    and $\phi$ are two complex vector bundles over a shared paracompact base space $B$. Glossary Eucledian Vector
    Bundles A Eucledian vector bundles are such that each fiber has the structure of Eucledian vector space. Meaning
    that a Euclidean vector bundle is defined as a real vector bundle $\xi$ combined with a continuous function:
    $\mu:E(\xi)\rightarrow\mathbb{R}$ where the $\mu$ restriction for each fiber of $\xi$ is positive definite and
    quadratic. Such that the Eucledian vector space is a real vector space $V$ with a postive definite quadratic
    function: $\mu:V\rightarrow\mathbb{R}$ Hermitian Metric Hermitian metrics play a role in complex vector bundles,
    just as Euclidean metrics play a role in real or eucledian vector bundles. A Hermitian metric on a complex vector
    bundle $\omega$ is a Euclidean metric: $v\mapsto|v|^{2}\geq0$ where the underlying real vector bundle satisifes the
    identity: $|iv|=|v|$ Group Cohomology Group cohomology is a set of mathematical tools that are used to study groups
    using cohomology. Cohomology Cohomology is apart of homology theory and algebraic topology. Cohomology refers to a
    sequence of albenian groups that are apart of topological space or cochain complex. Topolgical Space You can read
    about topological space here. Chern Classes Introduction Chern classes are part of algebraic topology, as well as
    other math groups, and are characteristic classes related to complex vector bundles. The first Chern class is the
    only nontrivial Chern class and is an element of the second cohomology group of $X$, where $X$ is a topological
    space of closure-finite weak CW complex. Chern Classes Defined The Chern classes $c_{i}(\omega)\in
    H^{2i}(B;\mathbb{Z})$, where $B$ is a paracompact base space admitting a Hermation metric, are found through
    induction on the complex dimension $n$ of the complex real vector bundle $\omega$, defined with in following
    conditionals. The top Chern class $c_{n}(\omega)$ is equivalent to the Euler class $e(\omega\mathbb{R})$. For any
    given Chern class less than the top Chern classes, where $n > i$, it is defined as:
    $c_{i}(\omega)=\pi_{0}^{*-1}c_{i}(\omega_{0})$, s.t. $\pi_{0}^{*-1}:H^{2i}(B)\rightarrow H^{2i}(E_{0})$ where
    $E_{0}$ denotes the deleted total space in the real case of $E_{0}(\omega)$ and $\pi_{0}^{*-1}$ is an isomorphism
    for $n > i$. Lastly, for all $c_{i}(\omega)$ where $i > n$, the Chern classes are defined as zero. The formal sum of
    $c_{\omega}=1+c_{1}(\omega)+c_{2}(\omega)...+c_{n}(\omega)$ for the given ring $H^{\Pi}(B;\mathbb{Z})$ is termed the
    total Chern class of the complex $n$-plane bundle $\omega$. Construction of Chern Classes For the construction of
    chern classes, let's give an inductive definition for the characterstics of classes for a complex $n$-plane bundle
    $\omega$. Note, that there exists an underlying real vector bundle $\omega_{\mathbb{R}}$ which has a prefered
    canonical orientation for the complex vector bundle $\omega$. Where the oriented bundle to $(\omega\oplus
    w')_{\mathbb{R}}$ is an isomoprhic bundle, but only if $\omega'$ is a complex $m$-plane bundle over the same basis
    as $\omega$. Consider an inductive definition of characterstics classes for a complex $(n-1)$-plane bundle. To start
    the construction, first, we need to build a canonical $(n-1)$-plane bundle $\omega_{0}$ over $E_{0}$, which denotes
    the deleted total space. The deleted total space in the real case $E_{0}=E_{0}(\omega)$, dentoes the set of all of
    the non-zero vectors in the total space $E_{0}=E_{0}(\omega_{\mathbb{R}})$. There is a point on the set of all
    non-zero vectors $E_{0}$ is specified with a fiber $F$ of $\omega$ and a non-zero vector $v$ in that fiber $F$.
    Given a Hermatian metric defined on $\omega$, the fiber of $\omega_{0}$ over the non-zero vector $v$ in the total
    space of non-zero vectors, by defintion, is orthogonal complement of the vector $v$ in the vector space $F$, by
    definition. This new ($n-1$) dimensional complex vector space is where all vector spaces can be considered as new
    vector bundle $w_{0}$ over $E_{0}$. Product Theorem for Chern Classes The following formula shows that the total
    Chern class of Whiteny sum of $\omega\oplus\phi$ is equivalant to the total Chern clesses of $\omega$ and $\phi$:
    $c(\omega\oplus\phi)=c(\omega)c(\phi)$ where $\omega$ and $\phi$ are two complex vector bundles over a shared
    paracompact base space $B$. Glossary Eucledian Vector Bundles A Eucledian vector bundles are such that each fiber
    has the structure of Eucledian vector space. Meaning that a Euclidean vector bundle is defined as a real vector
    bundle $\xi$ combined with a continuous function: $\mu:E(\xi)\rightarrow\mathbb{R}$ where the $\mu$ restriction for
    each fiber of $\xi$ is positive definite and quadratic. Such that the Eucledian vector space is a real vector space
    $V$ with a postive definite quadratic function: $\mu:V\rightarrow\mathbb{R}$ Hermitian Metric Hermitian metrics play
    a role in complex vector bundles, just as Euclidean metrics play a role in real or eucledian vector bundles. A
    Hermitian metric on a complex vector bundle $\omega$ is a Euclidean metric: $v\mapsto|v|^{2}\geq0$ where the
    underlying real vector bundle satisifes the identity: $|iv|=|v|$ Group Cohomology Group cohomology is a set of
    mathematical tools that are used to study groups using cohomology. Cohomology Cohomology is apart of homology theory
    and algebraic topology. Cohomology refers to a sequence of albenian groups that are apart of topological space or
    cochain complex. Topolgical Space You can read about topological space here. Chern Classes Introduction Chern
    classes are part of algebraic topology, as well as other math groups, and are characteristic classes related to
    complex vector bundles. The first Chern class is the only nontrivial Chern class and is an element of the second
    cohomology group of $X$, where $X$ is a topological space of closure-finite weak CW complex. Chern Classes Defined
    The Chern classes $c_{i}(\omega)\in H^{2i}(B;\mathbb{Z})$, where $B$ is a paracompact base space admitting a
    Hermation metric, are found through induction on the complex dimension $n$ of the complex real vector bundle
    $\omega$, defined with in following conditionals. The top Chern class $c_{n}(\omega)$ is equivalent to the Euler
    class $e(\omega\mathbb{R})$. For any given Chern class less than the top Chern classes, where $n > i$, it is defined
    as: $c_{i}(\omega)=\pi_{0}^{*-1}c_{i}(\omega_{0})$, s.t. $\pi_{0}^{*-1}:H^{2i}(B)\rightarrow H^{2i}(E_{0})$ where
    $E_{0}$ denotes the deleted total space in the real case of $E_{0}(\omega)$ and $\pi_{0}^{*-1}$ is an isomorphism
    for $n > i$. Lastly, for all $c_{i}(\omega)$ where $i > n$, the Chern classes are defined as zero. The formal sum of
    $c_{\omega}=1+c_{1}(\omega)+c_{2}(\omega)...+c_{n}(\omega)$ for the given ring $H^{\Pi}(B;\mathbb{Z})$ is termed the
    total Chern class of the complex $n$-plane bundle $\omega$. Construction of Chern Classes For the construction of
    chern classes, let's give an inductive definition for the characterstics of classes for a complex $n$-plane bundle
    $\omega$. Note, that there exists an underlying real vector bundle $\omega_{\mathbb{R}}$ which has a prefered
    canonical orientation for the complex vector bundle $\omega$. Where the oriented bundle to $(\omega\oplus
    w')_{\mathbb{R}}$ is an isomoprhic bundle, but only if $\omega'$ is a complex $m$-plane bundle over the same basis
    as $\omega$. Consider an inductive definition of characterstics classes for a complex $(n-1)$-plane bundle. To start
    the construction, first, we need to build a canonical $(n-1)$-plane bundle $\omega_{0}$ over $E_{0}$, which denotes
    the deleted total space. The deleted total space in the real case $E_{0}=E_{0}(\omega)$, dentoes the set of all of
    the non-zero vectors in the total space $E_{0}=E_{0}(\omega_{\mathbb{R}})$. There is a point on the set of all
    non-zero vectors $E_{0}$ is specified with a fiber $F$ of $\omega$ and a non-zero vector $v$ in that fiber $F$.
    Given a Hermatian metric defined on $\omega$, the fiber of $\omega_{0}$ over the non-zero vector $v$ in the total
    space of non-zero vectors, by defintion, is orthogonal complement of the vector $v$ in the vector space $F$, by
    definition. This new ($n-1$) dimensional complex vector space is where all vector spaces can be considered as new
    vector bundle $w_{0}$ over $E_{0}$. Product Theorem for Chern Classes The following formula shows that the total
    Chern class of Whiteny sum of $\omega\oplus\phi$ is equivalant to the total Chern clesses of $\omega$ and $\phi$:
    $c(\omega\oplus\phi)=c(\omega)c(\phi)$ where $\omega$ and $\phi$ are two complex vector bundles over a shared
    paracompact base space $B$. Glossary Eucledian Vector Bundles A Eucledian vector bundles are such that each fiber
    has the structure of Eucledian vector space. Meaning that a Euclidean vector bundle is defined as a real vector
    bundle $\xi$ combined with a continuous function: $\mu:E(\xi)\rightarrow\mathbb{R}$ where the $\mu$ restriction for
    each fiber of $\xi$ is positive definite and quadratic. Such that the Eucledian vector space is a real vector space
    $V$ with a postive definite quadratic function: $\mu:V\rightarrow\mathbb{R}$ Hermitian Metric Hermitian metrics play
    a role in complex vector bundles, just as Euclidean metrics play a role in real or eucledian vector bundles. A
    Hermitian metric on a complex vector bundle $\omega$ is a Euclidean metric: $v\mapsto|v|^{2}\geq0$ where the
    underlying real vector bundle satisifes the identity: $|iv|=|v|$ Group Cohomology Group cohomology is a set of
    mathematical tools that are used to study groups using cohomology. Cohomology Cohomology is apart of homology theory
    and algebraic topology. Cohomology refers to a sequence of albenian groups that are apart of topological space or
    cochain complex. Topolgical Space You can read about topological space here. Chern ClassesIntroduction Chern classes
    are part of algebraic topology, as well as other math groups, and are characteristic classes related to complex
    vector bundles. The first Chern class is the only nontrivial Chern class and is an element of the second cohomology
    group of $X$, where $X$ is a topological space of closure-finite weak CW complex. IntroductionChern classes are part
    of algebraic topology, as well as other math groups, and are characteristic classes related to complex vector
    bundles. The first Chern class is the only nontrivial Chern class and is an element of the second cohomology group
    of $X$, where $X$ is a topological space of closure-finite weak CW complex. vector bundlescohomology
    grouptopological spaceChern Classes Defined The Chern classes $c_{i}(\omega)\in H^{2i}(B;\mathbb{Z})$, where $B$ is
    a paracompact base space admitting a Hermation metric, are found through induction on the complex dimension $n$ of
    the complex real vector bundle $\omega$, defined with in following conditionals. The top Chern class $c_{n}(\omega)$
    is equivalent to the Euler class $e(\omega\mathbb{R})$. For any given Chern class less than the top Chern classes,
    where $n > i$, it is defined as: $c_{i}(\omega)=\pi_{0}^{*-1}c_{i}(\omega_{0})$, s.t.
    $\pi_{0}^{*-1}:H^{2i}(B)\rightarrow H^{2i}(E_{0})$ where $E_{0}$ denotes the deleted total space in the real case of
    $E_{0}(\omega)$ and $\pi_{0}^{*-1}$ is an isomorphism for $n > i$. Lastly, for all $c_{i}(\omega)$ where $i > n$,
    the Chern classes are defined as zero. The formal sum of $c_{\omega}=1+c_{1}(\omega)+c_{2}(\omega)...+c_{n}(\omega)$
    for the given ring $H^{\Pi}(B;\mathbb{Z})$ is termed the total Chern class of the complex $n$-plane bundle $\omega$.
    Chern Classes DefinedThe Chern classes $c_{i}(\omega)\in H^{2i}(B;\mathbb{Z})$, where $B$ is a paracompact base
    space admitting a Hermation metric, are found through induction on the complex dimension $n$ of the complex real
    vector bundle $\omega$, defined with in following conditionals. The top Chern class $c_{n}(\omega)$ is equivalent to
    the Euler class $e(\omega\mathbb{R})$. For any given Chern class less than the top Chern classes, where $n > i$, it
    is defined as: $c_{i}(\omega)=\pi_{0}^{*-1}c_{i}(\omega_{0})$, s.t. $\pi_{0}^{*-1}:H^{2i}(B)\rightarrow
    H^{2i}(E_{0})$ where $E_{0}$ denotes the deleted total space in the real case of $E_{0}(\omega)$ and $\pi_{0}^{*-1}$
    is an isomorphism for $n > i$. Lastly, for all $c_{i}(\omega)$ where $i > n$, the Chern classes are defined as zero.
    The formal sum of $c_{\omega}=1+c_{1}(\omega)+c_{2}(\omega)...+c_{n}(\omega)$ for the given ring
    $H^{\Pi}(B;\mathbb{Z})$ is termed the total Chern class of the complex $n$-plane bundle $\omega$. Construction of
    Chern Classes For the construction of chern classes, let's give an inductive definition for the characterstics of
    classes for a complex $n$-plane bundle $\omega$. Note, that there exists an underlying real vector bundle
    $\omega_{\mathbb{R}}$ which has a prefered canonical orientation for the complex vector bundle $\omega$. Where the
    oriented bundle to $(\omega\oplus w')_{\mathbb{R}}$ is an isomoprhic bundle, but only if $\omega'$ is a complex
    $m$-plane bundle over the same basis as $\omega$. Consider an inductive definition of characterstics classes for a
    complex $(n-1)$-plane bundle. To start the construction, first, we need to build a canonical $(n-1)$-plane bundle
    $\omega_{0}$ over $E_{0}$, which denotes the deleted total space. The deleted total space in the real case
    $E_{0}=E_{0}(\omega)$, dentoes the set of all of the non-zero vectors in the total space
    $E_{0}=E_{0}(\omega_{\mathbb{R}})$. There is a point on the set of all non-zero vectors $E_{0}$ is specified with a
    fiber $F$ of $\omega$ and a non-zero vector $v$ in that fiber $F$. Given a Hermatian metric defined on $\omega$, the
    fiber of $\omega_{0}$ over the non-zero vector $v$ in the total space of non-zero vectors, by defintion, is
    orthogonal complement of the vector $v$ in the vector space $F$, by definition. This new ($n-1$) dimensional complex
    vector space is where all vector spaces can be considered as new vector bundle $w_{0}$ over $E_{0}$. Construction of
    Chern ClassesFor the construction of chern classes, let's give an inductive definition for the characterstics of
    classes for a complex $n$-plane bundle $\omega$. Note, that there exists an underlying real vector bundle
    $\omega_{\mathbb{R}}$ which has a prefered canonical orientation for the complex vector bundle $\omega$. Where the
    oriented bundle to $(\omega\oplus w')_{\mathbb{R}}$ is an isomoprhic bundle, but only if $\omega'$ is a complex
    $m$-plane bundle over the same basis as $\omega$. Consider an inductive definition of characterstics classes for a
    complex $(n-1)$-plane bundle. To start the construction, first, we need to build a canonical $(n-1)$-plane bundle
    $\omega_{0}$ over $E_{0}$, which denotes the deleted total space. The deleted total space in the real case
    $E_{0}=E_{0}(\omega)$, dentoes the set of all of the non-zero vectors in the total space
    $E_{0}=E_{0}(\omega_{\mathbb{R}})$. There is a point on the set of all non-zero vectors $E_{0}$ is specified with a
    fiber $F$ of $\omega$ and a non-zero vector $v$ in that fiber $F$. Given a Hermatian metric defined on $\omega$, the
    fiber of $\omega_{0}$ over the non-zero vector $v$ in the total space of non-zero vectors, by defintion, is
    orthogonal complement of the vector $v$ in the vector space $F$, by definition. This new ($n-1$) dimensional complex
    vector space is where all vector spaces can be considered as new vector bundle $w_{0}$ over $E_{0}$. Consider an
    inductive definition of characterstics classes for a complex $(n-1)$-plane bundle. To start the construction, first,
    we need to build a canonical $(n-1)$-plane bundle $\omega_{0}$ over $E_{0}$, which denotes the deleted total space.
    The deleted total space in the real case $E_{0}=E_{0}(\omega)$, dentoes the set of all of the non-zero vectors in
    the total space $E_{0}=E_{0}(\omega_{\mathbb{R}})$. There is a point on the set of all non-zero vectors $E_{0}$ is
    specified with a fiber $F$ of $\omega$ and a non-zero vector $v$ in that fiber $F$. Given a Hermatian metric defined
    on $\omega$, the fiber of $\omega_{0}$ over the non-zero vector $v$ in the total space of non-zero vectors, by
    defintion, is orthogonal complement of the vector $v$ in the vector space $F$, by definition. This new ($n-1$)
    dimensional complex vector space is where all vector spaces can be considered as new vector bundle $w_{0}$ over
    $E_{0}$. Consider an inductive definition of characterstics classes for a complex $(n-1)$-plane bundle. To start the
    construction, first, we need to build a canonical $(n-1)$-plane bundle $\omega_{0}$ over $E_{0}$, which denotes the
    deleted total space. The deleted total space in the real case $E_{0}=E_{0}(\omega)$, dentoes the set of all of the
    non-zero vectors in the total space $E_{0}=E_{0}(\omega_{\mathbb{R}})$. There is a point on the set of all non-zero
    vectors $E_{0}$ is specified with a fiber $F$ of $\omega$ and a non-zero vector $v$ in that fiber $F$. Given a
    Hermatian metric defined on $\omega$, the fiber of $\omega_{0}$ over the non-zero vector $v$ in the total space of
    non-zero vectors, by defintion, is orthogonal complement of the vector $v$ in the vector space $F$, by definition.
    Hermatian metricThis new ($n-1$) dimensional complex vector space is where all vector spaces can be considered as
    new vector bundle $w_{0}$ over $E_{0}$. Product Theorem for Chern Classes The following formula shows that the total
    Chern class of Whiteny sum of $\omega\oplus\phi$ is equivalant to the total Chern clesses of $\omega$ and $\phi$:
    $c(\omega\oplus\phi)=c(\omega)c(\phi)$ where $\omega$ and $\phi$ are two complex vector bundles over a shared
    paracompact base space $B$. Product Theorem for Chern ClassesThe following formula shows that the total Chern class
    of Whiteny sum of $\omega\oplus\phi$ is equivalant to the total Chern clesses of $\omega$ and $\phi$:
    $c(\omega\oplus\phi)=c(\omega)c(\phi)$ where $\omega$ and $\phi$ are two complex vector bundles over a shared
    paracompact base space $B$. Glossary Eucledian Vector Bundles A Eucledian vector bundles are such that each fiber
    has the structure of Eucledian vector space. Meaning that a Euclidean vector bundle is defined as a real vector
    bundle $\xi$ combined with a continuous function: $\mu:E(\xi)\rightarrow\mathbb{R}$ where the $\mu$ restriction for
    each fiber of $\xi$ is positive definite and quadratic. Such that the Eucledian vector space is a real vector space
    $V$ with a postive definite quadratic function: $\mu:V\rightarrow\mathbb{R}$ Hermitian Metric Hermitian metrics play
    a role in complex vector bundles, just as Euclidean metrics play a role in real or eucledian vector bundles. A
    Hermitian metric on a complex vector bundle $\omega$ is a Euclidean metric: $v\mapsto|v|^{2}\geq0$ where the
    underlying real vector bundle satisifes the identity: $|iv|=|v|$ Group Cohomology Group cohomology is a set of
    mathematical tools that are used to study groups using cohomology. Cohomology Cohomology is apart of homology theory
    and algebraic topology. Cohomology refers to a sequence of albenian groups that are apart of topological space or
    cochain complex. Topolgical Space You can read about topological space here. GlossaryEucledian Vector Bundles A
    Eucledian vector bundles are such that each fiber has the structure of Eucledian vector space. Meaning that a
    Euclidean vector bundle is defined as a real vector bundle $\xi$ combined with a continuous function:
    $\mu:E(\xi)\rightarrow\mathbb{R}$ where the $\mu$ restriction for each fiber of $\xi$ is positive definite and
    quadratic. Such that the Eucledian vector space is a real vector space $V$ with a postive definite quadratic
    function: $\mu:V\rightarrow\mathbb{R}$ Eucledian Vector BundlesA Eucledian vector bundles are such that each fiber
    has the structure of Eucledian vector space. Meaning that a Euclidean vector bundle is defined as a real vector
    bundle $\xi$ combined with a continuous function: $\mu:E(\xi)\rightarrow\mathbb{R}$ where the $\mu$ restriction for
    each fiber of $\xi$ is positive definite and quadratic. Such that the Eucledian vector space is a real vector space
    $V$ with a postive definite quadratic function: $\mu:V\rightarrow\mathbb{R}$ Hermitian Metric Hermitian metrics play
    a role in complex vector bundles, just as Euclidean metrics play a role in real or eucledian vector bundles. A
    Hermitian metric on a complex vector bundle $\omega$ is a Euclidean metric: $v\mapsto|v|^{2}\geq0$ where the
    underlying real vector bundle satisifes the identity: $|iv|=|v|$ Hermitian MetricHermitian metrics play a role in
    complex vector bundles, just as Euclidean metrics play a role in real or eucledian vector bundles. A Hermitian
    metric on a complex vector bundle $\omega$ is a Euclidean metric: $v\mapsto|v|^{2}\geq0$ where the underlying real
    vector bundle satisifes the identity: $|iv|=|v|$ Group Cohomology Group cohomology is a set of mathematical tools
    that are used to study groups using cohomology. Group CohomologyGroup cohomology is a set of mathematical tools that
    are used to study groups using cohomology. cohomologyCohomology Cohomology is apart of homology theory and algebraic
    topology. Cohomology refers to a sequence of albenian groups that are apart of topological space or cochain complex.
    Cohomologyalbenian groupstopological spaceTopolgical Space You can read about topological space here. Topolgical
    SpaceYou can read about topological space here. here
    [https://www.contextswitching.org/my/presentations/qcnn/]
    You are being redirected by your loyal guides 🐶🐱 You are being redirected by your loyal guides 🐶🐱 You are being
    redirected by your loyal guides 🐶🐱
    [https://www.contextswitching.org/tcs/informationtheory]
    Information Theory - Context Switching Information Theory Quantities of Information Shannon Information Three
    properties were required by Shannon: $I(p) \geq 0$, i.e. information is a real non-negative measure.
    $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$ for independent events. $I(p)$ is a continous function of $p$. The mathematical
    function that satisfies these requirements is: $I(p)=k\;log(p)$ In the equation, the value of $k$ is arbitrary, so
    we choose $k=-1$ to make the math more convient. $I(p)=-\log(p)=\log(\frac{1}{p})$ The base of the logarithm is
    representative of the units of measure for the given information, but can also be chosen arbitarily. However, today
    units of information are exclusively use base 2 logarithms, in units of bits.
    $I(p)=-\log_{2}(p)=\log_{2}(\frac{1}{p})$ bits of information. Shannon Entropy and Average Code Length Let
    $S=\left\{s_{1}, s_{2},\ldots s_{n}\right\}$ some information source with $n$ symbols. Let $P=\left\{p_{1}, p_{2},
    \ldots p_{n}\right\}$ be the corresponding probability distribution. Entropy of the source is the defined as:
    $H(S)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$ $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ Another important number is
    the average code length $L_{avg}$, defined as: $L_{avg}=\sum_{i=1}^{n}p_{i}
    \left(\lceil\log_{2}\frac{1}{p_{i}}\rceil+1\right)$ Where, $c_{i}$ is some code string of the code set $C$ and
    $|\;|$ is the cardinality of $c_{i}$. Note, the entropy of the source is a lower bound on the average code length
    that can be achieved, given by $H(S) \leq L_{avg}$. Meaning, that the source entropy can be reach the compression
    limit, but can never exceed it. Kraft Inequality $K=\sum_{i=1}^{q}\frac{1}{r^{l^{i}}}\leq 1$ Where: $K$ is the Kraft
    sum $q$ is the number of source symbols $r$ is the radix of the channel alphabet $l_{i}$ is the lengths of the coded
    symbols. Note that the craft inequality must be satisfied for codes that called uniquely decodable, instantaneous
    codes, or prefix-free codes. Proof of Achieving Entropy Bound We can prove the entropy bound that can be achieved
    using a sufficiently large extension. We can first define that: $H(S)\leq L_{avg} < H(S)+1$ We take the extension
        for any code to the $n^{th}$ term, resulting in: $H(S^{n})\leq L_{n} < H(S^{n})+1$ We sequence $n$ symbols from
        $S$, taken from the extended source $S^{n}$, which now has $q^{n}$ symbols. Lastly, we divide our previous
        equation by $n$, giving us: $H(S)\leq \frac{L_{n}}{n} < H(S)+\frac{1}{n}$ What this shows, is that by choosing
        $n$ sufficently large, the average code length can come arbitrarily closer to source entropy, $H(S)$. This being
        said, there are practical limitations to choosing a large extension on our source symbols. As the extensions on
        our souce become larger, the returns on efficecny deminish and have a higher cost on delay, stroage, and
        computation. System Entropies The system entropies represent the input and output entropies. The source entropy
        can be defined as: $H_{r}(A)=\sum_{i=1}^{q}p(a_{i})\log_{r}\left(\frac{1}{p(a_{i})}\right)$ Recall that
        properties of the entropy function are: $H_{r}(A)\geq 0$ $H_{r}(A)\leq \log_{r}q$, where $q$ is the number of
        input symbols $H_{r}(A)=\log_{r}q$ when all source symbols are equally likely The output entropy can be defined
        as: $H_{r}(B)=\sum_{j=1}^{s}p(b_{j})\log_{r}\left[\frac{1}{p(b_{j})}\right]$ Conditional Entropy To continue.
        Mutual Information To continue. Channel Capacity To continue. Hamming Code and Error Correcting We will look at
        the encoding of a binary source for transmission on a noisy channel and then build a Hamming Code to transmit
        one bit of information. The Hamman Coding Model illustrated below shows two valid codewords spheres in
        $3$-dimensional Hamming Space: Image Source:
        https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226
        Mathematical Tools Hamming Distance Given two $N$-bit binary symbols, the Hamming distance between them is the
        number of symbols that differ. For example, if the symbols are $0110$ and $1010$ is $2$. Note, that binary
        numbers can be represented as points in an $N$-dimensional Hamming Space. N-Dimensional Hamming Space The volume
        of an $N$-dimensional Hamming space is based on the number of binary numbers or points in the $N$-bit binary
        number and is defined as $2^{N}$. The hamming space is represented with a set of $N$ coordinate axes, where each
        axis contains the discrete values $\left\{0,1\right\}$. The figure below is an example of $N=4$ dimensional
        space cube representing a $4$-dimensional Hamming space. Hamming space gets harder to visualize when the number
        of dimensions gets larger, but we must trust the math. Encoding Source Symbols Let's use a $3$-dimensional
        Hamming space for the encoding of a binary source on a noisy channel. Image Source: Methods of Mathematics
        Applied to Calculus, Probability, and Statistics by Richard Wesley Hamming Let the source alphabet be
        $\left\{s_{0}, s_{1}\right\}$ with triple repetition coding $s_{0}=000, s_{1}=111$. Encoding a binary source for
        transmission over a noisy channel can cause bit error, so the decoder needs to try to catch and fix this. To do,
        the decoder will map the recievecd codeword to the closest legal coderword, which is smallest Hamming distance.
        By looking at our $3$-dimensional Hamming space, we can see that the encoder needs to encode the source symbols
        by a sequence of channel symbols with a "good" distance. This allows for the decoder to have an eaiser time
        catching and correcting bit errors. However, the decoder can still be foolder by multiple bit errors. Hamming
        Distance of a Code The Hamming distance of a code can be defined as the minimum pairwise Hamming distance
        between all of the codeword pairs and can allows us to measure the error correction capability of a code. The
        Hamming distance for the example $3$-dimensional Hamming space below is $3$. There exists a subset of points in
        $N$-dimensional Hamming space that is the set of legal codewords. To optimize error correction capability of a
        code, we need to spread out the set of legal codewords in $N$-dimensional space such that they all have the
        maximum possible mututal Hamming distances. Hamming Code Hamming Codes Hamming codes are a family of perfect
        single error correcting codes that can correct all $1$-bit errors or detect all $2$-bit errors and are described
        by $(n,m)$. Where $n$ is the total number of bits in a codeword and $n=2^{k}-1$. $m$ is the number of data bits
        in a codeword and then lastly $k$ is the number of parity or check bits in a codeword and $k=n-m$, so $n=m+k$.
        Codeword Examples n (# of bits) m (# of data bits) k (# of parity bits) 7 4 3 15 11 3 31 26 5 $(7,4)$ Hamming
        Code The $(7,4)$ Hamming code, as with the rest of the Hamming codes in the family, are perfect single error
        correcting codes and can correct all $1$-bit errors or detect all $2$-bit errors. The $(7,4)$ Hamming code has
        $7$ total number of bits in the codeword and $4$ number of data bits in the codeword. Thus, given that $k=n-m$,
        the number of parity bits is $3=7-4$. The $(7,4)$ Hamming code has $2^{m}=2^{4}=16$ codewords with a Hamming
        distance found to be $3$. Given a $7$-dimensional Hamming space there exists $2^{N}=2^{7}=128$ points. Every
        codeword has a subset of neighboring codewords seperated by $1$ vertex that we will call a sphere with radius of
        $r=1$. Each sphere contains $\binom{N}{r}+\binom{N}{0}$ which is $\binom{7}{1}+\binom{7}{0}=8$ points. Given the
        $128$ points in $7$-dimensional Hamming space and that there are $16$ spheres and $8$ points per sphere
        ($6*8=128$) in the $(7,4)$ Hammaning code, all points are covered. Thus, the $(7,4)$ Hamming code is a perfect
        code. The sphere has with $r=1$ has a point called the center and $\binom{n}{1}=n$ points on the surface, thus,
        the volume of the sphere or total points in the sphere is: $center\;+\binom{n}{1}=1+n$ Example of a sphere with
        $r=2$: $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$ volume or total points in the sphere where the center is a
        codeword. Hamming Decoding To continue. Error Correcting Error Correcting Capability The error correcting
        capability $t$ of a given code with some Hamming distance $d$ is defined as: $t=\left(\frac{d-1}{2}\right)$
        Meaning, that for the Hamming Code to correct $t$ errors, the sphere with $r=t$ of some codeword in Hamming
        Space, must not intersect with any other sphere. So, some Hamming Code can correct $t$ errors when $d>t$, but
        fails otherwise. For example, looking at the $(7,4)$ Hamming Code, where $t=1$ and $d=3$, $d$ can be visualized
        as the Hamming distance between two codewords. Thus, the $(7,4)$ Hamming Code can correct $t$ errors when $t <
            3$, but fails otherwise. Sphere Packing Bound The important part of sphere packing in Hamming Space, is that
            valid codewords speheres cannot intersect. To satisfy this requirment, for any code that can correct $t$
            errors, $2^{m}$ spheres each with radius $t$ is required. However, all spheres must fit in the $2^{n}$
            volume of Hamming space. So, we are limited by the following: $\frac{v_{HS}}{v_{S_{i}}}\geq max(S)$ Where
            $v_{HS}$ is the total volume of the Hamming Space $HS$, $v_{S_{i}}$ is the volume of the given sphere
            $S_{i}$ from the set of spheres $S$, and $max(S)$ is the maximum number of spheres. Using this, we can now
            figure out the minimum number if check bits or parity bits $k$ that are acheieve any error correction
            capability. Recall, that $k=n-m$ where $n$ is the total number of bits in a codeword and $m$ is the number
            of data bits in a codeword. Where $n$ and $m$ are described by $(n,m)$ of the Hamming Code, such as $(7,4)$
            Hamming Code. Hamming Code Example To continue. Compression Algorithms Huffman Coding Algorithm To continue.
            Lempel-Ziv Compression To continue. Arithmetic Coding To continue. Glossary Cardinality The cardinality of a
            set represents the number of elements in the set. Uniquely Decodable Codes that are uniquely decodable,
            instantaneous codes, or prefix-free codes are called such because the decoder that scans the code instantly
            recognizes the end of the codeword. Information Theory - Context Switching Information Theory - Context
            SwitchingInformation Theory Quantities of Information Shannon Information Three properties were required by
            Shannon: $I(p) \geq 0$, i.e. information is a real non-negative measure. $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
            for independent events. $I(p)$ is a continous function of $p$. The mathematical function that satisfies
            these requirements is: $I(p)=k\;log(p)$ In the equation, the value of $k$ is arbitrary, so we choose $k=-1$
            to make the math more convient. $I(p)=-\log(p)=\log(\frac{1}{p})$ The base of the logarithm is
            representative of the units of measure for the given information, but can also be chosen arbitarily.
            However, today units of information are exclusively use base 2 logarithms, in units of bits.
            $I(p)=-\log_{2}(p)=\log_{2}(\frac{1}{p})$ bits of information. Shannon Entropy and Average Code Length Let
            $S=\left\{s_{1}, s_{2},\ldots s_{n}\right\}$ some information source with $n$ symbols. Let $P=\left\{p_{1},
            p_{2}, \ldots p_{n}\right\}$ be the corresponding probability distribution. Entropy of the source is the
            defined as: $H(S)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$ $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ Another
            important number is the average code length $L_{avg}$, defined as: $L_{avg}=\sum_{i=1}^{n}p_{i}
            \left(\lceil\log_{2}\frac{1}{p_{i}}\rceil+1\right)$ Where, $c_{i}$ is some code string of the code set $C$
            and $|\;|$ is the cardinality of $c_{i}$. Note, the entropy of the source is a lower bound on the average
            code length that can be achieved, given by $H(S) \leq L_{avg}$. Meaning, that the source entropy can be
            reach the compression limit, but can never exceed it. Kraft Inequality
            $K=\sum_{i=1}^{q}\frac{1}{r^{l^{i}}}\leq 1$ Where: $K$ is the Kraft sum $q$ is the number of source symbols
            $r$ is the radix of the channel alphabet $l_{i}$ is the lengths of the coded symbols. Note that the craft
            inequality must be satisfied for codes that called uniquely decodable, instantaneous codes, or prefix-free
            codes. Proof of Achieving Entropy Bound We can prove the entropy bound that can be achieved using a
            sufficiently large extension. We can first define that: $H(S)\leq L_{avg} < H(S)+1$ We take the extension
            for any code to the $n^{th}$ term, resulting in: $H(S^{n})\leq L_{n} < H(S^{n})+1$ We sequence $n$ symbols
            from $S$, taken from the extended source $S^{n}$, which now has $q^{n}$ symbols. Lastly, we divide our
            previous equation by $n$, giving us: $H(S)\leq \frac{L_{n}}{n} < H(S)+\frac{1}{n}$ What this shows, is that
            by choosing $n$ sufficently large, the average code length can come arbitrarily closer to source entropy,
            $H(S)$. This being said, there are practical limitations to choosing a large extension on our source
            symbols. As the extensions on our souce become larger, the returns on efficecny deminish and have a higher
            cost on delay, stroage, and computation. System Entropies The system entropies represent the input and
            output entropies. The source entropy can be defined as:
            $H_{r}(A)=\sum_{i=1}^{q}p(a_{i})\log_{r}\left(\frac{1}{p(a_{i})}\right)$ Recall that properties of the
            entropy function are: $H_{r}(A)\geq 0$ $H_{r}(A)\leq \log_{r}q$, where $q$ is the number of input symbols
            $H_{r}(A)=\log_{r}q$ when all source symbols are equally likely The output entropy can be defined as:
            $H_{r}(B)=\sum_{j=1}^{s}p(b_{j})\log_{r}\left[\frac{1}{p(b_{j})}\right]$ Conditional Entropy To continue.
            Mutual Information To continue. Channel Capacity To continue. Hamming Code and Error Correcting We will look
            at the encoding of a binary source for transmission on a noisy channel and then build a Hamming Code to
            transmit one bit of information. The Hamman Coding Model illustrated below shows two valid codewords spheres
            in $3$-dimensional Hamming Space: Image Source:
            https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226
            Mathematical Tools Hamming Distance Given two $N$-bit binary symbols, the Hamming distance between them is
            the number of symbols that differ. For example, if the symbols are $0110$ and $1010$ is $2$. Note, that
            binary numbers can be represented as points in an $N$-dimensional Hamming Space. N-Dimensional Hamming Space
            The volume of an $N$-dimensional Hamming space is based on the number of binary numbers or points in the
            $N$-bit binary number and is defined as $2^{N}$. The hamming space is represented with a set of $N$
            coordinate axes, where each axis contains the discrete values $\left\{0,1\right\}$. The figure below is an
            example of $N=4$ dimensional space cube representing a $4$-dimensional Hamming space. Hamming space gets
            harder to visualize when the number of dimensions gets larger, but we must trust the math. Encoding Source
            Symbols Let's use a $3$-dimensional Hamming space for the encoding of a binary source on a noisy channel.
            Image Source: Methods of Mathematics Applied to Calculus, Probability, and Statistics by Richard Wesley
            Hamming Let the source alphabet be $\left\{s_{0}, s_{1}\right\}$ with triple repetition coding $s_{0}=000,
            s_{1}=111$. Encoding a binary source for transmission over a noisy channel can cause bit error, so the
            decoder needs to try to catch and fix this. To do, the decoder will map the recievecd codeword to the
            closest legal coderword, which is smallest Hamming distance. By looking at our $3$-dimensional Hamming
            space, we can see that the encoder needs to encode the source symbols by a sequence of channel symbols with
            a "good" distance. This allows for the decoder to have an eaiser time catching and correcting bit errors.
            However, the decoder can still be foolder by multiple bit errors. Hamming Distance of a Code The Hamming
            distance of a code can be defined as the minimum pairwise Hamming distance between all of the codeword pairs
            and can allows us to measure the error correction capability of a code. The Hamming distance for the example
            $3$-dimensional Hamming space below is $3$. There exists a subset of points in $N$-dimensional Hamming space
            that is the set of legal codewords. To optimize error correction capability of a code, we need to spread out
            the set of legal codewords in $N$-dimensional space such that they all have the maximum possible mututal
            Hamming distances. Hamming Code Hamming Codes Hamming codes are a family of perfect single error correcting
            codes that can correct all $1$-bit errors or detect all $2$-bit errors and are described by $(n,m)$. Where
            $n$ is the total number of bits in a codeword and $n=2^{k}-1$. $m$ is the number of data bits in a codeword
            and then lastly $k$ is the number of parity or check bits in a codeword and $k=n-m$, so $n=m+k$. Codeword
            Examples n (# of bits) m (# of data bits) k (# of parity bits) 7 4 3 15 11 3 31 26 5 $(7,4)$ Hamming Code
            The $(7,4)$ Hamming code, as with the rest of the Hamming codes in the family, are perfect single error
            correcting codes and can correct all $1$-bit errors or detect all $2$-bit errors. The $(7,4)$ Hamming code
            has $7$ total number of bits in the codeword and $4$ number of data bits in the codeword. Thus, given that
            $k=n-m$, the number of parity bits is $3=7-4$. The $(7,4)$ Hamming code has $2^{m}=2^{4}=16$ codewords with
            a Hamming distance found to be $3$. Given a $7$-dimensional Hamming space there exists $2^{N}=2^{7}=128$
            points. Every codeword has a subset of neighboring codewords seperated by $1$ vertex that we will call a
            sphere with radius of $r=1$. Each sphere contains $\binom{N}{r}+\binom{N}{0}$ which is
            $\binom{7}{1}+\binom{7}{0}=8$ points. Given the $128$ points in $7$-dimensional Hamming space and that there
            are $16$ spheres and $8$ points per sphere ($6*8=128$) in the $(7,4)$ Hammaning code, all points are
            covered. Thus, the $(7,4)$ Hamming code is a perfect code. The sphere has with $r=1$ has a point called the
            center and $\binom{n}{1}=n$ points on the surface, thus, the volume of the sphere or total points in the
            sphere is: $center\;+\binom{n}{1}=1+n$ Example of a sphere with $r=2$:
            $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$ volume or total points in the sphere where the center is a
            codeword. Hamming Decoding To continue. Error Correcting Error Correcting Capability The error correcting
            capability $t$ of a given code with some Hamming distance $d$ is defined as: $t=\left(\frac{d-1}{2}\right)$
            Meaning, that for the Hamming Code to correct $t$ errors, the sphere with $r=t$ of some codeword in Hamming
            Space, must not intersect with any other sphere. So, some Hamming Code can correct $t$ errors when $d>t$,
            but fails otherwise. For example, looking at the $(7,4)$ Hamming Code, where $t=1$ and $d=3$, $d$ can be
            visualized as the Hamming distance between two codewords. Thus, the $(7,4)$ Hamming Code can correct $t$
            errors when $t < 3$, but fails otherwise. Sphere Packing Bound The important part of sphere packing in
                Hamming Space, is that valid codewords speheres cannot intersect. To satisfy this requirment, for any
                code that can correct $t$ errors, $2^{m}$ spheres each with radius $t$ is required. However, all spheres
                must fit in the $2^{n}$ volume of Hamming space. So, we are limited by the following:
                $\frac{v_{HS}}{v_{S_{i}}}\geq max(S)$ Where $v_{HS}$ is the total volume of the Hamming Space $HS$,
                $v_{S_{i}}$ is the volume of the given sphere $S_{i}$ from the set of spheres $S$, and $max(S)$ is the
                maximum number of spheres. Using this, we can now figure out the minimum number if check bits or parity
                bits $k$ that are acheieve any error correction capability. Recall, that $k=n-m$ where $n$ is the total
                number of bits in a codeword and $m$ is the number of data bits in a codeword. Where $n$ and $m$ are
                described by $(n,m)$ of the Hamming Code, such as $(7,4)$ Hamming Code. Hamming Code Example To
                continue. Compression Algorithms Huffman Coding Algorithm To continue. Lempel-Ziv Compression To
                continue. Arithmetic Coding To continue. Glossary Cardinality The cardinality of a set represents the
                number of elements in the set. Uniquely Decodable Codes that are uniquely decodable, instantaneous
                codes, or prefix-free codes are called such because the decoder that scans the code instantly recognizes
                the end of the codeword. Information Theory Quantities of Information Shannon Information Three
                properties were required by Shannon: $I(p) \geq 0$, i.e. information is a real non-negative measure.
                $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$ for independent events. $I(p)$ is a continous function of $p$. The
                mathematical function that satisfies these requirements is: $I(p)=k\;log(p)$ In the equation, the value
                of $k$ is arbitrary, so we choose $k=-1$ to make the math more convient.
                $I(p)=-\log(p)=\log(\frac{1}{p})$ The base of the logarithm is representative of the units of measure
                for the given information, but can also be chosen arbitarily. However, today units of information are
                exclusively use base 2 logarithms, in units of bits. $I(p)=-\log_{2}(p)=\log_{2}(\frac{1}{p})$ bits of
                information. Shannon Entropy and Average Code Length Let $S=\left\{s_{1}, s_{2},\ldots s_{n}\right\}$
                some information source with $n$ symbols. Let $P=\left\{p_{1}, p_{2}, \ldots p_{n}\right\}$ be the
                corresponding probability distribution. Entropy of the source is the defined as:
                $H(S)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$ $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ Another
                important number is the average code length $L_{avg}$, defined as: $L_{avg}=\sum_{i=1}^{n}p_{i}
                \left(\lceil\log_{2}\frac{1}{p_{i}}\rceil+1\right)$ Where, $c_{i}$ is some code string of the code set
                $C$ and $|\;|$ is the cardinality of $c_{i}$. Note, the entropy of the source is a lower bound on the
                average code length that can be achieved, given by $H(S) \leq L_{avg}$. Meaning, that the source entropy
                can be reach the compression limit, but can never exceed it. Kraft Inequality
                $K=\sum_{i=1}^{q}\frac{1}{r^{l^{i}}}\leq 1$ Where: $K$ is the Kraft sum $q$ is the number of source
                symbols $r$ is the radix of the channel alphabet $l_{i}$ is the lengths of the coded symbols. Note that
                the craft inequality must be satisfied for codes that called uniquely decodable, instantaneous codes, or
                prefix-free codes. Proof of Achieving Entropy Bound We can prove the entropy bound that can be achieved
                using a sufficiently large extension. We can first define that: $H(S)\leq L_{avg} < H(S)+1$ We take the
                extension for any code to the $n^{th}$ term, resulting in: $H(S^{n})\leq L_{n} < H(S^{n})+1$ We sequence
                $n$ symbols from $S$, taken from the extended source $S^{n}$, which now has $q^{n}$ symbols. Lastly, we
                divide our previous equation by $n$, giving us: $H(S)\leq \frac{L_{n}}{n} < H(S)+\frac{1}{n}$ What this
                shows, is that by choosing $n$ sufficently large, the average code length can come arbitrarily closer to
                source entropy, $H(S)$. This being said, there are practical limitations to choosing a large extension
                on our source symbols. As the extensions on our souce become larger, the returns on efficecny deminish
                and have a higher cost on delay, stroage, and computation. System Entropies The system entropies
                represent the input and output entropies. The source entropy can be defined as:
                $H_{r}(A)=\sum_{i=1}^{q}p(a_{i})\log_{r}\left(\frac{1}{p(a_{i})}\right)$ Recall that properties of the
                entropy function are: $H_{r}(A)\geq 0$ $H_{r}(A)\leq \log_{r}q$, where $q$ is the number of input
                symbols $H_{r}(A)=\log_{r}q$ when all source symbols are equally likely The output entropy can be
                defined as: $H_{r}(B)=\sum_{j=1}^{s}p(b_{j})\log_{r}\left[\frac{1}{p(b_{j})}\right]$ Conditional Entropy
                To continue. Mutual Information To continue. Channel Capacity To continue. Hamming Code and Error
                Correcting We will look at the encoding of a binary source for transmission on a noisy channel and then
                build a Hamming Code to transmit one bit of information. The Hamman Coding Model illustrated below shows
                two valid codewords spheres in $3$-dimensional Hamming Space: Image Source:
                https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226
                Mathematical Tools Hamming Distance Given two $N$-bit binary symbols, the Hamming distance between them
                is the number of symbols that differ. For example, if the symbols are $0110$ and $1010$ is $2$. Note,
                that binary numbers can be represented as points in an $N$-dimensional Hamming Space. N-Dimensional
                Hamming Space The volume of an $N$-dimensional Hamming space is based on the number of binary numbers or
                points in the $N$-bit binary number and is defined as $2^{N}$. The hamming space is represented with a
                set of $N$ coordinate axes, where each axis contains the discrete values $\left\{0,1\right\}$. The
                figure below is an example of $N=4$ dimensional space cube representing a $4$-dimensional Hamming space.
                Hamming space gets harder to visualize when the number of dimensions gets larger, but we must trust the
                math. Encoding Source Symbols Let's use a $3$-dimensional Hamming space for the encoding of a binary
                source on a noisy channel. Image Source: Methods of Mathematics Applied to Calculus, Probability, and
                Statistics by Richard Wesley Hamming Let the source alphabet be $\left\{s_{0}, s_{1}\right\}$ with
                triple repetition coding $s_{0}=000, s_{1}=111$. Encoding a binary source for transmission over a noisy
                channel can cause bit error, so the decoder needs to try to catch and fix this. To do, the decoder will
                map the recievecd codeword to the closest legal coderword, which is smallest Hamming distance. By
                looking at our $3$-dimensional Hamming space, we can see that the encoder needs to encode the source
                symbols by a sequence of channel symbols with a "good" distance. This allows for the decoder to have an
                eaiser time catching and correcting bit errors. However, the decoder can still be foolder by multiple
                bit errors. Hamming Distance of a Code The Hamming distance of a code can be defined as the minimum
                pairwise Hamming distance between all of the codeword pairs and can allows us to measure the error
                correction capability of a code. The Hamming distance for the example $3$-dimensional Hamming space
                below is $3$. There exists a subset of points in $N$-dimensional Hamming space that is the set of legal
                codewords. To optimize error correction capability of a code, we need to spread out the set of legal
                codewords in $N$-dimensional space such that they all have the maximum possible mututal Hamming
                distances. Hamming Code Hamming Codes Hamming codes are a family of perfect single error correcting
                codes that can correct all $1$-bit errors or detect all $2$-bit errors and are described by $(n,m)$.
                Where $n$ is the total number of bits in a codeword and $n=2^{k}-1$. $m$ is the number of data bits in a
                codeword and then lastly $k$ is the number of parity or check bits in a codeword and $k=n-m$, so
                $n=m+k$. Codeword Examples n (# of bits) m (# of data bits) k (# of parity bits) 7 4 3 15 11 3 31 26 5
                $(7,4)$ Hamming Code The $(7,4)$ Hamming code, as with the rest of the Hamming codes in the family, are
                perfect single error correcting codes and can correct all $1$-bit errors or detect all $2$-bit errors.
                The $(7,4)$ Hamming code has $7$ total number of bits in the codeword and $4$ number of data bits in the
                codeword. Thus, given that $k=n-m$, the number of parity bits is $3=7-4$. The $(7,4)$ Hamming code has
                $2^{m}=2^{4}=16$ codewords with a Hamming distance found to be $3$. Given a $7$-dimensional Hamming
                space there exists $2^{N}=2^{7}=128$ points. Every codeword has a subset of neighboring codewords
                seperated by $1$ vertex that we will call a sphere with radius of $r=1$. Each sphere contains
                $\binom{N}{r}+\binom{N}{0}$ which is $\binom{7}{1}+\binom{7}{0}=8$ points. Given the $128$ points in
                $7$-dimensional Hamming space and that there are $16$ spheres and $8$ points per sphere ($6*8=128$) in
                the $(7,4)$ Hammaning code, all points are covered. Thus, the $(7,4)$ Hamming code is a perfect code.
                The sphere has with $r=1$ has a point called the center and $\binom{n}{1}=n$ points on the surface,
                thus, the volume of the sphere or total points in the sphere is: $center\;+\binom{n}{1}=1+n$ Example of
                a sphere with $r=2$: $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$ volume or total points in the sphere where
                the center is a codeword. Hamming Decoding To continue. Error Correcting Error Correcting Capability The
                error correcting capability $t$ of a given code with some Hamming distance $d$ is defined as:
                $t=\left(\frac{d-1}{2}\right)$ Meaning, that for the Hamming Code to correct $t$ errors, the sphere with
                $r=t$ of some codeword in Hamming Space, must not intersect with any other sphere. So, some Hamming Code
                can correct $t$ errors when $d>t$, but fails otherwise. For example, looking at the $(7,4)$ Hamming
                Code, where $t=1$ and $d=3$, $d$ can be visualized as the Hamming distance between two codewords. Thus,
                the $(7,4)$ Hamming Code can correct $t$ errors when $t < 3$, but fails otherwise. Sphere Packing Bound
                    The important part of sphere packing in Hamming Space, is that valid codewords speheres cannot
                    intersect. To satisfy this requirment, for any code that can correct $t$ errors, $2^{m}$ spheres
                    each with radius $t$ is required. However, all spheres must fit in the $2^{n}$ volume of Hamming
                    space. So, we are limited by the following: $\frac{v_{HS}}{v_{S_{i}}}\geq max(S)$ Where $v_{HS}$ is
                    the total volume of the Hamming Space $HS$, $v_{S_{i}}$ is the volume of the given sphere $S_{i}$
                    from the set of spheres $S$, and $max(S)$ is the maximum number of spheres. Using this, we can now
                    figure out the minimum number if check bits or parity bits $k$ that are acheieve any error
                    correction capability. Recall, that $k=n-m$ where $n$ is the total number of bits in a codeword and
                    $m$ is the number of data bits in a codeword. Where $n$ and $m$ are described by $(n,m)$ of the
                    Hamming Code, such as $(7,4)$ Hamming Code. Hamming Code Example To continue. Compression Algorithms
                    Huffman Coding Algorithm To continue. Lempel-Ziv Compression To continue. Arithmetic Coding To
                    continue. Glossary Cardinality The cardinality of a set represents the number of elements in the
                    set. Uniquely Decodable Codes that are uniquely decodable, instantaneous codes, or prefix-free codes
                    are called such because the decoder that scans the code instantly recognizes the end of the
                    codeword. Information Theory Quantities of Information Shannon Information Three properties were
                    required by Shannon: $I(p) \geq 0$, i.e. information is a real non-negative measure.
                    $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$ for independent events. $I(p)$ is a continous function of $p$.
                    The mathematical function that satisfies these requirements is: $I(p)=k\;log(p)$ In the equation,
                    the value of $k$ is arbitrary, so we choose $k=-1$ to make the math more convient.
                    $I(p)=-\log(p)=\log(\frac{1}{p})$ The base of the logarithm is representative of the units of
                    measure for the given information, but can also be chosen arbitarily. However, today units of
                    information are exclusively use base 2 logarithms, in units of bits.
                    $I(p)=-\log_{2}(p)=\log_{2}(\frac{1}{p})$ bits of information. Shannon Entropy and Average Code
                    Length Let $S=\left\{s_{1}, s_{2},\ldots s_{n}\right\}$ some information source with $n$ symbols.
                    Let $P=\left\{p_{1}, p_{2}, \ldots p_{n}\right\}$ be the corresponding probability distribution.
                    Entropy of the source is the defined as: $H(S)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$
                    $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ Another important number is the average code length
                    $L_{avg}$, defined as: $L_{avg}=\sum_{i=1}^{n}p_{i}
                    \left(\lceil\log_{2}\frac{1}{p_{i}}\rceil+1\right)$ Where, $c_{i}$ is some code string of the code
                    set $C$ and $|\;|$ is the cardinality of $c_{i}$. Note, the entropy of the source is a lower bound
                    on the average code length that can be achieved, given by $H(S) \leq L_{avg}$. Meaning, that the
                    source entropy can be reach the compression limit, but can never exceed it. Kraft Inequality
                    $K=\sum_{i=1}^{q}\frac{1}{r^{l^{i}}}\leq 1$ Where: $K$ is the Kraft sum $q$ is the number of source
                    symbols $r$ is the radix of the channel alphabet $l_{i}$ is the lengths of the coded symbols. Note
                    that the craft inequality must be satisfied for codes that called uniquely decodable, instantaneous
                    codes, or prefix-free codes. Proof of Achieving Entropy Bound We can prove the entropy bound that
                    can be achieved using a sufficiently large extension. We can first define that: $H(S)\leq L_{avg} <
                    H(S)+1$ We take the extension for any code to the $n^{th}$ term, resulting in: $H(S^{n})\leq L_{n} <
                    H(S^{n})+1$ We sequence $n$ symbols from $S$, taken from the extended source $S^{n}$, which now has
                    $q^{n}$ symbols. Lastly, we divide our previous equation by $n$, giving us: $H(S)\leq
                    \frac{L_{n}}{n} < H(S)+\frac{1}{n}$ What this shows, is that by choosing $n$ sufficently large, the
                    average code length can come arbitrarily closer to source entropy, $H(S)$. This being said, there
                    are practical limitations to choosing a large extension on our source symbols. As the extensions on
                    our souce become larger, the returns on efficecny deminish and have a higher cost on delay, stroage,
                    and computation. System Entropies The system entropies represent the input and output entropies. The
                    source entropy can be defined as:
                    $H_{r}(A)=\sum_{i=1}^{q}p(a_{i})\log_{r}\left(\frac{1}{p(a_{i})}\right)$ Recall that properties of
                    the entropy function are: $H_{r}(A)\geq 0$ $H_{r}(A)\leq \log_{r}q$, where $q$ is the number of
                    input symbols $H_{r}(A)=\log_{r}q$ when all source symbols are equally likely The output entropy can
                    be defined as: $H_{r}(B)=\sum_{j=1}^{s}p(b_{j})\log_{r}\left[\frac{1}{p(b_{j})}\right]$ Conditional
                    Entropy To continue. Mutual Information To continue. Channel Capacity To continue. Hamming Code and
                    Error Correcting We will look at the encoding of a binary source for transmission on a noisy channel
                    and then build a Hamming Code to transmit one bit of information. The Hamman Coding Model
                    illustrated below shows two valid codewords spheres in $3$-dimensional Hamming Space: Image Source:
                    https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226
                    Mathematical Tools Hamming Distance Given two $N$-bit binary symbols, the Hamming distance between
                    them is the number of symbols that differ. For example, if the symbols are $0110$ and $1010$ is $2$.
                    Note, that binary numbers can be represented as points in an $N$-dimensional Hamming Space.
                    N-Dimensional Hamming Space The volume of an $N$-dimensional Hamming space is based on the number of
                    binary numbers or points in the $N$-bit binary number and is defined as $2^{N}$. The hamming space
                    is represented with a set of $N$ coordinate axes, where each axis contains the discrete values
                    $\left\{0,1\right\}$. The figure below is an example of $N=4$ dimensional space cube representing a
                    $4$-dimensional Hamming space. Hamming space gets harder to visualize when the number of dimensions
                    gets larger, but we must trust the math. Encoding Source Symbols Let's use a $3$-dimensional Hamming
                    space for the encoding of a binary source on a noisy channel. Image Source: Methods of Mathematics
                    Applied to Calculus, Probability, and Statistics by Richard Wesley Hamming Let the source alphabet
                    be $\left\{s_{0}, s_{1}\right\}$ with triple repetition coding $s_{0}=000, s_{1}=111$. Encoding a
                    binary source for transmission over a noisy channel can cause bit error, so the decoder needs to try
                    to catch and fix this. To do, the decoder will map the recievecd codeword to the closest legal
                    coderword, which is smallest Hamming distance. By looking at our $3$-dimensional Hamming space, we
                    can see that the encoder needs to encode the source symbols by a sequence of channel symbols with
                    a "good" distance. This allows for the decoder to have an eaiser time catching and correcting bit
                    errors. However, the decoder can still be foolder by multiple bit errors. Hamming Distance of a Code
                    The Hamming distance of a code can be defined as the minimum pairwise Hamming distance between all
                    of the codeword pairs and can allows us to measure the error correction capability of a code. The
                    Hamming distance for the example $3$-dimensional Hamming space below is $3$. There exists a subset
                    of points in $N$-dimensional Hamming space that is the set of legal codewords. To optimize error
                    correction capability of a code, we need to spread out the set of legal codewords in $N$-dimensional
                    space such that they all have the maximum possible mututal Hamming distances. Hamming Code Hamming
                    Codes Hamming codes are a family of perfect single error correcting codes that can correct all
                    $1$-bit errors or detect all $2$-bit errors and are described by $(n,m)$. Where $n$ is the total
                    number of bits in a codeword and $n=2^{k}-1$. $m$ is the number of data bits in a codeword and then
                    lastly $k$ is the number of parity or check bits in a codeword and $k=n-m$, so $n=m+k$. Codeword
                    Examples n (# of bits) m (# of data bits) k (# of parity bits) 7 4 3 15 11 3 31 26 5 $(7,4)$ Hamming
                    Code The $(7,4)$ Hamming code, as with the rest of the Hamming codes in the family, are perfect
                    single error correcting codes and can correct all $1$-bit errors or detect all $2$-bit errors. The
                    $(7,4)$ Hamming code has $7$ total number of bits in the codeword and $4$ number of data bits in the
                    codeword. Thus, given that $k=n-m$, the number of parity bits is $3=7-4$. The $(7,4)$ Hamming code
                    has $2^{m}=2^{4}=16$ codewords with a Hamming distance found to be $3$. Given a $7$-dimensional
                    Hamming space there exists $2^{N}=2^{7}=128$ points. Every codeword has a subset of neighboring
                    codewords seperated by $1$ vertex that we will call a sphere with radius of $r=1$. Each sphere
                    contains $\binom{N}{r}+\binom{N}{0}$ which is $\binom{7}{1}+\binom{7}{0}=8$ points. Given the $128$
                    points in $7$-dimensional Hamming space and that there are $16$ spheres and $8$ points per sphere
                    ($6*8=128$) in the $(7,4)$ Hammaning code, all points are covered. Thus, the $(7,4)$ Hamming code is
                    a perfect code. The sphere has with $r=1$ has a point called the center and $\binom{n}{1}=n$ points
                    on the surface, thus, the volume of the sphere or total points in the sphere is:
                    $center\;+\binom{n}{1}=1+n$ Example of a sphere with $r=2$: $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$
                    volume or total points in the sphere where the center is a codeword. Hamming Decoding To continue.
                    Error Correcting Error Correcting Capability The error correcting capability $t$ of a given code
                    with some Hamming distance $d$ is defined as: $t=\left(\frac{d-1}{2}\right)$ Meaning, that for the
                    Hamming Code to correct $t$ errors, the sphere with $r=t$ of some codeword in Hamming Space, must
                    not intersect with any other sphere. So, some Hamming Code can correct $t$ errors when $d>t$, but
                    fails otherwise. For example, looking at the $(7,4)$ Hamming Code, where $t=1$ and $d=3$, $d$ can be
                    visualized as the Hamming distance between two codewords. Thus, the $(7,4)$ Hamming Code can correct
                    $t$ errors when $t < 3$, but fails otherwise. Sphere Packing Bound The important part of sphere
                        packing in Hamming Space, is that valid codewords speheres cannot intersect. To satisfy this
                        requirment, for any code that can correct $t$ errors, $2^{m}$ spheres each with radius $t$ is
                        required. However, all spheres must fit in the $2^{n}$ volume of Hamming space. So, we are
                        limited by the following: $\frac{v_{HS}}{v_{S_{i}}}\geq max(S)$ Where $v_{HS}$ is the total
                        volume of the Hamming Space $HS$, $v_{S_{i}}$ is the volume of the given sphere $S_{i}$ from the
                        set of spheres $S$, and $max(S)$ is the maximum number of spheres. Using this, we can now figure
                        out the minimum number if check bits or parity bits $k$ that are acheieve any error correction
                        capability. Recall, that $k=n-m$ where $n$ is the total number of bits in a codeword and $m$ is
                        the number of data bits in a codeword. Where $n$ and $m$ are described by $(n,m)$ of the Hamming
                        Code, such as $(7,4)$ Hamming Code. Hamming Code Example To continue. Compression Algorithms
                        Huffman Coding Algorithm To continue. Lempel-Ziv Compression To continue. Arithmetic Coding To
                        continue. Glossary Cardinality The cardinality of a set represents the number of elements in the
                        set. Uniquely Decodable Codes that are uniquely decodable, instantaneous codes, or prefix-free
                        codes are called such because the decoder that scans the code instantly recognizes the end of
                        the codeword. Information TheoryQuantities of Information Shannon Information Three properties
                        were required by Shannon: $I(p) \geq 0$, i.e. information is a real non-negative measure.
                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$ for independent events. $I(p)$ is a continous function of
                        $p$. The mathematical function that satisfies these requirements is: $I(p)=k\;log(p)$ In the
                        equation, the value of $k$ is arbitrary, so we choose $k=-1$ to make the math more convient.
                        $I(p)=-\log(p)=\log(\frac{1}{p})$ The base of the logarithm is representative of the units of
                        measure for the given information, but can also be chosen arbitarily. However, today units of
                        information are exclusively use base 2 logarithms, in units of bits.
                        $I(p)=-\log_{2}(p)=\log_{2}(\frac{1}{p})$ bits of information. Shannon Entropy and Average Code
                        Length Let $S=\left\{s_{1}, s_{2},\ldots s_{n}\right\}$ some information source with $n$
                        symbols. Let $P=\left\{p_{1}, p_{2}, \ldots p_{n}\right\}$ be the corresponding probability
                        distribution. Entropy of the source is the defined as:
                        $H(S)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$ $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ Another
                        important number is the average code length $L_{avg}$, defined as: $L_{avg}=\sum_{i=1}^{n}p_{i}
                        \left(\lceil\log_{2}\frac{1}{p_{i}}\rceil+1\right)$ Where, $c_{i}$ is some code string of the
                        code set $C$ and $|\;|$ is the cardinality of $c_{i}$. Note, the entropy of the source is a
                        lower bound on the average code length that can be achieved, given by $H(S) \leq L_{avg}$.
                        Meaning, that the source entropy can be reach the compression limit, but can never exceed it.
                        Kraft Inequality $K=\sum_{i=1}^{q}\frac{1}{r^{l^{i}}}\leq 1$ Where: $K$ is the Kraft sum $q$ is
                        the number of source symbols $r$ is the radix of the channel alphabet $l_{i}$ is the lengths of
                        the coded symbols. Note that the craft inequality must be satisfied for codes that called
                        uniquely decodable, instantaneous codes, or prefix-free codes. Proof of Achieving Entropy Bound
                        We can prove the entropy bound that can be achieved using a sufficiently large extension. We can
                        first define that: $H(S)\leq L_{avg} < H(S)+1$ We take the extension for any code to the
                        $n^{th}$ term, resulting in: $H(S^{n})\leq L_{n} < H(S^{n})+1$ We sequence $n$ symbols from $S$,
                        taken from the extended source $S^{n}$, which now has $q^{n}$ symbols. Lastly, we divide our
                        previous equation by $n$, giving us: $H(S)\leq \frac{L_{n}}{n} < H(S)+\frac{1}{n}$ What this
                        shows, is that by choosing $n$ sufficently large, the average code length can come arbitrarily
                        closer to source entropy, $H(S)$. This being said, there are practical limitations to choosing a
                        large extension on our source symbols. As the extensions on our souce become larger, the returns
                        on efficecny deminish and have a higher cost on delay, stroage, and computation. System
                        Entropies The system entropies represent the input and output entropies. The source entropy can
                        be defined as: $H_{r}(A)=\sum_{i=1}^{q}p(a_{i})\log_{r}\left(\frac{1}{p(a_{i})}\right)$ Recall
                        that properties of the entropy function are: $H_{r}(A)\geq 0$ $H_{r}(A)\leq \log_{r}q$, where
                        $q$ is the number of input symbols $H_{r}(A)=\log_{r}q$ when all source symbols are equally
                        likely The output entropy can be defined as:
                        $H_{r}(B)=\sum_{j=1}^{s}p(b_{j})\log_{r}\left[\frac{1}{p(b_{j})}\right]$ Conditional Entropy To
                        continue. Mutual Information To continue. Channel Capacity To continue. Quantities of
                        InformationShannon Information Three properties were required by Shannon: $I(p) \geq 0$, i.e.
                        information is a real non-negative measure. $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$ for independent
                        events. $I(p)$ is a continous function of $p$. The mathematical function that satisfies these
                        requirements is: $I(p)=k\;log(p)$ In the equation, the value of $k$ is arbitrary, so we choose
                        $k=-1$ to make the math more convient. $I(p)=-\log(p)=\log(\frac{1}{p})$ The base of the
                        logarithm is representative of the units of measure for the given information, but can also be
                        chosen arbitarily. However, today units of information are exclusively use base 2 logarithms, in
                        units of bits. $I(p)=-\log_{2}(p)=\log_{2}(\frac{1}{p})$ bits of information. Shannon
                        InformationThree properties were required by Shannon: $I(p) \geq 0$, i.e. information is a real
                        non-negative measure. $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$ for independent events. $I(p)$ is a
                        continous function of $p$. $I(p) \geq 0$, i.e. information is a real non-negative
                        measure.$I(p_{1},p_{2})=I(p_{1})+I(p_{2})$ for independent events.$I(p)$ is a continous function
                        of $p$.The mathematical function that satisfies these requirements is: $I(p)=k\;log(p)$ In the
                        equation, the value of $k$ is arbitrary, so we choose $k=-1$ to make the math more convient.
                        $I(p)=-\log(p)=\log(\frac{1}{p})$ The base of the logarithm is representative of the units of
                        measure for the given information, but can also be chosen arbitarily. However, today units of
                        information are exclusively use base 2 logarithms, in units of bits.
                        $I(p)=-\log_{2}(p)=\log_{2}(\frac{1}{p})$ bits of information. Shannon Entropy and Average Code
                        Length Let $S=\left\{s_{1}, s_{2},\ldots s_{n}\right\}$ some information source with $n$
                        symbols. Let $P=\left\{p_{1}, p_{2}, \ldots p_{n}\right\}$ be the corresponding probability
                        distribution. Entropy of the source is the defined as:
                        $H(S)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$ $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ Another
                        important number is the average code length $L_{avg}$, defined as: $L_{avg}=\sum_{i=1}^{n}p_{i}
                        \left(\lceil\log_{2}\frac{1}{p_{i}}\rceil+1\right)$ Where, $c_{i}$ is some code string of the
                        code set $C$ and $|\;|$ is the cardinality of $c_{i}$. Note, the entropy of the source is a
                        lower bound on the average code length that can be achieved, given by $H(S) \leq L_{avg}$.
                        Meaning, that the source entropy can be reach the compression limit, but can never exceed it.
                        Shannon Entropy and Average Code LengthLet $S=\left\{s_{1}, s_{2},\ldots s_{n}\right\}$ some
                        information source with $n$ symbols. Let $P=\left\{p_{1}, p_{2}, \ldots p_{n}\right\}$ be the
                        corresponding probability distribution. Entropy of the source is the defined as:
                        $H(S)=-\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$ $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ Another
                        important number is the average code length $L_{avg}$, defined as: $L_{avg}=\sum_{i=1}^{n}p_{i}
                        \left(\lceil\log_{2}\frac{1}{p_{i}}\rceil+1\right)$ Where, $c_{i}$ is some code string of the
                        code set $C$ and $|\;|$ is the cardinality of $c_{i}$. cardinality Note, the entropy of the
                        source is a lower bound on the average code length that can be achieved, given by $H(S) \leq
                        L_{avg}$. Meaning, that the source entropy can be reach the compression limit, but can never
                        exceed it. Kraft Inequality $K=\sum_{i=1}^{q}\frac{1}{r^{l^{i}}}\leq 1$ Where: $K$ is the Kraft
                        sum $q$ is the number of source symbols $r$ is the radix of the channel alphabet $l_{i}$ is the
                        lengths of the coded symbols. Note that the craft inequality must be satisfied for codes that
                        called uniquely decodable, instantaneous codes, or prefix-free codes. Kraft
                        Inequality$K=\sum_{i=1}^{q}\frac{1}{r^{l^{i}}}\leq 1$ Where: $K$ is the Kraft sum $q$ is the
                        number of source symbols $r$ is the radix of the channel alphabet $l_{i}$ is the lengths of the
                        coded symbols. $K$ is the Kraft sum $q$ is the number of source symbols $r$ is the radix of the
                        channel alphabet $l_{i}$ is the lengths of the coded symbols. $K$ is the Kraft sum$q$ is the
                        number of source symbols$r$ is the radix of the channel alphabet$l_{i}$ is the lengths of the
                        coded symbols.Note that the craft inequality must be satisfied for codes that called uniquely
                        decodable, instantaneous codes, or prefix-free codes. uniquely decodableProof of Achieving
                        Entropy Bound We can prove the entropy bound that can be achieved using a sufficiently large
                        extension. We can first define that: $H(S)\leq L_{avg} < H(S)+1$ We take the extension for any
                        code to the $n^{th}$ term, resulting in: $H(S^{n})\leq L_{n} < H(S^{n})+1$ We sequence $n$
                        symbols from $S$, taken from the extended source $S^{n}$, which now has $q^{n}$ symbols. Lastly,
                        we divide our previous equation by $n$, giving us: $H(S)\leq \frac{L_{n}}{n} < H(S)+\frac{1}{n}$
                        What this shows, is that by choosing $n$ sufficently large, the average code length can come
                        arbitrarily closer to source entropy, $H(S)$. This being said, there are practical limitations
                        to choosing a large extension on our source symbols. As the extensions on our souce become
                        larger, the returns on efficecny deminish and have a higher cost on delay, stroage, and
                        computation. Proof of Achieving Entropy Bound We can prove the entropy bound that can be
                        achieved using a sufficiently large extension. We can first define that: $H(S)\leq L_{avg} <
                        H(S)+1$ We take the extension for any code to the $n^{th}$ term, resulting in: $H(S^{n})\leq
                        L_{n} < H(S^{n})+1$ We sequence $n$ symbols from $S$, taken from the extended source $S^{n}$,
                        which now has $q^{n}$ symbols. Lastly, we divide our previous equation by $n$, giving us:
                        $H(S)\leq \frac{L_{n}}{n} < H(S)+\frac{1}{n}$ What this shows, is that by choosing $n$
                        sufficently large, the average code length can come arbitrarily closer to source entropy,
                        $H(S)$. This being said, there are practical limitations to choosing a large extension on our
                        source symbols. As the extensions on our souce become larger, the returns on efficecny deminish
                        and have a higher cost on delay, stroage, and computation. System Entropies The system entropies
                        represent the input and output entropies. The source entropy can be defined as:
                        $H_{r}(A)=\sum_{i=1}^{q}p(a_{i})\log_{r}\left(\frac{1}{p(a_{i})}\right)$ Recall that properties
                        of the entropy function are: $H_{r}(A)\geq 0$ $H_{r}(A)\leq \log_{r}q$, where $q$ is the number
                        of input symbols $H_{r}(A)=\log_{r}q$ when all source symbols are equally likely The output
                        entropy can be defined as:
                        $H_{r}(B)=\sum_{j=1}^{s}p(b_{j})\log_{r}\left[\frac{1}{p(b_{j})}\right]$ System EntropiesThe
                        system entropies represent the input and output entropies. The source entropy can be defined as:
                        $H_{r}(A)=\sum_{i=1}^{q}p(a_{i})\log_{r}\left(\frac{1}{p(a_{i})}\right)$ Recall that properties
                        of the entropy function are: $H_{r}(A)\geq 0$ $H_{r}(A)\leq \log_{r}q$, where $q$ is the number
                        of input symbols $H_{r}(A)=\log_{r}q$ when all source symbols are equally likely $H_{r}(A)\geq
                        0$ $H_{r}(A)\leq \log_{r}q$, where $q$ is the number of input symbols $H_{r}(A)=\log_{r}q$ when
                        all source symbols are equally likely $H_{r}(A)\geq 0$$H_{r}(A)\leq \log_{r}q$, where $q$ is the
                        number of input symbols$H_{r}(A)=\log_{r}q$ when all source symbols are equally likelyThe output
                        entropy can be defined as:
                        $H_{r}(B)=\sum_{j=1}^{s}p(b_{j})\log_{r}\left[\frac{1}{p(b_{j})}\right]$ Conditional Entropy To
                        continue. Conditional EntropyTo continue. Mutual Information To continue. Mutual InformationTo
                        continue. Channel Capacity To continue. Channel CapacityTo continue. Hamming Code and Error
                        Correcting We will look at the encoding of a binary source for transmission on a noisy channel
                        and then build a Hamming Code to transmit one bit of information. The Hamman Coding Model
                        illustrated below shows two valid codewords spheres in $3$-dimensional Hamming Space: Image
                        Source:
                        https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226
                        Mathematical Tools Hamming Distance Given two $N$-bit binary symbols, the Hamming distance
                        between them is the number of symbols that differ. For example, if the symbols are $0110$ and
                        $1010$ is $2$. Note, that binary numbers can be represented as points in an $N$-dimensional
                        Hamming Space. N-Dimensional Hamming Space The volume of an $N$-dimensional Hamming space is
                        based on the number of binary numbers or points in the $N$-bit binary number and is defined as
                        $2^{N}$. The hamming space is represented with a set of $N$ coordinate axes, where each axis
                        contains the discrete values $\left\{0,1\right\}$. The figure below is an example of $N=4$
                        dimensional space cube representing a $4$-dimensional Hamming space. Hamming space gets harder
                        to visualize when the number of dimensions gets larger, but we must trust the math. Encoding
                        Source Symbols Let's use a $3$-dimensional Hamming space for the encoding of a binary source on
                        a noisy channel. Image Source: Methods of Mathematics Applied to Calculus, Probability, and
                        Statistics by Richard Wesley Hamming Let the source alphabet be $\left\{s_{0}, s_{1}\right\}$
                        with triple repetition coding $s_{0}=000, s_{1}=111$. Encoding a binary source for transmission
                        over a noisy channel can cause bit error, so the decoder needs to try to catch and fix this. To
                        do, the decoder will map the recievecd codeword to the closest legal coderword, which is
                        smallest Hamming distance. By looking at our $3$-dimensional Hamming space, we can see that the
                        encoder needs to encode the source symbols by a sequence of channel symbols with a "good"
                        distance. This allows for the decoder to have an eaiser time catching and correcting bit errors.
                        However, the decoder can still be foolder by multiple bit errors. Hamming Distance of a Code The
                        Hamming distance of a code can be defined as the minimum pairwise Hamming distance between all
                        of the codeword pairs and can allows us to measure the error correction capability of a code.
                        The Hamming distance for the example $3$-dimensional Hamming space below is $3$. There exists a
                        subset of points in $N$-dimensional Hamming space that is the set of legal codewords. To
                        optimize error correction capability of a code, we need to spread out the set of legal codewords
                        in $N$-dimensional space such that they all have the maximum possible mututal Hamming distances.
                        Hamming Code Hamming Codes Hamming codes are a family of perfect single error correcting codes
                        that can correct all $1$-bit errors or detect all $2$-bit errors and are described by $(n,m)$.
                        Where $n$ is the total number of bits in a codeword and $n=2^{k}-1$. $m$ is the number of data
                        bits in a codeword and then lastly $k$ is the number of parity or check bits in a codeword and
                        $k=n-m$, so $n=m+k$. Codeword Examples n (# of bits) m (# of data bits) k (# of parity bits) 7 4
                        3 15 11 3 31 26 5 $(7,4)$ Hamming Code The $(7,4)$ Hamming code, as with the rest of the Hamming
                        codes in the family, are perfect single error correcting codes and can correct all $1$-bit
                        errors or detect all $2$-bit errors. The $(7,4)$ Hamming code has $7$ total number of bits in
                        the codeword and $4$ number of data bits in the codeword. Thus, given that $k=n-m$, the number
                        of parity bits is $3=7-4$. The $(7,4)$ Hamming code has $2^{m}=2^{4}=16$ codewords with a
                        Hamming distance found to be $3$. Given a $7$-dimensional Hamming space there exists
                        $2^{N}=2^{7}=128$ points. Every codeword has a subset of neighboring codewords seperated by $1$
                        vertex that we will call a sphere with radius of $r=1$. Each sphere contains
                        $\binom{N}{r}+\binom{N}{0}$ which is $\binom{7}{1}+\binom{7}{0}=8$ points. Given the $128$
                        points in $7$-dimensional Hamming space and that there are $16$ spheres and $8$ points per
                        sphere ($6*8=128$) in the $(7,4)$ Hammaning code, all points are covered. Thus, the $(7,4)$
                        Hamming code is a perfect code. The sphere has with $r=1$ has a point called the center and
                        $\binom{n}{1}=n$ points on the surface, thus, the volume of the sphere or total points in the
                        sphere is: $center\;+\binom{n}{1}=1+n$ Example of a sphere with $r=2$:
                        $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$ volume or total points in the sphere where the center
                        is a codeword. Hamming Decoding To continue. Error Correcting Error Correcting Capability The
                        error correcting capability $t$ of a given code with some Hamming distance $d$ is defined as:
                        $t=\left(\frac{d-1}{2}\right)$ Meaning, that for the Hamming Code to correct $t$ errors, the
                        sphere with $r=t$ of some codeword in Hamming Space, must not intersect with any other sphere.
                        So, some Hamming Code can correct $t$ errors when $d>t$, but fails otherwise. For example,
                        looking at the $(7,4)$ Hamming Code, where $t=1$ and $d=3$, $d$ can be visualized as the Hamming
                        distance between two codewords. Thus, the $(7,4)$ Hamming Code can correct $t$ errors when $t <
                            3$, but fails otherwise. Sphere Packing Bound The important part of sphere packing in
                            Hamming Space, is that valid codewords speheres cannot intersect. To satisfy this
                            requirment, for any code that can correct $t$ errors, $2^{m}$ spheres each with radius $t$
                            is required. However, all spheres must fit in the $2^{n}$ volume of Hamming space. So, we
                            are limited by the following: $\frac{v_{HS}}{v_{S_{i}}}\geq max(S)$ Where $v_{HS}$ is the
                            total volume of the Hamming Space $HS$, $v_{S_{i}}$ is the volume of the given sphere
                            $S_{i}$ from the set of spheres $S$, and $max(S)$ is the maximum number of spheres. Using
                            this, we can now figure out the minimum number if check bits or parity bits $k$ that are
                            acheieve any error correction capability. Recall, that $k=n-m$ where $n$ is the total number
                            of bits in a codeword and $m$ is the number of data bits in a codeword. Where $n$ and $m$
                            are described by $(n,m)$ of the Hamming Code, such as $(7,4)$ Hamming Code. Hamming Code
                            Example To continue. Hamming Code and Error CorrectingWe will look at the encoding of a
                            binary source for transmission on a noisy channel and then build a Hamming Code to transmit
                            one bit of information. The Hamman Coding Model illustrated below shows two valid codewords
                            spheres in $3$-dimensional Hamming Space: Image Source:
                            https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226
                            https://www.researchgate.net/figure/a-A-Hamming-code-to-transmit-one-bit-of-information-k1-n3-Consider-a-hypersphere_fig3_5588226Mathematical
                            Tools Hamming Distance Given two $N$-bit binary symbols, the Hamming distance between them
                            is the number of symbols that differ. For example, if the symbols are $0110$ and $1010$ is
                            $2$. Note, that binary numbers can be represented as points in an $N$-dimensional Hamming
                            Space. N-Dimensional Hamming Space The volume of an $N$-dimensional Hamming space is based
                            on the number of binary numbers or points in the $N$-bit binary number and is defined as
                            $2^{N}$. The hamming space is represented with a set of $N$ coordinate axes, where each axis
                            contains the discrete values $\left\{0,1\right\}$. The figure below is an example of $N=4$
                            dimensional space cube representing a $4$-dimensional Hamming space. Hamming space gets
                            harder to visualize when the number of dimensions gets larger, but we must trust the math.
                            Encoding Source Symbols Let's use a $3$-dimensional Hamming space for the encoding of a
                            binary source on a noisy channel. Image Source: Methods of Mathematics Applied to Calculus,
                            Probability, and Statistics by Richard Wesley Hamming Let the source alphabet be
                            $\left\{s_{0}, s_{1}\right\}$ with triple repetition coding $s_{0}=000, s_{1}=111$. Encoding
                            a binary source for transmission over a noisy channel can cause bit error, so the decoder
                            needs to try to catch and fix this. To do, the decoder will map the recievecd codeword to
                            the closest legal coderword, which is smallest Hamming distance. By looking at our
                            $3$-dimensional Hamming space, we can see that the encoder needs to encode the source
                            symbols by a sequence of channel symbols with a "good" distance. This allows for the decoder
                            to have an eaiser time catching and correcting bit errors. However, the decoder can still be
                            foolder by multiple bit errors. Hamming Distance of a Code The Hamming distance of a code
                            can be defined as the minimum pairwise Hamming distance between all of the codeword pairs
                            and can allows us to measure the error correction capability of a code. The Hamming distance
                            for the example $3$-dimensional Hamming space below is $3$. There exists a subset of points
                            in $N$-dimensional Hamming space that is the set of legal codewords. To optimize error
                            correction capability of a code, we need to spread out the set of legal codewords in
                            $N$-dimensional space such that they all have the maximum possible mututal Hamming
                            distances. Mathematical ToolsHamming Distance Given two $N$-bit binary symbols, the Hamming
                            distance between them is the number of symbols that differ. For example, if the symbols are
                            $0110$ and $1010$ is $2$. Note, that binary numbers can be represented as points in an
                            $N$-dimensional Hamming Space. N-Dimensional Hamming Space The volume of an $N$-dimensional
                            Hamming space is based on the number of binary numbers or points in the $N$-bit binary
                            number and is defined as $2^{N}$. The hamming space is represented with a set of $N$
                            coordinate axes, where each axis contains the discrete values $\left\{0,1\right\}$. The
                            figure below is an example of $N=4$ dimensional space cube representing a $4$-dimensional
                            Hamming space. Hamming space gets harder to visualize when the number of dimensions gets
                            larger, but we must trust the math. Hamming DistanceGiven two $N$-bit binary symbols, the
                            Hamming distance between them is the number of symbols that differ. For example, if the
                            symbols are $0110$ and $1010$ is $2$. Note, that binary numbers can be represented as points
                            in an $N$-dimensional Hamming Space. N-Dimensional Hamming Space The volume of an
                            $N$-dimensional Hamming space is based on the number of binary numbers or points in the
                            $N$-bit binary number and is defined as $2^{N}$. The hamming space is represented with a set
                            of $N$ coordinate axes, where each axis contains the discrete values $\left\{0,1\right\}$.
                            The figure below is an example of $N=4$ dimensional space cube representing a
                            $4$-dimensional Hamming space. Hamming space gets harder to visualize when the number of
                            dimensions gets larger, but we must trust the math. N-Dimensional Hamming Space The volume
                            of an $N$-dimensional Hamming space is based on the number of binary numbers or points in
                            the $N$-bit binary number and is defined as $2^{N}$. The hamming space is represented with a
                            set of $N$ coordinate axes, where each axis contains the discrete values
                            $\left\{0,1\right\}$. The figure below is an example of $N=4$ dimensional space cube
                            representing a $4$-dimensional Hamming space. Hamming space gets harder to visualize when
                            the number of dimensions gets larger, but we must trust the math. Encoding Source Symbols
                            Let's use a $3$-dimensional Hamming space for the encoding of a binary source on a noisy
                            channel. Image Source: Methods of Mathematics Applied to Calculus, Probability, and
                            Statistics by Richard Wesley Hamming Let the source alphabet be $\left\{s_{0},
                            s_{1}\right\}$ with triple repetition coding $s_{0}=000, s_{1}=111$. Encoding a binary
                            source for transmission over a noisy channel can cause bit error, so the decoder needs to
                            try to catch and fix this. To do, the decoder will map the recievecd codeword to the closest
                            legal coderword, which is smallest Hamming distance. By looking at our $3$-dimensional
                            Hamming space, we can see that the encoder needs to encode the source symbols by a sequence
                            of channel symbols with a "good" distance. This allows for the decoder to have an eaiser
                            time catching and correcting bit errors. However, the decoder can still be foolder by
                            multiple bit errors. Encoding Source SymbolsLet's use a $3$-dimensional Hamming space for
                            the encoding of a binary source on a noisy channel. Image Source: Methods of Mathematics
                            Applied to Calculus, Probability, and Statistics by Richard Wesley Hamming Let the source
                            alphabet be $\left\{s_{0}, s_{1}\right\}$ with triple repetition coding $s_{0}=000,
                            s_{1}=111$. Encoding a binary source for transmission over a noisy channel can cause bit
                            error, so the decoder needs to try to catch and fix this. To do, the decoder will map the
                            recievecd codeword to the closest legal coderword, which is smallest Hamming distance. By
                            looking at our $3$-dimensional Hamming space, we can see that the encoder needs to encode
                            the source symbols by a sequence of channel symbols with a "good" distance. This allows for
                            the decoder to have an eaiser time catching and correcting bit errors. However, the decoder
                            can still be foolder by multiple bit errors. Hamming Distance of a Code The Hamming distance
                            of a code can be defined as the minimum pairwise Hamming distance between all of the
                            codeword pairs and can allows us to measure the error correction capability of a code. The
                            Hamming distance for the example $3$-dimensional Hamming space below is $3$. There exists a
                            subset of points in $N$-dimensional Hamming space that is the set of legal codewords. To
                            optimize error correction capability of a code, we need to spread out the set of legal
                            codewords in $N$-dimensional space such that they all have the maximum possible mututal
                            Hamming distances. Hamming Distance of a CodeThe Hamming distance of a code can be defined
                            as the minimum pairwise Hamming distance between all of the codeword pairs and can allows us
                            to measure the error correction capability of a code. The Hamming distance for the example
                            $3$-dimensional Hamming space below is $3$. There exists a subset of points in
                            $N$-dimensional Hamming space that is the set of legal codewords. To optimize error
                            correction capability of a code, we need to spread out the set of legal codewords in
                            $N$-dimensional space such that they all have the maximum possible mututal Hamming
                            distances. Hamming Code Hamming Codes Hamming codes are a family of perfect single error
                            correcting codes that can correct all $1$-bit errors or detect all $2$-bit errors and are
                            described by $(n,m)$. Where $n$ is the total number of bits in a codeword and $n=2^{k}-1$.
                            $m$ is the number of data bits in a codeword and then lastly $k$ is the number of parity or
                            check bits in a codeword and $k=n-m$, so $n=m+k$. Codeword Examples n (# of bits) m (# of
                            data bits) k (# of parity bits) 7 4 3 15 11 3 31 26 5 $(7,4)$ Hamming Code The $(7,4)$
                            Hamming code, as with the rest of the Hamming codes in the family, are perfect single error
                            correcting codes and can correct all $1$-bit errors or detect all $2$-bit errors. The
                            $(7,4)$ Hamming code has $7$ total number of bits in the codeword and $4$ number of data
                            bits in the codeword. Thus, given that $k=n-m$, the number of parity bits is $3=7-4$. The
                            $(7,4)$ Hamming code has $2^{m}=2^{4}=16$ codewords with a Hamming distance found to be $3$.
                            Given a $7$-dimensional Hamming space there exists $2^{N}=2^{7}=128$ points. Every codeword
                            has a subset of neighboring codewords seperated by $1$ vertex that we will call a sphere
                            with radius of $r=1$. Each sphere contains $\binom{N}{r}+\binom{N}{0}$ which is
                            $\binom{7}{1}+\binom{7}{0}=8$ points. Given the $128$ points in $7$-dimensional Hamming
                            space and that there are $16$ spheres and $8$ points per sphere ($6*8=128$) in the $(7,4)$
                            Hammaning code, all points are covered. Thus, the $(7,4)$ Hamming code is a perfect code.
                            The sphere has with $r=1$ has a point called the center and $\binom{n}{1}=n$ points on the
                            surface, thus, the volume of the sphere or total points in the sphere is:
                            $center\;+\binom{n}{1}=1+n$ Example of a sphere with $r=2$:
                            $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$ volume or total points in the sphere where the
                            center is a codeword. Hamming Decoding To continue. Hamming CodeHamming Codes Hamming codes
                            are a family of perfect single error correcting codes that can correct all $1$-bit errors or
                            detect all $2$-bit errors and are described by $(n,m)$. Where $n$ is the total number of
                            bits in a codeword and $n=2^{k}-1$. $m$ is the number of data bits in a codeword and then
                            lastly $k$ is the number of parity or check bits in a codeword and $k=n-m$, so $n=m+k$.
                            Codeword Examples n (# of bits) m (# of data bits) k (# of parity bits) 7 4 3 15 11 3 31 26
                            5 $(7,4)$ Hamming Code The $(7,4)$ Hamming code, as with the rest of the Hamming codes in
                            the family, are perfect single error correcting codes and can correct all $1$-bit errors or
                            detect all $2$-bit errors. The $(7,4)$ Hamming code has $7$ total number of bits in the
                            codeword and $4$ number of data bits in the codeword. Thus, given that $k=n-m$, the number
                            of parity bits is $3=7-4$. The $(7,4)$ Hamming code has $2^{m}=2^{4}=16$ codewords with a
                            Hamming distance found to be $3$. Given a $7$-dimensional Hamming space there exists
                            $2^{N}=2^{7}=128$ points. Every codeword has a subset of neighboring codewords seperated by
                            $1$ vertex that we will call a sphere with radius of $r=1$. Each sphere contains
                            $\binom{N}{r}+\binom{N}{0}$ which is $\binom{7}{1}+\binom{7}{0}=8$ points. Given the $128$
                            points in $7$-dimensional Hamming space and that there are $16$ spheres and $8$ points per
                            sphere ($6*8=128$) in the $(7,4)$ Hammaning code, all points are covered. Thus, the $(7,4)$
                            Hamming code is a perfect code. The sphere has with $r=1$ has a point called the center and
                            $\binom{n}{1}=n$ points on the surface, thus, the volume of the sphere or total points in
                            the sphere is: $center\;+\binom{n}{1}=1+n$ Example of a sphere with $r=2$:
                            $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$ volume or total points in the sphere where the
                            center is a codeword. Hamming CodesHamming codes are a family of perfect single error
                            correcting codes that can correct all $1$-bit errors or detect all $2$-bit errors and are
                            described by $(n,m)$. Where $n$ is the total number of bits in a codeword and $n=2^{k}-1$.
                            $m$ is the number of data bits in a codeword and then lastly $k$ is the number of parity or
                            check bits in a codeword and $k=n-m$, so $n=m+k$. Codeword Examples n (# of bits) m (# of
                            data bits) k (# of parity bits) 7 4 3 15 11 3 31 26 5 Codeword Examples n (# of bits) m (#
                            of data bits) k (# of parity bits) n (# of bits) m (# of data bits) k (# of parity bits) n
                            (# of bits)m (# of data bits)k (# of parity bits)7 4 3 15 11 3 31 26 5 7 4 3 7 4 3 4315 11 3
                            15 11 3 11331 26 5 31 26 5 265$(7,4)$ Hamming Code The $(7,4)$ Hamming code, as with the
                            rest of the Hamming codes in the family, are perfect single error correcting codes and can
                            correct all $1$-bit errors or detect all $2$-bit errors. The $(7,4)$ Hamming code has $7$
                            total number of bits in the codeword and $4$ number of data bits in the codeword. Thus,
                            given that $k=n-m$, the number of parity bits is $3=7-4$. The $(7,4)$ Hamming code has
                            $2^{m}=2^{4}=16$ codewords with a Hamming distance found to be $3$. Given a $7$-dimensional
                            Hamming space there exists $2^{N}=2^{7}=128$ points. Every codeword has a subset of
                            neighboring codewords seperated by $1$ vertex that we will call a sphere with radius of
                            $r=1$. Each sphere contains $\binom{N}{r}+\binom{N}{0}$ which is
                            $\binom{7}{1}+\binom{7}{0}=8$ points. Given the $128$ points in $7$-dimensional Hamming
                            space and that there are $16$ spheres and $8$ points per sphere ($6*8=128$) in the $(7,4)$
                            Hammaning code, all points are covered. Thus, the $(7,4)$ Hamming code is a perfect code.
                            The sphere has with $r=1$ has a point called the center and $\binom{n}{1}=n$ points on the
                            surface, thus, the volume of the sphere or total points in the sphere is:
                            $center\;+\binom{n}{1}=1+n$ Example of a sphere with $r=2$:
                            $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$ volume or total points in the sphere where the
                            center is a codeword. $(7,4)$ Hamming CodeThe $(7,4)$ Hamming code, as with the rest of the
                            Hamming codes in the family, are perfect single error correcting codes and can correct all
                            $1$-bit errors or detect all $2$-bit errors. The $(7,4)$ Hamming code has $7$ total number
                            of bits in the codeword and $4$ number of data bits in the codeword. Thus, given that
                            $k=n-m$, the number of parity bits is $3=7-4$. The $(7,4)$ Hamming code has $2^{m}=2^{4}=16$
                            codewords with a Hamming distance found to be $3$. Given a $7$-dimensional Hamming space
                            there exists $2^{N}=2^{7}=128$ points. Every codeword has a subset of neighboring codewords
                            seperated by $1$ vertex that we will call a sphere with radius of $r=1$. Each sphere
                            contains $\binom{N}{r}+\binom{N}{0}$ which is $\binom{7}{1}+\binom{7}{0}=8$ points. Given
                            the $128$ points in $7$-dimensional Hamming space and that there are $16$ spheres and $8$
                            points per sphere ($6*8=128$) in the $(7,4)$ Hammaning code, all points are covered. Thus,
                            the $(7,4)$ Hamming code is a perfect code. The sphere has with $r=1$ has a point called the
                            center and $\binom{n}{1}=n$ points on the surface, thus, the volume of the sphere or total
                            points in the sphere is: $center\;+\binom{n}{1}=1+n$ Example of a sphere with $r=2$:
                            $\binom{n}{0}+\binom{n}{1}+\binom{n}{2}$ volume or total points in the sphere where the
                            center is a codeword. Hamming Decoding To continue. Hamming DecodingTo continue. Error
                            Correcting Error Correcting Capability The error correcting capability $t$ of a given code
                            with some Hamming distance $d$ is defined as: $t=\left(\frac{d-1}{2}\right)$ Meaning, that
                            for the Hamming Code to correct $t$ errors, the sphere with $r=t$ of some codeword in
                            Hamming Space, must not intersect with any other sphere. So, some Hamming Code can correct
                            $t$ errors when $d>t$, but fails otherwise. For example, looking at the $(7,4)$ Hamming
                            Code, where $t=1$ and $d=3$, $d$ can be visualized as the Hamming distance between two
                            codewords. Thus, the $(7,4)$ Hamming Code can correct $t$ errors when $t < 3$, but fails
                                otherwise. Sphere Packing Bound The important part of sphere packing in Hamming Space,
                                is that valid codewords speheres cannot intersect. To satisfy this requirment, for any
                                code that can correct $t$ errors, $2^{m}$ spheres each with radius $t$ is required.
                                However, all spheres must fit in the $2^{n}$ volume of Hamming space. So, we are limited
                                by the following: $\frac{v_{HS}}{v_{S_{i}}}\geq max(S)$ Where $v_{HS}$ is the total
                                volume of the Hamming Space $HS$, $v_{S_{i}}$ is the volume of the given sphere $S_{i}$
                                from the set of spheres $S$, and $max(S)$ is the maximum number of spheres. Using this,
                                we can now figure out the minimum number if check bits or parity bits $k$ that are
                                acheieve any error correction capability. Recall, that $k=n-m$ where $n$ is the total
                                number of bits in a codeword and $m$ is the number of data bits in a codeword. Where $n$
                                and $m$ are described by $(n,m)$ of the Hamming Code, such as $(7,4)$ Hamming Code.
                                Error CorrectingError Correcting Capability The error correcting capability $t$ of a
                                given code with some Hamming distance $d$ is defined as: $t=\left(\frac{d-1}{2}\right)$
                                Meaning, that for the Hamming Code to correct $t$ errors, the sphere with $r=t$ of some
                                codeword in Hamming Space, must not intersect with any other sphere. So, some Hamming
                                Code can correct $t$ errors when $d>t$, but fails otherwise. For example, looking at the
                                $(7,4)$ Hamming Code, where $t=1$ and $d=3$, $d$ can be visualized as the Hamming
                                distance between two codewords. Thus, the $(7,4)$ Hamming Code can correct $t$ errors
                                when $t < 3$, but fails otherwise. Error Correcting CapabilityThe error correcting
                                    capability $t$ of a given code with some Hamming distance $d$ is defined as:
                                    $t=\left(\frac{d-1}{2}\right)$ Meaning, that for the Hamming Code to correct $t$
                                    errors, the sphere with $r=t$ of some codeword in Hamming Space, must not intersect
                                    with any other sphere. So, some Hamming Code can correct $t$ errors when $d>t$, but
                                    fails otherwise. For example, looking at the $(7,4)$ Hamming Code, where $t=1$ and
                                    $d=3$, $d$ can be visualized as the Hamming distance between two codewords. Thus,
                                    the $(7,4)$ Hamming Code can correct $t$ errors when $t < 3$, but fails
                                        otherwise.Sphere Packing Bound The important part of sphere packing in Hamming
                                        Space, is that valid codewords speheres cannot intersect. To satisfy this
                                        requirment, for any code that can correct $t$ errors, $2^{m}$ spheres each with
                                        radius $t$ is required. However, all spheres must fit in the $2^{n}$ volume of
                                        Hamming space. So, we are limited by the following:
                                        $\frac{v_{HS}}{v_{S_{i}}}\geq max(S)$ Where $v_{HS}$ is the total volume of the
                                        Hamming Space $HS$, $v_{S_{i}}$ is the volume of the given sphere $S_{i}$ from
                                        the set of spheres $S$, and $max(S)$ is the maximum number of spheres. Using
                                        this, we can now figure out the minimum number if check bits or parity bits $k$
                                        that are acheieve any error correction capability. Recall, that $k=n-m$ where
                                        $n$ is the total number of bits in a codeword and $m$ is the number of data bits
                                        in a codeword. Where $n$ and $m$ are described by $(n,m)$ of the Hamming Code,
                                        such as $(7,4)$ Hamming Code. Sphere Packing BoundThe important part of sphere
                                        packing in Hamming Space, is that valid codewords speheres cannot intersect. To
                                        satisfy this requirment, for any code that can correct $t$ errors, $2^{m}$
                                        spheres each with radius $t$ is required. However, all spheres must fit in the
                                        $2^{n}$ volume of Hamming space. So, we are limited by the following:
                                        $\frac{v_{HS}}{v_{S_{i}}}\geq max(S)$ Where $v_{HS}$ is the total volume of the
                                        Hamming Space $HS$, $v_{S_{i}}$ is the volume of the given sphere $S_{i}$ from
                                        the set of spheres $S$, and $max(S)$ is the maximum number of spheres. Using
                                        this, we can now figure out the minimum number if check bits or parity bits $k$
                                        that are acheieve any error correction capability. Recall, that $k=n-m$ where
                                        $n$ is the total number of bits in a codeword and $m$ is the number of data bits
                                        in a codeword. Where $n$ and $m$ are described by $(n,m)$ of the Hamming Code,
                                        such as $(7,4)$ Hamming Code. check bits or parity bitsHamming Code Example To
                                        continue. Hamming Code ExampleTo continue. Compression Algorithms Huffman Coding
                                        Algorithm To continue. Lempel-Ziv Compression To continue. Arithmetic Coding To
                                        continue. Compression AlgorithmsHuffman Coding Algorithm To continue. Huffman
                                        Coding AlgorithmTo continue. Lempel-Ziv Compression To continue. Lempel-Ziv
                                        CompressionTo continue. Arithmetic Coding To continue. Arithmetic CodingTo
                                        continue. Glossary Cardinality The cardinality of a set represents the number of
                                        elements in the set. GlossaryCardinality The cardinality of a set represents the
                                        number of elements in the set. CardinalityThe cardinality of a set represents
                                        the number of elements in the set. Uniquely Decodable Codes that are uniquely
                                        decodable, instantaneous codes, or prefix-free codes are called such because the
                                        decoder that scans the code instantly recognizes the end of the codeword.
                                        Uniquely DecodableCodes that are uniquely decodable, instantaneous codes, or
                                        prefix-free codes are called such because the decoder that scans the code
                                        instantly recognizes the end of the codeword.
                                        [https://www.contextswitching.org/my/researchandwork/] Research and Work -
                                        Context Switching Research and Work Papers Name: Final Paper for Hardware for
                                        Deep Learning: Quantum Algorithms for Deep Convolutional Neural Networks [PDF]
                                        Preview: Computer scientists utilize principles of quantum mechanics,
                                        mathematics, and computer science in quantum computing. By borrowing concepts
                                        from each field scientists can rigorously define both a broad and narrow
                                        theoretical model of a quantum computer and later apply it to the real world.
                                        These theoretical models, such as the result... Presentations Name: Quantum
                                        Algorithms for Deep Convolutional Neural Networks [PDF] Abstract: Current
                                        problem: it is difficult to implement non linearities with quantum unitaries
                                        Suggested solution: a new quantum tomography algorithm with norm guarantees, and
                                        new applications of probabilistic sampling in the context of information
                                        processing Goal: The QCNN is particularly interesting for deep networks and
                                        could allow new frontiers in image recognition, by using more or larger
                                        convolution kernels, larger or deeper inputs Name: Quantum Enhanced Feature
                                        Space [PDF] Abstract: Current problem: limitations on successful solution for
                                        problems when feature space becomes large high-dimensional Suggested solution:
                                        utilize controlled entanglement and interference to exploit exponentially
                                        growing quantum state space Goal: present new class of tools for exploring the
                                        applications of noisy intermediate scale quantum computers to machine learning
                                        for improved computational power and efficacy Current Projects Name: Credit Card
                                        Fraud Detection using a Quantum Support Vector Machine Description: Applying a
                                        Quantum Support Vector Machine for credit card fraud detection in Qiskit. The
                                        applied QSVM makes use of quantum enhanced feature space optimization based on
                                        the research paper, Supervised Learning with Quantum Enhanced Feature Spaces.
                                        The classifier used is a Variational Quantum Classifier with 29-dimensional ZZ
                                        feature mapping for a 2-qubit quantum kernel... view code Name: My Cloud Based
                                        Artifical Intelligence with Robot Interface Description: A robot I built that
                                        communicates with a remote server running artifical intelligence software I
                                        designed and programmed from scratch. Meaning, you can talk to the exact same
                                        Jarcey that is running in this robot, either on this website context switching,
                                        on jarcey's own website, or any device with an internet connection. This is
                                        because all communication, data, and programs are transimitted to, from, and
                                        processed on the same one remote server... my A.I's website Your browser does
                                        not support the video tag. Name: Autonomous AI Agents Ecosystem Simulator
                                        Description: Machine learning autonomous agents exploring and learning in a
                                        dynamically generated virtual ecosystem... view code Research and Work - Context
                                        Switching Research and Work - Context SwitchingResearch and Work Papers Name:
                                        Final Paper for Hardware for Deep Learning: Quantum Algorithms for Deep
                                        Convolutional Neural Networks [PDF] Preview: Computer scientists utilize
                                        principles of quantum mechanics, mathematics, and computer science in quantum
                                        computing. By borrowing concepts from each field scientists can rigorously
                                        define both a broad and narrow theoretical model of a quantum computer and later
                                        apply it to the real world. These theoretical models, such as the result...
                                        Presentations Name: Quantum Algorithms for Deep Convolutional Neural Networks
                                        [PDF] Abstract: Current problem: it is difficult to implement non linearities
                                        with quantum unitaries Suggested solution: a new quantum tomography algorithm
                                        with norm guarantees, and new applications of probabilistic sampling in the
                                        context of information processing Goal: The QCNN is particularly interesting for
                                        deep networks and could allow new frontiers in image recognition, by using more
                                        or larger convolution kernels, larger or deeper inputs Name: Quantum Enhanced
                                        Feature Space [PDF] Abstract: Current problem: limitations on successful
                                        solution for problems when feature space becomes large high-dimensional
                                        Suggested solution: utilize controlled entanglement and interference to exploit
                                        exponentially growing quantum state space Goal: present new class of tools for
                                        exploring the applications of noisy intermediate scale quantum computers to
                                        machine learning for improved computational power and efficacy Current Projects
                                        Name: Credit Card Fraud Detection using a Quantum Support Vector Machine
                                        Description: Applying a Quantum Support Vector Machine for credit card fraud
                                        detection in Qiskit. The applied QSVM makes use of quantum enhanced feature
                                        space optimization based on the research paper, Supervised Learning with Quantum
                                        Enhanced Feature Spaces. The classifier used is a Variational Quantum Classifier
                                        with 29-dimensional ZZ feature mapping for a 2-qubit quantum kernel... view code
                                        Name: My Cloud Based Artifical Intelligence with Robot Interface Description: A
                                        robot I built that communicates with a remote server running artifical
                                        intelligence software I designed and programmed from scratch. Meaning, you can
                                        talk to the exact same Jarcey that is running in this robot, either on this
                                        website context switching, on jarcey's own website, or any device with an
                                        internet connection. This is because all communication, data, and programs are
                                        transimitted to, from, and processed on the same one remote server... my A.I's
                                        website Your browser does not support the video tag. Name: Autonomous AI Agents
                                        Ecosystem Simulator Description: Machine learning autonomous agents exploring
                                        and learning in a dynamically generated virtual ecosystem... view code function
                                        go(loc) { document.getElementById("run").src="https://replit.com/@n113/" + loc
                                        + "?embed=true" ; } Research and Work Research and Work Research and Work Papers
                                        Papers Name: Final Paper for Hardware for Deep Learning: Quantum Algorithms for
                                        Deep Convolutional Neural Networks [PDF] Preview: Computer scientists utilize
                                        principles of quantum mechanics, mathematics, and computer science in quantum
                                        computing. By borrowing concepts from each field scientists can rigorously
                                        define both a broad and narrow theoretical model of a quantum computer and later
                                        apply it to the real world. These theoretical models, such as the result...
                                        Presentations Name: Quantum Algorithms for Deep Convolutional Neural Networks
                                        [PDF] Abstract: Current problem: it is difficult to implement non linearities
                                        with quantum unitaries Suggested solution: a new quantum tomography algorithm
                                        with norm guarantees, and new applications of probabilistic sampling in the
                                        context of information processing Goal: The QCNN is particularly interesting for
                                        deep networks and could allow new frontiers in image recognition, by using more
                                        or larger convolution kernels, larger or deeper inputs Name: Quantum Enhanced
                                        Feature Space [PDF] Abstract: Current problem: limitations on successful
                                        solution for problems when feature space becomes large high-dimensional
                                        Suggested solution: utilize controlled entanglement and interference to exploit
                                        exponentially growing quantum state space Goal: present new class of tools for
                                        exploring the applications of noisy intermediate scale quantum computers to
                                        machine learning for improved computational power and efficacy Current Projects
                                        Name: Credit Card Fraud Detection using a Quantum Support Vector Machine
                                        Description: Applying a Quantum Support Vector Machine for credit card fraud
                                        detection in Qiskit. The applied QSVM makes use of quantum enhanced feature
                                        space optimization based on the research paper, Supervised Learning with Quantum
                                        Enhanced Feature Spaces. The classifier used is a Variational Quantum Classifier
                                        with 29-dimensional ZZ feature mapping for a 2-qubit quantum kernel... view code
                                        Name: My Cloud Based Artifical Intelligence with Robot Interface Description: A
                                        robot I built that communicates with a remote server running artifical
                                        intelligence software I designed and programmed from scratch. Meaning, you can
                                        talk to the exact same Jarcey that is running in this robot, either on this
                                        website context switching, on jarcey's own website, or any device with an
                                        internet connection. This is because all communication, data, and programs are
                                        transimitted to, from, and processed on the same one remote server... my A.I's
                                        website Your browser does not support the video tag. Name: Autonomous AI Agents
                                        Ecosystem Simulator Description: Machine learning autonomous agents exploring
                                        and learning in a dynamically generated virtual ecosystem... view code Name:
                                        Final Paper for Hardware for Deep Learning: Quantum Algorithms for Deep
                                        Convolutional Neural Networks [PDF] Preview: Computer scientists utilize
                                        principles of quantum mechanics, mathematics, and computer science in quantum
                                        computing. By borrowing concepts from each field scientists can rigorously
                                        define both a broad and narrow theoretical model of a quantum computer and later
                                        apply it to the real world. These theoretical models, such as the result...
                                        Name: Final Paper for Hardware for Deep Learning: Quantum Algorithms for Deep
                                        Convolutional Neural Networks [PDF] Preview: Computer scientists utilize
                                        principles of quantum mechanics, mathematics, and computer science in quantum
                                        computing. By borrowing concepts from each field scientists can rigorously
                                        define both a broad and narrow theoretical model of a quantum computer and later
                                        apply it to the real world. These theoretical models, such as the result...
                                        Name[PDF]Preview Presentations Presentations Name: Quantum Algorithms for Deep
                                        Convolutional Neural Networks [PDF] Abstract: Current problem: it is difficult
                                        to implement non linearities with quantum unitaries Suggested solution: a new
                                        quantum tomography algorithm with norm guarantees, and new applications of
                                        probabilistic sampling in the context of information processing Goal: The QCNN
                                        is particularly interesting for deep networks and could allow new frontiers in
                                        image recognition, by using more or larger convolution kernels, larger or deeper
                                        inputs Name: Quantum Enhanced Feature Space [PDF] Abstract: Current problem:
                                        limitations on successful solution for problems when feature space becomes large
                                        high-dimensional Suggested solution: utilize controlled entanglement and
                                        interference to exploit exponentially growing quantum state space Goal: present
                                        new class of tools for exploring the applications of noisy intermediate scale
                                        quantum computers to machine learning for improved computational power and
                                        efficacy Current Projects Name: Credit Card Fraud Detection using a Quantum
                                        Support Vector Machine Description: Applying a Quantum Support Vector Machine
                                        for credit card fraud detection in Qiskit. The applied QSVM makes use of quantum
                                        enhanced feature space optimization based on the research paper, Supervised
                                        Learning with Quantum Enhanced Feature Spaces. The classifier used is a
                                        Variational Quantum Classifier with 29-dimensional ZZ feature mapping for a
                                        2-qubit quantum kernel... view code Name: My Cloud Based Artifical Intelligence
                                        with Robot Interface Description: A robot I built that communicates with a
                                        remote server running artifical intelligence software I designed and programmed
                                        from scratch. Meaning, you can talk to the exact same Jarcey that is running in
                                        this robot, either on this website context switching, on jarcey's own website,
                                        or any device with an internet connection. This is because all communication,
                                        data, and programs are transimitted to, from, and processed on the same one
                                        remote server... my A.I's website Your browser does not support the video tag.
                                        Name: Autonomous AI Agents Ecosystem Simulator Description: Machine learning
                                        autonomous agents exploring and learning in a dynamically generated virtual
                                        ecosystem... view code Name: Quantum Algorithms for Deep Convolutional Neural
                                        Networks [PDF] Abstract: Current problem: it is difficult to implement non
                                        linearities with quantum unitaries Suggested solution: a new quantum tomography
                                        algorithm with norm guarantees, and new applications of probabilistic sampling
                                        in the context of information processing Goal: The QCNN is particularly
                                        interesting for deep networks and could allow new frontiers in image
                                        recognition, by using more or larger convolution kernels, larger or deeper
                                        inputs Name: Quantum Algorithms for Deep Convolutional Neural Networks [PDF]
                                        Abstract: Current problem: it is difficult to implement non linearities with
                                        quantum unitaries Suggested solution: a new quantum tomography algorithm with
                                        norm guarantees, and new applications of probabilistic sampling in the context
                                        of information processing Goal: The QCNN is particularly interesting for deep
                                        networks and could allow new frontiers in image recognition, by using more or
                                        larger convolution kernels, larger or deeper inputs Name[PDF]AbstractName:
                                        Quantum Enhanced Feature Space [PDF] Abstract: Current problem: limitations on
                                        successful solution for problems when feature space becomes large
                                        high-dimensional Suggested solution: utilize controlled entanglement and
                                        interference to exploit exponentially growing quantum state space Goal: present
                                        new class of tools for exploring the applications of noisy intermediate scale
                                        quantum computers to machine learning for improved computational power and
                                        efficacy Name: Quantum Enhanced Feature Space [PDF] Abstract: Current problem:
                                        limitations on successful solution for problems when feature space becomes large
                                        high-dimensional Suggested solution: utilize controlled entanglement and
                                        interference to exploit exponentially growing quantum state space Goal: present
                                        new class of tools for exploring the applications of noisy intermediate scale
                                        quantum computers to machine learning for improved computational power and
                                        efficacy Name: Quantum Enhanced Feature Space [PDF] Abstract: Current problem:
                                        limitations on successful solution for problems when feature space becomes large
                                        high-dimensional Suggested solution: utilize controlled entanglement and
                                        interference to exploit exponentially growing quantum state space Goal: present
                                        new class of tools for exploring the applications of noisy intermediate scale
                                        quantum computers to machine learning for improved computational power and
                                        efficacy Name[PDF]Abstract Current Projects Current Projects Name: Credit Card
                                        Fraud Detection using a Quantum Support Vector Machine Description: Applying a
                                        Quantum Support Vector Machine for credit card fraud detection in Qiskit. The
                                        applied QSVM makes use of quantum enhanced feature space optimization based on
                                        the research paper, Supervised Learning with Quantum Enhanced Feature Spaces.
                                        The classifier used is a Variational Quantum Classifier with 29-dimensional ZZ
                                        feature mapping for a 2-qubit quantum kernel... view code Name: Credit Card
                                        Fraud Detection using a Quantum Support Vector Machine Description: Applying a
                                        Quantum Support Vector Machine for credit card fraud detection in Qiskit. The
                                        applied QSVM makes use of quantum enhanced feature space optimization based on
                                        the research paper, Supervised Learning with Quantum Enhanced Feature Spaces.
                                        The classifier used is a Variational Quantum Classifier with 29-dimensional ZZ
                                        feature mapping for a 2-qubit quantum kernel... view code Name: Credit Card
                                        Fraud Detection using a Quantum Support Vector Machine Description: Applying a
                                        Quantum Support Vector Machine for credit card fraud detection in Qiskit. The
                                        applied QSVM makes use of quantum enhanced feature space optimization based on
                                        the research paper, Supervised Learning with Quantum Enhanced Feature Spaces.
                                        The classifier used is a Variational Quantum Classifier with 29-dimensional ZZ
                                        feature mapping for a 2-qubit quantum kernel... view code NameDescriptionview
                                        codeName: My Cloud Based Artifical Intelligence with Robot Interface
                                        Description: A robot I built that communicates with a remote server running
                                        artifical intelligence software I designed and programmed from scratch. Meaning,
                                        you can talk to the exact same Jarcey that is running in this robot, either on
                                        this website context switching, on jarcey's own website, or any device with an
                                        internet connection. This is because all communication, data, and programs are
                                        transimitted to, from, and processed on the same one remote server... my A.I's
                                        website Your browser does not support the video tag. Name: My Cloud Based
                                        Artifical Intelligence with Robot Interface Description: A robot I built that
                                        communicates with a remote server running artifical intelligence software I
                                        designed and programmed from scratch. Meaning, you can talk to the exact same
                                        Jarcey that is running in this robot, either on this website context switching,
                                        on jarcey's own website, or any device with an internet connection. This is
                                        because all communication, data, and programs are transimitted to, from, and
                                        processed on the same one remote server... my A.I's website Your browser does
                                        not support the video tag. Name: My Cloud Based Artifical Intelligence with
                                        Robot Interface Description: A robot I built that communicates with a remote
                                        server running artifical intelligence software I designed and programmed from
                                        scratch. Meaning, you can talk to the exact same Jarcey that is running in this
                                        robot, either on this website context switching, on jarcey's own website, or any
                                        device with an internet connection. This is because all communication, data, and
                                        programs are transimitted to, from, and processed on the same one remote
                                        server... my A.I's website Your browser does not support the video tag.
                                        NameDescriptionmy A.I's website Your browser does not support the video tag.
                                        Your browser does not support the video tag. Name: Autonomous AI Agents
                                        Ecosystem Simulator Description: Machine learning autonomous agents exploring
                                        and learning in a dynamically generated virtual ecosystem... view code Name:
                                        Autonomous AI Agents Ecosystem Simulator Description: Machine learning
                                        autonomous agents exploring and learning in a dynamically generated virtual
                                        ecosystem... view code Name: Autonomous AI Agents Ecosystem Simulator
                                        Description: Machine learning autonomous agents exploring and learning in a
                                        dynamically generated virtual ecosystem... view code NameDescriptionview code
                                        [https://www.contextswitching.org/tcs/algorithmicanalysis/] Algorithmic Analysis
                                        - Context Switching Algorithmic Anaylsis Introduction What is it Why we use it
                                        Algorithmic analysis is used to help computer scientists understand the
                                        resources required by an algorithm for time, storage, and other uses.
                                        Algorithmic anlysis must analyze algorithms in a methodical, universal, and fair
                                        way. To do this computer scientist implement mathematical models that describe
                                        the resources used by algorithms. This work, although theoretical, extends
                                        beyound theory and applies mathematical models to real world problems.
                                        Algorithmic anlayis is apart of app development, website development,
                                        simulations, artificial intelligence, and anything that implements algorithms in
                                        its design. This is because computer scientist or programmers need to implement
                                        algorithms into applications that take minimum time and space, while still being
                                        reltively easy to code. In the rest of this articule we will review the
                                        mathematical models that computer scientists and programmers use to analyze
                                        algorithms. Mathematical Models Best-Case Time Complexity Equation Omega
                                        $\Omega$ Lower-bounds The best-case time complexity can be mathematically
                                        defined as: $\Omega(g(n))=\exists \, c> 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq
                                        f(n)$ $\forall \, n \geq n _{0}$ I will break this down into individual parts,
                                        then piece them together to arrive at the final equation. First, lets start with
                                        what's left of the equal sign. $\Omega(g(n)$ $\Omega$ is Big-Omega, which
                                        denotes the best-case time complexity or the lower-bounds for an algorithm. $n$
                                        is the input for the function $g$. $g$ is the function that describes the
                                        original algorithm. Next, lets look at $\exists \, c > 0, n_{0} > 0$ The
                                        $\exists $ denotes, "there exists". So the equation states that there exists two
                                        constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$ is simply a
                                        constant that accounts for operations that are not proportional to the input $n$
                                        for $g$. $n_{0}$ is a theoretical size for the input $n$ wherein only after
                                        input size does $\Omega$ actually maintain the lower-bound for $g$. Remember, we
                                        do not give actual values for the variables, but leave them ambigious so that we
                                        have a lower-bound model to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0
                                        \leq cg(n) \leq f(n)$ states that our lower bounds, $g(n)$ times some constant
                                        $c$ lies between $0$ and our original function $f(n)$. This makes sense as the
                                        lower-bound runtime for our algorithm cannot be less than 0 but also needs to be
                                        lower than or equal to the runtime of our original function. Otherwise it would
                                        not be much of a lower bound. Then there is $0 \leq cg(n) \leq f(n)$. This means
                                        that $cg(n)$ lies between $0$ and $f(n)$. In this instance, $f(n)$ is the
                                        original asymptotic function, the one that is derived from the algorithm itself.
                                        $cg(n)$ denotes the lower-bounds of our original asymptotic function times some
                                        constant $c$. Thus, this is saying that the lower-bounds of the original
                                        asymptotic function for our algorithm lies at or below the original asymptotic
                                        function or at or above $0$. $\exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                        \leq f(n)$ Now, to put the last two parts together. This segment of the equation
                                        is saying that the lower-bounds of our original function $g(n)$ times some
                                        constant $c$ that is greater than 0, lies at or below the original function
                                        $f(n)$ or at or above $0$. So looking at the final equation again: $\Omega(g(n))
                                        = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n
                                        \geq n _{0}$ This translates to, "There exists some contant $c$ greater than $0$
                                        and some constant $n_{0}$ such that $cg(n)$ is greater than or equal to $0$ and
                                        less than or equal to $f(n)$ where all $n$ are greater than or equal to
                                        $n_{0}$". From what we have learned we can say that this equation here says that
                                        there exists a constant $c$ and $n$, $c$ being runtime constants, that are
                                        greater than $0$ such that our lower-bound exists between $0$ and the original
                                        asymptotic function. Best-Case Time Complexity Graph Worst Case Time Complexity
                                        - $O$ Big-O $O$ Upper-bounds So we know that best case of time-complexity is
                                        when an algorithm runs is the least amount of time it could theoretically
                                        operate in. So, it follows that the worst-case time-complexity is the worst time
                                        an algorith could theoretically operate in. Now, what is an example of this.
                                        Lets take our previous example of insertion sort. Now, recall, that the type of
                                        input for the algorithm can affect its time complexity. So, I want you to take a
                                        moment to come up with an example. Lets continue. An example for which the time
                                        complexity is worse is when all of the input data is sorted in descending order.
                                        Algorithmic Analysis - Context Switching Algorithmic Analysis - Context
                                        SwitchingAlgorithmic Anaylsis Introduction What is it Why we use it Algorithmic
                                        analysis is used to help computer scientists understand the resources required
                                        by an algorithm for time, storage, and other uses. Algorithmic anlysis must
                                        analyze algorithms in a methodical, universal, and fair way. To do this computer
                                        scientist implement mathematical models that describe the resources used by
                                        algorithms. This work, although theoretical, extends beyound theory and applies
                                        mathematical models to real world problems. Algorithmic anlayis is apart of app
                                        development, website development, simulations, artificial intelligence, and
                                        anything that implements algorithms in its design. This is because computer
                                        scientist or programmers need to implement algorithms into applications that
                                        take minimum time and space, while still being reltively easy to code. In the
                                        rest of this articule we will review the mathematical models that computer
                                        scientists and programmers use to analyze algorithms. Mathematical Models
                                        Best-Case Time Complexity Equation Omega $\Omega$ Lower-bounds The best-case
                                        time complexity can be mathematically defined as: $\Omega(g(n)) = \exists \, c >
                                        0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ I will
                                        break this down into individual parts, then piece them together to arrive at the
                                        final equation. First, lets start with what's left of the equal sign.
                                        $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case time complexity
                                        or the lower-bounds for an algorithm. $n$ is the input for the function $g$. $g$
                                        is the function that describes the original algorithm. Next, lets look at
                                        $\exists \, c > 0, n_{0} > 0$ The $\exists $ denotes, "there exists". So the
                                        equation states that there exists two constants, $c$ and $n_{0}$ whose values
                                        both are greater than 0. $c$ is simply a constant that accounts for operations
                                        that are not proportional to the input $n$ for $g$. $n_{0}$ is a theoretical
                                        size for the input $n$ wherein only after input size does $\Omega$ actually
                                        maintain the lower-bound for $g$. Remember, we do not give actual values for the
                                        variables, but leave them ambigious so that we have a lower-bound model to use
                                        for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq f(n)$ states
                                        that our lower bounds, $g(n)$ times some constant $c$ lies between $0$ and our
                                        original function $f(n)$. This makes sense as the lower-bound runtime for our
                                        algorithm cannot be less than 0 but also needs to be lower than or equal to the
                                        runtime of our original function. Otherwise it would not be much of a lower
                                        bound. Then there is $0 \leq cg(n) \leq f(n)$. This means that $cg(n)$ lies
                                        between $0$ and $f(n)$. In this instance, $f(n)$ is the original asymptotic
                                        function, the one that is derived from the algorithm itself. $cg(n)$ denotes the
                                        lower-bounds of our original asymptotic function times some constant $c$. Thus,
                                        this is saying that the lower-bounds of the original asymptotic function for our
                                        algorithm lies at or below the original asymptotic function or at or above $0$.
                                        $\exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the
                                        last two parts together. This segment of the equation is saying that the
                                        lower-bounds of our original function $g(n)$ times some constant $c$ that is
                                        greater than 0, lies at or below the original function $f(n)$ or at or above
                                        $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \, c > 0,
                                        n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ This
                                        translates to, "There exists some contant $c$ greater than $0$ and some constant
                                        $n_{0}$ such that $cg(n)$ is greater than or equal to $0$ and less than or equal
                                        to $f(n)$ where all $n$ are greater than or equal to $n_{0}$". From what we have
                                        learned we can say that this equation here says that there exists a constant $c$
                                        and $n$, $c$ being runtime constants, that are greater than $0$ such that our
                                        lower-bound exists between $0$ and the original asymptotic function. Best-Case
                                        Time Complexity Graph Worst Case Time Complexity - $O$ Big-O $O$ Upper-bounds So
                                        we know that best case of time-complexity is when an algorithm runs is the least
                                        amount of time it could theoretically operate in. So, it follows that the
                                        worst-case time-complexity is the worst time an algorith could theoretically
                                        operate in. Now, what is an example of this. Lets take our previous example of
                                        insertion sort. Now, recall, that the type of input for the algorithm can affect
                                        its time complexity. So, I want you to take a moment to come up with an example.
                                        Lets continue. An example for which the time complexity is worse is when all of
                                        the input data is sorted in descending order. Algorithmic Anaylsis Introduction
                                        What is it Why we use it Algorithmic analysis is used to help computer
                                        scientists understand the resources required by an algorithm for time, storage,
                                        and other uses. Algorithmic anlysis must analyze algorithms in a methodical,
                                        universal, and fair way. To do this computer scientist implement mathematical
                                        models that describe the resources used by algorithms. This work, although
                                        theoretical, extends beyound theory and applies mathematical models to real
                                        world problems. Algorithmic anlayis is apart of app development, website
                                        development, simulations, artificial intelligence, and anything that implements
                                        algorithms in its design. This is because computer scientist or programmers need
                                        to implement algorithms into applications that take minimum time and space,
                                        while still being reltively easy to code. In the rest of this articule we will
                                        review the mathematical models that computer scientists and programmers use to
                                        analyze algorithms. Mathematical Models Best-Case Time Complexity Equation Omega
                                        $\Omega$ Lower-bounds The best-case time complexity can be mathematically
                                        defined as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                        \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into individual
                                        parts, then piece them together to arrive at the final equation. First, lets
                                        start with what's left of the equal sign. $\Omega(g(n)$ $\Omega$ is Big-Omega,
                                        which denotes the best-case time complexity or the lower-bounds for an
                                        algorithm. $n$ is the input for the function $g$. $g$ is the function that
                                        describes the original algorithm. Next, lets look at $\exists \, c > 0, n_{0} >
                                        0$ The $\exists $ denotes, "there exists". So the equation states that there
                                        exists two constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$
                                        is simply a constant that accounts for operations that are not proportional to
                                        the input $n$ for $g$. $n_{0}$ is a theoretical size for the input $n$ wherein
                                        only after input size does $\Omega$ actually maintain the lower-bound for $g$.
                                        Remember, we do not give actual values for the variables, but leave them
                                        ambigious so that we have a lower-bound model to use for all algorithms. $0 \leq
                                        cg(n) \leq f(n)$ $0 \leq cg(n) \leq f(n)$ states that our lower bounds, $g(n)$
                                        times some constant $c$ lies between $0$ and our original function $f(n)$. This
                                        makes sense as the lower-bound runtime for our algorithm cannot be less than 0
                                        but also needs to be lower than or equal to the runtime of our original
                                        function. Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                        cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$. In this
                                        instance, $f(n)$ is the original asymptotic function, the one that is derived
                                        from the algorithm itself. $cg(n)$ denotes the lower-bounds of our original
                                        asymptotic function times some constant $c$. Thus, this is saying that the
                                        lower-bounds of the original asymptotic function for our algorithm lies at or
                                        below the original asymptotic function or at or above $0$. $\exists \, c > 0,
                                        n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                        together. This segment of the equation is saying that the lower-bounds of our
                                        original function $g(n)$ times some constant $c$ that is greater than 0, lies at
                                        or below the original function $f(n)$ or at or above $0$. So looking at the
                                        final equation again: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0
                                        \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ This translates to, "There
                                        exists some contant $c$ greater than $0$ and some constant $n_{0}$ such that
                                        $cg(n)$ is greater than or equal to $0$ and less than or equal to $f(n)$ where
                                        all $n$ are greater than or equal to $n_{0}$". From what we have learned we can
                                        say that this equation here says that there exists a constant $c$ and $n$, $c$
                                        being runtime constants, that are greater than $0$ such that our lower-bound
                                        exists between $0$ and the original asymptotic function. Best-Case Time
                                        Complexity Graph Worst Case Time Complexity - $O$ Big-O $O$ Upper-bounds So we
                                        know that best case of time-complexity is when an algorithm runs is the least
                                        amount of time it could theoretically operate in. So, it follows that the
                                        worst-case time-complexity is the worst time an algorith could theoretically
                                        operate in. Now, what is an example of this. Lets take our previous example of
                                        insertion sort. Now, recall, that the type of input for the algorithm can affect
                                        its time complexity. So, I want you to take a moment to come up with an example.
                                        Lets continue. An example for which the time complexity is worse is when all of
                                        the input data is sorted in descending order. Algorithmic Anaylsis Introduction
                                        What is it Why we use it Algorithmic analysis is used to help computer
                                        scientists understand the resources required by an algorithm for time, storage,
                                        and other uses. Algorithmic anlysis must analyze algorithms in a methodical,
                                        universal, and fair way. To do this computer scientist implement mathematical
                                        models that describe the resources used by algorithms. This work, although
                                        theoretical, extends beyound theory and applies mathematical models to real
                                        world problems. Algorithmic anlayis is apart of app development, website
                                        development, simulations, artificial intelligence, and anything that implements
                                        algorithms in its design. This is because computer scientist or programmers need
                                        to implement algorithms into applications that take minimum time and space,
                                        while still being reltively easy to code. In the rest of this articule we will
                                        review the mathematical models that computer scientists and programmers use to
                                        analyze algorithms. Mathematical Models Best-Case Time Complexity Equation Omega
                                        $\Omega$ Lower-bounds The best-case time complexity can be mathematically
                                        defined as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                        \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into individual
                                        parts, then piece them together to arrive at the final equation. First, lets
                                        start with what's left of the equal sign. $\Omega(g(n)$ $\Omega$ is Big-Omega,
                                        which denotes the best-case time complexity or the lower-bounds for an
                                        algorithm. $n$ is the input for the function $g$. $g$ is the function that
                                        describes the original algorithm. Next, lets look at $\exists \, c > 0, n_{0} >
                                        0$ The $\exists $ denotes, "there exists". So the equation states that there
                                        exists two constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$
                                        is simply a constant that accounts for operations that are not proportional to
                                        the input $n$ for $g$. $n_{0}$ is a theoretical size for the input $n$ wherein
                                        only after input size does $\Omega$ actually maintain the lower-bound for $g$.
                                        Remember, we do not give actual values for the variables, but leave them
                                        ambigious so that we have a lower-bound model to use for all algorithms. $0 \leq
                                        cg(n) \leq f(n)$ $0 \leq cg(n) \leq f(n)$ states that our lower bounds, $g(n)$
                                        times some constant $c$ lies between $0$ and our original function $f(n)$. This
                                        makes sense as the lower-bound runtime for our algorithm cannot be less than 0
                                        but also needs to be lower than or equal to the runtime of our original
                                        function. Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                        cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$. In this
                                        instance, $f(n)$ is the original asymptotic function, the one that is derived
                                        from the algorithm itself. $cg(n)$ denotes the lower-bounds of our original
                                        asymptotic function times some constant $c$. Thus, this is saying that the
                                        lower-bounds of the original asymptotic function for our algorithm lies at or
                                        below the original asymptotic function or at or above $0$. $\exists \, c > 0,
                                        n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                        together. This segment of the equation is saying that the lower-bounds of our
                                        original function $g(n)$ times some constant $c$ that is greater than 0, lies at
                                        or below the original function $f(n)$ or at or above $0$. So looking at the
                                        final equation again: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0
                                        \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ This translates to, "There
                                        exists some contant $c$ greater than $0$ and some constant $n_{0}$ such that
                                        $cg(n)$ is greater than or equal to $0$ and less than or equal to $f(n)$ where
                                        all $n$ are greater than or equal to $n_{0}$". From what we have learned we can
                                        say that this equation here says that there exists a constant $c$ and $n$, $c$
                                        being runtime constants, that are greater than $0$ such that our lower-bound
                                        exists between $0$ and the original asymptotic function. Best-Case Time
                                        Complexity Graph Worst Case Time Complexity - $O$ Big-O $O$ Upper-bounds So we
                                        know that best case of time-complexity is when an algorithm runs is the least
                                        amount of time it could theoretically operate in. So, it follows that the
                                        worst-case time-complexity is the worst time an algorith could theoretically
                                        operate in. Now, what is an example of this. Lets take our previous example of
                                        insertion sort. Now, recall, that the type of input for the algorithm can affect
                                        its time complexity. So, I want you to take a moment to come up with an example.
                                        Lets continue. An example for which the time complexity is worse is when all of
                                        the input data is sorted in descending order. Algorithmic AnaylsisIntroduction
                                        What is it Why we use it Algorithmic analysis is used to help computer
                                        scientists understand the resources required by an algorithm for time, storage,
                                        and other uses. Algorithmic anlysis must analyze algorithms in a methodical,
                                        universal, and fair way. To do this computer scientist implement mathematical
                                        models that describe the resources used by algorithms. This work, although
                                        theoretical, extends beyound theory and applies mathematical models to real
                                        world problems. Algorithmic anlayis is apart of app development, website
                                        development, simulations, artificial intelligence, and anything that implements
                                        algorithms in its design. This is because computer scientist or programmers need
                                        to implement algorithms into applications that take minimum time and space,
                                        while still being reltively easy to code. In the rest of this articule we will
                                        review the mathematical models that computer scientists and programmers use to
                                        analyze algorithms. IntroductionWhat is it Why we use it What is itWhy we use
                                        itAlgorithmic analysis is used to help computer scientists understand the
                                        resources required by an algorithm for time, storage, and other uses.
                                        Algorithmic anlysis must analyze algorithms in a methodical, universal, and fair
                                        way. To do this computer scientist implement mathematical models that describe
                                        the resources used by algorithms. This work, although theoretical, extends
                                        beyound theory and applies mathematical models to real world problems.
                                        Algorithmic anlayis is apart of app development, website development,
                                        simulations, artificial intelligence, and anything that implements algorithms in
                                        its design. This is because computer scientist or programmers need to implement
                                        algorithms into applications that take minimum time and space, while still being
                                        reltively easy to code. In the rest of this articule we will review the
                                        mathematical models that computer scientists and programmers use to analyze
                                        algorithms. Mathematical Models Best-Case Time Complexity Equation Omega
                                        $\Omega$ Lower-bounds The best-case time complexity can be mathematically
                                        defined as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                        \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into individual
                                        parts, then piece them together to arrive at the final equation. First, lets
                                        start with what's left of the equal sign. $\Omega(g(n)$ $\Omega$ is Big-Omega,
                                        which denotes the best-case time complexity or the lower-bounds for an
                                        algorithm. $n$ is the input for the function $g$. $g$ is the function that
                                        describes the original algorithm. Next, lets look at $\exists \, c > 0, n_{0} >
                                        0$ The $\exists $ denotes, "there exists". So the equation states that there
                                        exists two constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$
                                        is simply a constant that accounts for operations that are not proportional to
                                        the input $n$ for $g$. $n_{0}$ is a theoretical size for the input $n$ wherein
                                        only after input size does $\Omega$ actually maintain the lower-bound for $g$.
                                        Remember, we do not give actual values for the variables, but leave them
                                        ambigious so that we have a lower-bound model to use for all algorithms. $0 \leq
                                        cg(n) \leq f(n)$ $0 \leq cg(n) \leq f(n)$ states that our lower bounds, $g(n)$
                                        times some constant $c$ lies between $0$ and our original function $f(n)$. This
                                        makes sense as the lower-bound runtime for our algorithm cannot be less than 0
                                        but also needs to be lower than or equal to the runtime of our original
                                        function. Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                        cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$. In this
                                        instance, $f(n)$ is the original asymptotic function, the one that is derived
                                        from the algorithm itself. $cg(n)$ denotes the lower-bounds of our original
                                        asymptotic function times some constant $c$. Thus, this is saying that the
                                        lower-bounds of the original asymptotic function for our algorithm lies at or
                                        below the original asymptotic function or at or above $0$. $\exists \, c > 0,
                                        n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                        together. This segment of the equation is saying that the lower-bounds of our
                                        original function $g(n)$ times some constant $c$ that is greater than 0, lies at
                                        or below the original function $f(n)$ or at or above $0$. So looking at the
                                        final equation again: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0
                                        \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ This translates to, "There
                                        exists some contant $c$ greater than $0$ and some constant $n_{0}$ such that
                                        $cg(n)$ is greater than or equal to $0$ and less than or equal to $f(n)$ where
                                        all $n$ are greater than or equal to $n_{0}$". From what we have learned we can
                                        say that this equation here says that there exists a constant $c$ and $n$, $c$
                                        being runtime constants, that are greater than $0$ such that our lower-bound
                                        exists between $0$ and the original asymptotic function. Best-Case Time
                                        Complexity Graph Mathematical ModelsBest-Case Time Complexity Equation Omega
                                        $\Omega$ Lower-bounds The best-case time complexity can be mathematically
                                        defined as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                        \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into individual
                                        parts, then piece them together to arrive at the final equation. First, lets
                                        start with what's left of the equal sign. $\Omega(g(n)$ $\Omega$ is Big-Omega,
                                        which denotes the best-case time complexity or the lower-bounds for an
                                        algorithm. $n$ is the input for the function $g$. $g$ is the function that
                                        describes the original algorithm. Next, lets look at $\exists \, c > 0, n_{0} >
                                        0$ The $\exists $ denotes, "there exists". So the equation states that there
                                        exists two constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$
                                        is simply a constant that accounts for operations that are not proportional to
                                        the input $n$ for $g$. $n_{0}$ is a theoretical size for the input $n$ wherein
                                        only after input size does $\Omega$ actually maintain the lower-bound for $g$.
                                        Remember, we do not give actual values for the variables, but leave them
                                        ambigious so that we have a lower-bound model to use for all algorithms. $0 \leq
                                        cg(n) \leq f(n)$ $0 \leq cg(n) \leq f(n)$ states that our lower bounds, $g(n)$
                                        times some constant $c$ lies between $0$ and our original function $f(n)$. This
                                        makes sense as the lower-bound runtime for our algorithm cannot be less than 0
                                        but also needs to be lower than or equal to the runtime of our original
                                        function. Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                        cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$. In this
                                        instance, $f(n)$ is the original asymptotic function, the one that is derived
                                        from the algorithm itself. $cg(n)$ denotes the lower-bounds of our original
                                        asymptotic function times some constant $c$. Thus, this is saying that the
                                        lower-bounds of the original asymptotic function for our algorithm lies at or
                                        below the original asymptotic function or at or above $0$. $\exists \, c > 0,
                                        n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                        together. This segment of the equation is saying that the lower-bounds of our
                                        original function $g(n)$ times some constant $c$ that is greater than 0, lies at
                                        or below the original function $f(n)$ or at or above $0$. So looking at the
                                        final equation again: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0
                                        \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ This translates to, "There
                                        exists some contant $c$ greater than $0$ and some constant $n_{0}$ such that
                                        $cg(n)$ is greater than or equal to $0$ and less than or equal to $f(n)$ where
                                        all $n$ are greater than or equal to $n_{0}$". From what we have learned we can
                                        say that this equation here says that there exists a constant $c$ and $n$, $c$
                                        being runtime constants, that are greater than $0$ such that our lower-bound
                                        exists between $0$ and the original asymptotic function. Best-Case Time
                                        Complexity EquationOmega $\Omega$ Lower-bounds Omega $\Omega$Lower-boundsThe
                                        best-case time complexity can be mathematically defined as:$\Omega(g(n)) =
                                        \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq
                                        n _{0}$ I will break this down into individual parts, then piece them together
                                        to arrive at the final equation. First, lets start with what's left of the equal
                                        sign. $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case time
                                        complexity or the lower-bounds for an algorithm. $n$ is the input for the
                                        function $g$. $g$ is the function that describes the original algorithm. Next,
                                        lets look at $\exists \, c > 0, n_{0} > 0$ The $\exists $ denotes, "there
                                        exists". So the equation states that there exists two constants, $c$ and $n_{0}$
                                        whose values both are greater than 0. $c$ is simply a constant that accounts for
                                        operations that are not proportional to the input $n$ for $g$. $n_{0}$ is a
                                        theoretical size for the input $n$ wherein only after input size does $\Omega$
                                        actually maintain the lower-bound for $g$. Remember, we do not give actual
                                        values for the variables, but leave them ambigious so that we have a lower-bound
                                        model to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq
                                        f(n)$ states that our lower bounds, $g(n)$ times some constant $c$ lies between
                                        $0$ and our original function $f(n)$. This makes sense as the lower-bound
                                        runtime for our algorithm cannot be less than 0 but also needs to be lower than
                                        or equal to the runtime of our original function. Otherwise it would not be much
                                        of a lower bound. Then there is $0 \leq cg(n) \leq f(n)$. This means that
                                        $cg(n)$ lies between $0$ and $f(n)$. In this instance, $f(n)$ is the original
                                        asymptotic function, the one that is derived from the algorithm itself. $cg(n)$
                                        denotes the lower-bounds of our original asymptotic function times some constant
                                        $c$. Thus, this is saying that the lower-bounds of the original asymptotic
                                        function for our algorithm lies at or below the original asymptotic function or
                                        at or above $0$. $\exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$
                                        Now, to put the last two parts together. This segment of the equation is saying
                                        that the lower-bounds of our original function $g(n)$ times some constant $c$
                                        that is greater than 0, lies at or below the original function $f(n)$ or at or
                                        above $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \, c
                                        > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ This
                                        translates to, "There exists some contant $c$ greater than $0$ and some constant
                                        $n_{0}$ such that $cg(n)$ is greater than or equal to $0$ and less than or equal
                                        to $f(n)$ where all $n$ are greater than or equal to $n_{0}$". From what we have
                                        learned we can say that this equation here says that there exists a constant $c$
                                        and $n$, $c$ being runtime constants, that are greater than $0$ such that our
                                        lower-bound exists between $0$ and the original asymptotic function. Best-Case
                                        Time Complexity Graph Best-Case Time Complexity GraphWorst Case Time Complexity
                                        - $O$ Big-O $O$ Upper-bounds So we know that best case of time-complexity is
                                        when an algorithm runs is the least amount of time it could theoretically
                                        operate in. So, it follows that the worst-case time-complexity is the worst time
                                        an algorith could theoretically operate in. Now, what is an example of this.
                                        Lets take our previous example of insertion sort. Now, recall, that the type of
                                        input for the algorithm can affect its time complexity. So, I want you to take a
                                        moment to come up with an example. Lets continue. An example for which the time
                                        complexity is worse is when all of the input data is sorted in descending order.
                                        Worst Case Time Complexity - $O$Big-O $O$ Upper-bounds Big-O $O$Upper-boundsSo
                                        we know that best case of time-complexity is when an algorithm runs is the least
                                        amount of time it could theoretically operate in. So, it follows that the
                                        worst-case time-complexity is the worst time an algorith could theoretically
                                        operate in. Now, what is an example of this. Lets take our previous example of
                                        insertion sort. Now, recall, that the type of input for the algorithm can affect
                                        its time complexity. So, I want you to take a moment to come up with an example.
                                        typeLets continue. An example for which the time complexity is worse is when all
                                        of the input data is sorted in descending order.
                                        [https://www.contextswitching.org/neuro]
                                        Neuroscience - Context Switching Neuroscience Thalamic Nuclei The thalamic
                                        nuclei are paired structures of the thalamus divided into three main groups: the
                                        lateral nuclear, medial nuclear, and anterior nuclear groups. The internal
                                        medullary lamina, a Y-shaped structure that splits these groups, is present on
                                        each side of the thalamus. A midline, thin thalamic nuclei, adjacent to the...
                                        read more Multi-Store Memory Model In neuroscience, the central executive is
                                        responsible for controlled processing and allocation of data to subsystems in
                                        working memory. Such subsystems include the visuospatial sketchpad and
                                        phonological loop, where the phonological loop can further be subdivided into
                                        the phonological store and the articulatory process. The primary functions of
                                        the central... read more Anatomy and Histology of the Hippocampus The
                                        hippocampus is distinguished externally as a layer of densely packed neurons
                                        that form a S-shaped structure and extends to the temporal lobe of the cerebral
                                        cortex. It is also a is a sub-cortical structure in the limbic lobe, and
                                        contains two parts: cornu ammonis and dentate gyrus, where the hippocampal
                                        sulcus separates both parts. The parts curve into each other and below the
                                        sulcus lies the subiculum. Since the hippocampus is a part of the allocortex or
                                        archicortex, there exists a zone that... read more Connections in the Human
                                        Structural Connectome From the human structural connectome, the authors attempt
                                        to extract architectural feautres using diffusion spectrum imaging DSI and
                                        encode the data required into triplcate as undirected, weighted network. They do
                                        so to capute as much information about possible paths which transmit as human
                                        process and preform complex behaviors... read more Topological Neuroscience One
                                        main theoretical framework that is used to model, estimate, and simulate brain
                                        networks from complex network science is graph theory. A graph being a
                                        composition of a set of intereconnected elements know as vertices and edges. The
                                        vertices in a network can represent brain areas, while edges can represent...
                                        read more Molecular Bases of Memory Formation Molecular neuroscience is an area
                                        of chemical neuroscience that studies the molecular basis of intercellular
                                        activity applied to animals' nervous systems. This area of research covers
                                        molecular neuroanatomy, mechanisms of molecular signaling in the nervous system,
                                        and the molecular basis of neuroplasticity and neurodegenerative disease, which
                                        we will focus on... read more ... And More Soon! Neuroscience - Context
                                        Switching Neuroscience - Context SwitchingNeuroscience Thalamic Nuclei The
                                        thalamic nuclei are paired structures of the thalamus divided into three main
                                        groups: the lateral nuclear, medial nuclear, and anterior nuclear groups. The
                                        internal medullary lamina, a Y-shaped structure that splits these groups, is
                                        present on each side of the thalamus. A midline, thin thalamic nuclei, adjacent
                                        to the... read more Multi-Store Memory Model In neuroscience, the central
                                        executive is responsible for controlled processing and allocation of data to
                                        subsystems in working memory. Such subsystems include the visuospatial sketchpad
                                        and phonological loop, where the phonological loop can further be subdivided
                                        into the phonological store and the articulatory process. The primary functions
                                        of the central... read more Anatomy and Histology of the Hippocampus The
                                        hippocampus is distinguished externally as a layer of densely packed neurons
                                        that form a S-shaped structure and extends to the temporal lobe of the cerebral
                                        cortex. It is also a is a sub-cortical structure in the limbic lobe, and
                                        contains two parts: cornu ammonis and dentate gyrus, where the hippocampal
                                        sulcus separates both parts. The parts curve into each other and below the
                                        sulcus lies the subiculum. Since the hippocampus is a part of the allocortex or
                                        archicortex, there exists a zone that... read more Connections in the Human
                                        Structural Connectome From the human structural connectome, the authors attempt
                                        to extract architectural feautres using diffusion spectrum imaging DSI and
                                        encode the data required into triplcate as undirected, weighted network. They do
                                        so to capute as much information about possible paths which transmit as human
                                        process and preform complex behaviors... read more Topological Neuroscience One
                                        main theoretical framework that is used to model, estimate, and simulate brain
                                        networks from complex network science is graph theory. A graph being a
                                        composition of a set of intereconnected elements know as vertices and edges. The
                                        vertices in a network can represent brain areas, while edges can represent...
                                        read more Molecular Bases of Memory Formation Molecular neuroscience is an area
                                        of chemical neuroscience that studies the molecular basis of intercellular
                                        activity applied to animals' nervous systems. This area of research covers
                                        molecular neuroanatomy, mechanisms of molecular signaling in the nervous system,
                                        and the molecular basis of neuroplasticity and neurodegenerative disease, which
                                        we will focus on... read more ... And More Soon! Neuroscience Neuroscience
                                        Neuroscience Thalamic Nuclei Thalamic Nuclei Thalamic Nuclei The thalamic nuclei
                                        are paired structures of the thalamus divided into three main groups: the
                                        lateral nuclear, medial nuclear, and anterior nuclear groups. The internal
                                        medullary lamina, a Y-shaped structure that splits these groups, is present on
                                        each side of the thalamus. A midline, thin thalamic nuclei, adjacent to the...
                                        read more The thalamic nuclei are paired structures of the thalamus divided into
                                        three main groups: the lateral nuclear, medial nuclear, and anterior nuclear
                                        groups. The internal medullary lamina, a Y-shaped structure that splits these
                                        groups, is present on each side of the thalamus. A midline, thin thalamic
                                        nuclei, adjacent to the... read more read more Multi-Store Memory Model
                                        Multi-Store Memory Model Multi-Store Memory Model In neuroscience, the central
                                        executive is responsible for controlled processing and allocation of data to
                                        subsystems in working memory. Such subsystems include the visuospatial sketchpad
                                        and phonological loop, where the phonological loop can further be subdivided
                                        into the phonological store and the articulatory process. The primary functions
                                        of the central... read more In neuroscience, the central executive is
                                        responsible for controlled processing and allocation of data to subsystems in
                                        working memory. Such subsystems include the visuospatial sketchpad and
                                        phonological loop, where the phonological loop can further be subdivided into
                                        the phonological store and the articulatory process. The primary functions of
                                        the central... read more In neuroscience, the central executive is responsible
                                        for controlled processing and allocation of data to subsystems in working
                                        memory. Such subsystems include the visuospatial sketchpad and phonological
                                        loop, where the phonological loop can further be subdivided into the
                                        phonological store and the articulatory process. The primary functions of the
                                        central... read more read more Anatomy and Histology of the Hippocampus Anatomy
                                        and Histology of the Hippocampus Anatomy and Histology of the Hippocampus The
                                        hippocampus is distinguished externally as a layer of densely packed neurons
                                        that form a S-shaped structure and extends to the temporal lobe of the cerebral
                                        cortex. It is also a is a sub-cortical structure in the limbic lobe, and
                                        contains two parts: cornu ammonis and dentate gyrus, where the hippocampal
                                        sulcus separates both parts. The parts curve into each other and below the
                                        sulcus lies the subiculum. Since the hippocampus is a part of the allocortex or
                                        archicortex, there exists a zone that... read more The hippocampus is
                                        distinguished externally as a layer of densely packed neurons that form a
                                        S-shaped structure and extends to the temporal lobe of the cerebral cortex. It
                                        is also a is a sub-cortical structure in the limbic lobe, and contains two
                                        parts: cornu ammonis and dentate gyrus, where the hippocampal sulcus separates
                                        both parts. The parts curve into each other and below the sulcus lies the
                                        subiculum. Since the hippocampus is a part of the allocortex or archicortex,
                                        there exists a zone that... read more The hippocampus is distinguished
                                        externally as a layer of densely packed neurons that form a S-shaped structure
                                        and extends to the temporal lobe of the cerebral cortex. It is also a is a
                                        sub-cortical structure in the limbic lobe, and contains two parts: cornu ammonis
                                        and dentate gyrus, where the hippocampal sulcus separates both parts. The parts
                                        curve into each other and below the sulcus lies the subiculum. Since the
                                        hippocampus is a part of the allocortex or archicortex, there exists a zone
                                        that... read more read more Connections in the Human Structural Connectome
                                        Connections in the Human Structural Connectome Connections in the Human
                                        Structural Connectome From the human structural connectome, the authors attempt
                                        to extract architectural feautres using diffusion spectrum imaging DSI and
                                        encode the data required into triplcate as undirected, weighted network. They do
                                        so to capute as much information about possible paths which transmit as human
                                        process and preform complex behaviors... read more From the human structural
                                        connectome, the authors attempt to extract architectural feautres using
                                        diffusion spectrum imaging DSI and encode the data required into triplcate as
                                        undirected, weighted network. They do so to capute as much information about
                                        possible paths which transmit as human process and preform complex behaviors...
                                        read more From the human structural connectome, the authors attempt to extract
                                        architectural feautres using diffusion spectrum imaging DSI and encode the data
                                        required into triplcate as undirected, weighted network. They do so to capute as
                                        much information about possible paths which transmit as human process and
                                        preform complex behaviors... read more read more Topological Neuroscience
                                        Topological Neuroscience Topological Neuroscience One main theoretical framework
                                        that is used to model, estimate, and simulate brain networks from complex
                                        network science is graph theory. A graph being a composition of a set of
                                        intereconnected elements know as vertices and edges. The vertices in a network
                                        can represent brain areas, while edges can represent... read more One main
                                        theoretical framework that is used to model, estimate, and simulate brain
                                        networks from complex network science is graph theory. A graph being a
                                        composition of a set of intereconnected elements know as vertices and edges. The
                                        vertices in a network can represent brain areas, while edges can represent...
                                        read more One main theoretical framework that is used to model, estimate, and
                                        simulate brain networks from complex network science is graph theory. A graph
                                        being a composition of a set of intereconnected elements know as vertices and
                                        edges. The vertices in a network can represent brain areas, while edges can
                                        represent... read more read more Molecular Bases of Memory Formation Molecular
                                        Bases of Memory Formation Molecular Bases of Memory Formation Molecular
                                        neuroscience is an area of chemical neuroscience that studies the molecular
                                        basis of intercellular activity applied to animals' nervous systems. This area
                                        of research covers molecular neuroanatomy, mechanisms of molecular signaling in
                                        the nervous system, and the molecular basis of neuroplasticity and
                                        neurodegenerative disease, which we will focus on... read more ... And More
                                        Soon! Molecular neuroscience is an area of chemical neuroscience that studies
                                        the molecular basis of intercellular activity applied to animals' nervous
                                        systems. This area of research covers molecular neuroanatomy, mechanisms of
                                        molecular signaling in the nervous system, and the molecular basis of
                                        neuroplasticity and neurodegenerative disease, which we will focus on... read
                                        more Molecular neuroscience is an area of chemical neuroscience that studies the
                                        molecular basis of intercellular activity applied to animals' nervous systems.
                                        This area of research covers molecular neuroanatomy, mechanisms of molecular
                                        signaling in the nervous system, and the molecular basis of neuroplasticity and
                                        neurodegenerative disease, which we will focus on... read more read more ... And
                                        More Soon! ... And More Soon! ... And More Soon!
                                        [https://www.contextswitching.org/tcs/multilevelcongitionforai]
                                        Multilevel Development of Cognitive Abilities for Artificial Intelligence -
                                        Context Switching Multilevel Development of Cognitive Abilities for Artificial
                                        Intelligence Motivation In biological intelliegent systems there are multiple
                                        mechanisms working in congruence on multiple levels, both at the structural and
                                        neurobiological level to develop complex cognitive abilities. What remains
                                        unknown is which mechanisms are necessary and sufficent to synthetically
                                        replicate these cognitive abilities for artificial intelligence. A
                                        neurocomputational model is offered of the devloping brain that spans the
                                        sensorimotor, cognitive, and conscious levels. We will look at how we can
                                        replicated this model artifically using known computational algorithms and data
                                        strucutres. The ending goal for the final model is to solve three tasks that
                                        increase in complexity. The first task is visual recognition, then cogintive
                                        manipulation, then maintenance of consious percepts. These tasks contain two
                                        fundemental mechainisms for the multilevel development of cognitive abilities in
                                        biolgical neural networks. The first fundemental mechanism of coginition seen in
                                        biolgical neural networks is epigensis with Hebbian learning at the local scale
                                        and with reinforcment learning at the global scale. Again, by modeling local and
                                        global scales, the hope is to better replicate biological multilevel congruence
                                        of mechanisms to acheive higher advanced cognition for artifical intelligence.
                                        Neurobiological Models Image Source: Multilevel development of cognitive
                                        abilities in an artificial neural network $^{[2]}$ Adaptive Topologies To
                                        continue. To continue. Structural Models Prelimenaries Topological Neuroscience
                                        You can read about Topological Neuroscience on my Topological Neuroscience page.
                                        Image Source: A Large-Scale Model of the Functioning Brain $^{[1]}$ System
                                        Models Reinfrocment Learning To continue. Glossary Hebbian Theory Hebbian theory
                                        attempts to explain synaptic plasticity or adaptation of brain neurons during
                                        the learning process. The neuropsychological theory introduced first by Donald
                                        Hebb, claims that the increase of synaptic efficacy is caused by the presynaptic
                                        cells repeated and persistent stimulation of a postsynaptic cell. An excerpt
                                        from Hebb's book, The Orginization of Behavior, explains his idea further: "When
                                        an axon of cell A is near enough to excite a cell B and repeatedly or
                                        persistently takes part in firing it, some growth process or metabolic change
                                        takes place in one or both cells such that A’s efficiency, as one of the cells
                                        firing B, is increased." Multilevel Development of Cognitive Abilities for
                                        Artificial Intelligence - Context Switching Multilevel Development of Cognitive
                                        Abilities for Artificial Intelligence - Context SwitchingMultilevel Development
                                        of Cognitive Abilities for Artificial Intelligence Motivation In biological
                                        intelliegent systems there are multiple mechanisms working in congruence on
                                        multiple levels, both at the structural and neurobiological level to develop
                                        complex cognitive abilities. What remains unknown is which mechanisms are
                                        necessary and sufficent to synthetically replicate these cognitive abilities for
                                        artificial intelligence. A neurocomputational model is offered of the devloping
                                        brain that spans the sensorimotor, cognitive, and conscious levels. We will look
                                        at how we can replicated this model artifically using known computational
                                        algorithms and data strucutres. The ending goal for the final model is to solve
                                        three tasks that increase in complexity. The first task is visual recognition,
                                        then cogintive manipulation, then maintenance of consious percepts. These tasks
                                        contain two fundemental mechainisms for the multilevel development of cognitive
                                        abilities in biolgical neural networks. The first fundemental mechanism of
                                        coginition seen in biolgical neural networks is epigensis with Hebbian learning
                                        at the local scale and with reinforcment learning at the global scale. Again, by
                                        modeling local and global scales, the hope is to better replicate biological
                                        multilevel congruence of mechanisms to acheive higher advanced cognition for
                                        artifical intelligence. Neurobiological Models Image Source: Multilevel
                                        development of cognitive abilities in an artificial neural network $^{[2]}$
                                        Adaptive Topologies To continue. To continue. Structural Models Prelimenaries
                                        Topological Neuroscience You can read about Topological Neuroscience on my
                                        Topological Neuroscience page. Image Source: A Large-Scale Model of the
                                        Functioning Brain $^{[1]}$ System Models Reinfrocment Learning To continue.
                                        Glossary Hebbian Theory Hebbian theory attempts to explain synaptic plasticity
                                        or adaptation of brain neurons during the learning process. The
                                        neuropsychological theory introduced first by Donald Hebb, claims that the
                                        increase of synaptic efficacy is caused by the presynaptic cells repeated and
                                        persistent stimulation of a postsynaptic cell. An excerpt from Hebb's book, The
                                        Orginization of Behavior, explains his idea further: "When an axon of cell A is
                                        near enough to excite a cell B and repeatedly or persistently takes part in
                                        firing it, some growth process or metabolic change takes place in one or both
                                        cells such that A’s efficiency, as one of the cells firing B, is increased."
                                        Multilevel Development of Cognitive Abilities for Artificial Intelligence
                                        Motivation In biological intelliegent systems there are multiple mechanisms
                                        working in congruence on multiple levels, both at the structural and
                                        neurobiological level to develop complex cognitive abilities. What remains
                                        unknown is which mechanisms are necessary and sufficent to synthetically
                                        replicate these cognitive abilities for artificial intelligence. A
                                        neurocomputational model is offered of the devloping brain that spans the
                                        sensorimotor, cognitive, and conscious levels. We will look at how we can
                                        replicated this model artifically using known computational algorithms and data
                                        strucutres. The ending goal for the final model is to solve three tasks that
                                        increase in complexity. The first task is visual recognition, then cogintive
                                        manipulation, then maintenance of consious percepts. These tasks contain two
                                        fundemental mechainisms for the multilevel development of cognitive abilities in
                                        biolgical neural networks. The first fundemental mechanism of coginition seen in
                                        biolgical neural networks is epigensis with Hebbian learning at the local scale
                                        and with reinforcment learning at the global scale. Again, by modeling local and
                                        global scales, the hope is to better replicate biological multilevel congruence
                                        of mechanisms to acheive higher advanced cognition for artifical intelligence.
                                        Neurobiological Models Image Source: Multilevel development of cognitive
                                        abilities in an artificial neural network $^{[2]}$ Adaptive Topologies To
                                        continue. To continue. Structural Models Prelimenaries Topological Neuroscience
                                        You can read about Topological Neuroscience on my Topological Neuroscience page.
                                        Image Source: A Large-Scale Model of the Functioning Brain $^{[1]}$ System
                                        Models Reinfrocment Learning To continue. Glossary Hebbian Theory Hebbian theory
                                        attempts to explain synaptic plasticity or adaptation of brain neurons during
                                        the learning process. The neuropsychological theory introduced first by Donald
                                        Hebb, claims that the increase of synaptic efficacy is caused by the presynaptic
                                        cells repeated and persistent stimulation of a postsynaptic cell. An excerpt
                                        from Hebb's book, The Orginization of Behavior, explains his idea further: "When
                                        an axon of cell A is near enough to excite a cell B and repeatedly or
                                        persistently takes part in firing it, some growth process or metabolic change
                                        takes place in one or both cells such that A’s efficiency, as one of the cells
                                        firing B, is increased." Multilevel Development of Cognitive Abilities for
                                        Artificial Intelligence Motivation In biological intelliegent systems there are
                                        multiple mechanisms working in congruence on multiple levels, both at the
                                        structural and neurobiological level to develop complex cognitive abilities.
                                        What remains unknown is which mechanisms are necessary and sufficent to
                                        synthetically replicate these cognitive abilities for artificial intelligence. A
                                        neurocomputational model is offered of the devloping brain that spans the
                                        sensorimotor, cognitive, and conscious levels. We will look at how we can
                                        replicated this model artifically using known computational algorithms and data
                                        strucutres. The ending goal for the final model is to solve three tasks that
                                        increase in complexity. The first task is visual recognition, then cogintive
                                        manipulation, then maintenance of consious percepts. These tasks contain two
                                        fundemental mechainisms for the multilevel development of cognitive abilities in
                                        biolgical neural networks. The first fundemental mechanism of coginition seen in
                                        biolgical neural networks is epigensis with Hebbian learning at the local scale
                                        and with reinforcment learning at the global scale. Again, by modeling local and
                                        global scales, the hope is to better replicate biological multilevel congruence
                                        of mechanisms to acheive higher advanced cognition for artifical intelligence.
                                        Neurobiological Models Image Source: Multilevel development of cognitive
                                        abilities in an artificial neural network $^{[2]}$ Adaptive Topologies To
                                        continue. To continue. Structural Models Prelimenaries Topological Neuroscience
                                        You can read about Topological Neuroscience on my Topological Neuroscience page.
                                        Image Source: A Large-Scale Model of the Functioning Brain $^{[1]}$ System
                                        Models Reinfrocment Learning To continue. Glossary Hebbian Theory Hebbian theory
                                        attempts to explain synaptic plasticity or adaptation of brain neurons during
                                        the learning process. The neuropsychological theory introduced first by Donald
                                        Hebb, claims that the increase of synaptic efficacy is caused by the presynaptic
                                        cells repeated and persistent stimulation of a postsynaptic cell. An excerpt
                                        from Hebb's book, The Orginization of Behavior, explains his idea further: "When
                                        an axon of cell A is near enough to excite a cell B and repeatedly or
                                        persistently takes part in firing it, some growth process or metabolic change
                                        takes place in one or both cells such that A’s efficiency, as one of the cells
                                        firing B, is increased." Multilevel Development of Cognitive Abilities for
                                        Artificial IntelligenceMotivation In biological intelliegent systems there are
                                        multiple mechanisms working in congruence on multiple levels, both at the
                                        structural and neurobiological level to develop complex cognitive abilities.
                                        What remains unknown is which mechanisms are necessary and sufficent to
                                        synthetically replicate these cognitive abilities for artificial intelligence. A
                                        neurocomputational model is offered of the devloping brain that spans the
                                        sensorimotor, cognitive, and conscious levels. We will look at how we can
                                        replicated this model artifically using known computational algorithms and data
                                        strucutres. The ending goal for the final model is to solve three tasks that
                                        increase in complexity. The first task is visual recognition, then cogintive
                                        manipulation, then maintenance of consious percepts. These tasks contain two
                                        fundemental mechainisms for the multilevel development of cognitive abilities in
                                        biolgical neural networks. The first fundemental mechanism of coginition seen in
                                        biolgical neural networks is epigensis with Hebbian learning at the local scale
                                        and with reinforcment learning at the global scale. Again, by modeling local and
                                        global scales, the hope is to better replicate biological multilevel congruence
                                        of mechanisms to acheive higher advanced cognition for artifical intelligence.
                                        MotivationIn biological intelliegent systems there are multiple mechanisms
                                        working in congruence on multiple levels, both at the structural and
                                        neurobiological level to develop complex cognitive abilities. What remains
                                        unknown is which mechanisms are necessary and sufficent to synthetically
                                        replicate these cognitive abilities for artificial intelligence. A
                                        neurocomputational model is offered of the devloping brain that spans the
                                        sensorimotor, cognitive, and conscious levels. We will look at how we can
                                        replicated this model artifically using known computational algorithms and data
                                        strucutres. The ending goal for the final model is to solve three tasks that
                                        increase in complexity. The first task is visual recognition, then cogintive
                                        manipulation, then maintenance of consious percepts. These tasks contain two
                                        fundemental mechainisms for the multilevel development of cognitive abilities in
                                        biolgical neural networks. The first fundemental mechanism of coginition seen in
                                        biolgical neural networks is epigensis with Hebbian learning at the local scale
                                        and with reinforcment learning at the global scale. Again, by modeling local and
                                        global scales, the hope is to better replicate biological multilevel congruence
                                        of mechanisms to acheive higher advanced cognition for artifical intelligence.
                                        Hebbian learningNeurobiological Models Image Source: Multilevel development of
                                        cognitive abilities in an artificial neural network $^{[2]}$ Adaptive Topologies
                                        To continue. To continue. Neurobiological ModelsImage Source: Multilevel
                                        development of cognitive abilities in an artificial neural network $^{[2]}$
                                        Multilevel development of cognitive abilities in an artificial neural network
                                        $^{[2]}$ Multilevel development of cognitive abilities in an artificial neural
                                        network Adaptive Topologies To continue. Adaptive TopologiesTo continue. To
                                        continue. Structural Models Prelimenaries Topological Neuroscience You can read
                                        about Topological Neuroscience on my Topological Neuroscience page. Image
                                        Source: A Large-Scale Model of the Functioning Brain $^{[1]}$ Structural
                                        ModelsPrelimenaries Topological Neuroscience You can read about Topological
                                        Neuroscience on my Topological Neuroscience page. PrelimenariesTopological
                                        Neuroscience You can read about Topological Neuroscience on my Topological
                                        Neuroscience page. Topological NeuroscienceYou can read about Topological
                                        Neuroscience on my Topological Neuroscience page. Topological NeuroscienceImage
                                        Source: A Large-Scale Model of the Functioning Brain $^{[1]}$ A Large-Scale
                                        Model of the Functioning Brain $^{[1]}$ A Large-Scale Model of the Functioning
                                        Brain System Models Reinfrocment Learning To continue. System ModelsReinfrocment
                                        Learning To continue. Reinfrocment LearningTo continue. Glossary Hebbian Theory
                                        Hebbian theory attempts to explain synaptic plasticity or adaptation of brain
                                        neurons during the learning process. The neuropsychological theory introduced
                                        first by Donald Hebb, claims that the increase of synaptic efficacy is caused by
                                        the presynaptic cells repeated and persistent stimulation of a postsynaptic
                                        cell. An excerpt from Hebb's book, The Orginization of Behavior, explains his
                                        idea further: "When an axon of cell A is near enough to excite a cell B and
                                        repeatedly or persistently takes part in firing it, some growth process or
                                        metabolic change takes place in one or both cells such that A’s efficiency, as
                                        one of the cells firing B, is increased." GlossaryHebbian Theory Hebbian theory
                                        attempts to explain synaptic plasticity or adaptation of brain neurons during
                                        the learning process. The neuropsychological theory introduced first by Donald
                                        Hebb, claims that the increase of synaptic efficacy is caused by the presynaptic
                                        cells repeated and persistent stimulation of a postsynaptic cell. An excerpt
                                        from Hebb's book, The Orginization of Behavior, explains his idea further: "When
                                        an axon of cell A is near enough to excite a cell B and repeatedly or
                                        persistently takes part in firing it, some growth process or metabolic change
                                        takes place in one or both cells such that A’s efficiency, as one of the cells
                                        firing B, is increased." Hebbian TheoryHebbian theory attempts to explain
                                        synaptic plasticity or adaptation of brain neurons during the learning process.
                                        The neuropsychological theory introduced first by Donald Hebb, claims that the
                                        increase of synaptic efficacy is caused by the presynaptic cells repeated and
                                        persistent stimulation of a postsynaptic cell. An excerpt from Hebb's book, The
                                        Orginization of Behavior, explains his idea further: "When an axon of cell A is
                                        near enough to excite a cell B and repeatedly or persistently takes part in
                                        firing it, some growth process or metabolic change takes place in one or both
                                        cells such that A’s efficiency, as one of the cells firing B, is increased."
                                        [https://www.contextswitching.org/tcs/quantumcomputingtheory/]
                                        Quantum Computing Theory - Context Switching Quantum Computing Theory
                                        Introduction Quantum Computing Theory is a field of computer science that uses
                                        the principles of quantum mechanics, mathematics, and computer science. By
                                        borrowing concepts from each field scientists can rigorously define both a broad
                                        and narrow theoretical model of a quantum computer, and later apply it to the
                                        real world. These theoretical models, such as the result of a quantum system
                                        manipulating subatomic particles, the theoretical circuits quantum computers
                                        implement to perform larger operations, and how to optimize the resource
                                        complexity for quantum systems, are just a few of the fundamental concepts in
                                        quantum computing theory. 1-Qubit Qubit A qubit, short for quantum bit, is a
                                        two-level quantum system and is a part of two-dimensional Hilbert space $H_{2}$,
                                        where Hilbert space $H$ is nondenumerable infinite complex vector space. The
                                        two-dimensional complex vector space $H_{2}$ comes with a fixed orthonormal
                                        basis states $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is
                                        in the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis states,
                                        denoted with Dirac notation. The states of the quantum system or qubit can be
                                        denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This vector has a
                                        unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and
                                        $|\beta|^{2}$ are the probabilities of the system being in the representative
                                        states. Meaning that the probabilities that when the qubit is measured will give
                                        a state $0$ or $1$ in this two-level quantum system. We will go more in depth
                                        into probabilites here soon. First, however, we will look at formal definition
                                        of the inner dot product of some given some qubit $\theta$. For $|\theta\rangle$
                                        the unit length is equivalant to its inner product. Where, for ket
                                        $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex
                                        numbers and both have two real numbers. The inner dot product is
                                        $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                        formulated as:
                                        $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                        For some quantum state for the qubit $\psi$ can be defined as:
                                        $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$ $=\langle
                                        v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where, squaring our
                                        projections, $\langle v|\psi\rangle$ and $\langle h|\psi\rangle$, onto axis an
                                        axis gives us our respective probabilites for $|v\rangle$ and $|h\rangle$
                                        respectfully. An example of a qubit is the spin of an electron. The two levels
                                        of this quibit are spin up or spin down. What differs from a classical system is
                                        that quantum mechanics allows for the qubit to be in a coherent superposition of
                                        both states simultaneously. Measuring a qubit in a basis gives a projective
                                        measurement of a qubit of state $\phi$ in its computational basis can be
                                        expressed as a linear combination of state vectors, such as:
                                        $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured in a
                                        basis, collapses the qubit to either the quantum state $|0\rangle$ or
                                        $|1\rangle$ given by the respective norm-square of the probability amplitudes
                                        $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch Sphere We can use a
                                        bloch ball or sphere to help us visualize sping down $0$ and spin up $1$ of a
                                        single qubit. The bloch spehere has a radius of 1, meaning that $|0\rangle$
                                        corresponds to $(x,y,z)$ point $(x,y,1)$ and $|1\rangle$ corresponds to
                                        $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows a spin up or sping down.
                                        Superposition In classical computing, states $0$ and $1$ would be the only
                                        states that exist for the bit. However, in quantum mechanics a quibit can be
                                        both in the state of $|0\rangle$ and $|1\rangle$. This is what gives quantum
                                        computers more processing power, as a single qubit can be in more states and
                                        therefore represent more information than a single classical bit. This means
                                        that the qubit can have an $80$% of being in state $|0\rangle$ and $20$% of
                                        being in state $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of
                                        being in state $|1\rangle$. Unlike a classical bit where there is either a
                                        $100$% of the classical bit being in state $0$ and $0$% of being a $1$ or a $0$%
                                        of the classical bit being a $0$ and $100$% of being a $1$. To allow for the
                                        qubit to be in superposition, we need to levearage Hilbert Space. Again, Hilbert
                                        space is represented using complex vector space. We use complex vector space,
                                        because it is the easiest way for the math to work. Let's represent this qubit
                                        in superposition as a vector using dirac notation. For the qubit have, for
                                        example, a $50$% of being in state $|0\rangle$ and $50$% of being in state
                                        $|1\rangle$, the vector should look like:
                                        $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$ are
                                        both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$. Pauli Matrices
                                        Pauli matrices have a core importance in quantum physics, and when combinex with
                                        an identity matrix, pauli matrices form a basis for all single quantum gates.
                                        Pauli matrices are defined as: $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0
                                        \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                        $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is usally
                                        refered to as the NOT gate and can be written as $X$. The NOT gate inverts
                                        $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$ are phase shifting
                                        gates. Multiple Qubits Quantum Registers A system that contains more than one
                                        qubit is represented in a quantum register. A system with two qubits is
                                        represented using a four-dimensional Hilbert space $H_{4} = H_{2} \otimes
                                        H_{2}$. The orthonormal basis is $\left\{|0\rangle |0\rangle, |0\rangle
                                        |1\rangle, |1\rangle |0\rangle, |1\rangle |1\rangle\ \right\}$. $|0\rangle
                                        |0\rangle$ can be more succinctly written as $|00\rangle$. The same is true for
                                        $|0\rangle |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a
                                        part from four-dimensional Hilbert space is a unit-length vector:
                                        $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$ Again,
                                        as it is with a single qubit, it is required that $|c_{0}|^{2} + |c_{1}|^{2} +
                                        |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit system will give
                                        $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2,
                                        |c_{3}|^2$, respectively. If we want to observe one of the qubits, then the
                                        standard rules of probabilities apply. It is important to note that the tensor
                                        product of these vectors does not commute. Meaning, that $|0\rangle|1\rangle
                                        \neq |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                        individually. Quantum Entanglement Maximally Entangled Bell States Bell states
                                        or Einstein, Podolski and Rosen pairs are the maximally entangled quantum states
                                        of a qubit system such that a quantum mechanical system is composed of two
                                        interacting two-level subsystems. The 4 types of maximially entangled Bell
                                        states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments The
                                        orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                        $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                        $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$ basis to
                                        the $X$ bases, we need to apply a Hadmard gate to the state, allowing for the
                                        state to be halfway: $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ Y
                                        Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway we need
                                        to apply $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                        -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Z Bases Measurment No transformations needed, since we measure in
                                        the "normal" bases. Operators Unitary Operator All quantum gates are unitary,
                                        which we will define in the following. Let us use the basis measurment of $Z$
                                        with the coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                        \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                        operation on a quibit, called an unary quantum gate, is a unitary mapping $U:
                                        H_{2} \rightarrow H_{2}$ with the following defining linear operation:
                                        $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle \mapsto c|0\rangle +
                                        d|1\rangle$ An important aspect of all quantum gates is that they are unitary.
                                        Meaning, that for some given matrix operation $U$, defined as: $\begin{pmatrix}
                                        a & b \\ c & d \end{pmatrix}$ it is neccesary that this matrix is unitary in
                                        order to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                        unitary and valid, the following equivalency must be true: $UU^{\dagger}=I$
                                        where $U^{\dagger}$ represents the conjugate transpose and $I$ is the identity
                                        matrix. Another important qualtiy of an unitary matrix is: $U^{\dagger}=U^{-1}$
                                        Represented as matrices, we get: $\begin{pmatrix} a & b \\ c & d \end{pmatrix}
                                        \begin{pmatrix} a^* & b^* \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\
                                        0 & 1 \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                        denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                        d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation $a^{*}$
                                        stands for the complex conjugate of the complex number $a$. The complex
                                        conjugate of a complex number is the number with an equal real part and an
                                        imaginary part equal in magnitude, but opposite in sign of the complex number.
                                        The mapping of for unary quantum operator, when represented in a quantum circut,
                                        can be a quantum gate. Where the output of the quantum gate must have the same
                                        dimensionality as its input. So, $U: H_{n} \rightarrow H_{n}$, where $n$ is the
                                        number of dimensions of $H$. Hermitian Operator A unitary operator is Hermitian
                                        if: $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                        matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian matrix is a
                                        special case of a unitary matrix, where all Hermitian operators or unitary
                                        operators, but not all unitary operators, are Hermitian. Natural Operator An
                                        Hermitian operator is Natural if: $U^{\dagger}U=UU^{\dagger}$, where
                                        $U^{\dagger}$ is the conjugate transpose of $U$. the operator has spectural
                                        decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum Algorithms
                                        Quantum Parallelism Suppose we want to evaluate a function $f(x)$, where the
                                        function $f$ expresses some computation or algorithm. A use case for quantum
                                        parallelism is to evaluate $f(x)$ with many different values for the output of
                                        the computation or algorithm on the input $x$ simultaneously. In essence, we can
                                        evaluate many different values of $x$ on $f$ in parallel by exploiting quantum
                                        effects. This quantum effect exploit feature is fundamental in many quantum
                                        algorithms. To continue, we will look at how quantum parallelism works. Consider
                                        the one-bit domain and range function $f(x):\{0,1\}\rightarrow\{0,1\}$. To
                                        compute this function $f$ on a quantum computer, we will use a two-qubit quantum
                                        computer with the starting state $|x,y\rangle$. The transformation on the domain
                                        or 'data' register to the range or 'target' register of this initial two qubit
                                        state is described by the following unitary function:
                                        $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                        represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts on $y$,
                                        and its value is $0$ then the value of the second qubit in the 'target' register
                                        is the value $f(x)$, given whatever function $f$ represents. The functions
                                        effect on $x$ is arbritrary for now. The final collapsed state $|\psi\rangle$ is
                                        an element of the set of final states or 'target' register $|x,y\oplus
                                        f(x)\rangle$, which again is given by the unitary transformation $U_{f}$ on the
                                        start state $|x,y\rangle$. Given the input $q_{0}=x=|0\rangle$, we will apply
                                        the Hadmard gate $H$ to $x$, such that now:
                                        $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting state
                                        is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the resulting new
                                        state is not apart of the starting computational basis $\{0,1\}$. Next, the
                                        unitary function or blackbox computation/algorithm $U_{f}$ can be applied to the
                                        current 'data' register. The resulting mapping of the unitary function $U_{f}$
                                        is: $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                        f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                        $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                        Meaning that the final resulting state for a two-qubit quantum computer has a
                                        $50$% chance of being $|0,f(0)\rangle$ and $50$% of being $.5|1,f(1)\rangle$.
                                        Given in the same form as the range for $U_{f}$ given above: $|x,y'\rangle$,
                                        where $y'=y\oplus f(x)$ All of this means that the information given by the
                                        mapping for $f(0)$ and $f(1)$ was simultaneously evaluated by applying
                                        superposition and the unitary function on the starting 'data' register. Thus,
                                        $f(x)$ has been computed for two values of $x$ in parellel. The resulting set of
                                        all possible states computed in parallel is given by the resulting 'target'
                                        register is given by quantum exploitation and aptly named 'quantum parallelism'.
                                        Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$ values
                                        of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit system such that
                                        $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply the NOT gate to $y$,
                                        giving the what will be the start state $|\psi_{0}\rangle=|01\rangle$. Next, we
                                        will apply a Hadmard gates to $x$ and $y$ individually, yielding the state:
                                        $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        where for state $|\psi_{1}\rangle$: $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                        $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary unitary
                                        function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                        $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        From this, we can then say that if $f(0)=f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$. If
                                        $f(0)=f(1)$, the resulting state is:
                                        $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$
                                        $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0) \oplus
                                        f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus, $f(0)$ interfers
                                        with $f(1)$ when we simultaneously evalute $f(x)$ with quantum parallelism.
                                        Grovers Search Algorithm The authors describe the process for Grovers Search
                                        Algorithm in the following sequential two main steps: Hadmard transformation and
                                        Grover iteration or Grover operator $G$. Hadmard Transformation The Hadmard
                                        transform puts the qubits of the quantum computer into equal superposition
                                        states, defined as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                        \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm implements a
                                        repeated quantum subroutine called Grover iteration or operator, denoted as $G$.
                                        This quantum iteration can be broken up in four steps: Apply oracle $O$ Apply
                                        Hadmard transform $H^{\otimes{n}}$ Apply conditional phase shift on quantum
                                        register, such that every quantum basis state except $|r\rangle$ is phased
                                        shifted $-1$. Meaning that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1
                                        \sin \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                        \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks $|w\rangle$
                                        only. Then, reflect the state around $|s\rangle$ using gate $R_{s}$, such that
                                        moving our state towards $|w\rangle$. Lastly, apply the Hadmard transform
                                        $H^{\otimes{n}}$ Where combined steps of 2, 3, 4, or Grovers iteration without
                                        the oracle step can be written as: $H^{\otimes n}(2|0\rangle\langle 0|-I)
                                        H^{\otimes n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly
                                        eighted superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                        Thus, including the oracle step now, Grovers iteration $G$ as a whole can be
                                        written, more generically for a given quantum state $\psi$ as
                                        $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image Source: $[1]$
                                        To continue. Quantum Computational Theory Quantum Automata Theory Quantum Turing
                                        Machine QTM $\delta$ Function A Quantum Turing Machine QTM can be expressed
                                        similarly to a traditional Turing Machine TM with all components reformulated
                                        canonically except for the transition function $\delta$. Below, is the formal
                                        definition of a QTM. Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma,
                                        \delta, q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite
                                        sets and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                        blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$
                                        and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way infinite, with
                                        squares indexed by the set of integers $\mathbb{Z}$ $\delta$ is a transition
                                        function described as $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times
                                        \Gamma \times \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state
                                        $q_{accept} \in Q$ is the accept state $q_{reject} \in Q$ is the reject state,
                                        where $q_{reject} \neq q_{accept}$ Image Source:
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                        Quantum Networks Superdense Coding and Quantum Teleportation Superdense Coding
                                        Given Alice wants to send classical information to Bob, quantum entanglement $n$
                                        qubits can store $2n$ qubits total of information. Say Alice needs to send one
                                        qubit of infromation, they can do so but needs Bob to already share a second
                                        qubit. So, given that Alice and Bob already share a pair of entangled qubits in
                                        state $|\Phi^{+}\rangle$, defined as: (Alice) --
                                        $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to send:
                                        $00$: Alice does nothing to their qubit, so the qubit is still in state:
                                        $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice applies
                                        $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that Bob has
                                        both qubits in one of four Bell basis, Bob will know what the qubit Alice wants
                                        to send by measuring the two qubits and seeing what state they are in. This is
                                        called a Bell measurment. If we then extrapolate this method, then if Alice and
                                        Bob want to share $n$ pairs of entangled qubits they can do so with $2n$ quibits
                                        in total. Quantum Teleportation To continue. Glossary Hilbert Space Hilbert
                                        Space is a nondenumerable infinite complex vector space. Complex space, being a
                                        collection of complex numbers $\mathbb{C}$ with an added structure. The infinite
                                        dimensions of Hilbert Space represents a continious spectra of alternative
                                        physical states. Alternative physical states, for example, being the position
                                        (coordinates) or momentum of a particle. Probabilistic Systems Pure States Mixed
                                        States The nature of a probabilistic system is that we do not know for certain
                                        the state of the system. However, we do know the probability distribution of the
                                        states. Our probabilistic distribution sums up to 1. The notation can be written
                                        as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands for the
                                        state the system is in with probability $p_{i}$. Where $p_{i} \ge 0$ and $p_{1}
                                        + ... + p_{n} = 1$. Our distribution defined above is a mixed state. Where
                                        $x_{i}$ is a pure state. It is important to note that our distribution is not an
                                        expected value or an average of the mixed state, but rather represents only the
                                        probability distribution for all states $x_{i}$. Quantum Mechanics Fun From here
                                        on out we used will use a Hilbert space formalism of quantum mechanics where the
                                        representation of quantum mechanical systems are represented as state vectors.
                                        We use this representation because state vectors are mathematically simpler that
                                        the more general ones. The quantum mechanical description of a physical system
                                        resembles the probabilistic systems we mentioned earlier: $p_{1}[x_{1}] +
                                        p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can
                                        be shown as a unit-length vector in $n$-dimensional complex vector space. We
                                        define this state space with $H_{n}$. Using ket-notation, which is a part of
                                        Dirac notation, we define our state space $H_{n}$ as an orthonormal basis
                                        $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We can now write
                                        any state of the quantum system as: $\alpha_{1}\left| x_{1} \right> +
                                        \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left| x_{n} \right>$ Here,
                                        $\alpha_{i}$ are probabilistic amplitudes. Finally, to meet our requirements
                                        defining our state space $H_{n}$ as unit-length we say that $|\alpha_{1}|^{2} +
                                        |\alpha_{2}|^{2} + ... + |\alpha_{n}|^{2} = 1$. This concludes most of the
                                        information neccesary for this page. However, if you are having fun with quantum
                                        mechanics, feel free to read more on my Quantum Mechanics page. Quantum
                                        Computing Theory - Context Switching Quantum Computing Theory - Context
                                        SwitchingQuantum Computing Theory Introduction Quantum Computing Theory is a
                                        field of computer science that uses the principles of quantum mechanics,
                                        mathematics, and computer science. By borrowing concepts from each field
                                        scientists can rigorously define both a broad and narrow theoretical model of a
                                        quantum computer, and later apply it to the real world. These theoretical
                                        models, such as the result of a quantum system manipulating subatomic particles,
                                        the theoretical circuits quantum computers implement to perform larger
                                        operations, and how to optimize the resource complexity for quantum systems, are
                                        just a few of the fundamental concepts in quantum computing theory. 1-Qubit
                                        Qubit A qubit, short for quantum bit, is a two-level quantum system and is a
                                        part of two-dimensional Hilbert space $H_{2}$, where Hilbert space $H$ is
                                        nondenumerable infinite complex vector space. The two-dimensional complex vector
                                        space $H_{2}$ comes with a fixed orthonormal basis states
                                        $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is in the $Z$
                                        basis. States $|0\rangle$ and $|1\rangle$ are the basis states, denoted with
                                        Dirac notation. The states of the quantum system or qubit can be denoted as a
                                        vector like: $\alpha|0\rangle + \beta|1\rangle$ This vector has a unit length of
                                        $1$, so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and $|\beta|^{2}$ are the
                                        probabilities of the system being in the representative states. Meaning that the
                                        probabilities that when the qubit is measured will give a state $0$ or $1$ in
                                        this two-level quantum system. We will go more in depth into probabilites here
                                        soon. First, however, we will look at formal definition of the inner dot product
                                        of some given some qubit $\theta$. For $|\theta\rangle$ the unit length is
                                        equivalant to its inner product. Where, for ket $|\theta\rangle$ and bra
                                        $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex numbers and both have two
                                        real numbers. The inner dot product is
                                        $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                        formulated as:
                                        $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                        For some quantum state for the qubit $\psi$ can be defined as:
                                        $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$ $=\langle
                                        v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where, squaring our
                                        projections, $\langle v|\psi\rangle$ and $\langle h|\psi\rangle$, onto axis an
                                        axis gives us our respective probabilites for $|v\rangle$ and $|h\rangle$
                                        respectfully. An example of a qubit is the spin of an electron. The two levels
                                        of this quibit are spin up or spin down. What differs from a classical system is
                                        that quantum mechanics allows for the qubit to be in a coherent superposition of
                                        both states simultaneously. Measuring a qubit in a basis gives a projective
                                        measurement of a qubit of state $\phi$ in its computational basis can be
                                        expressed as a linear combination of state vectors, such as:
                                        $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured in a
                                        basis, collapses the qubit to either the quantum state $|0\rangle$ or
                                        $|1\rangle$ given by the respective norm-square of the probability amplitudes
                                        $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch Sphere We can use a
                                        bloch ball or sphere to help us visualize sping down $0$ and spin up $1$ of a
                                        single qubit. The bloch spehere has a radius of 1, meaning that $|0\rangle$
                                        corresponds to $(x,y,z)$ point $(x,y,1)$ and $|1\rangle$ corresponds to
                                        $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows a spin up or sping down.
                                        Superposition In classical computing, states $0$ and $1$ would be the only
                                        states that exist for the bit. However, in quantum mechanics a quibit can be
                                        both in the state of $|0\rangle$ and $|1\rangle$. This is what gives quantum
                                        computers more processing power, as a single qubit can be in more states and
                                        therefore represent more information than a single classical bit. This means
                                        that the qubit can have an $80$% of being in state $|0\rangle$ and $20$% of
                                        being in state $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of
                                        being in state $|1\rangle$. Unlike a classical bit where there is either a
                                        $100$% of the classical bit being in state $0$ and $0$% of being a $1$ or a $0$%
                                        of the classical bit being a $0$ and $100$% of being a $1$. To allow for the
                                        qubit to be in superposition, we need to levearage Hilbert Space. Again, Hilbert
                                        space is represented using complex vector space. We use complex vector space,
                                        because it is the easiest way for the math to work. Let's represent this qubit
                                        in superposition as a vector using dirac notation. For the qubit have, for
                                        example, a $50$% of being in state $|0\rangle$ and $50$% of being in state
                                        $|1\rangle$, the vector should look like:
                                        $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$ are
                                        both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$. Pauli Matrices
                                        Pauli matrices have a core importance in quantum physics, and when combinex with
                                        an identity matrix, pauli matrices form a basis for all single quantum gates.
                                        Pauli matrices are defined as: $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0
                                        \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                        $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is usally
                                        refered to as the NOT gate and can be written as $X$. The NOT gate inverts
                                        $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$ are phase shifting
                                        gates. Multiple Qubits Quantum Registers A system that contains more than one
                                        qubit is represented in a quantum register. A system with two qubits is
                                        represented using a four-dimensional Hilbert space $H_{4} = H_{2} \otimes
                                        H_{2}$. The orthonormal basis is $\left\{|0\rangle |0\rangle, |0\rangle
                                        |1\rangle, |1\rangle |0\rangle, |1\rangle |1\rangle\ \right\}$. $|0\rangle
                                        |0\rangle$ can be more succinctly written as $|00\rangle$. The same is true for
                                        $|0\rangle |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a
                                        part from four-dimensional Hilbert space is a unit-length vector:
                                        $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$ Again,
                                        as it is with a single qubit, it is required that $|c_{0}|^{2} + |c_{1}|^{2} +
                                        |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit system will give
                                        $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2,
                                        |c_{3}|^2$, respectively. If we want to observe one of the qubits, then the
                                        standard rules of probabilities apply. It is important to note that the tensor
                                        product of these vectors does not commute. Meaning, that $|0\rangle|1\rangle
                                        \neq |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                        individually. Quantum Entanglement Maximally Entangled Bell States Bell states
                                        or Einstein, Podolski and Rosen pairs are the maximally entangled quantum states
                                        of a qubit system such that a quantum mechanical system is composed of two
                                        interacting two-level subsystems. The 4 types of maximially entangled Bell
                                        states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments The
                                        orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                        $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                        $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$ basis to
                                        the $X$ bases, we need to apply a Hadmard gate to the state, allowing for the
                                        state to be halfway: $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ Y
                                        Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway we need
                                        to apply $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                        -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Z Bases Measurment No transformations needed, since we measure in
                                        the "normal" bases. Operators Unitary Operator All quantum gates are unitary,
                                        which we will define in the following. Let us use the basis measurment of $Z$
                                        with the coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                        \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                        operation on a quibit, called an unary quantum gate, is a unitary mapping $U:
                                        H_{2} \rightarrow H_{2}$ with the following defining linear operation:
                                        $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle \mapsto c|0\rangle +
                                        d|1\rangle$ An important aspect of all quantum gates is that they are unitary.
                                        Meaning, that for some given matrix operation $U$, defined as: $\begin{pmatrix}
                                        a & b \\ c & d \end{pmatrix}$ it is neccesary that this matrix is unitary in
                                        order to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                        unitary and valid, the following equivalency must be true: $UU^{\dagger}=I$
                                        where $U^{\dagger}$ represents the conjugate transpose and $I$ is the identity
                                        matrix. Another important qualtiy of an unitary matrix is: $U^{\dagger}=U^{-1}$
                                        Represented as matrices, we get: $\begin{pmatrix} a & b \\ c & d \end{pmatrix}
                                        \begin{pmatrix} a^* & b^* \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\
                                        0 & 1 \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                        denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                        d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation $a^{*}$
                                        stands for the complex conjugate of the complex number $a$. The complex
                                        conjugate of a complex number is the number with an equal real part and an
                                        imaginary part equal in magnitude, but opposite in sign of the complex number.
                                        The mapping of for unary quantum operator, when represented in a quantum circut,
                                        can be a quantum gate. Where the output of the quantum gate must have the same
                                        dimensionality as its input. So, $U: H_{n} \rightarrow H_{n}$, where $n$ is the
                                        number of dimensions of $H$. Hermitian Operator A unitary operator is Hermitian
                                        if: $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                        matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian matrix is a
                                        special case of a unitary matrix, where all Hermitian operators or unitary
                                        operators, but not all unitary operators, are Hermitian. Natural Operator An
                                        Hermitian operator is Natural if: $U^{\dagger}U=UU^{\dagger}$, where
                                        $U^{\dagger}$ is the conjugate transpose of $U$. the operator has spectural
                                        decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum Algorithms
                                        Quantum Parallelism Suppose we want to evaluate a function $f(x)$, where the
                                        function $f$ expresses some computation or algorithm. A use case for quantum
                                        parallelism is to evaluate $f(x)$ with many different values for the output of
                                        the computation or algorithm on the input $x$ simultaneously. In essence, we can
                                        evaluate many different values of $x$ on $f$ in parallel by exploiting quantum
                                        effects. This quantum effect exploit feature is fundamental in many quantum
                                        algorithms. To continue, we will look at how quantum parallelism works. Consider
                                        the one-bit domain and range function $f(x):\{0,1\}\rightarrow\{0,1\}$. To
                                        compute this function $f$ on a quantum computer, we will use a two-qubit quantum
                                        computer with the starting state $|x,y\rangle$. The transformation on the domain
                                        or 'data' register to the range or 'target' register of this initial two qubit
                                        state is described by the following unitary function:
                                        $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                        represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts on $y$,
                                        and its value is $0$ then the value of the second qubit in the 'target' register
                                        is the value $f(x)$, given whatever function $f$ represents. The functions
                                        effect on $x$ is arbritrary for now. The final collapsed state $|\psi\rangle$ is
                                        an element of the set of final states or 'target' register $|x,y\oplus
                                        f(x)\rangle$, which again is given by the unitary transformation $U_{f}$ on the
                                        start state $|x,y\rangle$. Given the input $q_{0}=x=|0\rangle$, we will apply
                                        the Hadmard gate $H$ to $x$, such that now:
                                        $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting state
                                        is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the resulting new
                                        state is not apart of the starting computational basis $\{0,1\}$. Next, the
                                        unitary function or blackbox computation/algorithm $U_{f}$ can be applied to the
                                        current 'data' register. The resulting mapping of the unitary function $U_{f}$
                                        is: $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                        f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                        $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                        Meaning that the final resulting state for a two-qubit quantum computer has a
                                        $50$% chance of being $|0,f(0)\rangle$ and $50$% of being $.5|1,f(1)\rangle$.
                                        Given in the same form as the range for $U_{f}$ given above: $|x,y'\rangle$,
                                        where $y'=y\oplus f(x)$ All of this means that the information given by the
                                        mapping for $f(0)$ and $f(1)$ was simultaneously evaluated by applying
                                        superposition and the unitary function on the starting 'data' register. Thus,
                                        $f(x)$ has been computed for two values of $x$ in parellel. The resulting set of
                                        all possible states computed in parallel is given by the resulting 'target'
                                        register is given by quantum exploitation and aptly named 'quantum parallelism'.
                                        Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$ values
                                        of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit system such that
                                        $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply the NOT gate to $y$,
                                        giving the what will be the start state $|\psi_{0}\rangle=|01\rangle$. Next, we
                                        will apply a Hadmard gates to $x$ and $y$ individually, yielding the state:
                                        $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        where for state $|\psi_{1}\rangle$: $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                        $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary unitary
                                        function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                        $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        From this, we can then say that if $f(0)=f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$. If
                                        $f(0)=f(1)$, the resulting state is:
                                        $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$
                                        $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0) \oplus
                                        f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus, $f(0)$ interfers
                                        with $f(1)$ when we simultaneously evalute $f(x)$ with quantum parallelism.
                                        Grovers Search Algorithm The authors describe the process for Grovers Search
                                        Algorithm in the following sequential two main steps: Hadmard transformation and
                                        Grover iteration or Grover operator $G$. Hadmard Transformation The Hadmard
                                        transform puts the qubits of the quantum computer into equal superposition
                                        states, defined as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                        \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm implements a
                                        repeated quantum subroutine called Grover iteration or operator, denoted as $G$.
                                        This quantum iteration can be broken up in four steps: Apply oracle $O$ Apply
                                        Hadmard transform $H^{\otimes{n}}$ Apply conditional phase shift on quantum
                                        register, such that every quantum basis state except $|r\rangle$ is phased
                                        shifted $-1$. Meaning that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1
                                        \sin \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                        \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks $|w\rangle$
                                        only. Then, reflect the state around $|s\rangle$ using gate $R_{s}$, such that
                                        moving our state towards $|w\rangle$. Lastly, apply the Hadmard transform
                                        $H^{\otimes{n}}$ Where combined steps of 2, 3, 4, or Grovers iteration without
                                        the oracle step can be written as: $H^{\otimes n}(2|0\rangle\langle 0|-I)
                                        H^{\otimes n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly
                                        eighted superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                        Thus, including the oracle step now, Grovers iteration $G$ as a whole can be
                                        written, more generically for a given quantum state $\psi$ as
                                        $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image Source: $[1]$
                                        To continue. Quantum Computational Theory Quantum Automata Theory Quantum Turing
                                        Machine QTM $\delta$ Function A Quantum Turing Machine QTM can be expressed
                                        similarly to a traditional Turing Machine TM with all components reformulated
                                        canonically except for the transition function $\delta$. Below, is the formal
                                        definition of a QTM. Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma,
                                        \delta, q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite
                                        sets and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                        blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$
                                        and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way infinite, with
                                        squares indexed by the set of integers $\mathbb{Z}$ $\delta$ is a transition
                                        function described as $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times
                                        \Gamma \times \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state
                                        $q_{accept} \in Q$ is the accept state $q_{reject} \in Q$ is the reject state,
                                        where $q_{reject} \neq q_{accept}$ Image Source:
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                        Quantum Networks Superdense Coding and Quantum Teleportation Superdense Coding
                                        Given Alice wants to send classical information to Bob, quantum entanglement $n$
                                        qubits can store $2n$ qubits total of information. Say Alice needs to send one
                                        qubit of infromation, they can do so but needs Bob to already share a second
                                        qubit. So, given that Alice and Bob already share a pair of entangled qubits in
                                        state $|\Phi^{+}\rangle$, defined as: (Alice) --
                                        $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to send:
                                        $00$: Alice does nothing to their qubit, so the qubit is still in state:
                                        $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice applies
                                        $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that Bob has
                                        both qubits in one of four Bell basis, Bob will know what the qubit Alice wants
                                        to send by measuring the two qubits and seeing what state they are in. This is
                                        called a Bell measurment. If we then extrapolate this method, then if Alice and
                                        Bob want to share $n$ pairs of entangled qubits they can do so with $2n$ quibits
                                        in total. Quantum Teleportation To continue. Glossary Hilbert Space Hilbert
                                        Space is a nondenumerable infinite complex vector space. Complex space, being a
                                        collection of complex numbers $\mathbb{C}$ with an added structure. The infinite
                                        dimensions of Hilbert Space represents a continious spectra of alternative
                                        physical states. Alternative physical states, for example, being the position
                                        (coordinates) or momentum of a particle. Probabilistic Systems Pure States Mixed
                                        States The nature of a probabilistic system is that we do not know for certain
                                        the state of the system. However, we do know the probability distribution of the
                                        states. Our probabilistic distribution sums up to 1. The notation can be written
                                        as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands for the
                                        state the system is in with probability $p_{i}$. Where $p_{i} \ge 0$ and $p_{1}
                                        + ... + p_{n} = 1$. Our distribution defined above is a mixed state. Where
                                        $x_{i}$ is a pure state. It is important to note that our distribution is not an
                                        expected value or an average of the mixed state, but rather represents only the
                                        probability distribution for all states $x_{i}$. Quantum Mechanics Fun From here
                                        on out we used will use a Hilbert space formalism of quantum mechanics where the
                                        representation of quantum mechanical systems are represented as state vectors.
                                        We use this representation because state vectors are mathematically simpler that
                                        the more general ones. The quantum mechanical description of a physical system
                                        resembles the probabilistic systems we mentioned earlier: $p_{1}[x_{1}] +
                                        p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can
                                        be shown as a unit-length vector in $n$-dimensional complex vector space. We
                                        define this state space with $H_{n}$. Using ket-notation, which is a part of
                                        Dirac notation, we define our state space $H_{n}$ as an orthonormal basis
                                        $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We can now write
                                        any state of the quantum system as: $\alpha_{1}\left| x_{1} \right> +
                                        \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left| x_{n} \right>$ Here,
                                        $\alpha_{i}$ are probabilistic amplitudes. Finally, to meet our requirements
                                        defining our state space $H_{n}$ as unit-length we say that $|\alpha_{1}|^{2} +
                                        |\alpha_{2}|^{2} + ... + |\alpha_{n}|^{2} = 1$. This concludes most of the
                                        information neccesary for this page. However, if you are having fun with quantum
                                        mechanics, feel free to read more on my Quantum Mechanics page. Quantum
                                        Computing Theory Introduction Quantum Computing Theory is a field of computer
                                        science that uses the principles of quantum mechanics, mathematics, and computer
                                        science. By borrowing concepts from each field scientists can rigorously define
                                        both a broad and narrow theoretical model of a quantum computer, and later apply
                                        it to the real world. These theoretical models, such as the result of a quantum
                                        system manipulating subatomic particles, the theoretical circuits quantum
                                        computers implement to perform larger operations, and how to optimize the
                                        resource complexity for quantum systems, are just a few of the fundamental
                                        concepts in quantum computing theory. 1-Qubit Qubit A qubit, short for quantum
                                        bit, is a two-level quantum system and is a part of two-dimensional Hilbert
                                        space $H_{2}$, where Hilbert space $H$ is nondenumerable infinite complex vector
                                        space. The two-dimensional complex vector space $H_{2}$ comes with a fixed
                                        orthonormal basis states $B=\left\{|0\rangle,|1\rangle\right\}$ when the base
                                        measurment is in the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis
                                        states, denoted with Dirac notation. The states of the quantum system or qubit
                                        can be denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This vector
                                        has a unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and
                                        $|\beta|^{2}$ are the probabilities of the system being in the representative
                                        states. Meaning that the probabilities that when the qubit is measured will give
                                        a state $0$ or $1$ in this two-level quantum system. We will go more in depth
                                        into probabilites here soon. First, however, we will look at formal definition
                                        of the inner dot product of some given some qubit $\theta$. For $|\theta\rangle$
                                        the unit length is equivalant to its inner product. Where, for ket
                                        $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex
                                        numbers and both have two real numbers. The inner dot product is
                                        $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                        formulated as:
                                        $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                        For some quantum state for the qubit $\psi$ can be defined as:
                                        $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$ $=\langle
                                        v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where, squaring our
                                        projections, $\langle v|\psi\rangle$ and $\langle h|\psi\rangle$, onto axis an
                                        axis gives us our respective probabilites for $|v\rangle$ and $|h\rangle$
                                        respectfully. An example of a qubit is the spin of an electron. The two levels
                                        of this quibit are spin up or spin down. What differs from a classical system is
                                        that quantum mechanics allows for the qubit to be in a coherent superposition of
                                        both states simultaneously. Measuring a qubit in a basis gives a projective
                                        measurement of a qubit of state $\phi$ in its computational basis can be
                                        expressed as a linear combination of state vectors, such as:
                                        $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured in a
                                        basis, collapses the qubit to either the quantum state $|0\rangle$ or
                                        $|1\rangle$ given by the respective norm-square of the probability amplitudes
                                        $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch Sphere We can use a
                                        bloch ball or sphere to help us visualize sping down $0$ and spin up $1$ of a
                                        single qubit. The bloch spehere has a radius of 1, meaning that $|0\rangle$
                                        corresponds to $(x,y,z)$ point $(x,y,1)$ and $|1\rangle$ corresponds to
                                        $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows a spin up or sping down.
                                        Superposition In classical computing, states $0$ and $1$ would be the only
                                        states that exist for the bit. However, in quantum mechanics a quibit can be
                                        both in the state of $|0\rangle$ and $|1\rangle$. This is what gives quantum
                                        computers more processing power, as a single qubit can be in more states and
                                        therefore represent more information than a single classical bit. This means
                                        that the qubit can have an $80$% of being in state $|0\rangle$ and $20$% of
                                        being in state $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of
                                        being in state $|1\rangle$. Unlike a classical bit where there is either a
                                        $100$% of the classical bit being in state $0$ and $0$% of being a $1$ or a $0$%
                                        of the classical bit being a $0$ and $100$% of being a $1$. To allow for the
                                        qubit to be in superposition, we need to levearage Hilbert Space. Again, Hilbert
                                        space is represented using complex vector space. We use complex vector space,
                                        because it is the easiest way for the math to work. Let's represent this qubit
                                        in superposition as a vector using dirac notation. For the qubit have, for
                                        example, a $50$% of being in state $|0\rangle$ and $50$% of being in state
                                        $|1\rangle$, the vector should look like:
                                        $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$ are
                                        both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$. Pauli Matrices
                                        Pauli matrices have a core importance in quantum physics, and when combinex with
                                        an identity matrix, pauli matrices form a basis for all single quantum gates.
                                        Pauli matrices are defined as: $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0
                                        \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                        $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is usally
                                        refered to as the NOT gate and can be written as $X$. The NOT gate inverts
                                        $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$ are phase shifting
                                        gates. Multiple Qubits Quantum Registers A system that contains more than one
                                        qubit is represented in a quantum register. A system with two qubits is
                                        represented using a four-dimensional Hilbert space $H_{4} = H_{2} \otimes
                                        H_{2}$. The orthonormal basis is $\left\{|0\rangle |0\rangle, |0\rangle
                                        |1\rangle, |1\rangle |0\rangle, |1\rangle |1\rangle\ \right\}$. $|0\rangle
                                        |0\rangle$ can be more succinctly written as $|00\rangle$. The same is true for
                                        $|0\rangle |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a
                                        part from four-dimensional Hilbert space is a unit-length vector:
                                        $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$ Again,
                                        as it is with a single qubit, it is required that $|c_{0}|^{2} + |c_{1}|^{2} +
                                        |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit system will give
                                        $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2,
                                        |c_{3}|^2$, respectively. If we want to observe one of the qubits, then the
                                        standard rules of probabilities apply. It is important to note that the tensor
                                        product of these vectors does not commute. Meaning, that $|0\rangle|1\rangle
                                        \neq |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                        individually. Quantum Entanglement Maximally Entangled Bell States Bell states
                                        or Einstein, Podolski and Rosen pairs are the maximally entangled quantum states
                                        of a qubit system such that a quantum mechanical system is composed of two
                                        interacting two-level subsystems. The 4 types of maximially entangled Bell
                                        states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments The
                                        orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                        $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                        $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$ basis to
                                        the $X$ bases, we need to apply a Hadmard gate to the state, allowing for the
                                        state to be halfway: $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ Y
                                        Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway we need
                                        to apply $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                        -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Z Bases Measurment No transformations needed, since we measure in
                                        the "normal" bases. Operators Unitary Operator All quantum gates are unitary,
                                        which we will define in the following. Let us use the basis measurment of $Z$
                                        with the coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                        \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                        operation on a quibit, called an unary quantum gate, is a unitary mapping $U:
                                        H_{2} \rightarrow H_{2}$ with the following defining linear operation:
                                        $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle \mapsto c|0\rangle +
                                        d|1\rangle$ An important aspect of all quantum gates is that they are unitary.
                                        Meaning, that for some given matrix operation $U$, defined as: $\begin{pmatrix}
                                        a & b \\ c & d \end{pmatrix}$ it is neccesary that this matrix is unitary in
                                        order to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                        unitary and valid, the following equivalency must be true: $UU^{\dagger}=I$
                                        where $U^{\dagger}$ represents the conjugate transpose and $I$ is the identity
                                        matrix. Another important qualtiy of an unitary matrix is: $U^{\dagger}=U^{-1}$
                                        Represented as matrices, we get: $\begin{pmatrix} a & b \\ c & d \end{pmatrix}
                                        \begin{pmatrix} a^* & b^* \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\
                                        0 & 1 \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                        denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                        d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation $a^{*}$
                                        stands for the complex conjugate of the complex number $a$. The complex
                                        conjugate of a complex number is the number with an equal real part and an
                                        imaginary part equal in magnitude, but opposite in sign of the complex number.
                                        The mapping of for unary quantum operator, when represented in a quantum circut,
                                        can be a quantum gate. Where the output of the quantum gate must have the same
                                        dimensionality as its input. So, $U: H_{n} \rightarrow H_{n}$, where $n$ is the
                                        number of dimensions of $H$. Hermitian Operator A unitary operator is Hermitian
                                        if: $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                        matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian matrix is a
                                        special case of a unitary matrix, where all Hermitian operators or unitary
                                        operators, but not all unitary operators, are Hermitian. Natural Operator An
                                        Hermitian operator is Natural if: $U^{\dagger}U=UU^{\dagger}$, where
                                        $U^{\dagger}$ is the conjugate transpose of $U$. the operator has spectural
                                        decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum Algorithms
                                        Quantum Parallelism Suppose we want to evaluate a function $f(x)$, where the
                                        function $f$ expresses some computation or algorithm. A use case for quantum
                                        parallelism is to evaluate $f(x)$ with many different values for the output of
                                        the computation or algorithm on the input $x$ simultaneously. In essence, we can
                                        evaluate many different values of $x$ on $f$ in parallel by exploiting quantum
                                        effects. This quantum effect exploit feature is fundamental in many quantum
                                        algorithms. To continue, we will look at how quantum parallelism works. Consider
                                        the one-bit domain and range function $f(x):\{0,1\}\rightarrow\{0,1\}$. To
                                        compute this function $f$ on a quantum computer, we will use a two-qubit quantum
                                        computer with the starting state $|x,y\rangle$. The transformation on the domain
                                        or 'data' register to the range or 'target' register of this initial two qubit
                                        state is described by the following unitary function:
                                        $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                        represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts on $y$,
                                        and its value is $0$ then the value of the second qubit in the 'target' register
                                        is the value $f(x)$, given whatever function $f$ represents. The functions
                                        effect on $x$ is arbritrary for now. The final collapsed state $|\psi\rangle$ is
                                        an element of the set of final states or 'target' register $|x,y\oplus
                                        f(x)\rangle$, which again is given by the unitary transformation $U_{f}$ on the
                                        start state $|x,y\rangle$. Given the input $q_{0}=x=|0\rangle$, we will apply
                                        the Hadmard gate $H$ to $x$, such that now:
                                        $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting state
                                        is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the resulting new
                                        state is not apart of the starting computational basis $\{0,1\}$. Next, the
                                        unitary function or blackbox computation/algorithm $U_{f}$ can be applied to the
                                        current 'data' register. The resulting mapping of the unitary function $U_{f}$
                                        is: $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                        f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                        $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                        Meaning that the final resulting state for a two-qubit quantum computer has a
                                        $50$% chance of being $|0,f(0)\rangle$ and $50$% of being $.5|1,f(1)\rangle$.
                                        Given in the same form as the range for $U_{f}$ given above: $|x,y'\rangle$,
                                        where $y'=y\oplus f(x)$ All of this means that the information given by the
                                        mapping for $f(0)$ and $f(1)$ was simultaneously evaluated by applying
                                        superposition and the unitary function on the starting 'data' register. Thus,
                                        $f(x)$ has been computed for two values of $x$ in parellel. The resulting set of
                                        all possible states computed in parallel is given by the resulting 'target'
                                        register is given by quantum exploitation and aptly named 'quantum parallelism'.
                                        Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$ values
                                        of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit system such that
                                        $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply the NOT gate to $y$,
                                        giving the what will be the start state $|\psi_{0}\rangle=|01\rangle$. Next, we
                                        will apply a Hadmard gates to $x$ and $y$ individually, yielding the state:
                                        $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        where for state $|\psi_{1}\rangle$: $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                        $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary unitary
                                        function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                        $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        From this, we can then say that if $f(0)=f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$. If
                                        $f(0)=f(1)$, the resulting state is:
                                        $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$
                                        $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0) \oplus
                                        f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus, $f(0)$ interfers
                                        with $f(1)$ when we simultaneously evalute $f(x)$ with quantum parallelism.
                                        Grovers Search Algorithm The authors describe the process for Grovers Search
                                        Algorithm in the following sequential two main steps: Hadmard transformation and
                                        Grover iteration or Grover operator $G$. Hadmard Transformation The Hadmard
                                        transform puts the qubits of the quantum computer into equal superposition
                                        states, defined as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                        \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm implements a
                                        repeated quantum subroutine called Grover iteration or operator, denoted as $G$.
                                        This quantum iteration can be broken up in four steps: Apply oracle $O$ Apply
                                        Hadmard transform $H^{\otimes{n}}$ Apply conditional phase shift on quantum
                                        register, such that every quantum basis state except $|r\rangle$ is phased
                                        shifted $-1$. Meaning that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1
                                        \sin \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                        \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks $|w\rangle$
                                        only. Then, reflect the state around $|s\rangle$ using gate $R_{s}$, such that
                                        moving our state towards $|w\rangle$. Lastly, apply the Hadmard transform
                                        $H^{\otimes{n}}$ Where combined steps of 2, 3, 4, or Grovers iteration without
                                        the oracle step can be written as: $H^{\otimes n}(2|0\rangle\langle 0|-I)
                                        H^{\otimes n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly
                                        eighted superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                        Thus, including the oracle step now, Grovers iteration $G$ as a whole can be
                                        written, more generically for a given quantum state $\psi$ as
                                        $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image Source: $[1]$
                                        To continue. Quantum Computational Theory Quantum Automata Theory Quantum Turing
                                        Machine QTM $\delta$ Function A Quantum Turing Machine QTM can be expressed
                                        similarly to a traditional Turing Machine TM with all components reformulated
                                        canonically except for the transition function $\delta$. Below, is the formal
                                        definition of a QTM. Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma,
                                        \delta, q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite
                                        sets and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                        blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$
                                        and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way infinite, with
                                        squares indexed by the set of integers $\mathbb{Z}$ $\delta$ is a transition
                                        function described as $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times
                                        \Gamma \times \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state
                                        $q_{accept} \in Q$ is the accept state $q_{reject} \in Q$ is the reject state,
                                        where $q_{reject} \neq q_{accept}$ Image Source:
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                        Quantum Networks Superdense Coding and Quantum Teleportation Superdense Coding
                                        Given Alice wants to send classical information to Bob, quantum entanglement $n$
                                        qubits can store $2n$ qubits total of information. Say Alice needs to send one
                                        qubit of infromation, they can do so but needs Bob to already share a second
                                        qubit. So, given that Alice and Bob already share a pair of entangled qubits in
                                        state $|\Phi^{+}\rangle$, defined as: (Alice) --
                                        $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to send:
                                        $00$: Alice does nothing to their qubit, so the qubit is still in state:
                                        $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice applies
                                        $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that Bob has
                                        both qubits in one of four Bell basis, Bob will know what the qubit Alice wants
                                        to send by measuring the two qubits and seeing what state they are in. This is
                                        called a Bell measurment. If we then extrapolate this method, then if Alice and
                                        Bob want to share $n$ pairs of entangled qubits they can do so with $2n$ quibits
                                        in total. Quantum Teleportation To continue. Glossary Hilbert Space Hilbert
                                        Space is a nondenumerable infinite complex vector space. Complex space, being a
                                        collection of complex numbers $\mathbb{C}$ with an added structure. The infinite
                                        dimensions of Hilbert Space represents a continious spectra of alternative
                                        physical states. Alternative physical states, for example, being the position
                                        (coordinates) or momentum of a particle. Probabilistic Systems Pure States Mixed
                                        States The nature of a probabilistic system is that we do not know for certain
                                        the state of the system. However, we do know the probability distribution of the
                                        states. Our probabilistic distribution sums up to 1. The notation can be written
                                        as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands for the
                                        state the system is in with probability $p_{i}$. Where $p_{i} \ge 0$ and $p_{1}
                                        + ... + p_{n} = 1$. Our distribution defined above is a mixed state. Where
                                        $x_{i}$ is a pure state. It is important to note that our distribution is not an
                                        expected value or an average of the mixed state, but rather represents only the
                                        probability distribution for all states $x_{i}$. Quantum Mechanics Fun From here
                                        on out we used will use a Hilbert space formalism of quantum mechanics where the
                                        representation of quantum mechanical systems are represented as state vectors.
                                        We use this representation because state vectors are mathematically simpler that
                                        the more general ones. The quantum mechanical description of a physical system
                                        resembles the probabilistic systems we mentioned earlier: $p_{1}[x_{1}] +
                                        p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can
                                        be shown as a unit-length vector in $n$-dimensional complex vector space. We
                                        define this state space with $H_{n}$. Using ket-notation, which is a part of
                                        Dirac notation, we define our state space $H_{n}$ as an orthonormal basis
                                        $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We can now write
                                        any state of the quantum system as: $\alpha_{1}\left| x_{1} \right> +
                                        \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left| x_{n} \right>$ Here,
                                        $\alpha_{i}$ are probabilistic amplitudes. Finally, to meet our requirements
                                        defining our state space $H_{n}$ as unit-length we say that $|\alpha_{1}|^{2} +
                                        |\alpha_{2}|^{2} + ... + |\alpha_{n}|^{2} = 1$. This concludes most of the
                                        information neccesary for this page. However, if you are having fun with quantum
                                        mechanics, feel free to read more on my Quantum Mechanics page. Quantum
                                        Computing Theory Introduction Quantum Computing Theory is a field of computer
                                        science that uses the principles of quantum mechanics, mathematics, and computer
                                        science. By borrowing concepts from each field scientists can rigorously define
                                        both a broad and narrow theoretical model of a quantum computer, and later apply
                                        it to the real world. These theoretical models, such as the result of a quantum
                                        system manipulating subatomic particles, the theoretical circuits quantum
                                        computers implement to perform larger operations, and how to optimize the
                                        resource complexity for quantum systems, are just a few of the fundamental
                                        concepts in quantum computing theory. 1-Qubit Qubit A qubit, short for quantum
                                        bit, is a two-level quantum system and is a part of two-dimensional Hilbert
                                        space $H_{2}$, where Hilbert space $H$ is nondenumerable infinite complex vector
                                        space. The two-dimensional complex vector space $H_{2}$ comes with a fixed
                                        orthonormal basis states $B=\left\{|0\rangle,|1\rangle\right\}$ when the base
                                        measurment is in the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis
                                        states, denoted with Dirac notation. The states of the quantum system or qubit
                                        can be denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This vector
                                        has a unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and
                                        $|\beta|^{2}$ are the probabilities of the system being in the representative
                                        states. Meaning that the probabilities that when the qubit is measured will give
                                        a state $0$ or $1$ in this two-level quantum system. We will go more in depth
                                        into probabilites here soon. First, however, we will look at formal definition
                                        of the inner dot product of some given some qubit $\theta$. For $|\theta\rangle$
                                        the unit length is equivalant to its inner product. Where, for ket
                                        $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex
                                        numbers and both have two real numbers. The inner dot product is
                                        $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                        formulated as:
                                        $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                        For some quantum state for the qubit $\psi$ can be defined as:
                                        $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$ $=\langle
                                        v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where, squaring our
                                        projections, $\langle v|\psi\rangle$ and $\langle h|\psi\rangle$, onto axis an
                                        axis gives us our respective probabilites for $|v\rangle$ and $|h\rangle$
                                        respectfully. An example of a qubit is the spin of an electron. The two levels
                                        of this quibit are spin up or spin down. What differs from a classical system is
                                        that quantum mechanics allows for the qubit to be in a coherent superposition of
                                        both states simultaneously. Measuring a qubit in a basis gives a projective
                                        measurement of a qubit of state $\phi$ in its computational basis can be
                                        expressed as a linear combination of state vectors, such as:
                                        $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured in a
                                        basis, collapses the qubit to either the quantum state $|0\rangle$ or
                                        $|1\rangle$ given by the respective norm-square of the probability amplitudes
                                        $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch Sphere We can use a
                                        bloch ball or sphere to help us visualize sping down $0$ and spin up $1$ of a
                                        single qubit. The bloch spehere has a radius of 1, meaning that $|0\rangle$
                                        corresponds to $(x,y,z)$ point $(x,y,1)$ and $|1\rangle$ corresponds to
                                        $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows a spin up or sping down.
                                        Superposition In classical computing, states $0$ and $1$ would be the only
                                        states that exist for the bit. However, in quantum mechanics a quibit can be
                                        both in the state of $|0\rangle$ and $|1\rangle$. This is what gives quantum
                                        computers more processing power, as a single qubit can be in more states and
                                        therefore represent more information than a single classical bit. This means
                                        that the qubit can have an $80$% of being in state $|0\rangle$ and $20$% of
                                        being in state $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of
                                        being in state $|1\rangle$. Unlike a classical bit where there is either a
                                        $100$% of the classical bit being in state $0$ and $0$% of being a $1$ or a $0$%
                                        of the classical bit being a $0$ and $100$% of being a $1$. To allow for the
                                        qubit to be in superposition, we need to levearage Hilbert Space. Again, Hilbert
                                        space is represented using complex vector space. We use complex vector space,
                                        because it is the easiest way for the math to work. Let's represent this qubit
                                        in superposition as a vector using dirac notation. For the qubit have, for
                                        example, a $50$% of being in state $|0\rangle$ and $50$% of being in state
                                        $|1\rangle$, the vector should look like:
                                        $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$ are
                                        both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$. Pauli Matrices
                                        Pauli matrices have a core importance in quantum physics, and when combinex with
                                        an identity matrix, pauli matrices form a basis for all single quantum gates.
                                        Pauli matrices are defined as: $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0
                                        \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                        $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is usally
                                        refered to as the NOT gate and can be written as $X$. The NOT gate inverts
                                        $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$ are phase shifting
                                        gates. Multiple Qubits Quantum Registers A system that contains more than one
                                        qubit is represented in a quantum register. A system with two qubits is
                                        represented using a four-dimensional Hilbert space $H_{4} = H_{2} \otimes
                                        H_{2}$. The orthonormal basis is $\left\{|0\rangle |0\rangle, |0\rangle
                                        |1\rangle, |1\rangle |0\rangle, |1\rangle |1\rangle\ \right\}$. $|0\rangle
                                        |0\rangle$ can be more succinctly written as $|00\rangle$. The same is true for
                                        $|0\rangle |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a
                                        part from four-dimensional Hilbert space is a unit-length vector:
                                        $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$ Again,
                                        as it is with a single qubit, it is required that $|c_{0}|^{2} + |c_{1}|^{2} +
                                        |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit system will give
                                        $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2,
                                        |c_{3}|^2$, respectively. If we want to observe one of the qubits, then the
                                        standard rules of probabilities apply. It is important to note that the tensor
                                        product of these vectors does not commute. Meaning, that $|0\rangle|1\rangle
                                        \neq |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                        individually. Quantum Entanglement Maximally Entangled Bell States Bell states
                                        or Einstein, Podolski and Rosen pairs are the maximally entangled quantum states
                                        of a qubit system such that a quantum mechanical system is composed of two
                                        interacting two-level subsystems. The 4 types of maximially entangled Bell
                                        states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments The
                                        orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                        $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                        $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$ basis to
                                        the $X$ bases, we need to apply a Hadmard gate to the state, allowing for the
                                        state to be halfway: $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ Y
                                        Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway we need
                                        to apply $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                        -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Z Bases Measurment No transformations needed, since we measure in
                                        the "normal" bases. Operators Unitary Operator All quantum gates are unitary,
                                        which we will define in the following. Let us use the basis measurment of $Z$
                                        with the coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                        \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                        operation on a quibit, called an unary quantum gate, is a unitary mapping $U:
                                        H_{2} \rightarrow H_{2}$ with the following defining linear operation:
                                        $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle \mapsto c|0\rangle +
                                        d|1\rangle$ An important aspect of all quantum gates is that they are unitary.
                                        Meaning, that for some given matrix operation $U$, defined as: $\begin{pmatrix}
                                        a & b \\ c & d \end{pmatrix}$ it is neccesary that this matrix is unitary in
                                        order to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                        unitary and valid, the following equivalency must be true: $UU^{\dagger}=I$
                                        where $U^{\dagger}$ represents the conjugate transpose and $I$ is the identity
                                        matrix. Another important qualtiy of an unitary matrix is: $U^{\dagger}=U^{-1}$
                                        Represented as matrices, we get: $\begin{pmatrix} a & b \\ c & d \end{pmatrix}
                                        \begin{pmatrix} a^* & b^* \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\
                                        0 & 1 \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                        denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                        d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation $a^{*}$
                                        stands for the complex conjugate of the complex number $a$. The complex
                                        conjugate of a complex number is the number with an equal real part and an
                                        imaginary part equal in magnitude, but opposite in sign of the complex number.
                                        The mapping of for unary quantum operator, when represented in a quantum circut,
                                        can be a quantum gate. Where the output of the quantum gate must have the same
                                        dimensionality as its input. So, $U: H_{n} \rightarrow H_{n}$, where $n$ is the
                                        number of dimensions of $H$. Hermitian Operator A unitary operator is Hermitian
                                        if: $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                        matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian matrix is a
                                        special case of a unitary matrix, where all Hermitian operators or unitary
                                        operators, but not all unitary operators, are Hermitian. Natural Operator An
                                        Hermitian operator is Natural if: $U^{\dagger}U=UU^{\dagger}$, where
                                        $U^{\dagger}$ is the conjugate transpose of $U$. the operator has spectural
                                        decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum Algorithms
                                        Quantum Parallelism Suppose we want to evaluate a function $f(x)$, where the
                                        function $f$ expresses some computation or algorithm. A use case for quantum
                                        parallelism is to evaluate $f(x)$ with many different values for the output of
                                        the computation or algorithm on the input $x$ simultaneously. In essence, we can
                                        evaluate many different values of $x$ on $f$ in parallel by exploiting quantum
                                        effects. This quantum effect exploit feature is fundamental in many quantum
                                        algorithms. To continue, we will look at how quantum parallelism works. Consider
                                        the one-bit domain and range function $f(x):\{0,1\}\rightarrow\{0,1\}$. To
                                        compute this function $f$ on a quantum computer, we will use a two-qubit quantum
                                        computer with the starting state $|x,y\rangle$. The transformation on the domain
                                        or 'data' register to the range or 'target' register of this initial two qubit
                                        state is described by the following unitary function:
                                        $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                        represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts on $y$,
                                        and its value is $0$ then the value of the second qubit in the 'target' register
                                        is the value $f(x)$, given whatever function $f$ represents. The functions
                                        effect on $x$ is arbritrary for now. The final collapsed state $|\psi\rangle$ is
                                        an element of the set of final states or 'target' register $|x,y\oplus
                                        f(x)\rangle$, which again is given by the unitary transformation $U_{f}$ on the
                                        start state $|x,y\rangle$. Given the input $q_{0}=x=|0\rangle$, we will apply
                                        the Hadmard gate $H$ to $x$, such that now:
                                        $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting state
                                        is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the resulting new
                                        state is not apart of the starting computational basis $\{0,1\}$. Next, the
                                        unitary function or blackbox computation/algorithm $U_{f}$ can be applied to the
                                        current 'data' register. The resulting mapping of the unitary function $U_{f}$
                                        is: $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                        f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                        $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                        Meaning that the final resulting state for a two-qubit quantum computer has a
                                        $50$% chance of being $|0,f(0)\rangle$ and $50$% of being $.5|1,f(1)\rangle$.
                                        Given in the same form as the range for $U_{f}$ given above: $|x,y'\rangle$,
                                        where $y'=y\oplus f(x)$ All of this means that the information given by the
                                        mapping for $f(0)$ and $f(1)$ was simultaneously evaluated by applying
                                        superposition and the unitary function on the starting 'data' register. Thus,
                                        $f(x)$ has been computed for two values of $x$ in parellel. The resulting set of
                                        all possible states computed in parallel is given by the resulting 'target'
                                        register is given by quantum exploitation and aptly named 'quantum parallelism'.
                                        Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$ values
                                        of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit system such that
                                        $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply the NOT gate to $y$,
                                        giving the what will be the start state $|\psi_{0}\rangle=|01\rangle$. Next, we
                                        will apply a Hadmard gates to $x$ and $y$ individually, yielding the state:
                                        $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        where for state $|\psi_{1}\rangle$: $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                        $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary unitary
                                        function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                        $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        From this, we can then say that if $f(0)=f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$. If
                                        $f(0)=f(1)$, the resulting state is:
                                        $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$
                                        $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0) \oplus
                                        f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus, $f(0)$ interfers
                                        with $f(1)$ when we simultaneously evalute $f(x)$ with quantum parallelism.
                                        Grovers Search Algorithm The authors describe the process for Grovers Search
                                        Algorithm in the following sequential two main steps: Hadmard transformation and
                                        Grover iteration or Grover operator $G$. Hadmard Transformation The Hadmard
                                        transform puts the qubits of the quantum computer into equal superposition
                                        states, defined as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                        \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm implements a
                                        repeated quantum subroutine called Grover iteration or operator, denoted as $G$.
                                        This quantum iteration can be broken up in four steps: Apply oracle $O$ Apply
                                        Hadmard transform $H^{\otimes{n}}$ Apply conditional phase shift on quantum
                                        register, such that every quantum basis state except $|r\rangle$ is phased
                                        shifted $-1$. Meaning that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1
                                        \sin \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                        \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks $|w\rangle$
                                        only. Then, reflect the state around $|s\rangle$ using gate $R_{s}$, such that
                                        moving our state towards $|w\rangle$. Lastly, apply the Hadmard transform
                                        $H^{\otimes{n}}$ Where combined steps of 2, 3, 4, or Grovers iteration without
                                        the oracle step can be written as: $H^{\otimes n}(2|0\rangle\langle 0|-I)
                                        H^{\otimes n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly
                                        eighted superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                        Thus, including the oracle step now, Grovers iteration $G$ as a whole can be
                                        written, more generically for a given quantum state $\psi$ as
                                        $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image Source: $[1]$
                                        To continue. Quantum Computational Theory Quantum Automata Theory Quantum Turing
                                        Machine QTM $\delta$ Function A Quantum Turing Machine QTM can be expressed
                                        similarly to a traditional Turing Machine TM with all components reformulated
                                        canonically except for the transition function $\delta$. Below, is the formal
                                        definition of a QTM. Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma,
                                        \delta, q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite
                                        sets and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                        blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$
                                        and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way infinite, with
                                        squares indexed by the set of integers $\mathbb{Z}$ $\delta$ is a transition
                                        function described as $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times
                                        \Gamma \times \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state
                                        $q_{accept} \in Q$ is the accept state $q_{reject} \in Q$ is the reject state,
                                        where $q_{reject} \neq q_{accept}$ Image Source:
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                        Quantum Networks Superdense Coding and Quantum Teleportation Superdense Coding
                                        Given Alice wants to send classical information to Bob, quantum entanglement $n$
                                        qubits can store $2n$ qubits total of information. Say Alice needs to send one
                                        qubit of infromation, they can do so but needs Bob to already share a second
                                        qubit. So, given that Alice and Bob already share a pair of entangled qubits in
                                        state $|\Phi^{+}\rangle$, defined as: (Alice) --
                                        $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to send:
                                        $00$: Alice does nothing to their qubit, so the qubit is still in state:
                                        $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice applies
                                        $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that Bob has
                                        both qubits in one of four Bell basis, Bob will know what the qubit Alice wants
                                        to send by measuring the two qubits and seeing what state they are in. This is
                                        called a Bell measurment. If we then extrapolate this method, then if Alice and
                                        Bob want to share $n$ pairs of entangled qubits they can do so with $2n$ quibits
                                        in total. Quantum Teleportation To continue. Glossary Hilbert Space Hilbert
                                        Space is a nondenumerable infinite complex vector space. Complex space, being a
                                        collection of complex numbers $\mathbb{C}$ with an added structure. The infinite
                                        dimensions of Hilbert Space represents a continious spectra of alternative
                                        physical states. Alternative physical states, for example, being the position
                                        (coordinates) or momentum of a particle. Probabilistic Systems Pure States Mixed
                                        States The nature of a probabilistic system is that we do not know for certain
                                        the state of the system. However, we do know the probability distribution of the
                                        states. Our probabilistic distribution sums up to 1. The notation can be written
                                        as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands for the
                                        state the system is in with probability $p_{i}$. Where $p_{i} \ge 0$ and $p_{1}
                                        + ... + p_{n} = 1$. Our distribution defined above is a mixed state. Where
                                        $x_{i}$ is a pure state. It is important to note that our distribution is not an
                                        expected value or an average of the mixed state, but rather represents only the
                                        probability distribution for all states $x_{i}$. Quantum Mechanics Fun From here
                                        on out we used will use a Hilbert space formalism of quantum mechanics where the
                                        representation of quantum mechanical systems are represented as state vectors.
                                        We use this representation because state vectors are mathematically simpler that
                                        the more general ones. The quantum mechanical description of a physical system
                                        resembles the probabilistic systems we mentioned earlier: $p_{1}[x_{1}] +
                                        p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can
                                        be shown as a unit-length vector in $n$-dimensional complex vector space. We
                                        define this state space with $H_{n}$. Using ket-notation, which is a part of
                                        Dirac notation, we define our state space $H_{n}$ as an orthonormal basis
                                        $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We can now write
                                        any state of the quantum system as: $\alpha_{1}\left| x_{1} \right> +
                                        \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left| x_{n} \right>$ Here,
                                        $\alpha_{i}$ are probabilistic amplitudes. Finally, to meet our requirements
                                        defining our state space $H_{n}$ as unit-length we say that $|\alpha_{1}|^{2} +
                                        |\alpha_{2}|^{2} + ... + |\alpha_{n}|^{2} = 1$. This concludes most of the
                                        information neccesary for this page. However, if you are having fun with quantum
                                        mechanics, feel free to read more on my Quantum Mechanics page. Quantum
                                        Computing TheoryIntroduction Quantum Computing Theory is a field of computer
                                        science that uses the principles of quantum mechanics, mathematics, and computer
                                        science. By borrowing concepts from each field scientists can rigorously define
                                        both a broad and narrow theoretical model of a quantum computer, and later apply
                                        it to the real world. These theoretical models, such as the result of a quantum
                                        system manipulating subatomic particles, the theoretical circuits quantum
                                        computers implement to perform larger operations, and how to optimize the
                                        resource complexity for quantum systems, are just a few of the fundamental
                                        concepts in quantum computing theory. 1-Qubit Qubit A qubit, short for quantum
                                        bit, is a two-level quantum system and is a part of two-dimensional Hilbert
                                        space $H_{2}$, where Hilbert space $H$ is nondenumerable infinite complex vector
                                        space. The two-dimensional complex vector space $H_{2}$ comes with a fixed
                                        orthonormal basis states $B=\left\{|0\rangle,|1\rangle\right\}$ when the base
                                        measurment is in the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis
                                        states, denoted with Dirac notation. The states of the quantum system or qubit
                                        can be denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This vector
                                        has a unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and
                                        $|\beta|^{2}$ are the probabilities of the system being in the representative
                                        states. Meaning that the probabilities that when the qubit is measured will give
                                        a state $0$ or $1$ in this two-level quantum system. We will go more in depth
                                        into probabilites here soon. First, however, we will look at formal definition
                                        of the inner dot product of some given some qubit $\theta$. For $|\theta\rangle$
                                        the unit length is equivalant to its inner product. Where, for ket
                                        $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex
                                        numbers and both have two real numbers. The inner dot product is
                                        $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                        formulated as:
                                        $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                        For some quantum state for the qubit $\psi$ can be defined as:
                                        $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$ $=\langle
                                        v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where, squaring our
                                        projections, $\langle v|\psi\rangle$ and $\langle h|\psi\rangle$, onto axis an
                                        axis gives us our respective probabilites for $|v\rangle$ and $|h\rangle$
                                        respectfully. An example of a qubit is the spin of an electron. The two levels
                                        of this quibit are spin up or spin down. What differs from a classical system is
                                        that quantum mechanics allows for the qubit to be in a coherent superposition of
                                        both states simultaneously. Measuring a qubit in a basis gives a projective
                                        measurement of a qubit of state $\phi$ in its computational basis can be
                                        expressed as a linear combination of state vectors, such as:
                                        $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured in a
                                        basis, collapses the qubit to either the quantum state $|0\rangle$ or
                                        $|1\rangle$ given by the respective norm-square of the probability amplitudes
                                        $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch Sphere We can use a
                                        bloch ball or sphere to help us visualize sping down $0$ and spin up $1$ of a
                                        single qubit. The bloch spehere has a radius of 1, meaning that $|0\rangle$
                                        corresponds to $(x,y,z)$ point $(x,y,1)$ and $|1\rangle$ corresponds to
                                        $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows a spin up or sping down.
                                        Superposition In classical computing, states $0$ and $1$ would be the only
                                        states that exist for the bit. However, in quantum mechanics a quibit can be
                                        both in the state of $|0\rangle$ and $|1\rangle$. This is what gives quantum
                                        computers more processing power, as a single qubit can be in more states and
                                        therefore represent more information than a single classical bit. This means
                                        that the qubit can have an $80$% of being in state $|0\rangle$ and $20$% of
                                        being in state $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of
                                        being in state $|1\rangle$. Unlike a classical bit where there is either a
                                        $100$% of the classical bit being in state $0$ and $0$% of being a $1$ or a $0$%
                                        of the classical bit being a $0$ and $100$% of being a $1$. To allow for the
                                        qubit to be in superposition, we need to levearage Hilbert Space. Again, Hilbert
                                        space is represented using complex vector space. We use complex vector space,
                                        because it is the easiest way for the math to work. Let's represent this qubit
                                        in superposition as a vector using dirac notation. For the qubit have, for
                                        example, a $50$% of being in state $|0\rangle$ and $50$% of being in state
                                        $|1\rangle$, the vector should look like:
                                        $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$ are
                                        both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$. Pauli Matrices
                                        Pauli matrices have a core importance in quantum physics, and when combinex with
                                        an identity matrix, pauli matrices form a basis for all single quantum gates.
                                        Pauli matrices are defined as: $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0
                                        \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                        $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is usally
                                        refered to as the NOT gate and can be written as $X$. The NOT gate inverts
                                        $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$ are phase shifting
                                        gates. Multiple Qubits Quantum Registers A system that contains more than one
                                        qubit is represented in a quantum register. A system with two qubits is
                                        represented using a four-dimensional Hilbert space $H_{4} = H_{2} \otimes
                                        H_{2}$. The orthonormal basis is $\left\{|0\rangle |0\rangle, |0\rangle
                                        |1\rangle, |1\rangle |0\rangle, |1\rangle |1\rangle\ \right\}$. $|0\rangle
                                        |0\rangle$ can be more succinctly written as $|00\rangle$. The same is true for
                                        $|0\rangle |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a
                                        part from four-dimensional Hilbert space is a unit-length vector:
                                        $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$ Again,
                                        as it is with a single qubit, it is required that $|c_{0}|^{2} + |c_{1}|^{2} +
                                        |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit system will give
                                        $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2,
                                        |c_{3}|^2$, respectively. If we want to observe one of the qubits, then the
                                        standard rules of probabilities apply. It is important to note that the tensor
                                        product of these vectors does not commute. Meaning, that $|0\rangle|1\rangle
                                        \neq |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                        individually. Quantum Entanglement Maximally Entangled Bell States Bell states
                                        or Einstein, Podolski and Rosen pairs are the maximally entangled quantum states
                                        of a qubit system such that a quantum mechanical system is composed of two
                                        interacting two-level subsystems. The 4 types of maximially entangled Bell
                                        states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments The
                                        orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                        $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                        $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$ basis to
                                        the $X$ bases, we need to apply a Hadmard gate to the state, allowing for the
                                        state to be halfway: $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ Y
                                        Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway we need
                                        to apply $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                        -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Z Bases Measurment No transformations needed, since we measure in
                                        the "normal" bases. Operators Unitary Operator All quantum gates are unitary,
                                        which we will define in the following. Let us use the basis measurment of $Z$
                                        with the coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                        \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                        operation on a quibit, called an unary quantum gate, is a unitary mapping $U:
                                        H_{2} \rightarrow H_{2}$ with the following defining linear operation:
                                        $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle \mapsto c|0\rangle +
                                        d|1\rangle$ An important aspect of all quantum gates is that they are unitary.
                                        Meaning, that for some given matrix operation $U$, defined as: $\begin{pmatrix}
                                        a & b \\ c & d \end{pmatrix}$ it is neccesary that this matrix is unitary in
                                        order to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                        unitary and valid, the following equivalency must be true: $UU^{\dagger}=I$
                                        where $U^{\dagger}$ represents the conjugate transpose and $I$ is the identity
                                        matrix. Another important qualtiy of an unitary matrix is: $U^{\dagger}=U^{-1}$
                                        Represented as matrices, we get: $\begin{pmatrix} a & b \\ c & d \end{pmatrix}
                                        \begin{pmatrix} a^* & b^* \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\
                                        0 & 1 \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                        denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                        d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation $a^{*}$
                                        stands for the complex conjugate of the complex number $a$. The complex
                                        conjugate of a complex number is the number with an equal real part and an
                                        imaginary part equal in magnitude, but opposite in sign of the complex number.
                                        The mapping of for unary quantum operator, when represented in a quantum circut,
                                        can be a quantum gate. Where the output of the quantum gate must have the same
                                        dimensionality as its input. So, $U: H_{n} \rightarrow H_{n}$, where $n$ is the
                                        number of dimensions of $H$. Hermitian Operator A unitary operator is Hermitian
                                        if: $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                        matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian matrix is a
                                        special case of a unitary matrix, where all Hermitian operators or unitary
                                        operators, but not all unitary operators, are Hermitian. Natural Operator An
                                        Hermitian operator is Natural if: $U^{\dagger}U=UU^{\dagger}$, where
                                        $U^{\dagger}$ is the conjugate transpose of $U$. the operator has spectural
                                        decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$
                                        IntroductionQuantum Computing Theory is a field of computer science that uses
                                        the principles of quantum mechanics, mathematics, and computer science. By
                                        borrowing concepts from each field scientists can rigorously define both a broad
                                        and narrow theoretical model of a quantum computer, and later apply it to the
                                        real world. These theoretical models, such as the result of a quantum system
                                        manipulating subatomic particles, the theoretical circuits quantum computers
                                        implement to perform larger operations, and how to optimize the resource
                                        complexity for quantum systems, are just a few of the fundamental concepts in
                                        quantum computing theory. 1-Qubit Qubit A qubit, short for quantum bit, is a
                                        two-level quantum system and is a part of two-dimensional Hilbert space $H_{2}$,
                                        where Hilbert space $H$ is nondenumerable infinite complex vector space. The
                                        two-dimensional complex vector space $H_{2}$ comes with a fixed orthonormal
                                        basis states $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is
                                        in the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis states,
                                        denoted with Dirac notation. The states of the quantum system or qubit can be
                                        denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This vector has a
                                        unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and
                                        $|\beta|^{2}$ are the probabilities of the system being in the representative
                                        states. Meaning that the probabilities that when the qubit is measured will give
                                        a state $0$ or $1$ in this two-level quantum system. We will go more in depth
                                        into probabilites here soon. First, however, we will look at formal definition
                                        of the inner dot product of some given some qubit $\theta$. For $|\theta\rangle$
                                        the unit length is equivalant to its inner product. Where, for ket
                                        $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex
                                        numbers and both have two real numbers. The inner dot product is
                                        $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                        formulated as:
                                        $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                        For some quantum state for the qubit $\psi$ can be defined as:
                                        $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$ $=\langle
                                        v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where, squaring our
                                        projections, $\langle v|\psi\rangle$ and $\langle h|\psi\rangle$, onto axis an
                                        axis gives us our respective probabilites for $|v\rangle$ and $|h\rangle$
                                        respectfully. An example of a qubit is the spin of an electron. The two levels
                                        of this quibit are spin up or spin down. What differs from a classical system is
                                        that quantum mechanics allows for the qubit to be in a coherent superposition of
                                        both states simultaneously. Measuring a qubit in a basis gives a projective
                                        measurement of a qubit of state $\phi$ in its computational basis can be
                                        expressed as a linear combination of state vectors, such as:
                                        $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured in a
                                        basis, collapses the qubit to either the quantum state $|0\rangle$ or
                                        $|1\rangle$ given by the respective norm-square of the probability amplitudes
                                        $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch Sphere We can use a
                                        bloch ball or sphere to help us visualize sping down $0$ and spin up $1$ of a
                                        single qubit. The bloch spehere has a radius of 1, meaning that $|0\rangle$
                                        corresponds to $(x,y,z)$ point $(x,y,1)$ and $|1\rangle$ corresponds to
                                        $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows a spin up or sping down.
                                        Superposition In classical computing, states $0$ and $1$ would be the only
                                        states that exist for the bit. However, in quantum mechanics a quibit can be
                                        both in the state of $|0\rangle$ and $|1\rangle$. This is what gives quantum
                                        computers more processing power, as a single qubit can be in more states and
                                        therefore represent more information than a single classical bit. This means
                                        that the qubit can have an $80$% of being in state $|0\rangle$ and $20$% of
                                        being in state $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of
                                        being in state $|1\rangle$. Unlike a classical bit where there is either a
                                        $100$% of the classical bit being in state $0$ and $0$% of being a $1$ or a $0$%
                                        of the classical bit being a $0$ and $100$% of being a $1$. To allow for the
                                        qubit to be in superposition, we need to levearage Hilbert Space. Again, Hilbert
                                        space is represented using complex vector space. We use complex vector space,
                                        because it is the easiest way for the math to work. Let's represent this qubit
                                        in superposition as a vector using dirac notation. For the qubit have, for
                                        example, a $50$% of being in state $|0\rangle$ and $50$% of being in state
                                        $|1\rangle$, the vector should look like:
                                        $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$ are
                                        both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$. Pauli Matrices
                                        Pauli matrices have a core importance in quantum physics, and when combinex with
                                        an identity matrix, pauli matrices form a basis for all single quantum gates.
                                        Pauli matrices are defined as: $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0
                                        \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                        $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is usally
                                        refered to as the NOT gate and can be written as $X$. The NOT gate inverts
                                        $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$ are phase shifting
                                        gates. 1-QubitQubit A qubit, short for quantum bit, is a two-level quantum
                                        system and is a part of two-dimensional Hilbert space $H_{2}$, where Hilbert
                                        space $H$ is nondenumerable infinite complex vector space. The two-dimensional
                                        complex vector space $H_{2}$ comes with a fixed orthonormal basis states
                                        $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is in the $Z$
                                        basis. States $|0\rangle$ and $|1\rangle$ are the basis states, denoted with
                                        Dirac notation. The states of the quantum system or qubit can be denoted as a
                                        vector like: $\alpha|0\rangle + \beta|1\rangle$ This vector has a unit length of
                                        $1$, so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and $|\beta|^{2}$ are the
                                        probabilities of the system being in the representative states. Meaning that the
                                        probabilities that when the qubit is measured will give a state $0$ or $1$ in
                                        this two-level quantum system. We will go more in depth into probabilites here
                                        soon. First, however, we will look at formal definition of the inner dot product
                                        of some given some qubit $\theta$. For $|\theta\rangle$ the unit length is
                                        equivalant to its inner product. Where, for ket $|\theta\rangle$ and bra
                                        $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex numbers and both have two
                                        real numbers. The inner dot product is
                                        $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                        formulated as:
                                        $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                        For some quantum state for the qubit $\psi$ can be defined as:
                                        $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$ $=\langle
                                        v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where, squaring our
                                        projections, $\langle v|\psi\rangle$ and $\langle h|\psi\rangle$, onto axis an
                                        axis gives us our respective probabilites for $|v\rangle$ and $|h\rangle$
                                        respectfully. An example of a qubit is the spin of an electron. The two levels
                                        of this quibit are spin up or spin down. What differs from a classical system is
                                        that quantum mechanics allows for the qubit to be in a coherent superposition of
                                        both states simultaneously. Measuring a qubit in a basis gives a projective
                                        measurement of a qubit of state $\phi$ in its computational basis can be
                                        expressed as a linear combination of state vectors, such as:
                                        $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured in a
                                        basis, collapses the qubit to either the quantum state $|0\rangle$ or
                                        $|1\rangle$ given by the respective norm-square of the probability amplitudes
                                        $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Qubitbase measurmentDirac
                                        notationThe states of the quantum system or qubit can be denoted as a vector
                                        like: $\alpha|0\rangle + \beta|1\rangle$ This vector has a unit length of $1$,
                                        so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and $|\beta|^{2}$ are the
                                        probabilities of the system being in the representative states. Meaning that the
                                        probabilities that when the qubit is measured will give a state $0$ or $1$ in
                                        this two-level quantum system. We will go more in depth into probabilites here
                                        soon. First, however, we will look at formal definition of the inner dot product
                                        of some given some qubit $\theta$. For $|\theta\rangle$ the unit length is
                                        equivalant to its inner product. Where, for ket $|\theta\rangle$ and bra
                                        $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex numbers and both have two
                                        real numbers. The inner dot product is
                                        $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                        formulated as:
                                        $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                        For some quantum state for the qubit $\psi$ can be defined as:
                                        $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$ $=\langle
                                        v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where, squaring our
                                        projections, $\langle v|\psi\rangle$ and $\langle h|\psi\rangle$, onto axis an
                                        axis gives us our respective probabilites for $|v\rangle$ and $|h\rangle$
                                        respectfully. An example of a qubit is the spin of an electron. The two levels
                                        of this quibit are spin up or spin down. What differs from a classical system is
                                        that quantum mechanics allows for the qubit to be in a coherent superposition of
                                        both states simultaneously. Measuring a qubit in a basis gives a projective
                                        measurement of a qubit of state $\phi$ in its computational basis can be
                                        expressed as a linear combination of state vectors, such as:
                                        $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured in a
                                        basis, collapses the qubit to either the quantum state $|0\rangle$ or
                                        $|1\rangle$ given by the respective norm-square of the probability amplitudes
                                        $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch Sphere We can use a
                                        bloch ball or sphere to help us visualize sping down $0$ and spin up $1$ of a
                                        single qubit. The bloch spehere has a radius of 1, meaning that $|0\rangle$
                                        corresponds to $(x,y,z)$ point $(x,y,1)$ and $|1\rangle$ corresponds to
                                        $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows a spin up or sping down.
                                        Bloch SphereWe can use a bloch ball or sphere to help us visualize sping down
                                        $0$ and spin up $1$ of a single qubit. The bloch spehere has a radius of 1,
                                        meaning that $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and
                                        $|1\rangle$ corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows
                                        a spin up or sping down. Superposition In classical computing, states $0$ and
                                        $1$ would be the only states that exist for the bit. However, in quantum
                                        mechanics a quibit can be both in the state of $|0\rangle$ and $|1\rangle$. This
                                        is what gives quantum computers more processing power, as a single qubit can be
                                        in more states and therefore represent more information than a single classical
                                        bit. This means that the qubit can have an $80$% of being in state $|0\rangle$
                                        and $20$% of being in state $|1\rangle$, or $75$% of being in state $|0\rangle$
                                        and $25$% of being in state $|1\rangle$. Unlike a classical bit where there is
                                        either a $100$% of the classical bit being in state $0$ and $0$% of being a $1$
                                        or a $0$% of the classical bit being a $0$ and $100$% of being a $1$. To allow
                                        for the qubit to be in superposition, we need to levearage Hilbert Space. Again,
                                        Hilbert space is represented using complex vector space. We use complex vector
                                        space, because it is the easiest way for the math to work. Let's represent this
                                        qubit in superposition as a vector using dirac notation. For the qubit have, for
                                        example, a $50$% of being in state $|0\rangle$ and $50$% of being in state
                                        $|1\rangle$, the vector should look like:
                                        $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$ are
                                        both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$. SuperpositionIn
                                        classical computing, states $0$ and $1$ would be the only states that exist for
                                        the bit. However, in quantum mechanics a quibit can be both in the state of
                                        $|0\rangle$ and $|1\rangle$. This is what gives quantum computers more
                                        processing power, as a single qubit can be in more states and therefore
                                        represent more information than a single classical bit. This means that the
                                        qubit can have an $80$% of being in state $|0\rangle$ and $20$% of being in
                                        state $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of being in
                                        state $|1\rangle$. Unlike a classical bit where there is either a $100$% of the
                                        classical bit being in state $0$ and $0$% of being a $1$ or a $0$% of the
                                        classical bit being a $0$ and $100$% of being a $1$. To allow for the qubit to
                                        be in superposition, we need to levearage Hilbert Space. Again, Hilbert space is
                                        represented using complex vector space. We use complex vector space, because it
                                        is the easiest way for the math to work. Let's represent this qubit in
                                        superposition as a vector using dirac notation. For the qubit have, for example,
                                        a $50$% of being in state $|0\rangle$ and $50$% of being in state $|1\rangle$,
                                        the vector should look like: $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The
                                        coefficent for $0$ and $1$ are both $\frac{1}{\sqrt{2}}$ and therefore equal
                                        parts $0$ and $1$. Pauli Matrices Pauli matrices have a core importance in
                                        quantum physics, and when combinex with an identity matrix, pauli matrices form
                                        a basis for all single quantum gates. Pauli matrices are defined as:
                                        $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}$,
                                        $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                        $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is usally
                                        refered to as the NOT gate and can be written as $X$. The NOT gate inverts
                                        $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$ are phase shifting
                                        gates. Pauli MatricesPauli matrices have a core importance in quantum physics,
                                        and when combinex with an identity matrix, pauli matrices form a basis for all
                                        single quantum gates. Pauli matrices are defined as: $\sigma_{x}=\begin{pmatrix}
                                        0&1 \\ 1&0 \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0
                                        \end{pmatrix}$, $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$
                                        $\sigma_{x}$ is usally refered to as the NOT gate and can be written as $X$. The
                                        NOT gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$ are
                                        phase shifting gates. Multiple Qubits Quantum Registers A system that contains
                                        more than one qubit is represented in a quantum register. A system with two
                                        qubits is represented using a four-dimensional Hilbert space $H_{4} = H_{2}
                                        \otimes H_{2}$. The orthonormal basis is $\left\{|0\rangle |0\rangle, |0\rangle
                                        |1\rangle, |1\rangle |0\rangle, |1\rangle |1\rangle\ \right\}$. $|0\rangle
                                        |0\rangle$ can be more succinctly written as $|00\rangle$. The same is true for
                                        $|0\rangle |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a
                                        part from four-dimensional Hilbert space is a unit-length vector:
                                        $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$ Again,
                                        as it is with a single qubit, it is required that $|c_{0}|^{2} + |c_{1}|^{2} +
                                        |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit system will give
                                        $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2,
                                        |c_{3}|^2$, respectively. If we want to observe one of the qubits, then the
                                        standard rules of probabilities apply. It is important to note that the tensor
                                        product of these vectors does not commute. Meaning, that $|0\rangle|1\rangle
                                        \neq |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                        individually. Quantum Entanglement Maximally Entangled Bell States Bell states
                                        or Einstein, Podolski and Rosen pairs are the maximally entangled quantum states
                                        of a qubit system such that a quantum mechanical system is composed of two
                                        interacting two-level subsystems. The 4 types of maximially entangled Bell
                                        states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Multiple QubitsQuantum Registers A
                                        system that contains more than one qubit is represented in a quantum register. A
                                        system with two qubits is represented using a four-dimensional Hilbert space
                                        $H_{4} = H_{2} \otimes H_{2}$. The orthonormal basis is $\left\{|0\rangle
                                        |0\rangle, |0\rangle |1\rangle, |1\rangle |0\rangle, |1\rangle |1\rangle\
                                        \right\}$. $|0\rangle |0\rangle$ can be more succinctly written as $|00\rangle$.
                                        The same is true for $|0\rangle |1\rangle = |01\rangle$, etc. A state of this
                                        two-qubit system, a part from four-dimensional Hilbert space is a unit-length
                                        vector: $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$
                                        Again, as it is with a single qubit, it is required that $|c_{0}|^{2} +
                                        |c_{1}|^{2} + |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit
                                        system will give $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2,
                                        |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$, respectively. If we want to observe one of the
                                        qubits, then the standard rules of probabilities apply. It is important to note
                                        that the tensor product of these vectors does not commute. Meaning, that
                                        $|0\rangle|1\rangle \neq |1\rangle|0\rangle$. Linear ordering is used to address
                                        the qubits individually. Quantum RegistersA system that contains more than one
                                        qubit is represented in a quantum register. A system with two qubits is
                                        represented using a four-dimensional Hilbert space $H_{4} = H_{2} \otimes
                                        H_{2}$. The orthonormal basis is $\left\{|0\rangle |0\rangle, |0\rangle
                                        |1\rangle, |1\rangle |0\rangle, |1\rangle |1\rangle\ \right\}$. $|0\rangle
                                        |0\rangle$ can be more succinctly written as $|00\rangle$. The same is true for
                                        $|0\rangle |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a
                                        part from four-dimensional Hilbert space is a unit-length vector:
                                        $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$ Again,
                                        as it is with a single qubit, it is required that $|c_{0}|^{2} + |c_{1}|^{2} +
                                        |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit system will give
                                        $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2,
                                        |c_{3}|^2$, respectively. If we want to observe one of the qubits, then the
                                        standard rules of probabilities apply. It is important to note that the tensor
                                        product of these vectors does not commute. Meaning, that $|0\rangle|1\rangle
                                        \neq |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                        individually. Quantum Entanglement Maximally Entangled Bell States Bell states
                                        or Einstein, Podolski and Rosen pairs are the maximally entangled quantum states
                                        of a qubit system such that a quantum mechanical system is composed of two
                                        interacting two-level subsystems. The 4 types of maximially entangled Bell
                                        states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Quantum EntanglementMaximally Entangled
                                        Bell States Bell states or Einstein, Podolski and Rosen pairs are the maximally
                                        entangled quantum states of a qubit system such that a quantum mechanical system
                                        is composed of two interacting two-level subsystems. The 4 types of maximially
                                        entangled Bell states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Maximally Entangled Bell States Bell
                                        states or Einstein, Podolski and Rosen pairs are the maximally entangled quantum
                                        states of a qubit system such that a quantum mechanical system is composed of
                                        two interacting two-level subsystems. The 4 types of maximially entangled Bell
                                        states can be defined as: $|\Phi^{+}\rangle =
                                        \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                        \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                        \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                        \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments The
                                        orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                        $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                        $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$ basis to
                                        the $X$ bases, we need to apply a Hadmard gate to the state, allowing for the
                                        state to be halfway: $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ Y
                                        Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway we need
                                        to apply $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                        -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Z Bases Measurment No transformations needed, since we measure in
                                        the "normal" bases. MeasurmentsBases Measurments The orthagonal basis states for
                                        $Z$, $X$, and $Y$, are as follows: $Z=\{|0\rangle,|1\rangle\}$
                                        $X=\{|-\rangle,|+\rangle\}$ $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To
                                        measure from the $Z$ basis to the $X$ bases, we need to apply a Hadmard gate to
                                        the state, allowing for the state to be halfway:
                                        $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ Y
                                        Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway we need
                                        to apply $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                        -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Z Bases Measurment No transformations needed, since we measure in
                                        the "normal" bases. Bases MeasurmentsThe orthagonal basis states for $Z$, $X$,
                                        and $Y$, are as follows: $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                        $Y=\{|i\rangle,|-i\rangle\}$
                                        $Z=\{|0\rangle,|1\rangle\}$$X=\{|-\rangle,|+\rangle\}$$Y=\{|i\rangle,|-i\rangle\}$X
                                        Bases Measurment To measure from the $Z$ basis to the $X$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway:
                                        $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ X
                                        Bases MeasurmentTo measure from the $Z$ basis to the $X$ bases, we need to apply
                                        a Hadmard gate to the state, allowing for the state to be halfway:
                                        $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                        $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                        -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$ Y
                                        Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases, we need to
                                        apply a Hadmard gate to the state, allowing for the state to be halfway we need
                                        to apply $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                        -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Y Bases MeasurmentThen to measure from the $Z$ basis to the $Y$
                                        bases, we need to apply a Hadmard gate to the state, allowing for the state to
                                        be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                        $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                        \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                        \end{pmatrix}$ Z Bases Measurment No transformations needed, since we measure in
                                        the "normal" bases. Z Bases MeasurmentNo transformations needed, since we
                                        measure in the "normal" bases. Operators Unitary Operator All quantum gates are
                                        unitary, which we will define in the following. Let us use the basis measurment
                                        of $Z$ with the coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                        \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                        operation on a quibit, called an unary quantum gate, is a unitary mapping $U:
                                        H_{2} \rightarrow H_{2}$ with the following defining linear operation:
                                        $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle \mapsto c|0\rangle +
                                        d|1\rangle$ An important aspect of all quantum gates is that they are unitary.
                                        Meaning, that for some given matrix operation $U$, defined as: $\begin{pmatrix}
                                        a & b \\ c & d \end{pmatrix}$ it is neccesary that this matrix is unitary in
                                        order to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                        unitary and valid, the following equivalency must be true: $UU^{\dagger}=I$
                                        where $U^{\dagger}$ represents the conjugate transpose and $I$ is the identity
                                        matrix. Another important qualtiy of an unitary matrix is: $U^{\dagger}=U^{-1}$
                                        Represented as matrices, we get: $\begin{pmatrix} a & b \\ c & d \end{pmatrix}
                                        \begin{pmatrix} a^* & b^* \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\
                                        0 & 1 \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                        denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                        d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation $a^{*}$
                                        stands for the complex conjugate of the complex number $a$. The complex
                                        conjugate of a complex number is the number with an equal real part and an
                                        imaginary part equal in magnitude, but opposite in sign of the complex number.
                                        The mapping of for unary quantum operator, when represented in a quantum circut,
                                        can be a quantum gate. Where the output of the quantum gate must have the same
                                        dimensionality as its input. So, $U: H_{n} \rightarrow H_{n}$, where $n$ is the
                                        number of dimensions of $H$. Hermitian Operator A unitary operator is Hermitian
                                        if: $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                        matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian matrix is a
                                        special case of a unitary matrix, where all Hermitian operators or unitary
                                        operators, but not all unitary operators, are Hermitian. Natural Operator An
                                        Hermitian operator is Natural if: $U^{\dagger}U=UU^{\dagger}$, where
                                        $U^{\dagger}$ is the conjugate transpose of $U$. the operator has spectural
                                        decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ OperatorsUnitary
                                        Operator All quantum gates are unitary, which we will define in the following.
                                        Let us use the basis measurment of $Z$ with the coordinate representation
                                        $|0\rangle = \begin{bmatrix} 1\\ 0 \end{bmatrix}$ and $|1\rangle =
                                        \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An operation on a quibit, called an unary
                                        quantum gate, is a unitary mapping $U: H_{2} \rightarrow H_{2}$ with the
                                        following defining linear operation: $|0\rangle \mapsto a|0\rangle + b|1\rangle$
                                        $|1\rangle \mapsto c|0\rangle + d|1\rangle$ An important aspect of all quantum
                                        gates is that they are unitary. Meaning, that for some given matrix operation
                                        $U$, defined as: $\begin{pmatrix} a & b \\ c & d \end{pmatrix}$ it is neccesary
                                        that this matrix is unitary in order to be a valid quatnum gate. So, for some
                                        matrix or operator $U$ to be unitary and valid, the following equivalency must
                                        be true: $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                        and $I$ is the identity matrix. Another important qualtiy of an unitary matrix
                                        is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get: $\begin{pmatrix} a & b
                                        \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^* \\ c^* & d^* \end{pmatrix} =
                                        \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$ Written in vector form, the
                                        conjugate transpose of $U$ is denoted as $U^{\dagger}$ and can be wrriten as:
                                        $\begin{pmatrix}a\\ d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the
                                        notation $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                        complex conjugate of a complex number is the number with an equal real part and
                                        an imaginary part equal in magnitude, but opposite in sign of the complex
                                        number. The mapping of for unary quantum operator, when represented in a quantum
                                        circut, can be a quantum gate. Where the output of the quantum gate must have
                                        the same dimensionality as its input. So, $U: H_{n} \rightarrow H_{n}$, where
                                        $n$ is the number of dimensions of $H$. Unitary OperatorAll quantum gates are
                                        unitary, which we will define in the following. Let us use the basis measurment
                                        of $Z$ with the coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                        \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                        operation on a quibit, called an unary quantum gate, is a unitary mapping $U:
                                        H_{2} \rightarrow H_{2}$ with the following defining linear operation:
                                        $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle \mapsto c|0\rangle +
                                        d|1\rangle$ An important aspect of all quantum gates is that they are unitary.
                                        Meaning, that for some given matrix operation $U$, defined as: $\begin{pmatrix}
                                        a & b \\ c & d \end{pmatrix}$ it is neccesary that this matrix is unitary in
                                        order to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                        unitary and valid, the following equivalency must be true: $UU^{\dagger}=I$
                                        where $U^{\dagger}$ represents the conjugate transpose and $I$ is the identity
                                        matrix. Another important qualtiy of an unitary matrix is: $U^{\dagger}=U^{-1}$
                                        Represented as matrices, we get: $\begin{pmatrix} a & b \\ c & d \end{pmatrix}
                                        \begin{pmatrix} a^* & b^* \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\
                                        0 & 1 \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                        denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                        d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation $a^{*}$
                                        stands for the complex conjugate of the complex number $a$. The complex
                                        conjugate of a complex number is the number with an equal real part and an
                                        imaginary part equal in magnitude, but opposite in sign of the complex number.
                                        The mapping of for unary quantum operator, when represented in a quantum circut,
                                        can be a quantum gate. Where the output of the quantum gate must have the same
                                        dimensionality as its input. So, $U: H_{n} \rightarrow H_{n}$, where $n$ is the
                                        number of dimensions of $H$. Hermitian Operator A unitary operator is Hermitian
                                        if: $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                        matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian matrix is a
                                        special case of a unitary matrix, where all Hermitian operators or unitary
                                        operators, but not all unitary operators, are Hermitian. Hermitian OperatorA
                                        unitary operator is Hermitian if: unitary operator$U^{\dagger}=U$, where
                                        $U^{\dagger}$ is the conjugate transpose of the matrix $U$. $U^{2}=I$, where $I$
                                        is an identity matrix. $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate
                                        transpose of the matrix $U$. $U^{2}=I$, where $I$ is an identity matrix.A
                                        Hermatian matrix is a special case of a unitary matrix, where all Hermitian
                                        operators or unitary operators, but not all unitary operators, are Hermitian.
                                        Natural Operator An Hermitian operator is Natural if:
                                        $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate transpose of
                                        $U$. the operator has spectural decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Natural OperatorAn
                                        Hermitian operator is Natural if: Hermitian operator$U^{\dagger}U=UU^{\dagger}$,
                                        where $U^{\dagger}$ is the conjugate transpose of $U$. the operator has
                                        spectural decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$
                                        $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate transpose of
                                        $U$.the operator has spectural decomposition, where $U$ can be decomposed as:
                                        $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum Algorithms
                                        Quantum Parallelism Suppose we want to evaluate a function $f(x)$, where the
                                        function $f$ expresses some computation or algorithm. A use case for quantum
                                        parallelism is to evaluate $f(x)$ with many different values for the output of
                                        the computation or algorithm on the input $x$ simultaneously. In essence, we can
                                        evaluate many different values of $x$ on $f$ in parallel by exploiting quantum
                                        effects. This quantum effect exploit feature is fundamental in many quantum
                                        algorithms. To continue, we will look at how quantum parallelism works. Consider
                                        the one-bit domain and range function $f(x):\{0,1\}\rightarrow\{0,1\}$. To
                                        compute this function $f$ on a quantum computer, we will use a two-qubit quantum
                                        computer with the starting state $|x,y\rangle$. The transformation on the domain
                                        or 'data' register to the range or 'target' register of this initial two qubit
                                        state is described by the following unitary function:
                                        $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                        represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts on $y$,
                                        and its value is $0$ then the value of the second qubit in the 'target' register
                                        is the value $f(x)$, given whatever function $f$ represents. The functions
                                        effect on $x$ is arbritrary for now. The final collapsed state $|\psi\rangle$ is
                                        an element of the set of final states or 'target' register $|x,y\oplus
                                        f(x)\rangle$, which again is given by the unitary transformation $U_{f}$ on the
                                        start state $|x,y\rangle$. Given the input $q_{0}=x=|0\rangle$, we will apply
                                        the Hadmard gate $H$ to $x$, such that now:
                                        $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting state
                                        is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the resulting new
                                        state is not apart of the starting computational basis $\{0,1\}$. Next, the
                                        unitary function or blackbox computation/algorithm $U_{f}$ can be applied to the
                                        current 'data' register. The resulting mapping of the unitary function $U_{f}$
                                        is: $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                        f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                        $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                        Meaning that the final resulting state for a two-qubit quantum computer has a
                                        $50$% chance of being $|0,f(0)\rangle$ and $50$% of being $.5|1,f(1)\rangle$.
                                        Given in the same form as the range for $U_{f}$ given above: $|x,y'\rangle$,
                                        where $y'=y\oplus f(x)$ All of this means that the information given by the
                                        mapping for $f(0)$ and $f(1)$ was simultaneously evaluated by applying
                                        superposition and the unitary function on the starting 'data' register. Thus,
                                        $f(x)$ has been computed for two values of $x$ in parellel. The resulting set of
                                        all possible states computed in parallel is given by the resulting 'target'
                                        register is given by quantum exploitation and aptly named 'quantum parallelism'.
                                        Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$ values
                                        of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit system such that
                                        $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply the NOT gate to $y$,
                                        giving the what will be the start state $|\psi_{0}\rangle=|01\rangle$. Next, we
                                        will apply a Hadmard gates to $x$ and $y$ individually, yielding the state:
                                        $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        where for state $|\psi_{1}\rangle$: $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                        $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary unitary
                                        function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                        $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        From this, we can then say that if $f(0)=f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$. If
                                        $f(0)=f(1)$, the resulting state is:
                                        $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$
                                        $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0) \oplus
                                        f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus, $f(0)$ interfers
                                        with $f(1)$ when we simultaneously evalute $f(x)$ with quantum parallelism.
                                        Grovers Search Algorithm The authors describe the process for Grovers Search
                                        Algorithm in the following sequential two main steps: Hadmard transformation and
                                        Grover iteration or Grover operator $G$. Hadmard Transformation The Hadmard
                                        transform puts the qubits of the quantum computer into equal superposition
                                        states, defined as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                        \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm implements a
                                        repeated quantum subroutine called Grover iteration or operator, denoted as $G$.
                                        This quantum iteration can be broken up in four steps: Apply oracle $O$ Apply
                                        Hadmard transform $H^{\otimes{n}}$ Apply conditional phase shift on quantum
                                        register, such that every quantum basis state except $|r\rangle$ is phased
                                        shifted $-1$. Meaning that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1
                                        \sin \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                        \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks $|w\rangle$
                                        only. Then, reflect the state around $|s\rangle$ using gate $R_{s}$, such that
                                        moving our state towards $|w\rangle$. Lastly, apply the Hadmard transform
                                        $H^{\otimes{n}}$ Where combined steps of 2, 3, 4, or Grovers iteration without
                                        the oracle step can be written as: $H^{\otimes n}(2|0\rangle\langle 0|-I)
                                        H^{\otimes n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly
                                        eighted superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                        Thus, including the oracle step now, Grovers iteration $G$ as a whole can be
                                        written, more generically for a given quantum state $\psi$ as
                                        $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image Source: $[1]$
                                        To continue. Quantum AlgorithmsQuantum Parallelism Suppose we want to evaluate a
                                        function $f(x)$, where the function $f$ expresses some computation or algorithm.
                                        A use case for quantum parallelism is to evaluate $f(x)$ with many different
                                        values for the output of the computation or algorithm on the input $x$
                                        simultaneously. In essence, we can evaluate many different values of $x$ on $f$
                                        in parallel by exploiting quantum effects. This quantum effect exploit feature
                                        is fundamental in many quantum algorithms. To continue, we will look at how
                                        quantum parallelism works. Consider the one-bit domain and range function
                                        $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$ on a quantum
                                        computer, we will use a two-qubit quantum computer with the starting state
                                        $|x,y\rangle$. The transformation on the domain or 'data' register to the range
                                        or 'target' register of this initial two qubit state is described by the
                                        following unitary function: $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$
                                        where the $\oplus$ represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When
                                        $\oplus$ acts on $y$, and its value is $0$ then the value of the second qubit in
                                        the 'target' register is the value $f(x)$, given whatever function $f$
                                        represents. The functions effect on $x$ is arbritrary for now. The final
                                        collapsed state $|\psi\rangle$ is an element of the set of final states or
                                        'target' register $|x,y\oplus f(x)\rangle$, which again is given by the unitary
                                        transformation $U_{f}$ on the start state $|x,y\rangle$. Given the input
                                        $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$ to $x$, such that now:
                                        $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting state
                                        is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the resulting new
                                        state is not apart of the starting computational basis $\{0,1\}$. Next, the
                                        unitary function or blackbox computation/algorithm $U_{f}$ can be applied to the
                                        current 'data' register. The resulting mapping of the unitary function $U_{f}$
                                        is: $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                        f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                        $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                        Meaning that the final resulting state for a two-qubit quantum computer has a
                                        $50$% chance of being $|0,f(0)\rangle$ and $50$% of being $.5|1,f(1)\rangle$.
                                        Given in the same form as the range for $U_{f}$ given above: $|x,y'\rangle$,
                                        where $y'=y\oplus f(x)$ All of this means that the information given by the
                                        mapping for $f(0)$ and $f(1)$ was simultaneously evaluated by applying
                                        superposition and the unitary function on the starting 'data' register. Thus,
                                        $f(x)$ has been computed for two values of $x$ in parellel. The resulting set of
                                        all possible states computed in parallel is given by the resulting 'target'
                                        register is given by quantum exploitation and aptly named 'quantum parallelism'.
                                        Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$ values
                                        of $x$ simultaneously. Quantum ParallelismSuppose we want to evaluate a function
                                        $f(x)$, where the function $f$ expresses some computation or algorithm. A use
                                        case for quantum parallelism is to evaluate $f(x)$ with many different values
                                        for the output of the computation or algorithm on the input $x$ simultaneously.
                                        In essence, we can evaluate many different values of $x$ on $f$ in parallel by
                                        exploiting quantum effects. This quantum effect exploit feature is fundamental
                                        in many quantum algorithms. To continue, we will look at how quantum parallelism
                                        works. Consider the one-bit domain and range function
                                        $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$ on a quantum
                                        computer, we will use a two-qubit quantum computer with the starting state
                                        $|x,y\rangle$. The transformation on the domain or 'data' register to the range
                                        or 'target' register of this initial two qubit state is described by the
                                        following unitary function: $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$
                                        where the $\oplus$ represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When
                                        $\oplus$ acts on $y$, and its value is $0$ then the value of the second qubit in
                                        the 'target' register is the value $f(x)$, given whatever function $f$
                                        represents. The functions effect on $x$ is arbritrary for now. The final
                                        collapsed state $|\psi\rangle$ is an element of the set of final states or
                                        'target' register $|x,y\oplus f(x)\rangle$, which again is given by the unitary
                                        transformation $U_{f}$ on the start state $|x,y\rangle$. Given the input
                                        $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$ to $x$, such that now:
                                        $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting state
                                        is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the resulting new
                                        state is not apart of the starting computational basis $\{0,1\}$. Next, the
                                        unitary function or blackbox computation/algorithm $U_{f}$ can be applied to the
                                        current 'data' register. The resulting mapping of the unitary function $U_{f}$
                                        is: $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                        f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                        $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                        Meaning that the final resulting state for a two-qubit quantum computer has a
                                        $50$% chance of being $|0,f(0)\rangle$ and $50$% of being $.5|1,f(1)\rangle$.
                                        Given in the same form as the range for $U_{f}$ given above: $|x,y'\rangle$,
                                        where $y'=y\oplus f(x)$ All of this means that the information given by the
                                        mapping for $f(0)$ and $f(1)$ was simultaneously evaluated by applying
                                        superposition and the unitary function on the starting 'data' register. Thus,
                                        $f(x)$ has been computed for two values of $x$ in parellel. The resulting set of
                                        all possible states computed in parallel is given by the resulting 'target'
                                        register is given by quantum exploitation and aptly named 'quantum parallelism'.
                                        Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$ values
                                        of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit system such that
                                        $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply the NOT gate to $y$,
                                        giving the what will be the start state $|\psi_{0}\rangle=|01\rangle$. Next, we
                                        will apply a Hadmard gates to $x$ and $y$ individually, yielding the state:
                                        $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        where for state $|\psi_{1}\rangle$: $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                        $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary unitary
                                        function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                        $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        From this, we can then say that if $f(0)=f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$. If
                                        $f(0)=f(1)$, the resulting state is:
                                        $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$
                                        $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0) \oplus
                                        f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus, $f(0)$ interfers
                                        with $f(1)$ when we simultaneously evalute $f(x)$ with quantum parallelism.
                                        Deutsch's AlgorithmConsider a two-qubit system such that
                                        $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply the NOT gate to $y$,
                                        giving the what will be the start state $|\psi_{0}\rangle=|01\rangle$. Next, we
                                        will apply a Hadmard gates to $x$ and $y$ individually, yielding the state:
                                        $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        where for state $|\psi_{1}\rangle$: $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                        $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary unitary
                                        function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                        $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        From this, we can then say that if $f(0)=f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        or if $f(0)\neq f(1)$:
                                        $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$. If
                                        $f(0)=f(1)$, the resulting state is:
                                        $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$or
                                        if $f(0)\neq f(1)$
                                        $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                        This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0) \oplus
                                        f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus, $f(0)$ interfers
                                        with $f(1)$ when we simultaneously evalute $f(x)$ with quantum parallelism.
                                        Grovers Search Algorithm The authors describe the process for Grovers Search
                                        Algorithm in the following sequential two main steps: Hadmard transformation and
                                        Grover iteration or Grover operator $G$. Hadmard Transformation The Hadmard
                                        transform puts the qubits of the quantum computer into equal superposition
                                        states, defined as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                        \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm implements a
                                        repeated quantum subroutine called Grover iteration or operator, denoted as $G$.
                                        This quantum iteration can be broken up in four steps: Apply oracle $O$ Apply
                                        Hadmard transform $H^{\otimes{n}}$ Apply conditional phase shift on quantum
                                        register, such that every quantum basis state except $|r\rangle$ is phased
                                        shifted $-1$. Meaning that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1
                                        \sin \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                        \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks $|w\rangle$
                                        only. Then, reflect the state around $|s\rangle$ using gate $R_{s}$, such that
                                        moving our state towards $|w\rangle$. Lastly, apply the Hadmard transform
                                        $H^{\otimes{n}}$ Where combined steps of 2, 3, 4, or Grovers iteration without
                                        the oracle step can be written as: $H^{\otimes n}(2|0\rangle\langle 0|-I)
                                        H^{\otimes n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly
                                        eighted superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                        Thus, including the oracle step now, Grovers iteration $G$ as a whole can be
                                        written, more generically for a given quantum state $\psi$ as
                                        $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image Source: $[1]$
                                        To continue. Grovers Search AlgorithmThe authors describe the process for
                                        Grovers Search Algorithm in the following sequential two main steps: Hadmard
                                        transformation and Grover iteration or Grover operator $G$. Hadmard
                                        Transformation The Hadmard transform puts the qubits of the quantum computer
                                        into equal superposition states, defined as: $|s\rangle=|+\rangle^{\otimes
                                        n}=\frac{1}{\sqrt{N}} \sum_{x \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers
                                        search algorithm implements a repeated quantum subroutine called Grover
                                        iteration or operator, denoted as $G$. This quantum iteration can be broken up
                                        in four steps: Apply oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$ Apply
                                        conditional phase shift on quantum register, such that every quantum basis state
                                        except $|r\rangle$ is phased shifted $-1$. Meaning that for unitary:
                                        $\begin{aligned} U_f|s\rangle & =(-1)^1 \sin \theta|w\rangle \\ & +(-1)^0 \cos
                                        \theta|r\rangle \\ & =-\sin \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$
                                        This phase kicks $|w\rangle$ only. Then, reflect the state around $|s\rangle$
                                        using gate $R_{s}$, such that moving our state towards $|w\rangle$. Lastly,
                                        apply the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2, 3, 4, or
                                        Grovers iteration without the oracle step can be written as: $H^{\otimes
                                        n}(2|0\rangle\langle 0|-I) H^{\otimes n}=2|\psi\rangle\langle\psi|-I$ where
                                        $|\psi\rangle$ is equaly eighted superposition of states $\frac{1}{N^{1 / 2}}
                                        \sum_{x=0}^{N-1}|w\rangle$. Thus, including the oracle step now, Grovers
                                        iteration $G$ as a whole can be written, more generically for a given quantum
                                        state $\psi$ as $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as:
                                        Image Source: $[1]$ The authors describe the process for Grovers Search
                                        Algorithm in the following sequential two main steps: Hadmard transformation and
                                        Grover iteration or Grover operator $G$. Hadmard transformationGrover iteration
                                        or Grover operator $G$Hadmard TransformationHadmard TransformationThe Hadmard
                                        transform puts the qubits of the quantum computer into equal superposition
                                        states, defined as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                        \in\{0,1\}^n}|x\rangle$ Grover OperationGrover OperationGrovers search algorithm
                                        implements a repeated quantum subroutine called Grover iteration or operator,
                                        denoted as $G$. This quantum iteration can be broken up in four steps: Apply
                                        oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$ Apply conditional phase
                                        shift on quantum register, such that every quantum basis state except
                                        $|r\rangle$ is phased shifted $-1$. Meaning that for unitary: $\begin{aligned}
                                        U_f|s\rangle & =(-1)^1 \sin \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\
                                        & =-\sin \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                        $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate $R_{s}$,
                                        such that moving our state towards $|w\rangle$. Lastly, apply the Hadmard
                                        transform $H^{\otimes{n}}$ Apply oracle $O$Apply Hadmard transform
                                        $H^{\otimes{n}}$Apply conditional phase shift on quantum register, such that
                                        every quantum basis state except $|r\rangle$ is phased shifted $-1$. Meaning
                                        that for unitary:$\begin{aligned} U_f|s\rangle & =(-1)^1 \sin \theta|w\rangle \\
                                        & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin \theta|w\rangle+\cos \theta|r\rangle
                                        \end{aligned}$ This phase kicks $|w\rangle$ only. Then, reflect the state around
                                        $|s\rangle$ using gate $R_{s}$, such that moving our state towards $|w\rangle$.
                                        Lastly, apply the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2,
                                        3, 4, or Grovers iteration without the oracle step can be written as:
                                        $H^{\otimes n}(2|0\rangle\langle 0|-I) H^{\otimes
                                        n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly eighted
                                        superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$. Thus,
                                        including the oracle step now, Grovers iteration $G$ as a whole can be written,
                                        more generically for a given quantum state $\psi$ as
                                        $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image Source: $[1]$
                                        $[1]$ To continue. Quantum Computational Theory Quantum Automata Theory Quantum
                                        Turing Machine QTM $\delta$ Function A Quantum Turing Machine QTM can be
                                        expressed similarly to a traditional Turing Machine TM with all components
                                        reformulated canonically except for the transition function $\delta$. Below, is
                                        the formal definition of a QTM. Quantum Turing Machine is a 7-tuple: $(Q,
                                        \Sigma, \Gamma, \delta, q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma,
                                        \Gamma$ are all finite sets and: $Q$ is a set of states $\Sigma$ is an alphabet
                                        not containing the blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where
                                        $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to be
                                        two-way infinite, with squares indexed by the set of integers $\mathbb{Z}$
                                        $\delta$ is a transition function described as $\delta : Q \times \Gamma
                                        \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$ $q_{0}
                                        \in Q$ is the initial state $q_{accept} \in Q$ is the accept state $q_{reject}
                                        \in Q$ is the reject state, where $q_{reject} \neq q_{accept}$ Image Source:
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                        Quantum Computational TheoryQuantum Automata Theory Quantum Turing Machine QTM
                                        $\delta$ Function A Quantum Turing Machine QTM can be expressed similarly to a
                                        traditional Turing Machine TM with all components reformulated canonically
                                        except for the transition function $\delta$. Below, is the formal definition of
                                        a QTM. Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                        q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets and: $Q$
                                        is a set of states $\Sigma$ is an alphabet not containing the blank symbol
                                        $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$ and $\Sigma
                                        \subseteq \Gamma$. The tape is assumed to be two-way infinite, with squares
                                        indexed by the set of integers $\mathbb{Z}$ $\delta$ is a transition function
                                        described as $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times \Gamma
                                        \times \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state $q_{accept}
                                        \in Q$ is the accept state $q_{reject} \in Q$ is the reject state, where
                                        $q_{reject} \neq q_{accept}$ Image Source:
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                        Quantum Automata TheoryQuantum Turing Machine QTM $\delta$ Function A Quantum
                                        Turing Machine QTM can be expressed similarly to a traditional Turing Machine TM
                                        with all components reformulated canonically except for the transition function
                                        $\delta$. Below, is the formal definition of a QTM. Quantum Turing Machine is a
                                        7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{accept}, q_{reject})$ where $Q,
                                        \Sigma, \Gamma$ are all finite sets and: $Q$ is a set of states $\Sigma$ is an
                                        alphabet not containing the blank symbol $\sqcup$ $\Gamma$ is the tape alphabet,
                                        where $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to
                                        be two-way infinite, with squares indexed by the set of integers $\mathbb{Z}$
                                        $\delta$ is a transition function described as $\delta : Q \times \Gamma
                                        \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$ $q_{0}
                                        \in Q$ is the initial state $q_{accept} \in Q$ is the accept state $q_{reject}
                                        \in Q$ is the reject state, where $q_{reject} \neq q_{accept}$ Image Source:
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                        Quantum Turing MachineQTM $\delta$ Function QTM$\delta$ FunctionA Quantum Turing
                                        Machine QTM can be expressed similarly to a traditional Turing Machine TM with
                                        all components reformulated canonically except for the transition function
                                        $\delta$. Below, is the formal definition of a QTM. exceptQuantum Turing Machine
                                        is a 7-tuple:$(Q, \Sigma, \Gamma, \delta, q_{0}, q_{accept}, q_{reject})$ where
                                        $Q, \Sigma, \Gamma$ are all finite sets and: $Q$ is a set of states $\Sigma$ is
                                        an alphabet not containing the blank symbol $\sqcup$ $\Gamma$ is the tape
                                        alphabet, where $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is
                                        assumed to be two-way infinite, with squares indexed by the set of integers
                                        $\mathbb{Z}$ $\delta$ is a transition function described as $\delta : Q \times
                                        \Gamma \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$
                                        $q_{0} \in Q$ is the initial state $q_{accept} \in Q$ is the accept state
                                        $q_{reject} \in Q$ is the reject state, where $q_{reject} \neq q_{accept}$ $Q$
                                        is a set of states $\Sigma$ is an alphabet not containing the blank symbol
                                        $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$ and $\Sigma
                                        \subseteq \Gamma$. The tape is assumed to be two-way infinite, with squares
                                        indexed by the set of integers $\mathbb{Z}$ $\delta$ is a transition function
                                        described as $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times \Gamma
                                        \times \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state $q_{accept}
                                        \in Q$ is the accept state $q_{reject} \in Q$ is the reject state, where
                                        $q_{reject} \neq q_{accept}$ $Q$ is a set of states$\Sigma$ is an alphabet not
                                        containing the blank symbol $\sqcup$$\Gamma$ is the tape alphabet, where $\sqcup
                                        \in \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way
                                        infinite, with squares indexed by the set of integers $\mathbb{Z}$ $\delta$ is a
                                        transition function described as $\delta : Q \times \Gamma \rightarrow
                                        \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the
                                        initial state$q_{accept} \in Q$ is the accept state$q_{reject} \in Q$ is the
                                        reject state, where $q_{reject} \neq q_{accept}$Image Source:
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                        https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.pngQuantum
                                        Networks Superdense Coding and Quantum Teleportation Superdense Coding Given
                                        Alice wants to send classical information to Bob, quantum entanglement $n$
                                        qubits can store $2n$ qubits total of information. Say Alice needs to send one
                                        qubit of infromation, they can do so but needs Bob to already share a second
                                        qubit. So, given that Alice and Bob already share a pair of entangled qubits in
                                        state $|\Phi^{+}\rangle$, defined as: (Alice) --
                                        $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to send:
                                        $00$: Alice does nothing to their qubit, so the qubit is still in state:
                                        $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice applies
                                        $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that Bob has
                                        both qubits in one of four Bell basis, Bob will know what the qubit Alice wants
                                        to send by measuring the two qubits and seeing what state they are in. This is
                                        called a Bell measurment. If we then extrapolate this method, then if Alice and
                                        Bob want to share $n$ pairs of entangled qubits they can do so with $2n$ quibits
                                        in total. Quantum Teleportation To continue. Quantum NetworksSuperdense Coding
                                        and Quantum Teleportation Superdense Coding Given Alice wants to send classical
                                        information to Bob, quantum entanglement $n$ qubits can store $2n$ qubits total
                                        of information. Say Alice needs to send one qubit of infromation, they can do so
                                        but needs Bob to already share a second qubit. So, given that Alice and Bob
                                        already share a pair of entangled qubits in state $|\Phi^{+}\rangle$, defined
                                        as: (Alice) -- $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice
                                        wants to send: $00$: Alice does nothing to their qubit, so the qubit is still in
                                        state: $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice
                                        applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that Bob has
                                        both qubits in one of four Bell basis, Bob will know what the qubit Alice wants
                                        to send by measuring the two qubits and seeing what state they are in. This is
                                        called a Bell measurment. If we then extrapolate this method, then if Alice and
                                        Bob want to share $n$ pairs of entangled qubits they can do so with $2n$ quibits
                                        in total. Quantum Teleportation To continue. Superdense Coding and Quantum
                                        TeleportationSuperdense Coding Given Alice wants to send classical information
                                        to Bob, quantum entanglement $n$ qubits can store $2n$ qubits total of
                                        information. Say Alice needs to send one qubit of infromation, they can do so
                                        but needs Bob to already share a second qubit. So, given that Alice and Bob
                                        already share a pair of entangled qubits in state $|\Phi^{+}\rangle$, defined
                                        as: (Alice) -- $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice
                                        wants to send: $00$: Alice does nothing to their qubit, so the qubit is still in
                                        state: $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice
                                        applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that Bob has
                                        both qubits in one of four Bell basis, Bob will know what the qubit Alice wants
                                        to send by measuring the two qubits and seeing what state they are in. This is
                                        called a Bell measurment. If we then extrapolate this method, then if Alice and
                                        Bob want to share $n$ pairs of entangled qubits they can do so with $2n$ quibits
                                        in total. Superdense CodingGiven Alice wants to send classical information to
                                        Bob, quantum entanglement $n$ qubits can store $2n$ qubits total of information.
                                        Say Alice needs to send one qubit of infromation, they can do so but needs Bob
                                        to already share a second qubit. So, given that Alice and Bob already share a
                                        pair of entangled qubits in state $|\Phi^{+}\rangle$, defined as: already(Alice)
                                        -- $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to
                                        send:$00$: Alice does nothing to their qubit, so the qubit is still in state:
                                        $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice applies
                                        $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ $00$: Alice does
                                        nothing to their qubit, so the qubit is still in
                                        state:$|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice
                                        applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice applies
                                        $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice applies
                                        $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                        $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that Bob has
                                        both qubits in one of four Bell basis, Bob will know what the qubit Alice wants
                                        to send by measuring the two qubits and seeing what state they are in. This is
                                        called a Bell measurment. If we then extrapolate this method, then if Alice and
                                        Bob want to share $n$ pairs of entangled qubits they can do so with $2n$ quibits
                                        in total. Quantum Teleportation To continue. Quantum TeleportationTo continue.
                                        Glossary Hilbert Space Hilbert Space is a nondenumerable infinite complex vector
                                        space. Complex space, being a collection of complex numbers $\mathbb{C}$ with an
                                        added structure. The infinite dimensions of Hilbert Space represents a
                                        continious spectra of alternative physical states. Alternative physical states,
                                        for example, being the position (coordinates) or momentum of a particle.
                                        Probabilistic Systems Pure States Mixed States The nature of a probabilistic
                                        system is that we do not know for certain the state of the system. However, we
                                        do know the probability distribution of the states. Our probabilistic
                                        distribution sums up to 1. The notation can be written as: $p_{1}[x_{1}] +
                                        p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands for the state the system is in
                                        with probability $p_{i}$. Where $p_{i} \ge 0$ and $p_{1} + ... + p_{n} = 1$. Our
                                        distribution defined above is a mixed state. Where $x_{i}$ is a pure state. It
                                        is important to note that our distribution is not an expected value or an
                                        average of the mixed state, but rather represents only the probability
                                        distribution for all states $x_{i}$. Quantum Mechanics Fun From here on out we
                                        used will use a Hilbert space formalism of quantum mechanics where the
                                        representation of quantum mechanical systems are represented as state vectors.
                                        We use this representation because state vectors are mathematically simpler that
                                        the more general ones. The quantum mechanical description of a physical system
                                        resembles the probabilistic systems we mentioned earlier: $p_{1}[x_{1}] +
                                        p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can
                                        be shown as a unit-length vector in $n$-dimensional complex vector space. We
                                        define this state space with $H_{n}$. Using ket-notation, which is a part of
                                        Dirac notation, we define our state space $H_{n}$ as an orthonormal basis
                                        $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We can now write
                                        any state of the quantum system as: $\alpha_{1}\left| x_{1} \right> +
                                        \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left| x_{n} \right>$ Here,
                                        $\alpha_{i}$ are probabilistic amplitudes. Finally, to meet our requirements
                                        defining our state space $H_{n}$ as unit-length we say that $|\alpha_{1}|^{2} +
                                        |\alpha_{2}|^{2} + ... + |\alpha_{n}|^{2} = 1$. This concludes most of the
                                        information neccesary for this page. However, if you are having fun with quantum
                                        mechanics, feel free to read more on my Quantum Mechanics page. GlossaryHilbert
                                        Space Hilbert Space is a nondenumerable infinite complex vector space. Complex
                                        space, being a collection of complex numbers $\mathbb{C}$ with an added
                                        structure. The infinite dimensions of Hilbert Space represents a continious
                                        spectra of alternative physical states. Alternative physical states, for
                                        example, being the position (coordinates) or momentum of a particle. Hilbert
                                        SpaceHilbert Space is a nondenumerable infinite complex vector space. Complex
                                        space, being a collection of complex numbers $\mathbb{C}$ with an added
                                        structure. The infinite dimensions of Hilbert Space represents a continious
                                        spectra of alternative physical states. Alternative physical states, for
                                        example, being the position (coordinates) or momentum of a particle.
                                        Probabilistic Systems Pure States Mixed States The nature of a probabilistic
                                        system is that we do not know for certain the state of the system. However, we
                                        do know the probability distribution of the states. Our probabilistic
                                        distribution sums up to 1. The notation can be written as: $p_{1}[x_{1}] +
                                        p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands for the state the system is in
                                        with probability $p_{i}$. Where $p_{i} \ge 0$ and $p_{1} + ... + p_{n} = 1$. Our
                                        distribution defined above is a mixed state. Where $x_{i}$ is a pure state. It
                                        is important to note that our distribution is not an expected value or an
                                        average of the mixed state, but rather represents only the probability
                                        distribution for all states $x_{i}$. Probabilistic SystemsPure States Mixed
                                        States Pure StatesMixed StatesThe nature of a probabilistic system is that we do
                                        not know for certain the state of the system. However, we do know the
                                        probability distribution of the states. Our probabilistic distribution sums up
                                        to 1. The notation can be written as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... +
                                        p_{n}[x_{n}]$ $x_{i}$ stands for the state the system is in with probability
                                        $p_{i}$. Where $p_{i} \ge 0$ and $p_{1} + ... + p_{n} = 1$. Our distribution
                                        defined above is a mixed state. Where $x_{i}$ is a pure state. It is important
                                        to note that our distribution is not an expected value or an average of the
                                        mixed state, but rather represents only the probability distribution for all
                                        states $x_{i}$. onlyprobability distributionQuantum Mechanics Fun From here on
                                        out we used will use a Hilbert space formalism of quantum mechanics where the
                                        representation of quantum mechanical systems are represented as state vectors.
                                        We use this representation because state vectors are mathematically simpler that
                                        the more general ones. The quantum mechanical description of a physical system
                                        resembles the probabilistic systems we mentioned earlier: $p_{1}[x_{1}] +
                                        p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can
                                        be shown as a unit-length vector in $n$-dimensional complex vector space. We
                                        define this state space with $H_{n}$. Using ket-notation, which is a part of
                                        Dirac notation, we define our state space $H_{n}$ as an orthonormal basis
                                        $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We can now write
                                        any state of the quantum system as: $\alpha_{1}\left| x_{1} \right> +
                                        \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left| x_{n} \right>$ Here,
                                        $\alpha_{i}$ are probabilistic amplitudes. Finally, to meet our requirements
                                        defining our state space $H_{n}$ as unit-length we say that $|\alpha_{1}|^{2} +
                                        |\alpha_{2}|^{2} + ... + |\alpha_{n}|^{2} = 1$. This concludes most of the
                                        information neccesary for this page. However, if you are having fun with quantum
                                        mechanics, feel free to read more on my Quantum Mechanics page. Quantum
                                        Mechanics FunFrom here on out we used will use a Hilbert space formalism of
                                        quantum mechanics where the representation of quantum mechanical systems are
                                        represented as state vectors. We use this representation because state vectors
                                        are mathematically simpler that the more general ones. The quantum mechanical
                                        description of a physical system resembles the probabilistic systems we
                                        mentioned earlier: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An
                                        $n$-level system in quantum mechanics can be shown as a unit-length vector in
                                        $n$-dimensional complex vector space. We define this state space with $H_{n}$.
                                        Using ket-notation, which is a part of Dirac notation, we define our state space
                                        $H_{n}$ as an orthonormal basis $\left\{\left| x_{1} \right>,...,\left| x_{n}
                                        \right>\right\}$. We can now write any state of the quantum system as:
                                        $\alpha_{1}\left| x_{1} \right> + \alpha_{2}\left| x_{2} \right> + ... +
                                        \alpha_{n}\left| x_{n} \right>$ Here, $\alpha_{i}$ are probabilistic amplitudes.
                                        Finally, to meet our requirements defining our state space $H_{n}$ as
                                        unit-length we say that $|\alpha_{1}|^{2} + |\alpha_{2}|^{2} + ... +
                                        |\alpha_{n}|^{2} = 1$. This concludes most of the information neccesary for this
                                        page. However, if you are having fun with quantum mechanics, feel free to read
                                        more on my Quantum Mechanics page. Quantum Mechanics
                                        [https://www.contextswitching.org/misc/deontologyconsequentialismvirtue]
                                        Context Switching Context Switching by Alejandro Rosales For PK and Lucy, my
                                        best boy and best girl. Whose constant warmth I could never repay, even with a
                                        lifetime of dog treats and scratches. I coded this website from scratch to
                                        provide a place to share what I enjoy learning. I hope you enjoy it too! Jump
                                        to: Pinned New Explore  Pinned Quantum Support Vector Machine Using quantum
                                        computing, the authors exploit quantum mechanics for the algorithmic complexity
                                        optimization of a Support Vector Machine with high-dimensional feature space.
                                        Where the high-dimensional classical data is mapped non-linearly to Hilbert
                                        Space and a hyperplane in quantum space is used to separate and label the data.
                                        By using the... read more Molecular Bases of Memory Formation Molecular
                                        neuroscience is an area of chemical neuroscience that studies the molecular
                                        basis of intercellular activity applied to animals' nervous systems. This area
                                        of research covers molecular neuroanatomy, mechanisms of molecular signaling in
                                        the nervous system, and the molecular basis of neuroplasticity and
                                        neurodegenerative disease, which we will focus on... read more Compatification
                                        and Massless Scattering in Anti-de Sitter Space In theoretical physics,
                                        Minkowski Space is a particular type of $4$-dimensional Lorentzian space, with a
                                        Minkowski metric. Where the Minkowski metric is a metric tensor denoted as
                                        $d\tau^2$ with the form $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d
                                        x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of the study
                                        of spacetime within special relativity and is... read more  New Riemannian
                                        Manifold A Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                        denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns a
                                        vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The Riemanniam
                                        metric defines a positive-definite inner product: $g_p: T_p M \times T_p M
                                        \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M \rightarrow \mathbb{R}$
                                        defined as: $|v|_p=\sqrt{g_p(v, v)}$... read more Quantum [Forward Propagation]
                                        Time Complexity The quantum time complexity of one forward pass through the
                                        convolution layer $\ell$, where $\widetilde{O}$ hides the polylogarithmic
                                        factors, is: $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                        \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right )}}\right
                                        )$ which can be written also as: $\widetilde{O}\left(\sigma H^{\ell+1}
                                        W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                        \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$ is the
                                        fraction of sampled elements, where the number of elements is size $H^{\ell+1}
                                        W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the classical CNN
                                        layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot H W
                                        D^{\ell}\right )$... read more Thalamic Nuclei The thalamic nuclei are paired
                                        structures of the thalamus divided into three main groups: the lateral nuclear,
                                        medial nuclear, and anterior nuclear groups. The internal medullary lamina, a
                                        Y-shaped structure that splits these groups, is present on each side of the
                                        thalamus. A midline, thin thalamic nuclei, adjacent to the... read more The
                                        Maldacena Conjecture Given $AdS_{n+1}$ space of constant negative curvature, a
                                        Hyperboloid in $n+2$ dimensional flat spacetime with coordinates $(X^0, X^1,
                                        \ldots,$ $ X^n, X^{n+1})$ and metric $\eta_{a b}=$
                                        $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant: $\Lambda^2=$
                                        $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global coordinates
                                        in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$ $\Lambda \sec \rho \cos \tau$,
                                        $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$ $\Lambda \sec \rho \sin
                                        \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2
                                            \pi$. Note, since the time variable is $\tau$ is compact, we... read more
                                             Explore All Articles Computer Science Mathematics Neuroscience Physics
                                            Miscellaneous My Research and Work Context Switching Context
                                            SwitchingContext Switching by Alejandro Rosales For PK and Lucy, my best boy
                                            and best girl. Whose constant warmth I could never repay, even with a
                                            lifetime of dog treats and scratches. I coded this website from scratch to
                                            provide a place to share what I enjoy learning. I hope you enjoy it too!
                                            Jump to: Pinned New Explore  Pinned Quantum Support Vector Machine Using
                                            quantum computing, the authors exploit quantum mechanics for the algorithmic
                                            complexity optimization of a Support Vector Machine with high-dimensional
                                            feature space. Where the high-dimensional classical data is mapped
                                            non-linearly to Hilbert Space and a hyperplane in quantum space is used to
                                            separate and label the data. By using the... read more Molecular Bases of
                                            Memory Formation Molecular neuroscience is an area of chemical neuroscience
                                            that studies the molecular basis of intercellular activity applied to
                                            animals' nervous systems. This area of research covers molecular
                                            neuroanatomy, mechanisms of molecular signaling in the nervous system, and
                                            the molecular basis of neuroplasticity and neurodegenerative disease, which
                                            we will focus on... read more Compatification and Massless Scattering in
                                            Anti-de Sitter Space In theoretical physics, Minkowski Space is a particular
                                            type of $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                            Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                            $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity and is... read more  New Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ defined as: $|v|_p=\sqrt{g_p(v, v)}$... read more
                                            Quantum [Forward Propagation] Time Complexity The quantum time complexity of
                                            one forward pass through the convolution layer $\ell$, where $\widetilde{O}$
                                            hides the polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                            \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                            )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                            H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right )$... read more Thalamic Nuclei The thalamic nuclei
                                            are paired structures of the thalamus divided into three main groups: the
                                            lateral nuclear, medial nuclear, and anterior nuclear groups. The internal
                                            medullary lamina, a Y-shaped structure that splits these groups, is present
                                            on each side of the thalamus. A midline, thin thalamic nuclei, adjacent to
                                            the... read more The Maldacena Conjecture Given $AdS_{n+1}$ space of
                                            constant negative curvature, a Hyperboloid in $n+2$ dimensional flat
                                            spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric
                                            $\eta_{a b}=$ $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the
                                            constant: $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we
                                            introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$
                                            $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and
                                            $X_{n+1}=$ $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n \Omega_i^2=$
                                            $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time
                                            variable is $\tau$ is compact, we... read more  Explore All Articles
                                            Computer Science Mathematics Neuroscience Physics Miscellaneous My Research
                                            and Work Context Switching by Alejandro Rosales Context Switching by
                                            Alejandro Rosales Context SwitchingbyAlejandro RosalesAlejandro Rosales For
                                            PK and Lucy, my best boy and best girl. Whose constant warmth I could never
                                            repay, even with a lifetime of dog treats and scratches. I coded this
                                            website from scratch to provide a place to share what I enjoy learning. I
                                            hope you enjoy it too! For PK and Lucy, my best boy and best girl. Whose
                                            constant warmth I could never repay, even with a lifetime of dog treats and
                                            scratches. I coded this website from scratch to provide a place to share
                                            what I enjoy learning. I hope you enjoy it too! PK and LucyJump to: Pinned
                                            New Explore  Pinned Quantum Support Vector Machine Using quantum computing,
                                            the authors exploit quantum mechanics for the algorithmic complexity
                                            optimization of a Support Vector Machine with high-dimensional feature
                                            space. Where the high-dimensional classical data is mapped non-linearly to
                                            Hilbert Space and a hyperplane in quantum space is used to separate and
                                            label the data. By using the... read more Molecular Bases of Memory
                                            Formation Molecular neuroscience is an area of chemical neuroscience that
                                            studies the molecular basis of intercellular activity applied to animals'
                                            nervous systems. This area of research covers molecular neuroanatomy,
                                            mechanisms of molecular signaling in the nervous system, and the molecular
                                            basis of neuroplasticity and neurodegenerative disease, which we will focus
                                            on... read more Compatification and Massless Scattering in Anti-de Sitter
                                            Space In theoretical physics, Minkowski Space is a particular type of
                                            $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                            Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                            $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity and is... read more  New Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ defined as: $|v|_p=\sqrt{g_p(v, v)}$... read more
                                            Quantum [Forward Propagation] Time Complexity The quantum time complexity of
                                            one forward pass through the convolution layer $\ell$, where $\widetilde{O}$
                                            hides the polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                            \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                            )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                            H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right )$... read more Thalamic Nuclei The thalamic nuclei
                                            are paired structures of the thalamus divided into three main groups: the
                                            lateral nuclear, medial nuclear, and anterior nuclear groups. The internal
                                            medullary lamina, a Y-shaped structure that splits these groups, is present
                                            on each side of the thalamus. A midline, thin thalamic nuclei, adjacent to
                                            the... read more The Maldacena Conjecture Given $AdS_{n+1}$ space of
                                            constant negative curvature, a Hyperboloid in $n+2$ dimensional flat
                                            spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric
                                            $\eta_{a b}=$ $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the
                                            constant: $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we
                                            introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$
                                            $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and
                                            $X_{n+1}=$ $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n \Omega_i^2=$
                                            $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time
                                            variable is $\tau$ is compact, we... read more  Explore All Articles
                                            Computer Science Mathematics Neuroscience Physics Miscellaneous My Research
                                            and Work Jump to: Pinned New Explore Jump to:Pinned New Explore Pinned
                                            PinnedNew NewExplore Explore Pinned Quantum Support Vector Machine Using
                                            quantum computing, the authors exploit quantum mechanics for the algorithmic
                                            complexity optimization of a Support Vector Machine with high-dimensional
                                            feature space. Where the high-dimensional classical data is mapped
                                            non-linearly to Hilbert Space and a hyperplane in quantum space is used to
                                            separate and label the data. By using the... read more Molecular Bases of
                                            Memory Formation Molecular neuroscience is an area of chemical neuroscience
                                            that studies the molecular basis of intercellular activity applied to
                                            animals' nervous systems. This area of research covers molecular
                                            neuroanatomy, mechanisms of molecular signaling in the nervous system, and
                                            the molecular basis of neuroplasticity and neurodegenerative disease, which
                                            we will focus on... read more Compatification and Massless Scattering in
                                            Anti-de Sitter Space In theoretical physics, Minkowski Space is a particular
                                            type of $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                            Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                            $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity and is... read more  Pinned Quantum Support Vector
                                            Machine Quantum Support Vector Machine Quantum Support Vector Machine Using
                                            quantum computing, the authors exploit quantum mechanics for the algorithmic
                                            complexity optimization of a Support Vector Machine with high-dimensional
                                            feature space. Where the high-dimensional classical data is mapped
                                            non-linearly to Hilbert Space and a hyperplane in quantum space is used to
                                            separate and label the data. By using the... read more Using quantum
                                            computing, the authors exploit quantum mechanics for the algorithmic
                                            complexity optimization of a Support Vector Machine with high-dimensional
                                            feature space. Where the high-dimensional classical data is mapped
                                            non-linearly to Hilbert Space and a hyperplane in quantum space is used to
                                            separate and label the data. By using the... read more read more Molecular
                                            Bases of Memory Formation Molecular Bases of Memory Formation Molecular
                                            Bases of Memory Formation Molecular neuroscience is an area of chemical
                                            neuroscience that studies the molecular basis of intercellular activity
                                            applied to animals' nervous systems. This area of research covers molecular
                                            neuroanatomy, mechanisms of molecular signaling in the nervous system, and
                                            the molecular basis of neuroplasticity and neurodegenerative disease, which
                                            we will focus on... read more Molecular neuroscience is an area of chemical
                                            neuroscience that studies the molecular basis of intercellular activity
                                            applied to animals' nervous systems. This area of research covers molecular
                                            neuroanatomy, mechanisms of molecular signaling in the nervous system, and
                                            the molecular basis of neuroplasticity and neurodegenerative disease, which
                                            we will focus on... read more Molecular neuroscience is an area of chemical
                                            neuroscience that studies the molecular basis of intercellular activity
                                            applied to animals' nervous systems. This area of research covers molecular
                                            neuroanatomy, mechanisms of molecular signaling in the nervous system, and
                                            the molecular basis of neuroplasticity and neurodegenerative disease, which
                                            we will focus on... read more read more Compatification and Massless
                                            Scattering in Anti-de Sitter Space Compatification and Massless Scattering
                                            in Anti-de Sitter Space Compatification and Massless Scattering in Anti-de
                                            Sitter Space In theoretical physics, Minkowski Space is a particular type of
                                            $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                            Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                            $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity and is... read more In theoretical physics,
                                            Minkowski Space is a particular type of $4$-dimensional Lorentzian space,
                                            with a Minkowski metric. Where the Minkowski metric is a metric tensor
                                            denoted as $d\tau^2$ with the form $-\left(d^0\right)^2+\left(d
                                            x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski
                                            space forms the basis of the study of spacetime within special relativity
                                            and is... read more read more New Riemannian Manifold A Riemannian manifold
                                            is a smooth manifold $M$ with a Riemannian metric denoted $(M,g)$. For every
                                            point $p\in M$ the tangent bundle of $M$ assigns a vector space $T_{p}M$,
                                            termed the tangent space of $M$ at $p$. The Riemanniam metric defines a
                                            positive-definite inner product: $g_p: T_p M \times T_p M \rightarrow
                                            \mathbb{R}$ with a norm $|\cdot|_p: T_p M \rightarrow \mathbb{R}$ defined
                                            as: $|v|_p=\sqrt{g_p(v, v)}$... read more Quantum [Forward Propagation] Time
                                            Complexity The quantum time complexity of one forward pass through the
                                            convolution layer $\ell$, where $\widetilde{O}$ hides the polylogarithmic
                                            factors, is: $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                            )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                            H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right )$... read more Thalamic Nuclei The thalamic nuclei
                                            are paired structures of the thalamus divided into three main groups: the
                                            lateral nuclear, medial nuclear, and anterior nuclear groups. The internal
                                            medullary lamina, a Y-shaped structure that splits these groups, is present
                                            on each side of the thalamus. A midline, thin thalamic nuclei, adjacent to
                                            the... read more The Maldacena Conjecture Given $AdS_{n+1}$ space of
                                            constant negative curvature, a Hyperboloid in $n+2$ dimensional flat
                                            spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric
                                            $\eta_{a b}=$ $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the
                                            constant: $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we
                                            introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$
                                            $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and
                                            $X_{n+1}=$ $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n \Omega_i^2=$
                                            $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time
                                            variable is $\tau$ is compact, we... read more  New Riemannian Manifold
                                            Riemannian Manifold Riemannian Manifold A Riemannian manifold is a smooth
                                            manifold $M$ with a Riemannian metric denoted $(M,g)$. For every point $p\in
                                            M$ the tangent bundle of $M$ assigns a vector space $T_{p}M$, termed the
                                            tangent space of $M$ at $p$. The Riemanniam metric defines a
                                            positive-definite inner product: $g_p: T_p M \times T_p M \rightarrow
                                            \mathbb{R}$ with a norm $|\cdot|_p: T_p M \rightarrow \mathbb{R}$ defined
                                            as: $|v|_p=\sqrt{g_p(v, v)}$... read more A Riemannian manifold is a smooth
                                            manifold $M$ with a Riemannian metric denoted $(M,g)$. For every point $p\in
                                            M$ the tangent bundle of $M$ assigns a vector space $T_{p}M$, termed the
                                            tangent space of $M$ at $p$. The Riemanniam metric defines a
                                            positive-definite inner product: $g_p: T_p M \times T_p M \rightarrow
                                            \mathbb{R}$ with a norm $|\cdot|_p: T_p M \rightarrow \mathbb{R}$ defined
                                            as: $|v|_p=\sqrt{g_p(v, v)}$... read more read more Quantum [Forward
                                            Propagation] Time Complexity Quantum [Forward Propagation] Time Complexity
                                            Quantum [Forward Propagation] Time Complexity The quantum time complexity of
                                            one forward pass through the convolution layer $\ell$, where $\widetilde{O}$
                                            hides the polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                            \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                            )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                            H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right )$... read more The quantum time complexity of one
                                            forward pass through the convolution layer $\ell$, where $\widetilde{O}$
                                            hides the polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                            \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                            )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                            H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right )$... read more read more Thalamic Nuclei Thalamic
                                            Nuclei Thalamic Nuclei The thalamic nuclei are paired structures of the
                                            thalamus divided into three main groups: the lateral nuclear, medial
                                            nuclear, and anterior nuclear groups. The internal medullary lamina, a
                                            Y-shaped structure that splits these groups, is present on each side of the
                                            thalamus. A midline, thin thalamic nuclei, adjacent to the... read more The
                                            thalamic nuclei are paired structures of the thalamus divided into three
                                            main groups: the lateral nuclear, medial nuclear, and anterior nuclear
                                            groups. The internal medullary lamina, a Y-shaped structure that splits
                                            these groups, is present on each side of the thalamus. A midline, thin
                                            thalamic nuclei, adjacent to the... read more read more The Maldacena
                                            Conjecture The Maldacena Conjecture The Maldacena Conjecture Given
                                            $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid in $n+2$
                                            dimensional flat spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n,
                                            X^{n+1})$ and metric $\eta_{a b}=$ $\operatorname{diag}(+,-,-,\ldots,$
                                            $-,+)$, we get the constant: $\Lambda^2=$
                                            $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global
                                            coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$ $\Lambda \sec \rho
                                            \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$ $\Lambda
                                            \sec \rho \sin \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq \rho < \pi /
                                            2,0 $ $\leq \tau<2 \pi$. Note, since the time variable is $\tau$ is compact,
                                            we... read more Given $AdS_{n+1}$ space of constant negative curvature, a
                                            Hyperboloid in $n+2$ dimensional flat spacetime with coordinates $(X^0, X^1,
                                            \ldots,$ $ X^n, X^{n+1})$ and metric $\eta_{a b}=$
                                            $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant:
                                            $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce
                                            global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$ $\Lambda \sec
                                            \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$
                                            $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq
                                            \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time variable is
                                            $\tau$ is compact, we... read more read more Explore All Articles Computer
                                            Science Mathematics Neuroscience Physics Miscellaneous My Research and Work
                                             Explore All Articles All Articles All Articles Computer Science Computer
                                            Science Computer Science Mathematics Mathematics Mathematics Neuroscience
                                            Neuroscience Neuroscience Physics Physics Physics Miscellaneous
                                            Miscellaneous Miscellaneous My Research and Work My Research and Work My
                                            Research and Work [https://www.contextswitching.org/my/aboutme] About Me -
                                            Context Switching Alejandro Rosales Trying to be the person his dogs thought
                                            he was Linked GitHub Resume Some Intro Stuff My name is Alejandro Rosales
                                            and I coded this website from scratch. It's not much, but it's honest work.
                                            I have a Bachelors of Science in Computer Science with a minor in Philosophy
                                            and concentration in Statistics. I am working on my Masters of Science in
                                            Computer Science, with a research focus on quantum machine learning. My
                                            strongest skills are using abstract and deductive thinking, creativity, and
                                            inviting the skills and expertise of others. I love learning new things and
                                            pursing a deeper understanding of anything and everything. I also have a lot
                                            of fun problem solving and being creative, so naturally, I enjoy optimizing
                                            and building new things; things generally, technology especially. For
                                            example, computer programs I enjoyed building in my free time include, but
                                            of course are not limited to, machine learning programs, mobile apps, AWS
                                            cloud based programs, encryption programs, classical AI programs, stock
                                            trading algorithms, simulations, and websites! You can view the code for
                                            those programs on my GitHub (https://github.com/AlejandroJRosales). If you
                                            are interested in the research and work I am currently doing both as a
                                            graduate student and in my free time, feel free to view that on my Research
                                            and Work page. Some Cool Stuff This website I built is dedicated to both PK
                                            (Puppy-Kitty), pictured with one of his favorite toys, and Lucy, pictured at
                                            one of her favorite spots. They were my best boy and best girl and forever
                                            favorite chapters in my life. I enjoy supporting and sharing the work of
                                            passionate people as well. So, to continue with that theme and the theme of
                                            cool, please go checkout the amazing work of my girlfriend Hannah over on
                                            her website:Hannah's Hub (https://sites.google.com/view/hannahs-hub/) My
                                            favorite quote is, "Try to be the person your dog thinks you are." Two other
                                            quotes I like
                                            are, "Curiosity killed the cat, but satisfaction brought them back." As well
                                            as, "Be kind, for everyone you meet is fighting a hard battle." Uh, what
                                            else am I forgetting for these ice breakers, oh yeah! Lastly, my hobbies are
                                            playing the piano, cello, chess, sports, and learning about random stuff!
                                            Oh, and I can't wink, for whatever reason. Some Contact Stuff Like chess?
                                            Send me a virutal game request on chess.com (you will probably
                                            win):computron01 (https://www.chess.com/member/computron01) Have some
                                            thoughts, comments, concerns, notions, decrees, philosophies or choice words
                                            to share with me?Contact me at:alejand.j.rosales@gmail.com About Me -
                                            Context Switching About Me - Context SwitchingAlejandro Rosales Trying to be
                                            the person his dogs thought he was Linked GitHub Resume Some Intro Stuff My
                                            name is Alejandro Rosales and I coded this website from scratch. It's not
                                            much, but it's honest work. I have a Bachelors of Science in Computer
                                            Science with a minor in Philosophy and concentration in Statistics. I am
                                            working on my Masters of Science in Computer Science, with a research focus
                                            on quantum machine learning. My strongest skills are using abstract and
                                            deductive thinking, creativity, and inviting the skills and expertise of
                                            others. I love learning new things and pursing a deeper understanding of
                                            anything and everything. I also have a lot of fun problem solving and being
                                            creative, so naturally, I enjoy optimizing and building new things; things
                                            generally, technology especially. For example, computer programs I enjoyed
                                            building in my free time include, but of course are not limited to, machine
                                            learning programs, mobile apps, AWS cloud based programs, encryption
                                            programs, classical AI programs, stock trading algorithms, simulations, and
                                            websites! You can view the code for those programs on my GitHub
                                            (https://github.com/AlejandroJRosales). If you are interested in the
                                            research and work I am currently doing both as a graduate student and in my
                                            free time, feel free to view that on my Research and Work page. Some Cool
                                            Stuff This website I built is dedicated to both PK (Puppy-Kitty), pictured
                                            with one of his favorite toys, and Lucy, pictured at one of her favorite
                                            spots. They were my best boy and best girl and forever favorite chapters in
                                            my life. I enjoy supporting and sharing the work of passionate people as
                                            well. So, to continue with that theme and the theme of cool, please go
                                            checkout the amazing work of my girlfriend Hannah over on her
                                            website:Hannah's Hub (https://sites.google.com/view/hannahs-hub/) My
                                            favorite quote is, "Try to be the person your dog thinks you are." Two other
                                            quotes I like
                                            are, "Curiosity killed the cat, but satisfaction brought them back." As well
                                            as, "Be kind, for everyone you meet is fighting a hard battle." Uh, what
                                            else am I forgetting for these ice breakers, oh yeah! Lastly, my hobbies are
                                            playing the piano, cello, chess, sports, and learning about random stuff!
                                            Oh, and I can't wink, for whatever reason. Some Contact Stuff Like chess?
                                            Send me a virutal game request on chess.com (you will probably
                                            win):computron01 (https://www.chess.com/member/computron01) Have some
                                            thoughts, comments, concerns, notions, decrees, philosophies or choice words
                                            to share with me?Contact me at:alejand.j.rosales@gmail.com Alejandro Rosales
                                            Trying to be the person his dogs thought he was Alejandro Rosales Trying to
                                            be the person his dogs thought he was Alejandro Rosales Trying to be the
                                            person his dogs thought he was Alejandro RosalesTrying to be the person his
                                            dogs thought he was dogsLinked GitHub Resume Linked GitHub Resume Linked
                                            GitHub Resume Linked Linked GitHub GitHubResume ResumeSome Intro Stuff My
                                            name is Alejandro Rosales and I coded this website from scratch. It's not
                                            much, but it's honest work. I have a Bachelors of Science in Computer
                                            Science with a minor in Philosophy and concentration in Statistics. I am
                                            working on my Masters of Science in Computer Science, with a research focus
                                            on quantum machine learning. My strongest skills are using abstract and
                                            deductive thinking, creativity, and inviting the skills and expertise of
                                            others. I love learning new things and pursing a deeper understanding of
                                            anything and everything. I also have a lot of fun problem solving and being
                                            creative, so naturally, I enjoy optimizing and building new things; things
                                            generally, technology especially. For example, computer programs I enjoyed
                                            building in my free time include, but of course are not limited to, machine
                                            learning programs, mobile apps, AWS cloud based programs, encryption
                                            programs, classical AI programs, stock trading algorithms, simulations, and
                                            websites! You can view the code for those programs on my GitHub
                                            (https://github.com/AlejandroJRosales). If you are interested in the
                                            research and work I am currently doing both as a graduate student and in my
                                            free time, feel free to view that on my Research and Work page. Some Cool
                                            Stuff This website I built is dedicated to both PK (Puppy-Kitty), pictured
                                            with one of his favorite toys, and Lucy, pictured at one of her favorite
                                            spots. They were my best boy and best girl and forever favorite chapters in
                                            my life. I enjoy supporting and sharing the work of passionate people as
                                            well. So, to continue with that theme and the theme of cool, please go
                                            checkout the amazing work of my girlfriend Hannah over on her
                                            website:Hannah's Hub (https://sites.google.com/view/hannahs-hub/) My
                                            favorite quote is, "Try to be the person your dog thinks you are." Two other
                                            quotes I like
                                            are, "Curiosity killed the cat, but satisfaction brought them back." As well
                                            as, "Be kind, for everyone you meet is fighting a hard battle." Uh, what
                                            else am I forgetting for these ice breakers, oh yeah! Lastly, my hobbies are
                                            playing the piano, cello, chess, sports, and learning about random stuff!
                                            Oh, and I can't wink, for whatever reason. Some Contact Stuff Like chess?
                                            Send me a virutal game request on chess.com (you will probably
                                            win):computron01 (https://www.chess.com/member/computron01) Have some
                                            thoughts, comments, concerns, notions, decrees, philosophies or choice words
                                            to share with me?Contact me at:alejand.j.rosales@gmail.com Some Intro
                                            StuffMy name is Alejandro Rosales and I coded this website from scratch.
                                            It's not much, but it's honest work. I have a Bachelors of Science in
                                            Computer Science with a minor in Philosophy and concentration in Statistics.
                                            I am working on my Masters of Science in Computer Science, with a research
                                            focus on quantum machine learning. My strongest skills are using abstract
                                            and deductive thinking, creativity, and inviting the skills and expertise of
                                            others. I love learning new things and pursing a deeper understanding of
                                            anything and everything. I also have a lot of fun problem solving and being
                                            creative, so naturally, I enjoy optimizing and building new things; things
                                            generally, technology especially. For example, computer programs I enjoyed
                                            building in my free time include, but of course are not limited to, machine
                                            learning programs, mobile apps, AWS cloud based programs, encryption
                                            programs, classical AI programs, stock trading algorithms, simulations, and
                                            websites! You can view the code for those programs on my GitHub
                                            (https://github.com/AlejandroJRosales). If you are interested in the
                                            research and work I am currently doing both as a graduate student and in my
                                            free time, feel free to view that on my Research and Work page.
                                            GitHubResearch and WorkSome Cool StuffThis website I built is dedicated to
                                            both PK (Puppy-Kitty), pictured with one of his favorite toys, and Lucy,
                                            pictured at one of her favorite spots. They were my best boy and best girl
                                            and forever favorite chapters in my life. I enjoy supporting and sharing the
                                            work of passionate people as well. So, to continue with that theme and the
                                            theme of cool, please go checkout the amazing work of my girlfriend Hannah
                                            over on her website:Hannah's Hub
                                            (https://sites.google.com/view/hannahs-hub/) Hannah's HubMy favorite quote
                                            is, "Try to be the person your dog thinks you are." Two other quotes I like
                                            are, "Curiosity killed the cat, but satisfaction brought them back." As well
                                            as, "Be kind, for everyone you meet is fighting a hard battle." Uh, what
                                            else am I forgetting for these ice breakers, oh yeah! Lastly, my hobbies are
                                            playing the piano, cello, chess, sports, and learning about random stuff!
                                            Oh, and I can't wink, for whatever reason. Some Contact StuffLike chess?
                                            Send me a virutal game request on chess.com (you will probably
                                            win):computron01 (https://www.chess.com/member/computron01) computron01Have
                                            some thoughts, comments, concerns, notions, decrees, philosophies or choice
                                            words to share with me?Contact me at:alejand.j.rosales@gmail.com
                                            [https://www.contextswitching.org/math/differentialmanifolds] Differential
                                            Manifolds - Context Switching Differential Manifolds Differential Structures
                                            Manifolds Atlas Maps $C^{r}$ Structure An $n$-dimensional manifold is a
                                            topological space where each point has a neighborhood that is homeomorphic
                                            to an open subset on $n$-dimensional Euclidean space. Let $M$ be a
                                            topological space. A chart in $M$ consists of an open subset $U \subset M$
                                            and a homomorphism $h$ of $U$ onto an open subset of $R^{m}$. A $C^{r}$
                                            atlas on $M$ is a collection $\left\{U_{\alpha}, h_{\alpha}\right\}$ of
                                            charts such that the $U_{\alpha}$ cover $M$ and $h_{\beta}h_{\alpha}^{-1}$,
                                            the transition maps, are $C^{r}$ maps on $h_{\alpha}(U_{\alpha}\cap
                                            U_{\beta})$. A maximal atlas $C^{r}$ on our Manifold $M$ is called a $C^{r}$
                                            structure. A manifold of class $C^{r}$ that has a second coutnable Hausdorff
                                            space $M$, and has a $C^{r}$ structure on it, is called a differential
                                            manifold. Smooth Mainfold Manifold with Bounds Given an open subset
                                            $U\subset \mathbb{R}^{n}$, a function $f:U\rightarrow
                                            M\subset\mathbb{R}^{A}$, is smooth if for all functions
                                            $f_{\alpha}:U\rightarrow\mathbb{R}$ is smooth. Where $R^{A}$ denotes the
                                            vector space of all functions $x$. Given $f$ is smooth, the partial
                                            derivative $\frac{\partial_{f}}{\partial_{u_{i}}}$ is defined as a smooth
                                            function $U\rightarrow\mathbb{R}^{A}$, where for the $\alpha$-th coordinate
                                            is $\frac{\partial_{\alpha}}{\partial_{u_{i}}}$ for $i=1,...,n$. A good
                                            method of studying smooth manifolds is by putting together the smooth
                                            manifold with smaller pieces. These smaller pieces themselves are not smooth
                                            manifolds, but rather manifolds with boundary. The definition of a manifold
                                            parallels a manifold definition (defined in the Manifolds section) however,
                                            we need to update the definition of a homeomorphism. For a manifold with
                                            boundary, the homeomorphisms $h_{\alpha}$ onto the open subsets of
                                            $\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, where
                                            $\mathbb{R}^{m}_{+}=\left\{(x_{1},...,x_{m})\in\mathbb{R}^{m}|\;x_{m}\ge
                                            0\right\}$. Meaning, with our update when an open subset of our topological
                                            structure is passed through the homeomorphic functions $h_{\alpha}$, it can
                                            now be mapped surjectively to the eucledian space of $\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$. Where for $\mathbb{R}^{m}_{+}$, the last element in
                                            the ordered pair of real elements is $\ge 0$. To put it succently, the
                                            transtion maps now become maps of open subsets of $\mathbb{R}^{m}_{+}$. This
                                            type of map is smooth if it is locally a restriction of a smooth map defined
                                            on an open subset of $\mathbb{R}^{m}$, by definition. Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ given as: $|v|_p=\sqrt{g_p(v, v)}$ Partitions of
                                            Unity In differential topology, many constructions utilize partitions of
                                            unity. Before we continue, we will formally define a specific type of atlas.
                                            This is an atlas $\left\{U_{\alpha},h_{\alpha}\right\}$ on $M$ that is said
                                            to be adequate if it is locally finite,
                                            $h_{\alpha}(U_{\alpha})=\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, and
                                            $\bigcup_{\alpha}h^{-1}_{\alpha}(\mathring{D}^{m})=M$. To continue. Vector
                                            Bundle Family of Sets Vector Space Field A vector bundle is topological
                                            construction that has a family of vector spaces parameterized by another
                                            space $X$. This space $X$ can be a topolgical space, manifold, or another
                                            type of algebraic variety. By family, we are refering to more generally a
                                            family of sets. This family of sets is a collection $F$ of subsets over the
                                            given set $S$. More to our case we have a family of sets or vector spaces
                                            parameterized over the manifold space $M$. For vector space, we are refering
                                            to a set of vector elements that may be added together and multiplied by
                                            numbers or scalars. These scalars often are real numbers but can be complex
                                            numbers or another type of element of any field. Speaking more generally,
                                            for every point $x$ of the space $X$ we we associate a vector space $V(x)$
                                            such that these vector spaces fit together to form another space of the same
                                            kind as $X$. This resulting same kind of space is called a vector bundle
                                            over $X$. Trivial Bundle Trivial vector bundles is where we have a fixed
                                            vector space $V$ such that $V(x)=V$ for every $x$ in some space $X$. We
                                            define this as letting $E=B\times F$ and let $\pi: E \rightarrow B$ be
                                            defined as the projection onto the first factor. $\pi$ is a fiber bundle of
                                            a family of vector spaces $F$ over $B$. Note, here $E$ is globally a
                                            product. Finally, any such fiber bundle is called a trivial bundle. Smooth
                                            Vector Bundle Image Source: Theory and Numerical Simulation of Deep Rock
                                            Mass Based on a Non-Euclidean Model Let $\pi: E \rightarrow M$ be an
                                            $n$-dimensional vector bundle over a smooth manifold $M$. Let
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ be an atlas on $M$ where the vector
                                            bundle is trivial over the sets $U_{\alpha}$. Let $\phi_{a}$ represent that
                                            canonical map defined as $\pi^{-1}(U_{a})\rightarrow U_{a}
                                            \times\mathbb{R}^n$ with a projection onto $n$-dimensional Eucledian space
                                            $\mathbb{R}^n$. Here we are taking our open subset $U_{\alpha}$ of the
                                            manifold in topolgical space $M$ and mapping $U_{\alpha}$ to $\mathbb{R}^n$.
                                            With the smooth vector bundle, we take the family of open subsets
                                            $U_{\alpha}$ in atlas $\left\{U_{\alpha}, h_{\alpha}\right\}$ on toplogical
                                            space and pass it through the canonical map to $n$-dimensional Eucledian
                                            space. Operations on Manifolds Connected Sums This graph represents a
                                            connected sum between $m$-dimensional manifolds $M_{1}, M_{2}$. Using the
                                            notation: $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$
                                            $=(M_{1}-h_{1}(0))\cup_{g}(M_{2}-h_{2}(0))$ where $g=h_{2}\alpha h_{1}^{-1}$
                                            we can formally define the function for the connected sums between two
                                            manifolds. In our notation $M_{1}, M_{2}$ are two connected $m$-dimensional
                                            manifolds. We let $h_{i}: \mathbb{R}^{m} \rightarrow M_{i}, i=1,2$, be two
                                            imbeddings. $\alpha$ is an arbritary orientation reversing diffeomorphism
                                            whose domain is $(0, \infty)$ and codomain is $(0, \infty)$. Next, we define
                                            $\alpha_{m}: \bf \mathbb{R} - 0 \rightarrow \mathbb{R} - 0$ by:
                                            $\alpha(v)\;=\;\alpha(|v|) {v \over |v|}$ The connected sum is then written
                                            as $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$. Handle Presentation Theorem
                                            Cobordism Preliminary An ordered triple of manifolds
                                            $\mathscr{C}=\left\{V_0, W, V_1\right\}$ where $\partial W=V_0 \cup V_1$ and
                                            $V_{0}, V_{1}$ are disjoint open subsets of $\partial W$ is called a
                                            cobordism. Such that $V_0=\partial_{-} W$ is the left-hand boundary of $W$.
                                            $\{M \times\{0\}, M \times I, M \times\{1\}\}$, where $M$ is a compact
                                            closed manifold, is an example cobordism termed an elementary cobordism of
                                            $\lambda$. The result is attaching a $\lambda$-handle to the right-hand
                                            boundary of $M \times I$. Note, we will view a trivial cobordism as an
                                            elementary cobordismof index $-1$, as it is convient. Consider two
                                            cobordisms $\mathscr{C}=\left\{V_0, W, V_1\right\}$,
                                            $\mathscr{C}'=\left\{V'_{0}, W', V'_{1}\right\}$, and a diffeomorphism given
                                            as $h:V'_{1}\rightarrow V'_{0}$. Using $h$, we can join $W$ and $W'$, like
                                            so: $W_{1}=W\bigcup_{h}W'$. Let $\partial W_{1}=V_{0}\cup V'_{1}$ and
                                            $\left\{V_0, W_{1}, V'_1\right\}$ be a cobordism denoted as $\mathscr{C}\cup
                                            \mathscr{C}'$. Note, that this notation is symbolic and does not show the
                                            result dependent on the diffeomorphism $h$. That being said, given
                                            $\mathscr{C}$ is a trivial cobordism, the result does not depend on $h$ and
                                            therefore the result is given as $\mathscr{C}\cup \mathscr{C}'$ $=$
                                            $\mathscr{C}$. Handle Presentation Theorem and Morse Theory The Smale and
                                            Wallace theorem is the following. Let $\mathscr{C}=\left\{V_0, W,
                                            V_1\right\}$ be a cobordism, such that $\mathscr{C}$ $=$ $\mathscr{C}_1 \cup
                                            \mathscr{C}_2 \;\cup$ $\cdots$ $\cup \;\mathscr{C}_k$, where $\mathscr{C}_i$
                                            are elementary cobordisms. Given $i < j$ implies $\lambda(i) \leq
                                            \lambda(j)$, where $\lambda(i)$ denotes the index of $\mathscr{C}_i$. The
                                            corollary is there exists a sequence of manifolds $V_0 \times I$ $=$ $W_{-1}
                                            \subset W_0 \subset W_1 \subset$ $\cdots$ $\subset W_m=W$ such that $W_i$ is
                                            obtained from $W_{i-1}$ by attaching $i$-handles to the right-hand boundary
                                            of the cobordism $\mathscr{C}$. To continue. Glossary Topological Space
                                            Topologoical vs Eucledian Space Let us break this down. First, we will start
                                            with getting an idea of what a topological space is, since this is one of
                                            the first parts of our definition of a manifold. A topological space is
                                            roughly speaking where the closeness of the points in the space cannot be
                                            numerically defined, unlike in Euclidean space. What is Euclidean space? If
                                            you recall, in school we would plot a point in 2-d space using $(x, y)$ and
                                            an $x$ and $y$ axis with dashes along the side to help define the space
                                            between values. This is 2-dimensional Euclidean space. Or written as
                                            $\mathbb{R}^{2}$. Of course, our Euclidean space is not limited to 2
                                            dimensions. We can use similar descriptors for 3-d space, 11-space, and any
                                            number of dimensions, or $n$-d Euclidean space $\mathbb{R}^{n}$. However,
                                            continuing with 2-d space, using our numbered $x$ and $y$ axis and our
                                            points with coordinates, we can figure out the numerical value that
                                            represents how close one point is to other. However, a topological space,
                                            unlike our familiar Eucledian space, is more generalized than that. Rather,
                                            our space does not have a "numbered axis" to define one point relation to
                                            another. Rather our space is represented as a set of points whose elements
                                            are called points. In this set that defines our topological space, we have a
                                            structure called a topology. This topology is defined as a set of
                                            neighborhoods for every point that, by examining the other points or
                                            elements that are in this neighborhood set, gives an idea of closeness or
                                            relation of one point to another. So, we have a set of points that defines
                                            what makes up our topological space and a structure called a topology that
                                            is defined as a set of neighbors. Topological Structure Open Subsets
                                            Neighboorhoods A topological structure, roughly speaking, is an additional
                                            structure to a topological space that can be defined as a set of
                                            neighborhoods. A neighborhood is closely related to an open subset. This is
                                            why in out definition when we take an element from our topology we get an
                                            open subset. i.e. $U_{p} \in \tau_{M}$. Basically, a neighborhood of a point
                                            from a topological space is a set of points that contains that point and the
                                            points for which we can move around in any direction and still be in our set
                                            or neighborhood. This is why our point $p$ that we have is an element of
                                            $U$. Since when we take a point we want to examine a slice of our topology
                                            that contains the point we are examining and not some random point. We do
                                            this so we can keep properly traversing through our topology and keep a list
                                            of all of the charts and their respective homeomorphisms. This allows us to
                                            determine if all elements or open subsets in our topology when sent through
                                            their respective homeomorphisms, are a part of eucledian space $R^{d}$. i.e.
                                            if parts of our topological space $M$ examined locally are apart of
                                            eucledian space $R^{d}$. Chart Homeomorphism A chart in a topological space
                                            $M$ consists of $U_{\alpha}$ an open covering that is homeomorphic, via
                                            $h_{\alpha}$ a homeomorphism, to an open subset $U_{\alpha}$ in Eucledian
                                            space $\mathbb{R}^{n}$. Atlas An atlas $C^{r}$ on a topological space $M$ is
                                            a collection $\left\{U_{\alpha},h_{\alpha}\right\}$ of charts. $U_{\alpha}$
                                            is an open covering admitted on $M$ and $h_{\alpha}$ is a homeomorphism on
                                            $U_{\alpha}$ to the Eucledian space $\mathbb{R}^{n}$. The $C^{r}$ transition
                                            maps $h_{\beta}h_{\alpha}^{-1}$ are $C^{r}$ maps on
                                            $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. Tangent Bundle To continue. Informal
                                            Manifold Definition Short version: a manifold is a topological space that
                                            locally resembles Eucliden space. Medium version: an $n$-dimensional
                                            manifold, or $n$-manifold, is a topological space where each point has a
                                            neighboorhood that is homeomorphic to an open subset on $n$-dimensional
                                            Euclidean space. Differential Manifolds - Context Switching Differential
                                            Manifolds - Context SwitchingDifferential Manifolds Differential Structures
                                            Manifolds Atlas Maps $C^{r}$ Structure An $n$-dimensional manifold is a
                                            topological space where each point has a neighborhood that is homeomorphic
                                            to an open subset on $n$-dimensional Euclidean space. Let $M$ be a
                                            topological space. A chart in $M$ consists of an open subset $U \subset M$
                                            and a homomorphism $h$ of $U$ onto an open subset of $R^{m}$. A $C^{r}$
                                            atlas on $M$ is a collection $\left\{U_{\alpha}, h_{\alpha}\right\}$ of
                                            charts such that the $U_{\alpha}$ cover $M$ and $h_{\beta}h_{\alpha}^{-1}$,
                                            the transition maps, are $C^{r}$ maps on $h_{\alpha}(U_{\alpha}\cap
                                            U_{\beta})$. A maximal atlas $C^{r}$ on our Manifold $M$ is called a $C^{r}$
                                            structure. A manifold of class $C^{r}$ that has a second coutnable Hausdorff
                                            space $M$, and has a $C^{r}$ structure on it, is called a differential
                                            manifold. Smooth Mainfold Manifold with Bounds Given an open subset
                                            $U\subset \mathbb{R}^{n}$, a function $f:U\rightarrow
                                            M\subset\mathbb{R}^{A}$, is smooth if for all functions
                                            $f_{\alpha}:U\rightarrow\mathbb{R}$ is smooth. Where $R^{A}$ denotes the
                                            vector space of all functions $x$. Given $f$ is smooth, the partial
                                            derivative $\frac{\partial_{f}}{\partial_{u_{i}}}$ is defined as a smooth
                                            function $U\rightarrow\mathbb{R}^{A}$, where for the $\alpha$-th coordinate
                                            is $\frac{\partial_{\alpha}}{\partial_{u_{i}}}$ for $i=1,...,n$. A good
                                            method of studying smooth manifolds is by putting together the smooth
                                            manifold with smaller pieces. These smaller pieces themselves are not smooth
                                            manifolds, but rather manifolds with boundary. The definition of a manifold
                                            parallels a manifold definition (defined in the Manifolds section) however,
                                            we need to update the definition of a homeomorphism. For a manifold with
                                            boundary, the homeomorphisms $h_{\alpha}$ onto the open subsets of
                                            $\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, where
                                            $\mathbb{R}^{m}_{+}=\left\{(x_{1},...,x_{m})\in\mathbb{R}^{m}|\;x_{m}\ge
                                            0\right\}$. Meaning, with our update when an open subset of our topological
                                            structure is passed through the homeomorphic functions $h_{\alpha}$, it can
                                            now be mapped surjectively to the eucledian space of $\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$. Where for $\mathbb{R}^{m}_{+}$, the last element in
                                            the ordered pair of real elements is $\ge 0$. To put it succently, the
                                            transtion maps now become maps of open subsets of $\mathbb{R}^{m}_{+}$. This
                                            type of map is smooth if it is locally a restriction of a smooth map defined
                                            on an open subset of $\mathbb{R}^{m}$, by definition. Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ given as: $|v|_p=\sqrt{g_p(v, v)}$ Partitions of
                                            Unity In differential topology, many constructions utilize partitions of
                                            unity. Before we continue, we will formally define a specific type of atlas.
                                            This is an atlas $\left\{U_{\alpha},h_{\alpha}\right\}$ on $M$ that is said
                                            to be adequate if it is locally finite,
                                            $h_{\alpha}(U_{\alpha})=\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, and
                                            $\bigcup_{\alpha}h^{-1}_{\alpha}(\mathring{D}^{m})=M$. To continue. Vector
                                            Bundle Family of Sets Vector Space Field A vector bundle is topological
                                            construction that has a family of vector spaces parameterized by another
                                            space $X$. This space $X$ can be a topolgical space, manifold, or another
                                            type of algebraic variety. By family, we are refering to more generally a
                                            family of sets. This family of sets is a collection $F$ of subsets over the
                                            given set $S$. More to our case we have a family of sets or vector spaces
                                            parameterized over the manifold space $M$. For vector space, we are refering
                                            to a set of vector elements that may be added together and multiplied by
                                            numbers or scalars. These scalars often are real numbers but can be complex
                                            numbers or another type of element of any field. Speaking more generally,
                                            for every point $x$ of the space $X$ we we associate a vector space $V(x)$
                                            such that these vector spaces fit together to form another space of the same
                                            kind as $X$. This resulting same kind of space is called a vector bundle
                                            over $X$. Trivial Bundle Trivial vector bundles is where we have a fixed
                                            vector space $V$ such that $V(x)=V$ for every $x$ in some space $X$. We
                                            define this as letting $E=B\times F$ and let $\pi: E \rightarrow B$ be
                                            defined as the projection onto the first factor. $\pi$ is a fiber bundle of
                                            a family of vector spaces $F$ over $B$. Note, here $E$ is globally a
                                            product. Finally, any such fiber bundle is called a trivial bundle. Smooth
                                            Vector Bundle Image Source: Theory and Numerical Simulation of Deep Rock
                                            Mass Based on a Non-Euclidean Model Let $\pi: E \rightarrow M$ be an
                                            $n$-dimensional vector bundle over a smooth manifold $M$. Let
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ be an atlas on $M$ where the vector
                                            bundle is trivial over the sets $U_{\alpha}$. Let $\phi_{a}$ represent that
                                            canonical map defined as $\pi^{-1}(U_{a})\rightarrow U_{a}
                                            \times\mathbb{R}^n$ with a projection onto $n$-dimensional Eucledian space
                                            $\mathbb{R}^n$. Here we are taking our open subset $U_{\alpha}$ of the
                                            manifold in topolgical space $M$ and mapping $U_{\alpha}$ to $\mathbb{R}^n$.
                                            With the smooth vector bundle, we take the family of open subsets
                                            $U_{\alpha}$ in atlas $\left\{U_{\alpha}, h_{\alpha}\right\}$ on toplogical
                                            space and pass it through the canonical map to $n$-dimensional Eucledian
                                            space. Operations on Manifolds Connected Sums This graph represents a
                                            connected sum between $m$-dimensional manifolds $M_{1}, M_{2}$. Using the
                                            notation: $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$
                                            $=(M_{1}-h_{1}(0))\cup_{g}(M_{2}-h_{2}(0))$ where $g=h_{2}\alpha h_{1}^{-1}$
                                            we can formally define the function for the connected sums between two
                                            manifolds. In our notation $M_{1}, M_{2}$ are two connected $m$-dimensional
                                            manifolds. We let $h_{i}: \mathbb{R}^{m} \rightarrow M_{i}, i=1,2$, be two
                                            imbeddings. $\alpha$ is an arbritary orientation reversing diffeomorphism
                                            whose domain is $(0, \infty)$ and codomain is $(0, \infty)$. Next, we define
                                            $\alpha_{m}: \bf \mathbb{R} - 0 \rightarrow \mathbb{R} - 0$ by:
                                            $\alpha(v)\;=\;\alpha(|v|) {v \over |v|}$ The connected sum is then written
                                            as $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$. Handle Presentation Theorem
                                            Cobordism Preliminary An ordered triple of manifolds
                                            $\mathscr{C}=\left\{V_0, W, V_1\right\}$ where $\partial W=V_0 \cup V_1$ and
                                            $V_{0}, V_{1}$ are disjoint open subsets of $\partial W$ is called a
                                            cobordism. Such that $V_0=\partial_{-} W$ is the left-hand boundary of $W$.
                                            $\{M \times\{0\}, M \times I, M \times\{1\}\}$, where $M$ is a compact
                                            closed manifold, is an example cobordism termed an elementary cobordism of
                                            $\lambda$. The result is attaching a $\lambda$-handle to the right-hand
                                            boundary of $M \times I$. Note, we will view a trivial cobordism as an
                                            elementary cobordismof index $-1$, as it is convient. Consider two
                                            cobordisms $\mathscr{C}=\left\{V_0, W, V_1\right\}$,
                                            $\mathscr{C}'=\left\{V'_{0}, W', V'_{1}\right\}$, and a diffeomorphism given
                                            as $h:V'_{1}\rightarrow V'_{0}$. Using $h$, we can join $W$ and $W'$, like
                                            so: $W_{1}=W\bigcup_{h}W'$. Let $\partial W_{1}=V_{0}\cup V'_{1}$ and
                                            $\left\{V_0, W_{1}, V'_1\right\}$ be a cobordism denoted as $\mathscr{C}\cup
                                            \mathscr{C}'$. Note, that this notation is symbolic and does not show the
                                            result dependent on the diffeomorphism $h$. That being said, given
                                            $\mathscr{C}$ is a trivial cobordism, the result does not depend on $h$ and
                                            therefore the result is given as $\mathscr{C}\cup \mathscr{C}'$ $=$
                                            $\mathscr{C}$. Handle Presentation Theorem and Morse Theory The Smale and
                                            Wallace theorem is the following. Let $\mathscr{C}=\left\{V_0, W,
                                            V_1\right\}$ be a cobordism, such that $\mathscr{C}$ $=$ $\mathscr{C}_1 \cup
                                            \mathscr{C}_2 \;\cup$ $\cdots$ $\cup \;\mathscr{C}_k$, where $\mathscr{C}_i$
                                            are elementary cobordisms. Given $i < j$ implies $\lambda(i) \leq
                                            \lambda(j)$, where $\lambda(i)$ denotes the index of $\mathscr{C}_i$. The
                                            corollary is there exists a sequence of manifolds $V_0 \times I$ $=$ $W_{-1}
                                            \subset W_0 \subset W_1 \subset$ $\cdots$ $\subset W_m=W$ such that $W_i$ is
                                            obtained from $W_{i-1}$ by attaching $i$-handles to the right-hand boundary
                                            of the cobordism $\mathscr{C}$. To continue. Glossary Topological Space
                                            Topologoical vs Eucledian Space Let us break this down. First, we will start
                                            with getting an idea of what a topological space is, since this is one of
                                            the first parts of our definition of a manifold. A topological space is
                                            roughly speaking where the closeness of the points in the space cannot be
                                            numerically defined, unlike in Euclidean space. What is Euclidean space? If
                                            you recall, in school we would plot a point in 2-d space using $(x, y)$ and
                                            an $x$ and $y$ axis with dashes along the side to help define the space
                                            between values. This is 2-dimensional Euclidean space. Or written as
                                            $\mathbb{R}^{2}$. Of course, our Euclidean space is not limited to 2
                                            dimensions. We can use similar descriptors for 3-d space, 11-space, and any
                                            number of dimensions, or $n$-d Euclidean space $\mathbb{R}^{n}$. However,
                                            continuing with 2-d space, using our numbered $x$ and $y$ axis and our
                                            points with coordinates, we can figure out the numerical value that
                                            represents how close one point is to other. However, a topological space,
                                            unlike our familiar Eucledian space, is more generalized than that. Rather,
                                            our space does not have a "numbered axis" to define one point relation to
                                            another. Rather our space is represented as a set of points whose elements
                                            are called points. In this set that defines our topological space, we have a
                                            structure called a topology. This topology is defined as a set of
                                            neighborhoods for every point that, by examining the other points or
                                            elements that are in this neighborhood set, gives an idea of closeness or
                                            relation of one point to another. So, we have a set of points that defines
                                            what makes up our topological space and a structure called a topology that
                                            is defined as a set of neighbors. Topological Structure Open Subsets
                                            Neighboorhoods A topological structure, roughly speaking, is an additional
                                            structure to a topological space that can be defined as a set of
                                            neighborhoods. A neighborhood is closely related to an open subset. This is
                                            why in out definition when we take an element from our topology we get an
                                            open subset. i.e. $U_{p} \in \tau_{M}$. Basically, a neighborhood of a point
                                            from a topological space is a set of points that contains that point and the
                                            points for which we can move around in any direction and still be in our set
                                            or neighborhood. This is why our point $p$ that we have is an element of
                                            $U$. Since when we take a point we want to examine a slice of our topology
                                            that contains the point we are examining and not some random point. We do
                                            this so we can keep properly traversing through our topology and keep a list
                                            of all of the charts and their respective homeomorphisms. This allows us to
                                            determine if all elements or open subsets in our topology when sent through
                                            their respective homeomorphisms, are a part of eucledian space $R^{d}$. i.e.
                                            if parts of our topological space $M$ examined locally are apart of
                                            eucledian space $R^{d}$. Chart Homeomorphism A chart in a topological space
                                            $M$ consists of $U_{\alpha}$ an open covering that is homeomorphic, via
                                            $h_{\alpha}$ a homeomorphism, to an open subset $U_{\alpha}$ in Eucledian
                                            space $\mathbb{R}^{n}$. Atlas An atlas $C^{r}$ on a topological space $M$ is
                                            a collection $\left\{U_{\alpha},h_{\alpha}\right\}$ of charts. $U_{\alpha}$
                                            is an open covering admitted on $M$ and $h_{\alpha}$ is a homeomorphism on
                                            $U_{\alpha}$ to the Eucledian space $\mathbb{R}^{n}$. The $C^{r}$ transition
                                            maps $h_{\beta}h_{\alpha}^{-1}$ are $C^{r}$ maps on
                                            $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. Tangent Bundle To continue. Informal
                                            Manifold Definition Short version: a manifold is a topological space that
                                            locally resembles Eucliden space. Medium version: an $n$-dimensional
                                            manifold, or $n$-manifold, is a topological space where each point has a
                                            neighboorhood that is homeomorphic to an open subset on $n$-dimensional
                                            Euclidean space. Differential Manifolds Differential Structures Manifolds
                                            Atlas Maps $C^{r}$ Structure An $n$-dimensional manifold is a topological
                                            space where each point has a neighborhood that is homeomorphic to an open
                                            subset on $n$-dimensional Euclidean space. Let $M$ be a topological space. A
                                            chart in $M$ consists of an open subset $U \subset M$ and a homomorphism $h$
                                            of $U$ onto an open subset of $R^{m}$. A $C^{r}$ atlas on $M$ is a
                                            collection $\left\{U_{\alpha}, h_{\alpha}\right\}$ of charts such that the
                                            $U_{\alpha}$ cover $M$ and $h_{\beta}h_{\alpha}^{-1}$, the transition maps,
                                            are $C^{r}$ maps on $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. A maximal atlas
                                            $C^{r}$ on our Manifold $M$ is called a $C^{r}$ structure. A manifold of
                                            class $C^{r}$ that has a second coutnable Hausdorff space $M$, and has a
                                            $C^{r}$ structure on it, is called a differential manifold. Smooth Mainfold
                                            Manifold with Bounds Given an open subset $U\subset \mathbb{R}^{n}$, a
                                            function $f:U\rightarrow M\subset\mathbb{R}^{A}$, is smooth if for all
                                            functions $f_{\alpha}:U\rightarrow\mathbb{R}$ is smooth. Where $R^{A}$
                                            denotes the vector space of all functions $x$. Given $f$ is smooth, the
                                            partial derivative $\frac{\partial_{f}}{\partial_{u_{i}}}$ is defined as a
                                            smooth function $U\rightarrow\mathbb{R}^{A}$, where for the $\alpha$-th
                                            coordinate is $\frac{\partial_{\alpha}}{\partial_{u_{i}}}$ for $i=1,...,n$.
                                            A good method of studying smooth manifolds is by putting together the smooth
                                            manifold with smaller pieces. These smaller pieces themselves are not smooth
                                            manifolds, but rather manifolds with boundary. The definition of a manifold
                                            parallels a manifold definition (defined in the Manifolds section) however,
                                            we need to update the definition of a homeomorphism. For a manifold with
                                            boundary, the homeomorphisms $h_{\alpha}$ onto the open subsets of
                                            $\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, where
                                            $\mathbb{R}^{m}_{+}=\left\{(x_{1},...,x_{m})\in\mathbb{R}^{m}|\;x_{m}\ge
                                            0\right\}$. Meaning, with our update when an open subset of our topological
                                            structure is passed through the homeomorphic functions $h_{\alpha}$, it can
                                            now be mapped surjectively to the eucledian space of $\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$. Where for $\mathbb{R}^{m}_{+}$, the last element in
                                            the ordered pair of real elements is $\ge 0$. To put it succently, the
                                            transtion maps now become maps of open subsets of $\mathbb{R}^{m}_{+}$. This
                                            type of map is smooth if it is locally a restriction of a smooth map defined
                                            on an open subset of $\mathbb{R}^{m}$, by definition. Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ given as: $|v|_p=\sqrt{g_p(v, v)}$ Partitions of
                                            Unity In differential topology, many constructions utilize partitions of
                                            unity. Before we continue, we will formally define a specific type of atlas.
                                            This is an atlas $\left\{U_{\alpha},h_{\alpha}\right\}$ on $M$ that is said
                                            to be adequate if it is locally finite,
                                            $h_{\alpha}(U_{\alpha})=\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, and
                                            $\bigcup_{\alpha}h^{-1}_{\alpha}(\mathring{D}^{m})=M$. To continue. Vector
                                            Bundle Family of Sets Vector Space Field A vector bundle is topological
                                            construction that has a family of vector spaces parameterized by another
                                            space $X$. This space $X$ can be a topolgical space, manifold, or another
                                            type of algebraic variety. By family, we are refering to more generally a
                                            family of sets. This family of sets is a collection $F$ of subsets over the
                                            given set $S$. More to our case we have a family of sets or vector spaces
                                            parameterized over the manifold space $M$. For vector space, we are refering
                                            to a set of vector elements that may be added together and multiplied by
                                            numbers or scalars. These scalars often are real numbers but can be complex
                                            numbers or another type of element of any field. Speaking more generally,
                                            for every point $x$ of the space $X$ we we associate a vector space $V(x)$
                                            such that these vector spaces fit together to form another space of the same
                                            kind as $X$. This resulting same kind of space is called a vector bundle
                                            over $X$. Trivial Bundle Trivial vector bundles is where we have a fixed
                                            vector space $V$ such that $V(x)=V$ for every $x$ in some space $X$. We
                                            define this as letting $E=B\times F$ and let $\pi: E \rightarrow B$ be
                                            defined as the projection onto the first factor. $\pi$ is a fiber bundle of
                                            a family of vector spaces $F$ over $B$. Note, here $E$ is globally a
                                            product. Finally, any such fiber bundle is called a trivial bundle. Smooth
                                            Vector Bundle Image Source: Theory and Numerical Simulation of Deep Rock
                                            Mass Based on a Non-Euclidean Model Let $\pi: E \rightarrow M$ be an
                                            $n$-dimensional vector bundle over a smooth manifold $M$. Let
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ be an atlas on $M$ where the vector
                                            bundle is trivial over the sets $U_{\alpha}$. Let $\phi_{a}$ represent that
                                            canonical map defined as $\pi^{-1}(U_{a})\rightarrow U_{a}
                                            \times\mathbb{R}^n$ with a projection onto $n$-dimensional Eucledian space
                                            $\mathbb{R}^n$. Here we are taking our open subset $U_{\alpha}$ of the
                                            manifold in topolgical space $M$ and mapping $U_{\alpha}$ to $\mathbb{R}^n$.
                                            With the smooth vector bundle, we take the family of open subsets
                                            $U_{\alpha}$ in atlas $\left\{U_{\alpha}, h_{\alpha}\right\}$ on toplogical
                                            space and pass it through the canonical map to $n$-dimensional Eucledian
                                            space. Operations on Manifolds Connected Sums This graph represents a
                                            connected sum between $m$-dimensional manifolds $M_{1}, M_{2}$. Using the
                                            notation: $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$
                                            $=(M_{1}-h_{1}(0))\cup_{g}(M_{2}-h_{2}(0))$ where $g=h_{2}\alpha h_{1}^{-1}$
                                            we can formally define the function for the connected sums between two
                                            manifolds. In our notation $M_{1}, M_{2}$ are two connected $m$-dimensional
                                            manifolds. We let $h_{i}: \mathbb{R}^{m} \rightarrow M_{i}, i=1,2$, be two
                                            imbeddings. $\alpha$ is an arbritary orientation reversing diffeomorphism
                                            whose domain is $(0, \infty)$ and codomain is $(0, \infty)$. Next, we define
                                            $\alpha_{m}: \bf \mathbb{R} - 0 \rightarrow \mathbb{R} - 0$ by:
                                            $\alpha(v)\;=\;\alpha(|v|) {v \over |v|}$ The connected sum is then written
                                            as $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$. Handle Presentation Theorem
                                            Cobordism Preliminary An ordered triple of manifolds
                                            $\mathscr{C}=\left\{V_0, W, V_1\right\}$ where $\partial W=V_0 \cup V_1$ and
                                            $V_{0}, V_{1}$ are disjoint open subsets of $\partial W$ is called a
                                            cobordism. Such that $V_0=\partial_{-} W$ is the left-hand boundary of $W$.
                                            $\{M \times\{0\}, M \times I, M \times\{1\}\}$, where $M$ is a compact
                                            closed manifold, is an example cobordism termed an elementary cobordism of
                                            $\lambda$. The result is attaching a $\lambda$-handle to the right-hand
                                            boundary of $M \times I$. Note, we will view a trivial cobordism as an
                                            elementary cobordismof index $-1$, as it is convient. Consider two
                                            cobordisms $\mathscr{C}=\left\{V_0, W, V_1\right\}$,
                                            $\mathscr{C}'=\left\{V'_{0}, W', V'_{1}\right\}$, and a diffeomorphism given
                                            as $h:V'_{1}\rightarrow V'_{0}$. Using $h$, we can join $W$ and $W'$, like
                                            so: $W_{1}=W\bigcup_{h}W'$. Let $\partial W_{1}=V_{0}\cup V'_{1}$ and
                                            $\left\{V_0, W_{1}, V'_1\right\}$ be a cobordism denoted as $\mathscr{C}\cup
                                            \mathscr{C}'$. Note, that this notation is symbolic and does not show the
                                            result dependent on the diffeomorphism $h$. That being said, given
                                            $\mathscr{C}$ is a trivial cobordism, the result does not depend on $h$ and
                                            therefore the result is given as $\mathscr{C}\cup \mathscr{C}'$ $=$
                                            $\mathscr{C}$. Handle Presentation Theorem and Morse Theory The Smale and
                                            Wallace theorem is the following. Let $\mathscr{C}=\left\{V_0, W,
                                            V_1\right\}$ be a cobordism, such that $\mathscr{C}$ $=$ $\mathscr{C}_1 \cup
                                            \mathscr{C}_2 \;\cup$ $\cdots$ $\cup \;\mathscr{C}_k$, where $\mathscr{C}_i$
                                            are elementary cobordisms. Given $i < j$ implies $\lambda(i) \leq
                                            \lambda(j)$, where $\lambda(i)$ denotes the index of $\mathscr{C}_i$. The
                                            corollary is there exists a sequence of manifolds $V_0 \times I$ $=$ $W_{-1}
                                            \subset W_0 \subset W_1 \subset$ $\cdots$ $\subset W_m=W$ such that $W_i$ is
                                            obtained from $W_{i-1}$ by attaching $i$-handles to the right-hand boundary
                                            of the cobordism $\mathscr{C}$. To continue. Glossary Topological Space
                                            Topologoical vs Eucledian Space Let us break this down. First, we will start
                                            with getting an idea of what a topological space is, since this is one of
                                            the first parts of our definition of a manifold. A topological space is
                                            roughly speaking where the closeness of the points in the space cannot be
                                            numerically defined, unlike in Euclidean space. What is Euclidean space? If
                                            you recall, in school we would plot a point in 2-d space using $(x, y)$ and
                                            an $x$ and $y$ axis with dashes along the side to help define the space
                                            between values. This is 2-dimensional Euclidean space. Or written as
                                            $\mathbb{R}^{2}$. Of course, our Euclidean space is not limited to 2
                                            dimensions. We can use similar descriptors for 3-d space, 11-space, and any
                                            number of dimensions, or $n$-d Euclidean space $\mathbb{R}^{n}$. However,
                                            continuing with 2-d space, using our numbered $x$ and $y$ axis and our
                                            points with coordinates, we can figure out the numerical value that
                                            represents how close one point is to other. However, a topological space,
                                            unlike our familiar Eucledian space, is more generalized than that. Rather,
                                            our space does not have a "numbered axis" to define one point relation to
                                            another. Rather our space is represented as a set of points whose elements
                                            are called points. In this set that defines our topological space, we have a
                                            structure called a topology. This topology is defined as a set of
                                            neighborhoods for every point that, by examining the other points or
                                            elements that are in this neighborhood set, gives an idea of closeness or
                                            relation of one point to another. So, we have a set of points that defines
                                            what makes up our topological space and a structure called a topology that
                                            is defined as a set of neighbors. Topological Structure Open Subsets
                                            Neighboorhoods A topological structure, roughly speaking, is an additional
                                            structure to a topological space that can be defined as a set of
                                            neighborhoods. A neighborhood is closely related to an open subset. This is
                                            why in out definition when we take an element from our topology we get an
                                            open subset. i.e. $U_{p} \in \tau_{M}$. Basically, a neighborhood of a point
                                            from a topological space is a set of points that contains that point and the
                                            points for which we can move around in any direction and still be in our set
                                            or neighborhood. This is why our point $p$ that we have is an element of
                                            $U$. Since when we take a point we want to examine a slice of our topology
                                            that contains the point we are examining and not some random point. We do
                                            this so we can keep properly traversing through our topology and keep a list
                                            of all of the charts and their respective homeomorphisms. This allows us to
                                            determine if all elements or open subsets in our topology when sent through
                                            their respective homeomorphisms, are a part of eucledian space $R^{d}$. i.e.
                                            if parts of our topological space $M$ examined locally are apart of
                                            eucledian space $R^{d}$. Chart Homeomorphism A chart in a topological space
                                            $M$ consists of $U_{\alpha}$ an open covering that is homeomorphic, via
                                            $h_{\alpha}$ a homeomorphism, to an open subset $U_{\alpha}$ in Eucledian
                                            space $\mathbb{R}^{n}$. Atlas An atlas $C^{r}$ on a topological space $M$ is
                                            a collection $\left\{U_{\alpha},h_{\alpha}\right\}$ of charts. $U_{\alpha}$
                                            is an open covering admitted on $M$ and $h_{\alpha}$ is a homeomorphism on
                                            $U_{\alpha}$ to the Eucledian space $\mathbb{R}^{n}$. The $C^{r}$ transition
                                            maps $h_{\beta}h_{\alpha}^{-1}$ are $C^{r}$ maps on
                                            $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. Tangent Bundle To continue. Informal
                                            Manifold Definition Short version: a manifold is a topological space that
                                            locally resembles Eucliden space. Medium version: an $n$-dimensional
                                            manifold, or $n$-manifold, is a topological space where each point has a
                                            neighboorhood that is homeomorphic to an open subset on $n$-dimensional
                                            Euclidean space. Differential Manifolds Differential Structures Manifolds
                                            Atlas Maps $C^{r}$ Structure An $n$-dimensional manifold is a topological
                                            space where each point has a neighborhood that is homeomorphic to an open
                                            subset on $n$-dimensional Euclidean space. Let $M$ be a topological space. A
                                            chart in $M$ consists of an open subset $U \subset M$ and a homomorphism $h$
                                            of $U$ onto an open subset of $R^{m}$. A $C^{r}$ atlas on $M$ is a
                                            collection $\left\{U_{\alpha}, h_{\alpha}\right\}$ of charts such that the
                                            $U_{\alpha}$ cover $M$ and $h_{\beta}h_{\alpha}^{-1}$, the transition maps,
                                            are $C^{r}$ maps on $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. A maximal atlas
                                            $C^{r}$ on our Manifold $M$ is called a $C^{r}$ structure. A manifold of
                                            class $C^{r}$ that has a second coutnable Hausdorff space $M$, and has a
                                            $C^{r}$ structure on it, is called a differential manifold. Smooth Mainfold
                                            Manifold with Bounds Given an open subset $U\subset \mathbb{R}^{n}$, a
                                            function $f:U\rightarrow M\subset\mathbb{R}^{A}$, is smooth if for all
                                            functions $f_{\alpha}:U\rightarrow\mathbb{R}$ is smooth. Where $R^{A}$
                                            denotes the vector space of all functions $x$. Given $f$ is smooth, the
                                            partial derivative $\frac{\partial_{f}}{\partial_{u_{i}}}$ is defined as a
                                            smooth function $U\rightarrow\mathbb{R}^{A}$, where for the $\alpha$-th
                                            coordinate is $\frac{\partial_{\alpha}}{\partial_{u_{i}}}$ for $i=1,...,n$.
                                            A good method of studying smooth manifolds is by putting together the smooth
                                            manifold with smaller pieces. These smaller pieces themselves are not smooth
                                            manifolds, but rather manifolds with boundary. The definition of a manifold
                                            parallels a manifold definition (defined in the Manifolds section) however,
                                            we need to update the definition of a homeomorphism. For a manifold with
                                            boundary, the homeomorphisms $h_{\alpha}$ onto the open subsets of
                                            $\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, where
                                            $\mathbb{R}^{m}_{+}=\left\{(x_{1},...,x_{m})\in\mathbb{R}^{m}|\;x_{m}\ge
                                            0\right\}$. Meaning, with our update when an open subset of our topological
                                            structure is passed through the homeomorphic functions $h_{\alpha}$, it can
                                            now be mapped surjectively to the eucledian space of $\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$. Where for $\mathbb{R}^{m}_{+}$, the last element in
                                            the ordered pair of real elements is $\ge 0$. To put it succently, the
                                            transtion maps now become maps of open subsets of $\mathbb{R}^{m}_{+}$. This
                                            type of map is smooth if it is locally a restriction of a smooth map defined
                                            on an open subset of $\mathbb{R}^{m}$, by definition. Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ given as: $|v|_p=\sqrt{g_p(v, v)}$ Partitions of
                                            Unity In differential topology, many constructions utilize partitions of
                                            unity. Before we continue, we will formally define a specific type of atlas.
                                            This is an atlas $\left\{U_{\alpha},h_{\alpha}\right\}$ on $M$ that is said
                                            to be adequate if it is locally finite,
                                            $h_{\alpha}(U_{\alpha})=\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, and
                                            $\bigcup_{\alpha}h^{-1}_{\alpha}(\mathring{D}^{m})=M$. To continue. Vector
                                            Bundle Family of Sets Vector Space Field A vector bundle is topological
                                            construction that has a family of vector spaces parameterized by another
                                            space $X$. This space $X$ can be a topolgical space, manifold, or another
                                            type of algebraic variety. By family, we are refering to more generally a
                                            family of sets. This family of sets is a collection $F$ of subsets over the
                                            given set $S$. More to our case we have a family of sets or vector spaces
                                            parameterized over the manifold space $M$. For vector space, we are refering
                                            to a set of vector elements that may be added together and multiplied by
                                            numbers or scalars. These scalars often are real numbers but can be complex
                                            numbers or another type of element of any field. Speaking more generally,
                                            for every point $x$ of the space $X$ we we associate a vector space $V(x)$
                                            such that these vector spaces fit together to form another space of the same
                                            kind as $X$. This resulting same kind of space is called a vector bundle
                                            over $X$. Trivial Bundle Trivial vector bundles is where we have a fixed
                                            vector space $V$ such that $V(x)=V$ for every $x$ in some space $X$. We
                                            define this as letting $E=B\times F$ and let $\pi: E \rightarrow B$ be
                                            defined as the projection onto the first factor. $\pi$ is a fiber bundle of
                                            a family of vector spaces $F$ over $B$. Note, here $E$ is globally a
                                            product. Finally, any such fiber bundle is called a trivial bundle. Smooth
                                            Vector Bundle Image Source: Theory and Numerical Simulation of Deep Rock
                                            Mass Based on a Non-Euclidean Model Let $\pi: E \rightarrow M$ be an
                                            $n$-dimensional vector bundle over a smooth manifold $M$. Let
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ be an atlas on $M$ where the vector
                                            bundle is trivial over the sets $U_{\alpha}$. Let $\phi_{a}$ represent that
                                            canonical map defined as $\pi^{-1}(U_{a})\rightarrow U_{a}
                                            \times\mathbb{R}^n$ with a projection onto $n$-dimensional Eucledian space
                                            $\mathbb{R}^n$. Here we are taking our open subset $U_{\alpha}$ of the
                                            manifold in topolgical space $M$ and mapping $U_{\alpha}$ to $\mathbb{R}^n$.
                                            With the smooth vector bundle, we take the family of open subsets
                                            $U_{\alpha}$ in atlas $\left\{U_{\alpha}, h_{\alpha}\right\}$ on toplogical
                                            space and pass it through the canonical map to $n$-dimensional Eucledian
                                            space. Operations on Manifolds Connected Sums This graph represents a
                                            connected sum between $m$-dimensional manifolds $M_{1}, M_{2}$. Using the
                                            notation: $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$
                                            $=(M_{1}-h_{1}(0))\cup_{g}(M_{2}-h_{2}(0))$ where $g=h_{2}\alpha h_{1}^{-1}$
                                            we can formally define the function for the connected sums between two
                                            manifolds. In our notation $M_{1}, M_{2}$ are two connected $m$-dimensional
                                            manifolds. We let $h_{i}: \mathbb{R}^{m} \rightarrow M_{i}, i=1,2$, be two
                                            imbeddings. $\alpha$ is an arbritary orientation reversing diffeomorphism
                                            whose domain is $(0, \infty)$ and codomain is $(0, \infty)$. Next, we define
                                            $\alpha_{m}: \bf \mathbb{R} - 0 \rightarrow \mathbb{R} - 0$ by:
                                            $\alpha(v)\;=\;\alpha(|v|) {v \over |v|}$ The connected sum is then written
                                            as $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$. Differential ManifoldsDifferential
                                            Structures Manifolds Atlas Maps $C^{r}$ Structure An $n$-dimensional
                                            manifold is a topological space where each point has a neighborhood that is
                                            homeomorphic to an open subset on $n$-dimensional Euclidean space. Let $M$
                                            be a topological space. A chart in $M$ consists of an open subset $U \subset
                                            M$ and a homomorphism $h$ of $U$ onto an open subset of $R^{m}$. A $C^{r}$
                                            atlas on $M$ is a collection $\left\{U_{\alpha}, h_{\alpha}\right\}$ of
                                            charts such that the $U_{\alpha}$ cover $M$ and $h_{\beta}h_{\alpha}^{-1}$,
                                            the transition maps, are $C^{r}$ maps on $h_{\alpha}(U_{\alpha}\cap
                                            U_{\beta})$. A maximal atlas $C^{r}$ on our Manifold $M$ is called a $C^{r}$
                                            structure. A manifold of class $C^{r}$ that has a second coutnable Hausdorff
                                            space $M$, and has a $C^{r}$ structure on it, is called a differential
                                            manifold. Smooth Mainfold Manifold with Bounds Given an open subset
                                            $U\subset \mathbb{R}^{n}$, a function $f:U\rightarrow
                                            M\subset\mathbb{R}^{A}$, is smooth if for all functions
                                            $f_{\alpha}:U\rightarrow\mathbb{R}$ is smooth. Where $R^{A}$ denotes the
                                            vector space of all functions $x$. Given $f$ is smooth, the partial
                                            derivative $\frac{\partial_{f}}{\partial_{u_{i}}}$ is defined as a smooth
                                            function $U\rightarrow\mathbb{R}^{A}$, where for the $\alpha$-th coordinate
                                            is $\frac{\partial_{\alpha}}{\partial_{u_{i}}}$ for $i=1,...,n$. A good
                                            method of studying smooth manifolds is by putting together the smooth
                                            manifold with smaller pieces. These smaller pieces themselves are not smooth
                                            manifolds, but rather manifolds with boundary. The definition of a manifold
                                            parallels a manifold definition (defined in the Manifolds section) however,
                                            we need to update the definition of a homeomorphism. For a manifold with
                                            boundary, the homeomorphisms $h_{\alpha}$ onto the open subsets of
                                            $\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, where
                                            $\mathbb{R}^{m}_{+}=\left\{(x_{1},...,x_{m})\in\mathbb{R}^{m}|\;x_{m}\ge
                                            0\right\}$. Meaning, with our update when an open subset of our topological
                                            structure is passed through the homeomorphic functions $h_{\alpha}$, it can
                                            now be mapped surjectively to the eucledian space of $\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$. Where for $\mathbb{R}^{m}_{+}$, the last element in
                                            the ordered pair of real elements is $\ge 0$. To put it succently, the
                                            transtion maps now become maps of open subsets of $\mathbb{R}^{m}_{+}$. This
                                            type of map is smooth if it is locally a restriction of a smooth map defined
                                            on an open subset of $\mathbb{R}^{m}$, by definition. Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ given as: $|v|_p=\sqrt{g_p(v, v)}$ Partitions of
                                            Unity In differential topology, many constructions utilize partitions of
                                            unity. Before we continue, we will formally define a specific type of atlas.
                                            This is an atlas $\left\{U_{\alpha},h_{\alpha}\right\}$ on $M$ that is said
                                            to be adequate if it is locally finite,
                                            $h_{\alpha}(U_{\alpha})=\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, and
                                            $\bigcup_{\alpha}h^{-1}_{\alpha}(\mathring{D}^{m})=M$. To continue. Vector
                                            Bundle Family of Sets Vector Space Field A vector bundle is topological
                                            construction that has a family of vector spaces parameterized by another
                                            space $X$. This space $X$ can be a topolgical space, manifold, or another
                                            type of algebraic variety. By family, we are refering to more generally a
                                            family of sets. This family of sets is a collection $F$ of subsets over the
                                            given set $S$. More to our case we have a family of sets or vector spaces
                                            parameterized over the manifold space $M$. For vector space, we are refering
                                            to a set of vector elements that may be added together and multiplied by
                                            numbers or scalars. These scalars often are real numbers but can be complex
                                            numbers or another type of element of any field. Speaking more generally,
                                            for every point $x$ of the space $X$ we we associate a vector space $V(x)$
                                            such that these vector spaces fit together to form another space of the same
                                            kind as $X$. This resulting same kind of space is called a vector bundle
                                            over $X$. Trivial Bundle Trivial vector bundles is where we have a fixed
                                            vector space $V$ such that $V(x)=V$ for every $x$ in some space $X$. We
                                            define this as letting $E=B\times F$ and let $\pi: E \rightarrow B$ be
                                            defined as the projection onto the first factor. $\pi$ is a fiber bundle of
                                            a family of vector spaces $F$ over $B$. Note, here $E$ is globally a
                                            product. Finally, any such fiber bundle is called a trivial bundle. Smooth
                                            Vector Bundle Image Source: Theory and Numerical Simulation of Deep Rock
                                            Mass Based on a Non-Euclidean Model Let $\pi: E \rightarrow M$ be an
                                            $n$-dimensional vector bundle over a smooth manifold $M$. Let
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ be an atlas on $M$ where the vector
                                            bundle is trivial over the sets $U_{\alpha}$. Let $\phi_{a}$ represent that
                                            canonical map defined as $\pi^{-1}(U_{a})\rightarrow U_{a}
                                            \times\mathbb{R}^n$ with a projection onto $n$-dimensional Eucledian space
                                            $\mathbb{R}^n$. Here we are taking our open subset $U_{\alpha}$ of the
                                            manifold in topolgical space $M$ and mapping $U_{\alpha}$ to $\mathbb{R}^n$.
                                            With the smooth vector bundle, we take the family of open subsets
                                            $U_{\alpha}$ in atlas $\left\{U_{\alpha}, h_{\alpha}\right\}$ on toplogical
                                            space and pass it through the canonical map to $n$-dimensional Eucledian
                                            space. Differential StructuresManifolds Atlas Maps $C^{r}$ Structure An
                                            $n$-dimensional manifold is a topological space where each point has a
                                            neighborhood that is homeomorphic to an open subset on $n$-dimensional
                                            Euclidean space. Let $M$ be a topological space. A chart in $M$ consists of
                                            an open subset $U \subset M$ and a homomorphism $h$ of $U$ onto an open
                                            subset of $R^{m}$. A $C^{r}$ atlas on $M$ is a collection
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ of charts such that the $U_{\alpha}$
                                            cover $M$ and $h_{\beta}h_{\alpha}^{-1}$, the transition maps, are $C^{r}$
                                            maps on $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. A maximal atlas $C^{r}$ on
                                            our Manifold $M$ is called a $C^{r}$ structure. A manifold of class $C^{r}$
                                            that has a second coutnable Hausdorff space $M$, and has a $C^{r}$ structure
                                            on it, is called a differential manifold. Smooth Mainfold Manifold with
                                            Bounds Given an open subset $U\subset \mathbb{R}^{n}$, a function
                                            $f:U\rightarrow M\subset\mathbb{R}^{A}$, is smooth if for all functions
                                            $f_{\alpha}:U\rightarrow\mathbb{R}$ is smooth. Where $R^{A}$ denotes the
                                            vector space of all functions $x$. Given $f$ is smooth, the partial
                                            derivative $\frac{\partial_{f}}{\partial_{u_{i}}}$ is defined as a smooth
                                            function $U\rightarrow\mathbb{R}^{A}$, where for the $\alpha$-th coordinate
                                            is $\frac{\partial_{\alpha}}{\partial_{u_{i}}}$ for $i=1,...,n$. A good
                                            method of studying smooth manifolds is by putting together the smooth
                                            manifold with smaller pieces. These smaller pieces themselves are not smooth
                                            manifolds, but rather manifolds with boundary. The definition of a manifold
                                            parallels a manifold definition (defined in the Manifolds section) however,
                                            we need to update the definition of a homeomorphism. For a manifold with
                                            boundary, the homeomorphisms $h_{\alpha}$ onto the open subsets of
                                            $\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, where
                                            $\mathbb{R}^{m}_{+}=\left\{(x_{1},...,x_{m})\in\mathbb{R}^{m}|\;x_{m}\ge
                                            0\right\}$. Meaning, with our update when an open subset of our topological
                                            structure is passed through the homeomorphic functions $h_{\alpha}$, it can
                                            now be mapped surjectively to the eucledian space of $\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$. Where for $\mathbb{R}^{m}_{+}$, the last element in
                                            the ordered pair of real elements is $\ge 0$. To put it succently, the
                                            transtion maps now become maps of open subsets of $\mathbb{R}^{m}_{+}$. This
                                            type of map is smooth if it is locally a restriction of a smooth map defined
                                            on an open subset of $\mathbb{R}^{m}$, by definition. Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ given as: $|v|_p=\sqrt{g_p(v, v)}$ Partitions of
                                            Unity In differential topology, many constructions utilize partitions of
                                            unity. Before we continue, we will formally define a specific type of atlas.
                                            This is an atlas $\left\{U_{\alpha},h_{\alpha}\right\}$ on $M$ that is said
                                            to be adequate if it is locally finite,
                                            $h_{\alpha}(U_{\alpha})=\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, and
                                            $\bigcup_{\alpha}h^{-1}_{\alpha}(\mathring{D}^{m})=M$. To continue.
                                            ManifoldsAtlas Maps $C^{r}$ Structure AtlasMaps$C^{r}$ StructureAn
                                            $n$-dimensional manifold is a topological space where each point has a
                                            neighborhood that is homeomorphic to an open subset on $n$-dimensional
                                            Euclidean space. Let $M$ be a topological space. A chart in $M$ consists of
                                            an open subset $U \subset M$ and a homomorphism $h$ of $U$ onto an open
                                            subset of $R^{m}$. A $C^{r}$ atlas on $M$ is a collection
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ of charts such that the $U_{\alpha}$
                                            cover $M$ and $h_{\beta}h_{\alpha}^{-1}$, the transition maps, are $C^{r}$
                                            maps on $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. topological spacechartatlasA
                                            maximal atlas $C^{r}$ on our Manifold $M$ is called a $C^{r}$ structure. A
                                            manifold of class $C^{r}$ that has a second coutnable Hausdorff space $M$,
                                            and has a $C^{r}$ structure on it, is called a differential manifold. Smooth
                                            Mainfold Manifold with Bounds Given an open subset $U\subset
                                            \mathbb{R}^{n}$, a function $f:U\rightarrow M\subset\mathbb{R}^{A}$, is
                                            smooth if for all functions $f_{\alpha}:U\rightarrow\mathbb{R}$ is smooth.
                                            Where $R^{A}$ denotes the vector space of all functions $x$. Given $f$ is
                                            smooth, the partial derivative $\frac{\partial_{f}}{\partial_{u_{i}}}$ is
                                            defined as a smooth function $U\rightarrow\mathbb{R}^{A}$, where for the
                                            $\alpha$-th coordinate is $\frac{\partial_{\alpha}}{\partial_{u_{i}}}$ for
                                            $i=1,...,n$. A good method of studying smooth manifolds is by putting
                                            together the smooth manifold with smaller pieces. These smaller pieces
                                            themselves are not smooth manifolds, but rather manifolds with boundary. The
                                            definition of a manifold parallels a manifold definition (defined in the
                                            Manifolds section) however, we need to update the definition of a
                                            homeomorphism. For a manifold with boundary, the homeomorphisms $h_{\alpha}$
                                            onto the open subsets of $\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, where
                                            $\mathbb{R}^{m}_{+}=\left\{(x_{1},...,x_{m})\in\mathbb{R}^{m}|\;x_{m}\ge
                                            0\right\}$. Meaning, with our update when an open subset of our topological
                                            structure is passed through the homeomorphic functions $h_{\alpha}$, it can
                                            now be mapped surjectively to the eucledian space of $\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$. Where for $\mathbb{R}^{m}_{+}$, the last element in
                                            the ordered pair of real elements is $\ge 0$. To put it succently, the
                                            transtion maps now become maps of open subsets of $\mathbb{R}^{m}_{+}$. This
                                            type of map is smooth if it is locally a restriction of a smooth map defined
                                            on an open subset of $\mathbb{R}^{m}$, by definition. Smooth
                                            MainfoldManifold with Bounds Manifold with BoundsGiven an open subset
                                            $U\subset \mathbb{R}^{n}$, a function $f:U\rightarrow
                                            M\subset\mathbb{R}^{A}$, is smooth if for all functions
                                            $f_{\alpha}:U\rightarrow\mathbb{R}$ is smooth. Where $R^{A}$ denotes the
                                            vector space of all functions $x$. Given $f$ is smooth, the partial
                                            derivative $\frac{\partial_{f}}{\partial_{u_{i}}}$ is defined as a smooth
                                            function $U\rightarrow\mathbb{R}^{A}$, where for the $\alpha$-th coordinate
                                            is $\frac{\partial_{\alpha}}{\partial_{u_{i}}}$ for $i=1,...,n$. A good
                                            method of studying smooth manifolds is by putting together the smooth
                                            manifold with smaller pieces. These smaller pieces themselves are not smooth
                                            manifolds, but rather manifolds with boundary. The definition of a manifold
                                            parallels a manifold definition (defined in the Manifolds section) however,
                                            we need to update the definition of a homeomorphism. For a manifold with
                                            boundary, the homeomorphisms $h_{\alpha}$ onto the open subsets of
                                            $\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, where
                                            $\mathbb{R}^{m}_{+}=\left\{(x_{1},...,x_{m})\in\mathbb{R}^{m}|\;x_{m}\ge
                                            0\right\}$. Meaning, with our update when an open subset of our topological
                                            structure is passed through the homeomorphic functions $h_{\alpha}$, it can
                                            now be mapped surjectively to the eucledian space of $\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$. Where for $\mathbb{R}^{m}_{+}$, the last element in
                                            the ordered pair of real elements is $\ge 0$. To put it succently, the
                                            transtion maps now become maps of open subsets of $\mathbb{R}^{m}_{+}$. This
                                            type of map is smooth if it is locally a restriction of a smooth map defined
                                            on an open subset of $\mathbb{R}^{m}$, by definition. ManifoldsRiemannian
                                            Manifold A Riemannian manifold is a smooth manifold $M$ with a Riemannian
                                            metric denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$
                                            assigns a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ given as: $|v|_p=\sqrt{g_p(v, v)}$ Riemannian
                                            ManifoldA Riemannian manifold is a smooth manifold $M$ with a Riemannian
                                            metric denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$
                                            assigns a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: smooth
                                            manifoldtangent bundlewith a norm $|\cdot|_p: T_p M \rightarrow \mathbb{R}$
                                            given as: $|v|_p=\sqrt{g_p(v, v)}$ Partitions of Unity In differential
                                            topology, many constructions utilize partitions of unity. Before we
                                            continue, we will formally define a specific type of atlas. This is an atlas
                                            $\left\{U_{\alpha},h_{\alpha}\right\}$ on $M$ that is said to be adequate if
                                            it is locally finite, $h_{\alpha}(U_{\alpha})=\mathbb{R}^{m}$ or
                                            $\mathbb{R}^{m}_{+}$, and
                                            $\bigcup_{\alpha}h^{-1}_{\alpha}(\mathring{D}^{m})=M$. To continue.
                                            Partitions of UnityIn differential topology, many constructions utilize
                                            partitions of unity. Before we continue, we will formally define a specific
                                            type of atlas. This is an atlas $\left\{U_{\alpha},h_{\alpha}\right\}$ on
                                            $M$ that is said to be adequate if it is locally finite,
                                            $h_{\alpha}(U_{\alpha})=\mathbb{R}^{m}$ or $\mathbb{R}^{m}_{+}$, and
                                            $\bigcup_{\alpha}h^{-1}_{\alpha}(\mathring{D}^{m})=M$. To continue. Vector
                                            Bundle Family of Sets Vector Space Field A vector bundle is topological
                                            construction that has a family of vector spaces parameterized by another
                                            space $X$. This space $X$ can be a topolgical space, manifold, or another
                                            type of algebraic variety. By family, we are refering to more generally a
                                            family of sets. This family of sets is a collection $F$ of subsets over the
                                            given set $S$. More to our case we have a family of sets or vector spaces
                                            parameterized over the manifold space $M$. For vector space, we are refering
                                            to a set of vector elements that may be added together and multiplied by
                                            numbers or scalars. These scalars often are real numbers but can be complex
                                            numbers or another type of element of any field. Speaking more generally,
                                            for every point $x$ of the space $X$ we we associate a vector space $V(x)$
                                            such that these vector spaces fit together to form another space of the same
                                            kind as $X$. This resulting same kind of space is called a vector bundle
                                            over $X$. Trivial Bundle Trivial vector bundles is where we have a fixed
                                            vector space $V$ such that $V(x)=V$ for every $x$ in some space $X$. We
                                            define this as letting $E=B\times F$ and let $\pi: E \rightarrow B$ be
                                            defined as the projection onto the first factor. $\pi$ is a fiber bundle of
                                            a family of vector spaces $F$ over $B$. Note, here $E$ is globally a
                                            product. Finally, any such fiber bundle is called a trivial bundle. Smooth
                                            Vector Bundle Image Source: Theory and Numerical Simulation of Deep Rock
                                            Mass Based on a Non-Euclidean Model Let $\pi: E \rightarrow M$ be an
                                            $n$-dimensional vector bundle over a smooth manifold $M$. Let
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ be an atlas on $M$ where the vector
                                            bundle is trivial over the sets $U_{\alpha}$. Let $\phi_{a}$ represent that
                                            canonical map defined as $\pi^{-1}(U_{a})\rightarrow U_{a}
                                            \times\mathbb{R}^n$ with a projection onto $n$-dimensional Eucledian space
                                            $\mathbb{R}^n$. Here we are taking our open subset $U_{\alpha}$ of the
                                            manifold in topolgical space $M$ and mapping $U_{\alpha}$ to $\mathbb{R}^n$.
                                            With the smooth vector bundle, we take the family of open subsets
                                            $U_{\alpha}$ in atlas $\left\{U_{\alpha}, h_{\alpha}\right\}$ on toplogical
                                            space and pass it through the canonical map to $n$-dimensional Eucledian
                                            space. Vector BundleFamily of Sets Vector Space Field Family of SetsVector
                                            SpaceFieldA vector bundle is topological construction that has a family of
                                            vector spaces parameterized by another space $X$. This space $X$ can be a
                                            topolgical space, manifold, or another type of algebraic variety. By family,
                                            we are refering to more generally a family of sets. This family of sets is a
                                            collection $F$ of subsets over the given set $S$. More to our case we have a
                                            family of sets or vector spaces parameterized over the manifold space $M$.
                                            For vector space, we are refering to a set of vector elements that may be
                                            added together and multiplied by numbers or scalars. These scalars often are
                                            real numbers but can be complex numbers or another type of element of any
                                            field. Speaking more generally, for every point $x$ of the space $X$ we we
                                            associate a vector space $V(x)$ such that these vector spaces fit together
                                            to form another space of the same kind as $X$. This resulting same kind of
                                            space is called a vector bundle over $X$. Trivial Bundle Trivial vector
                                            bundles is where we have a fixed vector space $V$ such that $V(x)=V$ for
                                            every $x$ in some space $X$. We define this as letting $E=B\times F$ and let
                                            $\pi: E \rightarrow B$ be defined as the projection onto the first factor.
                                            $\pi$ is a fiber bundle of a family of vector spaces $F$ over $B$. Note,
                                            here $E$ is globally a product. Finally, any such fiber bundle is called a
                                            trivial bundle. Trivial BundleTrivial vector bundles is where we have a
                                            fixed vector space $V$ such that $V(x)=V$ for every $x$ in some space $X$.
                                            We define this as letting $E=B\times F$ and let $\pi: E \rightarrow B$ be
                                            defined as the projection onto the first factor. $\pi$ is a fiber bundle of
                                            a family of vector spaces $F$ over $B$. Note, here $E$ is globally a
                                            product. Finally, any such fiber bundle is called a trivial bundle. Smooth
                                            Vector Bundle Image Source: Theory and Numerical Simulation of Deep Rock
                                            Mass Based on a Non-Euclidean Model Let $\pi: E \rightarrow M$ be an
                                            $n$-dimensional vector bundle over a smooth manifold $M$. Let
                                            $\left\{U_{\alpha}, h_{\alpha}\right\}$ be an atlas on $M$ where the vector
                                            bundle is trivial over the sets $U_{\alpha}$. Let $\phi_{a}$ represent that
                                            canonical map defined as $\pi^{-1}(U_{a})\rightarrow U_{a}
                                            \times\mathbb{R}^n$ with a projection onto $n$-dimensional Eucledian space
                                            $\mathbb{R}^n$. Here we are taking our open subset $U_{\alpha}$ of the
                                            manifold in topolgical space $M$ and mapping $U_{\alpha}$ to $\mathbb{R}^n$.
                                            With the smooth vector bundle, we take the family of open subsets
                                            $U_{\alpha}$ in atlas $\left\{U_{\alpha}, h_{\alpha}\right\}$ on toplogical
                                            space and pass it through the canonical map to $n$-dimensional Eucledian
                                            space. Smooth Vector BundleImage Source: Theory and Numerical Simulation of
                                            Deep Rock Mass Based on a Non-Euclidean Model Theory and Numerical
                                            Simulation of Deep Rock Mass Based on a Non-Euclidean ModelLet $\pi: E
                                            \rightarrow M$ be an $n$-dimensional vector bundle over a smooth manifold
                                            $M$. Let $\left\{U_{\alpha}, h_{\alpha}\right\}$ be an atlas on $M$ where
                                            the vector bundle is trivial over the sets $U_{\alpha}$. Let $\phi_{a}$
                                            represent that canonical map defined as $\pi^{-1}(U_{a})\rightarrow U_{a}
                                            \times\mathbb{R}^n$ with a projection onto $n$-dimensional Eucledian space
                                            $\mathbb{R}^n$. Here we are taking our open subset $U_{\alpha}$ of the
                                            manifold in topolgical space $M$ and mapping $U_{\alpha}$ to $\mathbb{R}^n$.
                                            With the smooth vector bundle, we take the family of open subsets
                                            $U_{\alpha}$ in atlas $\left\{U_{\alpha}, h_{\alpha}\right\}$ on toplogical
                                            space and pass it through the canonical map to $n$-dimensional Eucledian
                                            space. Operations on Manifolds Connected Sums This graph represents a
                                            connected sum between $m$-dimensional manifolds $M_{1}, M_{2}$. Using the
                                            notation: $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$
                                            $=(M_{1}-h_{1}(0))\cup_{g}(M_{2}-h_{2}(0))$ where $g=h_{2}\alpha h_{1}^{-1}$
                                            we can formally define the function for the connected sums between two
                                            manifolds. In our notation $M_{1}, M_{2}$ are two connected $m$-dimensional
                                            manifolds. We let $h_{i}: \mathbb{R}^{m} \rightarrow M_{i}, i=1,2$, be two
                                            imbeddings. $\alpha$ is an arbritary orientation reversing diffeomorphism
                                            whose domain is $(0, \infty)$ and codomain is $(0, \infty)$. Next, we define
                                            $\alpha_{m}: \bf \mathbb{R} - 0 \rightarrow \mathbb{R} - 0$ by:
                                            $\alpha(v)\;=\;\alpha(|v|) {v \over |v|}$ The connected sum is then written
                                            as $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$. Operations on ManifoldsConnected
                                            Sums This graph represents a connected sum between $m$-dimensional manifolds
                                            $M_{1}, M_{2}$. Using the notation: $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$
                                            $=(M_{1}-h_{1}(0))\cup_{g}(M_{2}-h_{2}(0))$ where $g=h_{2}\alpha h_{1}^{-1}$
                                            we can formally define the function for the connected sums between two
                                            manifolds. In our notation $M_{1}, M_{2}$ are two connected $m$-dimensional
                                            manifolds. We let $h_{i}: \mathbb{R}^{m} \rightarrow M_{i}, i=1,2$, be two
                                            imbeddings. $\alpha$ is an arbritary orientation reversing diffeomorphism
                                            whose domain is $(0, \infty)$ and codomain is $(0, \infty)$. Next, we define
                                            $\alpha_{m}: \bf \mathbb{R} - 0 \rightarrow \mathbb{R} - 0$ by:
                                            $\alpha(v)\;=\;\alpha(|v|) {v \over |v|}$ The connected sum is then written
                                            as $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$. Connected SumsThis graph represents
                                            a connected sum between $m$-dimensional manifolds $M_{1}, M_{2}$. Using the
                                            notation: $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$
                                            $=(M_{1}-h_{1}(0))\cup_{g}(M_{2}-h_{2}(0))$ where $g=h_{2}\alpha h_{1}^{-1}$
                                            we can formally define the function for the connected sums between two
                                            manifolds. In our notation $M_{1}, M_{2}$ are two connected $m$-dimensional
                                            manifolds. We let $h_{i}: \mathbb{R}^{m} \rightarrow M_{i}, i=1,2$, be two
                                            imbeddings. $\alpha$ is an arbritary orientation reversing diffeomorphism
                                            whose domain is $(0, \infty)$ and codomain is $(0, \infty)$. Next, we define
                                            $\alpha_{m}: \bf \mathbb{R} - 0 \rightarrow \mathbb{R} - 0$ by:
                                            $\alpha(v)\;=\;\alpha(|v|) {v \over |v|}$ The connected sum is then written
                                            as $M_{1}\#M_{2}(h_{1}, h_{2}, \alpha)$. Handle Presentation Theorem
                                            Cobordism Preliminary An ordered triple of manifolds
                                            $\mathscr{C}=\left\{V_0, W, V_1\right\}$ where $\partial W=V_0 \cup V_1$ and
                                            $V_{0}, V_{1}$ are disjoint open subsets of $\partial W$ is called a
                                            cobordism. Such that $V_0=\partial_{-} W$ is the left-hand boundary of $W$.
                                            $\{M \times\{0\}, M \times I, M \times\{1\}\}$, where $M$ is a compact
                                            closed manifold, is an example cobordism termed an elementary cobordism of
                                            $\lambda$. The result is attaching a $\lambda$-handle to the right-hand
                                            boundary of $M \times I$. Note, we will view a trivial cobordism as an
                                            elementary cobordismof index $-1$, as it is convient. Consider two
                                            cobordisms $\mathscr{C}=\left\{V_0, W, V_1\right\}$,
                                            $\mathscr{C}'=\left\{V'_{0}, W', V'_{1}\right\}$, and a diffeomorphism given
                                            as $h:V'_{1}\rightarrow V'_{0}$. Using $h$, we can join $W$ and $W'$, like
                                            so: $W_{1}=W\bigcup_{h}W'$. Let $\partial W_{1}=V_{0}\cup V'_{1}$ and
                                            $\left\{V_0, W_{1}, V'_1\right\}$ be a cobordism denoted as $\mathscr{C}\cup
                                            \mathscr{C}'$. Note, that this notation is symbolic and does not show the
                                            result dependent on the diffeomorphism $h$. That being said, given
                                            $\mathscr{C}$ is a trivial cobordism, the result does not depend on $h$ and
                                            therefore the result is given as $\mathscr{C}\cup \mathscr{C}'$ $=$
                                            $\mathscr{C}$. Handle Presentation Theorem and Morse Theory The Smale and
                                            Wallace theorem is the following. Let $\mathscr{C}=\left\{V_0, W,
                                            V_1\right\}$ be a cobordism, such that $\mathscr{C}$ $=$ $\mathscr{C}_1 \cup
                                            \mathscr{C}_2 \;\cup$ $\cdots$ $\cup \;\mathscr{C}_k$, where $\mathscr{C}_i$
                                            are elementary cobordisms. Given $i < j$ implies $\lambda(i) \leq
                                            \lambda(j)$, where $\lambda(i)$ denotes the index of $\mathscr{C}_i$. The
                                            corollary is there exists a sequence of manifolds $V_0 \times I$ $=$ $W_{-1}
                                            \subset W_0 \subset W_1 \subset$ $\cdots$ $\subset W_m=W$ such that $W_i$ is
                                            obtained from $W_{i-1}$ by attaching $i$-handles to the right-hand boundary
                                            of the cobordism $\mathscr{C}$. To continue. Handle Presentation
                                            TheoremCobordism Preliminary An ordered triple of manifolds
                                            $\mathscr{C}=\left\{V_0, W, V_1\right\}$ where $\partial W=V_0 \cup V_1$ and
                                            $V_{0}, V_{1}$ are disjoint open subsets of $\partial W$ is called a
                                            cobordism. Such that $V_0=\partial_{-} W$ is the left-hand boundary of $W$.
                                            $\{M \times\{0\}, M \times I, M \times\{1\}\}$, where $M$ is a compact
                                            closed manifold, is an example cobordism termed an elementary cobordism of
                                            $\lambda$. The result is attaching a $\lambda$-handle to the right-hand
                                            boundary of $M \times I$. Note, we will view a trivial cobordism as an
                                            elementary cobordismof index $-1$, as it is convient. Consider two
                                            cobordisms $\mathscr{C}=\left\{V_0, W, V_1\right\}$,
                                            $\mathscr{C}'=\left\{V'_{0}, W', V'_{1}\right\}$, and a diffeomorphism given
                                            as $h:V'_{1}\rightarrow V'_{0}$. Using $h$, we can join $W$ and $W'$, like
                                            so: $W_{1}=W\bigcup_{h}W'$. Let $\partial W_{1}=V_{0}\cup V'_{1}$ and
                                            $\left\{V_0, W_{1}, V'_1\right\}$ be a cobordism denoted as $\mathscr{C}\cup
                                            \mathscr{C}'$. Note, that this notation is symbolic and does not show the
                                            result dependent on the diffeomorphism $h$. That being said, given
                                            $\mathscr{C}$ is a trivial cobordism, the result does not depend on $h$ and
                                            therefore the result is given as $\mathscr{C}\cup \mathscr{C}'$ $=$
                                            $\mathscr{C}$. Cobordism PreliminaryAn ordered triple of manifolds
                                            $\mathscr{C}=\left\{V_0, W, V_1\right\}$ where $\partial W=V_0 \cup V_1$ and
                                            $V_{0}, V_{1}$ are disjoint open subsets of $\partial W$ is called a
                                            cobordism. Such that $V_0=\partial_{-} W$ is the left-hand boundary of $W$.
                                            $\{M \times\{0\}, M \times I, M \times\{1\}\}$, where $M$ is a compact
                                            closed manifold, is an example cobordism termed an elementary cobordism of
                                            $\lambda$. The result is attaching a $\lambda$-handle to the right-hand
                                            boundary of $M \times I$. Note, we will view a trivial cobordism as an
                                            elementary cobordismof index $-1$, as it is convient. Consider two
                                            cobordisms $\mathscr{C}=\left\{V_0, W, V_1\right\}$,
                                            $\mathscr{C}'=\left\{V'_{0}, W', V'_{1}\right\}$, and a diffeomorphism given
                                            as $h:V'_{1}\rightarrow V'_{0}$. Using $h$, we can join $W$ and $W'$, like
                                            so: $W_{1}=W\bigcup_{h}W'$. Let $\partial W_{1}=V_{0}\cup V'_{1}$ and
                                            $\left\{V_0, W_{1}, V'_1\right\}$ be a cobordism denoted as $\mathscr{C}\cup
                                            \mathscr{C}'$. Note, that this notation is symbolic and does not show the
                                            result dependent on the diffeomorphism $h$. That being said, given
                                            $\mathscr{C}$ is a trivial cobordism, the result does not depend on $h$ and
                                            therefore the result is given as $\mathscr{C}\cup \mathscr{C}'$ $=$
                                            $\mathscr{C}$. Handle Presentation Theorem and Morse Theory The Smale and
                                            Wallace theorem is the following. Let $\mathscr{C}=\left\{V_0, W,
                                            V_1\right\}$ be a cobordism, such that $\mathscr{C}$ $=$ $\mathscr{C}_1 \cup
                                            \mathscr{C}_2 \;\cup$ $\cdots$ $\cup \;\mathscr{C}_k$, where $\mathscr{C}_i$
                                            are elementary cobordisms. Given $i < j$ implies $\lambda(i) \leq
                                            \lambda(j)$, where $\lambda(i)$ denotes the index of $\mathscr{C}_i$. The
                                            corollary is there exists a sequence of manifolds $V_0 \times I$ $=$ $W_{-1}
                                            \subset W_0 \subset W_1 \subset$ $\cdots$ $\subset W_m=W$ such that $W_i$ is
                                            obtained from $W_{i-1}$ by attaching $i$-handles to the right-hand boundary
                                            of the cobordism $\mathscr{C}$. To continue. Handle Presentation Theorem and
                                            Morse TheoryThe Smale and Wallace theorem is the following. Let
                                            $\mathscr{C}=\left\{V_0, W, V_1\right\}$ be a cobordism, such that
                                            $\mathscr{C}$ $=$ $\mathscr{C}_1 \cup \mathscr{C}_2 \;\cup$ $\cdots$ $\cup
                                            \;\mathscr{C}_k$, where $\mathscr{C}_i$ are elementary cobordisms. Given $i
                                            < j$ implies $\lambda(i) \leq \lambda(j)$, where $\lambda(i)$ denotes the
                                            index of $\mathscr{C}_i$. The corollary is there exists a sequence of
                                            manifolds $V_0 \times I$ $=$ $W_{-1} \subset W_0 \subset W_1 \subset$
                                            $\cdots$ $\subset W_m=W$ such that $W_i$ is obtained from $W_{i-1}$ by
                                            attaching $i$-handles to the right-hand boundary of the cobordism
                                            $\mathscr{C}$. To continue. Glossary Topological Space Topologoical vs
                                            Eucledian Space Let us break this down. First, we will start with getting an
                                            idea of what a topological space is, since this is one of the first parts of
                                            our definition of a manifold. A topological space is roughly speaking where
                                            the closeness of the points in the space cannot be numerically defined,
                                            unlike in Euclidean space. What is Euclidean space? If you recall, in school
                                            we would plot a point in 2-d space using $(x, y)$ and an $x$ and $y$ axis
                                            with dashes along the side to help define the space between values. This is
                                            2-dimensional Euclidean space. Or written as $\mathbb{R}^{2}$. Of course,
                                            our Euclidean space is not limited to 2 dimensions. We can use similar
                                            descriptors for 3-d space, 11-space, and any number of dimensions, or $n$-d
                                            Euclidean space $\mathbb{R}^{n}$. However, continuing with 2-d space, using
                                            our numbered $x$ and $y$ axis and our points with coordinates, we can figure
                                            out the numerical value that represents how close one point is to other.
                                            However, a topological space, unlike our familiar Eucledian space, is more
                                            generalized than that. Rather, our space does not have a "numbered axis" to
                                            define one point relation to another. Rather our space is represented as a
                                            set of points whose elements are called points. In this set that defines our
                                            topological space, we have a structure called a topology. This topology is
                                            defined as a set of neighborhoods for every point that, by examining the
                                            other points or elements that are in this neighborhood set, gives an idea of
                                            closeness or relation of one point to another. So, we have a set of points
                                            that defines what makes up our topological space and a structure called a
                                            topology that is defined as a set of neighbors. Topological Structure Open
                                            Subsets Neighboorhoods A topological structure, roughly speaking, is an
                                            additional structure to a topological space that can be defined as a set of
                                            neighborhoods. A neighborhood is closely related to an open subset. This is
                                            why in out definition when we take an element from our topology we get an
                                            open subset. i.e. $U_{p} \in \tau_{M}$. Basically, a neighborhood of a point
                                            from a topological space is a set of points that contains that point and the
                                            points for which we can move around in any direction and still be in our set
                                            or neighborhood. This is why our point $p$ that we have is an element of
                                            $U$. Since when we take a point we want to examine a slice of our topology
                                            that contains the point we are examining and not some random point. We do
                                            this so we can keep properly traversing through our topology and keep a list
                                            of all of the charts and their respective homeomorphisms. This allows us to
                                            determine if all elements or open subsets in our topology when sent through
                                            their respective homeomorphisms, are a part of eucledian space $R^{d}$. i.e.
                                            if parts of our topological space $M$ examined locally are apart of
                                            eucledian space $R^{d}$. Chart Homeomorphism A chart in a topological space
                                            $M$ consists of $U_{\alpha}$ an open covering that is homeomorphic, via
                                            $h_{\alpha}$ a homeomorphism, to an open subset $U_{\alpha}$ in Eucledian
                                            space $\mathbb{R}^{n}$. Atlas An atlas $C^{r}$ on a topological space $M$ is
                                            a collection $\left\{U_{\alpha},h_{\alpha}\right\}$ of charts. $U_{\alpha}$
                                            is an open covering admitted on $M$ and $h_{\alpha}$ is a homeomorphism on
                                            $U_{\alpha}$ to the Eucledian space $\mathbb{R}^{n}$. The $C^{r}$ transition
                                            maps $h_{\beta}h_{\alpha}^{-1}$ are $C^{r}$ maps on
                                            $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. Tangent Bundle To continue. Informal
                                            Manifold Definition Short version: a manifold is a topological space that
                                            locally resembles Eucliden space. Medium version: an $n$-dimensional
                                            manifold, or $n$-manifold, is a topological space where each point has a
                                            neighboorhood that is homeomorphic to an open subset on $n$-dimensional
                                            Euclidean space. GlossaryTopological Space Topologoical vs Eucledian Space
                                            Let us break this down. First, we will start with getting an idea of what a
                                            topological space is, since this is one of the first parts of our definition
                                            of a manifold. A topological space is roughly speaking where the closeness
                                            of the points in the space cannot be numerically defined, unlike in
                                            Euclidean space. What is Euclidean space? If you recall, in school we would
                                            plot a point in 2-d space using $(x, y)$ and an $x$ and $y$ axis with dashes
                                            along the side to help define the space between values. This is
                                            2-dimensional Euclidean space. Or written as $\mathbb{R}^{2}$. Of course,
                                            our Euclidean space is not limited to 2 dimensions. We can use similar
                                            descriptors for 3-d space, 11-space, and any number of dimensions, or $n$-d
                                            Euclidean space $\mathbb{R}^{n}$. However, continuing with 2-d space, using
                                            our numbered $x$ and $y$ axis and our points with coordinates, we can figure
                                            out the numerical value that represents how close one point is to other.
                                            However, a topological space, unlike our familiar Eucledian space, is more
                                            generalized than that. Rather, our space does not have a "numbered axis" to
                                            define one point relation to another. Rather our space is represented as a
                                            set of points whose elements are called points. In this set that defines our
                                            topological space, we have a structure called a topology. This topology is
                                            defined as a set of neighborhoods for every point that, by examining the
                                            other points or elements that are in this neighborhood set, gives an idea of
                                            closeness or relation of one point to another. So, we have a set of points
                                            that defines what makes up our topological space and a structure called a
                                            topology that is defined as a set of neighbors. Topological
                                            SpaceTopologoical vs Eucledian Space Topologoical vs Eucledian SpaceLet us
                                            break this down. First, we will start with getting an idea of what a
                                            topological space is, since this is one of the first parts of our definition
                                            of a manifold. A topological space is roughly speaking where the closeness
                                            of the points in the space cannot be numerically defined, unlike in
                                            Euclidean space. What is Euclidean space? If you recall, in school we would
                                            plot a point in 2-d space using $(x, y)$ and an $x$ and $y$ axis with dashes
                                            along the side to help define the space between values. This is
                                            2-dimensional Euclidean space. Or written as $\mathbb{R}^{2}$. Of course,
                                            our Euclidean space is not limited to 2 dimensions. We can use similar
                                            descriptors for 3-d space, 11-space, and any number of dimensions, or $n$-d
                                            Euclidean space $\mathbb{R}^{n}$. However, continuing with 2-d space, using
                                            our numbered $x$ and $y$ axis and our points with coordinates, we can figure
                                            out the numerical value that represents how close one point is to other.
                                            However, a topological space, unlike our familiar Eucledian space, is more
                                            generalized than that. Rather, our space does not have a "numbered axis" to
                                            define one point relation to another. Rather our space is represented as a
                                            set of points whose elements are called points. In this set that defines our
                                            topological space, we have a structure called a topology. This topology is
                                            defined as a set of neighborhoods for every point that, by examining the
                                            other points or elements that are in this neighborhood set, gives an idea of
                                            closeness or relation of one point to another. So, we have a set of points
                                            that defines what makes up our topological space and a structure called a
                                            topology that is defined as a set of neighbors.
                                            topologicalEucledianTopological Structure Open Subsets Neighboorhoods A
                                            topological structure, roughly speaking, is an additional structure to a
                                            topological space that can be defined as a set of neighborhoods. A
                                            neighborhood is closely related to an open subset. This is why in out
                                            definition when we take an element from our topology we get an open subset.
                                            i.e. $U_{p} \in \tau_{M}$. Basically, a neighborhood of a point from a
                                            topological space is a set of points that contains that point and the points
                                            for which we can move around in any direction and still be in our set or
                                            neighborhood. This is why our point $p$ that we have is an element of $U$.
                                            Since when we take a point we want to examine a slice of our topology that
                                            contains the point we are examining and not some random point. We do this so
                                            we can keep properly traversing through our topology and keep a list of all
                                            of the charts and their respective homeomorphisms. This allows us to
                                            determine if all elements or open subsets in our topology when sent through
                                            their respective homeomorphisms, are a part of eucledian space $R^{d}$. i.e.
                                            if parts of our topological space $M$ examined locally are apart of
                                            eucledian space $R^{d}$. Topological StructureOpen Subsets Neighboorhoods
                                            Open SubsetsNeighboorhoodsA topological structure, roughly speaking, is an
                                            additional structure to a topological space that can be defined as a set of
                                            neighborhoods. A neighborhood is closely related to an open subset. This is
                                            why in out definition when we take an element from our topology we get an
                                            open subset. i.e. $U_{p} \in \tau_{M}$. Basically, a neighborhood of a point
                                            from a topological space is a set of points that contains that point and the
                                            points for which we can move around in any direction and still be in our set
                                            or neighborhood. This is why our point $p$ that we have is an element of
                                            $U$. Since when we take a point we want to examine a slice of our topology
                                            that contains the point we are examining and not some random point. We do
                                            this so we can keep properly traversing through our topology and keep a list
                                            of all of the charts and their respective homeomorphisms. This allows us to
                                            determine if all elements or open subsets in our topology when sent through
                                            their respective homeomorphisms, are a part of eucledian space $R^{d}$. i.e.
                                            if parts of our topological space $M$ examined locally are apart of
                                            eucledian space $R^{d}$. Chart Homeomorphism A chart in a topological space
                                            $M$ consists of $U_{\alpha}$ an open covering that is homeomorphic, via
                                            $h_{\alpha}$ a homeomorphism, to an open subset $U_{\alpha}$ in Eucledian
                                            space $\mathbb{R}^{n}$. ChartHomeomorphism HomeomorphismA chart in a
                                            topological space $M$ consists of $U_{\alpha}$ an open covering that is
                                            homeomorphic, via $h_{\alpha}$ a homeomorphism, to an open subset
                                            $U_{\alpha}$ in Eucledian space $\mathbb{R}^{n}$. Atlas An atlas $C^{r}$ on
                                            a topological space $M$ is a collection
                                            $\left\{U_{\alpha},h_{\alpha}\right\}$ of charts. $U_{\alpha}$ is an open
                                            covering admitted on $M$ and $h_{\alpha}$ is a homeomorphism on $U_{\alpha}$
                                            to the Eucledian space $\mathbb{R}^{n}$. The $C^{r}$ transition maps
                                            $h_{\beta}h_{\alpha}^{-1}$ are $C^{r}$ maps on $h_{\alpha}(U_{\alpha}\cap
                                            U_{\beta})$. AtlasAn atlas $C^{r}$ on a topological space $M$ is a
                                            collection $\left\{U_{\alpha},h_{\alpha}\right\}$ of charts. $U_{\alpha}$ is
                                            an open covering admitted on $M$ and $h_{\alpha}$ is a homeomorphism on
                                            $U_{\alpha}$ to the Eucledian space $\mathbb{R}^{n}$. The $C^{r}$ transition
                                            maps $h_{\beta}h_{\alpha}^{-1}$ are $C^{r}$ maps on
                                            $h_{\alpha}(U_{\alpha}\cap U_{\beta})$. Tangent Bundle To continue. Tangent
                                            BundleTo continue. Informal Manifold Definition Short version: a manifold is
                                            a topological space that locally resembles Eucliden space. Medium version:
                                            an $n$-dimensional manifold, or $n$-manifold, is a topological space where
                                            each point has a neighboorhood that is homeomorphic to an open subset on
                                            $n$-dimensional Euclidean space. Informal Manifold DefinitionShort version:
                                            a manifold is a topological space that locally resembles Eucliden space.
                                            Medium version: an $n$-dimensional manifold, or $n$-manifold, is a
                                            topological space where each point has a neighboorhood that is homeomorphic
                                            to an open subset on $n$-dimensional Euclidean space.
                                            [https://www.contextswitching.org/phys/comptificationandscatteringinads]
                                            Compatification and Massless Scarttering in Anti-de Sitter Space - Context
                                            Switching Compatification and Massless Scattering in Anti-de Sitter Space In
                                            theoretical physics, Minkowski Space is a particular type of $4$-dimensional
                                            Lorentzian space, with a Minkowski metric. Where the Minkowski metric is a
                                            metric tensor denoted as $d\tau^2$ with the form
                                            $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity and is the primary space we will use throughout.
                                            Conformal Compactification of Asymptotically Flat Spacetimes We first define
                                            a flat metric on Minkowski space, resembling the standard Minkowski metric,
                                            given in the spherical coordinates as: $d s^2=-d t^2+d r^2+2 r^2 \gamma_{z
                                            \bar{z}} d z d \bar{z}$ Here, $\gamma_{z \bar{z}}=\frac{2}{(1+z \bar{z})^2}$
                                            is defined with polar coordinates $z=e^{i \phi} \tan \frac{\theta}{2}$ and
                                            the spherical coordinates are projective coordinates for the $S^{2}$ factor
                                            named the celestrial sphere $CS^{2}$. Next, let $u, v \in(-\infty, \infty)$,
                                            where the finite range $U,V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$
                                            covers a given manifold. Next, we define new coordinates $(T,R)$, using the
                                            double null coordinates $(U,V)$ such that $T=U+V$ and $R=V-U$. The Minkowski
                                            metric $ds^{2}$ relative to the coordinates $(T,R)$ is a conformal factor
                                            times the Lorentz metric on $S^{3}\times\mathbb{R}$ and can be written as:
                                            $\begin{aligned} ds^{2}=& \left(\frac{L^2}{4 \cos
                                            ^2\left(\frac{R-T}{2}\right) \cos ^2\left(\frac{R+T}{2}\right)}\right) \\ &
                                            -d T^2+d R^2+2 \sin ^2 R \gamma_{z \bar{z}} d z d \bar{z} \\=& \;\Omega^{-2}
                                            d \widetilde{s}^2 \end{aligned}$ Here, our conformal factor is denoted as
                                            $\Omega^{-2}$ and, as we can see from above, is defined as:
                                            $\Omega^{-2}=\frac{L^2}{4 \cos ^2\left(\frac{R-T}{2}\right) \cos
                                            ^2\left(\frac{R+T}{2}\right)}$ An important property of this conformal
                                            factor is that it is positive, thus, preserving the casual structure for the
                                            rescaled metric $d \widetilde{s}^2$. This is important, because preserving
                                            the casual structures in this compactification allows for curves that begin
                                            as timelike, null, or spacelike, to maintain as such with respect to the
                                            rescaled metric. Next, we will attach a boundary to this compactification so
                                            that we can better understand the behavior of inifinity as it relates to
                                            Minkowski space and asymptotically flat spacetimes. These boundaries are
                                            given by the author, and are as folllows: Massive particles following
                                            time-like trajectories will enter at past timelike infinity and will exit at
                                            future timelike infinity. Where past timelike infinity is denoted as $i^{-}$
                                            and paramterized by $(R,T)=(0,-\pi)$ and future timelike infinity is denoted
                                            as $i^{+}$ and parameterized by $(R,T)=(0,\pi)$. Massless particles enter
                                            along past null-infinity and exit along future null-infinity. Where past
                                            null-inifinity is denoted as $\mathcal{I}^{-}$ and paramterized by
                                            $U=-\frac{\pi}{2}, V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ and
                                            future null-infinity is denoted as $\mathcal{I}^{+}$ and paramterized by
                                            $V=\frac{\pi}{2}, U \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$. Moving
                                            along any spacelike infinity, results in spacelike infinity. Where spacelike
                                            infinity is denoted as $i^{0}$ and paramterized by $(R,T)=(\pi,0)$. Image
                                            Source: https://tikz.net/relativity_penrose_diagram/ To continue. Penrose
                                            Diagram of the Einstein Static Universe Image Source: [2] Above is a Penrose
                                            diagram for Minkowski space that is represented as a patch of the Einstein
                                            static universe unwrapped, where the antipodal points are $i^{-},i^{+}$.
                                            Massless trajectories travel at $45^{\circ}$ to enter and exit at
                                            $\mathcal{I}^{ \pm}$ while the geodesics for massive particles enter at
                                            $i^{-}$ and exit at $i^{+}$. Theoretically, many penrose-patches
                                            mathematically establish the space for white holes, parallel universes,
                                            antiverses, and Einstein-Rose bridges, however, I digress. I will probably
                                            learn and then write about this in the future though! An example of a
                                            massless particles traveling at $45^{\circ}$ through the Penrose diagram for
                                            Minkowski space are photons and lightcones, where a lightcone is a
                                            subdivision of Minkowski $4$-dimensional spacetime with respect to an event
                                            that has four disjoint sets. This all tells us that the Penrose diagram is
                                            conformal. Geometry of Conformal Inversions Section Reference: Equating
                                            Extrapolate Dictionaries for Massless Scattering by Eivind Jørstad, Sabrina
                                            Pasterski and Atul Sharma from Perimeter Institute for Theoretical Physics,
                                            Dept. of Physics & Astronomy, University of Waterloo, Center for the
                                            Fundamental Laws of Nature & Black Hole Initiative, Harvard University
                                            $^{[1]}$ Four-dimensional Lorentzian space $\mathbb{R}^{4}$ is know as
                                            Minkowski space and is denoted as $\mathbb{M}=\mathbb{R}^{1,3}$, with the
                                            metric signature $(1,3)$ and the Cartesian coordinates $X^\mu, \mu=0,1,2,3$.
                                            For this Lorentzian Space to be Minkowski Space, it must possess a type of
                                            metric tensor taken to be $\eta_{\mu \nu}=\operatorname{diag}(-1,1,1,1)$,
                                            aptly named the Minkowski metric or Minkowski tensor. The light cone of the
                                            origin to null infinity is mapped using the conformal inversion map
                                            $\mathbb{M}$. The reason being is the authors mention embeded space
                                            formalism is eaiser to study conformal transormations with conformal
                                            compactification of $\mathbb{M}$. Poincaré Patch on the Einstein Cylinder
                                            Minkowski Spacetime and Lightcone Minkowski spacetime is $4$-dimensional
                                            Eucledian space, with each point representing an event in space time. Such
                                            that, the fourth coordinate is an imaginary fourth spacetime coordinate. A
                                            subdivision of Minkowski spacetime with respect to an event that has four
                                            disjoint sets is called a light cone, portrayed below. Image Source:
                                            https://byjus.com/physics/minkowski-space/ In this subdivision of Minkowski
                                            spacetime, there exists the absolute future, the absolute past, and
                                            elsewhere. For a $4$-vector $\mathbf{x}=(x_{0},x_{1},x_{2},x_{3})$ in
                                            Minkowski spacetime, variable $x_{0}$ is time, while variables
                                            $x_{1},x_{2},x_{3}$ are space variables. Furthermore, the time variable
                                            $x_{0}$ is sometimes denoted $t$, and when used in general relativity it can
                                            be denoted as $c$ or $i\;c\;t$. Where $c$ represents the speed of light and
                                            $i=\sqrt{-1}$ is an imaginary unit. Celestrial Quadric Let $Z^{I}$ with
                                            index $I=-1,0,...,4$ be coordinates on embedding space $\mathbb{R}^{2,4}$
                                            where the light cone of origin $\mathbb{R}^{2,4}$ is cut out by:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$
                                            such that the origin $Z^{I}=0$ is removed. The remaining light cone is
                                            positively rescaled with quotient $Z^I \sim t Z^I, t \in \mathbb{R}_{+}$,
                                            arriving at Penrose's conformal compactification of $\mathbb{M}$. Given now
                                            that the light point does not contain an origin, it no longer contains
                                            points at $Z^{-1}=Z^{0}=0$, meaning for the quoitent we can chose $Z^{I}$ to
                                            satisfy:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$$=1$
                                            To do so, the authors mention performing the positive rescaling
                                            $Z^{I}\rightarrow Z^{I}/\sqrt{(Z^{-1})^{2}+(Z^{0})^{2}}$, where the
                                            resulting space is topoglogically $S^{1}\times S^{3}$. This resulting
                                            topological space is a celestial quadric, such that the complement of the
                                            hyperplane in the space $Z^{-1}+Z^{4}=0$ consists of two Poincaré Patches
                                            that are each a copy of $\mathbb{M}$. The embeddings for the positive
                                            rescalings are defined as: $X^\mu \mapsto Z^I=\pm\left(\frac{1+|X|^2}{2},
                                            X^\mu, \frac{1-|X|^2}{2}\right)$ where $|X|^2=\eta_{\mu \nu} X^\mu X^\nu$ is
                                            the Lorentzian norm. To continue. Glossary AdS/CFT Correspondence Anti-de
                                            Sitter/conformal theory correspondence AdS/CFT in theoretical physics is a
                                            conjecture that describes the relationship between two kinds of physical
                                            theories. AdS used in quantum gravtiy and is formulated in terms of string
                                            theory of M-theory, while, CFT are quantum field theories that include
                                            theories such as Yang-Mills theories describing elementatry particles. Brane
                                            Brane, in string theory and related supergravity theories, is a physical
                                            object thats use is to generalize the notion of a zero-dimensional point
                                            particle, a one-dimensional string, or a two-dimensional membrane to
                                            higher-dimensional objects. Lorentzian Space Lorentzian $n$-space is the
                                            inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional
                                            Lorentzian inner product. Where the vector space is a set that is closed
                                            under finite vector addition and scalar multiplication and inner product is
                                            defined as a vector space with an inner product on it. Minkowski Space The
                                            Minkowski Space is a particular type of Lorentzian space, specifically
                                            $4$-dimensional Lorentzian space, with a Minkowski metric or Minkowski
                                            tensor. Where the Minkowski metric is a type of metric tensor denoted as $d
                                            \tau^2$ with the form $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d
                                            x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of the
                                            study of spacetime within special relativity. Another relative feature of
                                            this space is that it unifies $\mathbb{R}^{3}$ plus time
                                            (the "fourth dimension" ) in Einstein's theory of special relativity.
                                            Minkowski Metric The Minkowski metric is defined as: $g_{\mu \nu} \approx
                                            \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0
                                            & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of
                                            generally curved spacetime. One application of the Minkowski metric is
                                            modeling the cosmological constant term in Einstein's field equation with
                                            stress-energy from a vaccum or not. Metric Signature In theoretical physics,
                                            the metric signature counts the number of time-like or space-like characters
                                            are in the spacetime. For example, in the case Minkowski metric signature is
                                            $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time
                                            direction. If the eigenvalue is defined in three spatial directions $x,y,z$,
                                            then the metric signature is $(1,3,0)^{-}$ or $(-,+,+,+)$. Compatification
                                            and Massless Scarttering in Anti-de Sitter Space - Context Switching
                                            Compatification and Massless Scarttering in Anti-de Sitter Space - Context
                                            SwitchingCompatification and Massless Scattering in Anti-de Sitter Space In
                                            theoretical physics, Minkowski Space is a particular type of $4$-dimensional
                                            Lorentzian space, with a Minkowski metric. Where the Minkowski metric is a
                                            metric tensor denoted as $d\tau^2$ with the form
                                            $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity and is the primary space we will use throughout.
                                            Conformal Compactification of Asymptotically Flat Spacetimes We first define
                                            a flat metric on Minkowski space, resembling the standard Minkowski metric,
                                            given in the spherical coordinates as: $d s^2=-d t^2+d r^2+2 r^2 \gamma_{z
                                            \bar{z}} d z d \bar{z}$ Here, $\gamma_{z \bar{z}}=\frac{2}{(1+z \bar{z})^2}$
                                            is defined with polar coordinates $z=e^{i \phi} \tan \frac{\theta}{2}$ and
                                            the spherical coordinates are projective coordinates for the $S^{2}$ factor
                                            named the celestrial sphere $CS^{2}$. Next, let $u, v \in(-\infty, \infty)$,
                                            where the finite range $U,V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$
                                            covers a given manifold. Next, we define new coordinates $(T,R)$, using the
                                            double null coordinates $(U,V)$ such that $T=U+V$ and $R=V-U$. The Minkowski
                                            metric $ds^{2}$ relative to the coordinates $(T,R)$ is a conformal factor
                                            times the Lorentz metric on $S^{3}\times\mathbb{R}$ and can be written as:
                                            $\begin{aligned} ds^{2}=& \left(\frac{L^2}{4 \cos
                                            ^2\left(\frac{R-T}{2}\right) \cos ^2\left(\frac{R+T}{2}\right)}\right) \\ &
                                            -d T^2+d R^2+2 \sin ^2 R \gamma_{z \bar{z}} d z d \bar{z} \\=& \;\Omega^{-2}
                                            d \widetilde{s}^2 \end{aligned}$ Here, our conformal factor is denoted as
                                            $\Omega^{-2}$ and, as we can see from above, is defined as:
                                            $\Omega^{-2}=\frac{L^2}{4 \cos ^2\left(\frac{R-T}{2}\right) \cos
                                            ^2\left(\frac{R+T}{2}\right)}$ An important property of this conformal
                                            factor is that it is positive, thus, preserving the casual structure for the
                                            rescaled metric $d \widetilde{s}^2$. This is important, because preserving
                                            the casual structures in this compactification allows for curves that begin
                                            as timelike, null, or spacelike, to maintain as such with respect to the
                                            rescaled metric. Next, we will attach a boundary to this compactification so
                                            that we can better understand the behavior of inifinity as it relates to
                                            Minkowski space and asymptotically flat spacetimes. These boundaries are
                                            given by the author, and are as folllows: Massive particles following
                                            time-like trajectories will enter at past timelike infinity and will exit at
                                            future timelike infinity. Where past timelike infinity is denoted as $i^{-}$
                                            and paramterized by $(R,T)=(0,-\pi)$ and future timelike infinity is denoted
                                            as $i^{+}$ and parameterized by $(R,T)=(0,\pi)$. Massless particles enter
                                            along past null-infinity and exit along future null-infinity. Where past
                                            null-inifinity is denoted as $\mathcal{I}^{-}$ and paramterized by
                                            $U=-\frac{\pi}{2}, V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ and
                                            future null-infinity is denoted as $\mathcal{I}^{+}$ and paramterized by
                                            $V=\frac{\pi}{2}, U \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$. Moving
                                            along any spacelike infinity, results in spacelike infinity. Where spacelike
                                            infinity is denoted as $i^{0}$ and paramterized by $(R,T)=(\pi,0)$. Image
                                            Source: https://tikz.net/relativity_penrose_diagram/ To continue. Penrose
                                            Diagram of the Einstein Static Universe Image Source: [2] Above is a Penrose
                                            diagram for Minkowski space that is represented as a patch of the Einstein
                                            static universe unwrapped, where the antipodal points are $i^{-},i^{+}$.
                                            Massless trajectories travel at $45^{\circ}$ to enter and exit at
                                            $\mathcal{I}^{ \pm}$ while the geodesics for massive particles enter at
                                            $i^{-}$ and exit at $i^{+}$. Theoretically, many penrose-patches
                                            mathematically establish the space for white holes, parallel universes,
                                            antiverses, and Einstein-Rose bridges, however, I digress. I will probably
                                            learn and then write about this in the future though! An example of a
                                            massless particles traveling at $45^{\circ}$ through the Penrose diagram for
                                            Minkowski space are photons and lightcones, where a lightcone is a
                                            subdivision of Minkowski $4$-dimensional spacetime with respect to an event
                                            that has four disjoint sets. This all tells us that the Penrose diagram is
                                            conformal. Geometry of Conformal Inversions Section Reference: Equating
                                            Extrapolate Dictionaries for Massless Scattering by Eivind Jørstad, Sabrina
                                            Pasterski and Atul Sharma from Perimeter Institute for Theoretical Physics,
                                            Dept. of Physics & Astronomy, University of Waterloo, Center for the
                                            Fundamental Laws of Nature & Black Hole Initiative, Harvard University
                                            $^{[1]}$ Four-dimensional Lorentzian space $\mathbb{R}^{4}$ is know as
                                            Minkowski space and is denoted as $\mathbb{M}=\mathbb{R}^{1,3}$, with the
                                            metric signature $(1,3)$ and the Cartesian coordinates $X^\mu, \mu=0,1,2,3$.
                                            For this Lorentzian Space to be Minkowski Space, it must possess a type of
                                            metric tensor taken to be $\eta_{\mu \nu}=\operatorname{diag}(-1,1,1,1)$,
                                            aptly named the Minkowski metric or Minkowski tensor. The light cone of the
                                            origin to null infinity is mapped using the conformal inversion map
                                            $\mathbb{M}$. The reason being is the authors mention embeded space
                                            formalism is eaiser to study conformal transormations with conformal
                                            compactification of $\mathbb{M}$. Poincaré Patch on the Einstein Cylinder
                                            Minkowski Spacetime and Lightcone Minkowski spacetime is $4$-dimensional
                                            Eucledian space, with each point representing an event in space time. Such
                                            that, the fourth coordinate is an imaginary fourth spacetime coordinate. A
                                            subdivision of Minkowski spacetime with respect to an event that has four
                                            disjoint sets is called a light cone, portrayed below. Image Source:
                                            https://byjus.com/physics/minkowski-space/ In this subdivision of Minkowski
                                            spacetime, there exists the absolute future, the absolute past, and
                                            elsewhere. For a $4$-vector $\mathbf{x}=(x_{0},x_{1},x_{2},x_{3})$ in
                                            Minkowski spacetime, variable $x_{0}$ is time, while variables
                                            $x_{1},x_{2},x_{3}$ are space variables. Furthermore, the time variable
                                            $x_{0}$ is sometimes denoted $t$, and when used in general relativity it can
                                            be denoted as $c$ or $i\;c\;t$. Where $c$ represents the speed of light and
                                            $i=\sqrt{-1}$ is an imaginary unit. Celestrial Quadric Let $Z^{I}$ with
                                            index $I=-1,0,...,4$ be coordinates on embedding space $\mathbb{R}^{2,4}$
                                            where the light cone of origin $\mathbb{R}^{2,4}$ is cut out by:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$
                                            such that the origin $Z^{I}=0$ is removed. The remaining light cone is
                                            positively rescaled with quotient $Z^I \sim t Z^I, t \in \mathbb{R}_{+}$,
                                            arriving at Penrose's conformal compactification of $\mathbb{M}$. Given now
                                            that the light point does not contain an origin, it no longer contains
                                            points at $Z^{-1}=Z^{0}=0$, meaning for the quoitent we can chose $Z^{I}$ to
                                            satisfy:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$$=1$
                                            To do so, the authors mention performing the positive rescaling
                                            $Z^{I}\rightarrow Z^{I}/\sqrt{(Z^{-1})^{2}+(Z^{0})^{2}}$, where the
                                            resulting space is topoglogically $S^{1}\times S^{3}$. This resulting
                                            topological space is a celestial quadric, such that the complement of the
                                            hyperplane in the space $Z^{-1}+Z^{4}=0$ consists of two Poincaré Patches
                                            that are each a copy of $\mathbb{M}$. The embeddings for the positive
                                            rescalings are defined as: $X^\mu \mapsto Z^I=\pm\left(\frac{1+|X|^2}{2},
                                            X^\mu, \frac{1-|X|^2}{2}\right)$ where $|X|^2=\eta_{\mu \nu} X^\mu X^\nu$ is
                                            the Lorentzian norm. To continue. Glossary AdS/CFT Correspondence Anti-de
                                            Sitter/conformal theory correspondence AdS/CFT in theoretical physics is a
                                            conjecture that describes the relationship between two kinds of physical
                                            theories. AdS used in quantum gravtiy and is formulated in terms of string
                                            theory of M-theory, while, CFT are quantum field theories that include
                                            theories such as Yang-Mills theories describing elementatry particles. Brane
                                            Brane, in string theory and related supergravity theories, is a physical
                                            object thats use is to generalize the notion of a zero-dimensional point
                                            particle, a one-dimensional string, or a two-dimensional membrane to
                                            higher-dimensional objects. Lorentzian Space Lorentzian $n$-space is the
                                            inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional
                                            Lorentzian inner product. Where the vector space is a set that is closed
                                            under finite vector addition and scalar multiplication and inner product is
                                            defined as a vector space with an inner product on it. Minkowski Space The
                                            Minkowski Space is a particular type of Lorentzian space, specifically
                                            $4$-dimensional Lorentzian space, with a Minkowski metric or Minkowski
                                            tensor. Where the Minkowski metric is a type of metric tensor denoted as $d
                                            \tau^2$ with the form $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d
                                            x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of the
                                            study of spacetime within special relativity. Another relative feature of
                                            this space is that it unifies $\mathbb{R}^{3}$ plus time
                                            (the "fourth dimension" ) in Einstein's theory of special relativity.
                                            Minkowski Metric The Minkowski metric is defined as: $g_{\mu \nu} \approx
                                            \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0
                                            & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of
                                            generally curved spacetime. One application of the Minkowski metric is
                                            modeling the cosmological constant term in Einstein's field equation with
                                            stress-energy from a vaccum or not. Metric Signature In theoretical physics,
                                            the metric signature counts the number of time-like or space-like characters
                                            are in the spacetime. For example, in the case Minkowski metric signature is
                                            $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time
                                            direction. If the eigenvalue is defined in three spatial directions $x,y,z$,
                                            then the metric signature is $(1,3,0)^{-}$ or $(-,+,+,+)$. Compatification
                                            and Massless Scattering in Anti-de Sitter Space In theoretical physics,
                                            Minkowski Space is a particular type of $4$-dimensional Lorentzian space,
                                            with a Minkowski metric. Where the Minkowski metric is a metric tensor
                                            denoted as $d\tau^2$ with the form $-\left(d^0\right)^2+\left(d
                                            x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski
                                            space forms the basis of the study of spacetime within special relativity
                                            and is the primary space we will use throughout. Conformal Compactification
                                            of Asymptotically Flat Spacetimes We first define a flat metric on Minkowski
                                            space, resembling the standard Minkowski metric, given in the spherical
                                            coordinates as: $d s^2=-d t^2+d r^2+2 r^2 \gamma_{z \bar{z}} d z d \bar{z}$
                                            Here, $\gamma_{z \bar{z}}=\frac{2}{(1+z \bar{z})^2}$ is defined with polar
                                            coordinates $z=e^{i \phi} \tan \frac{\theta}{2}$ and the spherical
                                            coordinates are projective coordinates for the $S^{2}$ factor named the
                                            celestrial sphere $CS^{2}$. Next, let $u, v \in(-\infty, \infty)$, where the
                                            finite range $U,V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ covers a
                                            given manifold. Next, we define new coordinates $(T,R)$, using the double
                                            null coordinates $(U,V)$ such that $T=U+V$ and $R=V-U$. The Minkowski metric
                                            $ds^{2}$ relative to the coordinates $(T,R)$ is a conformal factor times the
                                            Lorentz metric on $S^{3}\times\mathbb{R}$ and can be written as:
                                            $\begin{aligned} ds^{2}=& \left(\frac{L^2}{4 \cos
                                            ^2\left(\frac{R-T}{2}\right) \cos ^2\left(\frac{R+T}{2}\right)}\right) \\ &
                                            -d T^2+d R^2+2 \sin ^2 R \gamma_{z \bar{z}} d z d \bar{z} \\=& \;\Omega^{-2}
                                            d \widetilde{s}^2 \end{aligned}$ Here, our conformal factor is denoted as
                                            $\Omega^{-2}$ and, as we can see from above, is defined as:
                                            $\Omega^{-2}=\frac{L^2}{4 \cos ^2\left(\frac{R-T}{2}\right) \cos
                                            ^2\left(\frac{R+T}{2}\right)}$ An important property of this conformal
                                            factor is that it is positive, thus, preserving the casual structure for the
                                            rescaled metric $d \widetilde{s}^2$. This is important, because preserving
                                            the casual structures in this compactification allows for curves that begin
                                            as timelike, null, or spacelike, to maintain as such with respect to the
                                            rescaled metric. Next, we will attach a boundary to this compactification so
                                            that we can better understand the behavior of inifinity as it relates to
                                            Minkowski space and asymptotically flat spacetimes. These boundaries are
                                            given by the author, and are as folllows: Massive particles following
                                            time-like trajectories will enter at past timelike infinity and will exit at
                                            future timelike infinity. Where past timelike infinity is denoted as $i^{-}$
                                            and paramterized by $(R,T)=(0,-\pi)$ and future timelike infinity is denoted
                                            as $i^{+}$ and parameterized by $(R,T)=(0,\pi)$. Massless particles enter
                                            along past null-infinity and exit along future null-infinity. Where past
                                            null-inifinity is denoted as $\mathcal{I}^{-}$ and paramterized by
                                            $U=-\frac{\pi}{2}, V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ and
                                            future null-infinity is denoted as $\mathcal{I}^{+}$ and paramterized by
                                            $V=\frac{\pi}{2}, U \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$. Moving
                                            along any spacelike infinity, results in spacelike infinity. Where spacelike
                                            infinity is denoted as $i^{0}$ and paramterized by $(R,T)=(\pi,0)$. Image
                                            Source: https://tikz.net/relativity_penrose_diagram/ To continue. Penrose
                                            Diagram of the Einstein Static Universe Image Source: [2] Above is a Penrose
                                            diagram for Minkowski space that is represented as a patch of the Einstein
                                            static universe unwrapped, where the antipodal points are $i^{-},i^{+}$.
                                            Massless trajectories travel at $45^{\circ}$ to enter and exit at
                                            $\mathcal{I}^{ \pm}$ while the geodesics for massive particles enter at
                                            $i^{-}$ and exit at $i^{+}$. Theoretically, many penrose-patches
                                            mathematically establish the space for white holes, parallel universes,
                                            antiverses, and Einstein-Rose bridges, however, I digress. I will probably
                                            learn and then write about this in the future though! An example of a
                                            massless particles traveling at $45^{\circ}$ through the Penrose diagram for
                                            Minkowski space are photons and lightcones, where a lightcone is a
                                            subdivision of Minkowski $4$-dimensional spacetime with respect to an event
                                            that has four disjoint sets. This all tells us that the Penrose diagram is
                                            conformal. Geometry of Conformal Inversions Section Reference: Equating
                                            Extrapolate Dictionaries for Massless Scattering by Eivind Jørstad, Sabrina
                                            Pasterski and Atul Sharma from Perimeter Institute for Theoretical Physics,
                                            Dept. of Physics & Astronomy, University of Waterloo, Center for the
                                            Fundamental Laws of Nature & Black Hole Initiative, Harvard University
                                            $^{[1]}$ Four-dimensional Lorentzian space $\mathbb{R}^{4}$ is know as
                                            Minkowski space and is denoted as $\mathbb{M}=\mathbb{R}^{1,3}$, with the
                                            metric signature $(1,3)$ and the Cartesian coordinates $X^\mu, \mu=0,1,2,3$.
                                            For this Lorentzian Space to be Minkowski Space, it must possess a type of
                                            metric tensor taken to be $\eta_{\mu \nu}=\operatorname{diag}(-1,1,1,1)$,
                                            aptly named the Minkowski metric or Minkowski tensor. The light cone of the
                                            origin to null infinity is mapped using the conformal inversion map
                                            $\mathbb{M}$. The reason being is the authors mention embeded space
                                            formalism is eaiser to study conformal transormations with conformal
                                            compactification of $\mathbb{M}$. Poincaré Patch on the Einstein Cylinder
                                            Minkowski Spacetime and Lightcone Minkowski spacetime is $4$-dimensional
                                            Eucledian space, with each point representing an event in space time. Such
                                            that, the fourth coordinate is an imaginary fourth spacetime coordinate. A
                                            subdivision of Minkowski spacetime with respect to an event that has four
                                            disjoint sets is called a light cone, portrayed below. Image Source:
                                            https://byjus.com/physics/minkowski-space/ In this subdivision of Minkowski
                                            spacetime, there exists the absolute future, the absolute past, and
                                            elsewhere. For a $4$-vector $\mathbf{x}=(x_{0},x_{1},x_{2},x_{3})$ in
                                            Minkowski spacetime, variable $x_{0}$ is time, while variables
                                            $x_{1},x_{2},x_{3}$ are space variables. Furthermore, the time variable
                                            $x_{0}$ is sometimes denoted $t$, and when used in general relativity it can
                                            be denoted as $c$ or $i\;c\;t$. Where $c$ represents the speed of light and
                                            $i=\sqrt{-1}$ is an imaginary unit. Celestrial Quadric Let $Z^{I}$ with
                                            index $I=-1,0,...,4$ be coordinates on embedding space $\mathbb{R}^{2,4}$
                                            where the light cone of origin $\mathbb{R}^{2,4}$ is cut out by:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$
                                            such that the origin $Z^{I}=0$ is removed. The remaining light cone is
                                            positively rescaled with quotient $Z^I \sim t Z^I, t \in \mathbb{R}_{+}$,
                                            arriving at Penrose's conformal compactification of $\mathbb{M}$. Given now
                                            that the light point does not contain an origin, it no longer contains
                                            points at $Z^{-1}=Z^{0}=0$, meaning for the quoitent we can chose $Z^{I}$ to
                                            satisfy:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$$=1$
                                            To do so, the authors mention performing the positive rescaling
                                            $Z^{I}\rightarrow Z^{I}/\sqrt{(Z^{-1})^{2}+(Z^{0})^{2}}$, where the
                                            resulting space is topoglogically $S^{1}\times S^{3}$. This resulting
                                            topological space is a celestial quadric, such that the complement of the
                                            hyperplane in the space $Z^{-1}+Z^{4}=0$ consists of two Poincaré Patches
                                            that are each a copy of $\mathbb{M}$. The embeddings for the positive
                                            rescalings are defined as: $X^\mu \mapsto Z^I=\pm\left(\frac{1+|X|^2}{2},
                                            X^\mu, \frac{1-|X|^2}{2}\right)$ where $|X|^2=\eta_{\mu \nu} X^\mu X^\nu$ is
                                            the Lorentzian norm. To continue. Glossary AdS/CFT Correspondence Anti-de
                                            Sitter/conformal theory correspondence AdS/CFT in theoretical physics is a
                                            conjecture that describes the relationship between two kinds of physical
                                            theories. AdS used in quantum gravtiy and is formulated in terms of string
                                            theory of M-theory, while, CFT are quantum field theories that include
                                            theories such as Yang-Mills theories describing elementatry particles. Brane
                                            Brane, in string theory and related supergravity theories, is a physical
                                            object thats use is to generalize the notion of a zero-dimensional point
                                            particle, a one-dimensional string, or a two-dimensional membrane to
                                            higher-dimensional objects. Lorentzian Space Lorentzian $n$-space is the
                                            inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional
                                            Lorentzian inner product. Where the vector space is a set that is closed
                                            under finite vector addition and scalar multiplication and inner product is
                                            defined as a vector space with an inner product on it. Minkowski Space The
                                            Minkowski Space is a particular type of Lorentzian space, specifically
                                            $4$-dimensional Lorentzian space, with a Minkowski metric or Minkowski
                                            tensor. Where the Minkowski metric is a type of metric tensor denoted as $d
                                            \tau^2$ with the form $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d
                                            x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of the
                                            study of spacetime within special relativity. Another relative feature of
                                            this space is that it unifies $\mathbb{R}^{3}$ plus time
                                            (the "fourth dimension" ) in Einstein's theory of special relativity.
                                            Minkowski Metric The Minkowski metric is defined as: $g_{\mu \nu} \approx
                                            \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0
                                            & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of
                                            generally curved spacetime. One application of the Minkowski metric is
                                            modeling the cosmological constant term in Einstein's field equation with
                                            stress-energy from a vaccum or not. Metric Signature In theoretical physics,
                                            the metric signature counts the number of time-like or space-like characters
                                            are in the spacetime. For example, in the case Minkowski metric signature is
                                            $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time
                                            direction. If the eigenvalue is defined in three spatial directions $x,y,z$,
                                            then the metric signature is $(1,3,0)^{-}$ or $(-,+,+,+)$. Compatification
                                            and Massless Scattering in Anti-de Sitter Space In theoretical physics,
                                            Minkowski Space is a particular type of $4$-dimensional Lorentzian space,
                                            with a Minkowski metric. Where the Minkowski metric is a metric tensor
                                            denoted as $d\tau^2$ with the form $-\left(d^0\right)^2+\left(d
                                            x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski
                                            space forms the basis of the study of spacetime within special relativity
                                            and is the primary space we will use throughout. Conformal Compactification
                                            of Asymptotically Flat Spacetimes We first define a flat metric on Minkowski
                                            space, resembling the standard Minkowski metric, given in the spherical
                                            coordinates as: $d s^2=-d t^2+d r^2+2 r^2 \gamma_{z \bar{z}} d z d \bar{z}$
                                            Here, $\gamma_{z \bar{z}}=\frac{2}{(1+z \bar{z})^2}$ is defined with polar
                                            coordinates $z=e^{i \phi} \tan \frac{\theta}{2}$ and the spherical
                                            coordinates are projective coordinates for the $S^{2}$ factor named the
                                            celestrial sphere $CS^{2}$. Next, let $u, v \in(-\infty, \infty)$, where the
                                            finite range $U,V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ covers a
                                            given manifold. Next, we define new coordinates $(T,R)$, using the double
                                            null coordinates $(U,V)$ such that $T=U+V$ and $R=V-U$. The Minkowski metric
                                            $ds^{2}$ relative to the coordinates $(T,R)$ is a conformal factor times the
                                            Lorentz metric on $S^{3}\times\mathbb{R}$ and can be written as:
                                            $\begin{aligned} ds^{2}=& \left(\frac{L^2}{4 \cos
                                            ^2\left(\frac{R-T}{2}\right) \cos ^2\left(\frac{R+T}{2}\right)}\right) \\ &
                                            -d T^2+d R^2+2 \sin ^2 R \gamma_{z \bar{z}} d z d \bar{z} \\=& \;\Omega^{-2}
                                            d \widetilde{s}^2 \end{aligned}$ Here, our conformal factor is denoted as
                                            $\Omega^{-2}$ and, as we can see from above, is defined as:
                                            $\Omega^{-2}=\frac{L^2}{4 \cos ^2\left(\frac{R-T}{2}\right) \cos
                                            ^2\left(\frac{R+T}{2}\right)}$ An important property of this conformal
                                            factor is that it is positive, thus, preserving the casual structure for the
                                            rescaled metric $d \widetilde{s}^2$. This is important, because preserving
                                            the casual structures in this compactification allows for curves that begin
                                            as timelike, null, or spacelike, to maintain as such with respect to the
                                            rescaled metric. Next, we will attach a boundary to this compactification so
                                            that we can better understand the behavior of inifinity as it relates to
                                            Minkowski space and asymptotically flat spacetimes. These boundaries are
                                            given by the author, and are as folllows: Massive particles following
                                            time-like trajectories will enter at past timelike infinity and will exit at
                                            future timelike infinity. Where past timelike infinity is denoted as $i^{-}$
                                            and paramterized by $(R,T)=(0,-\pi)$ and future timelike infinity is denoted
                                            as $i^{+}$ and parameterized by $(R,T)=(0,\pi)$. Massless particles enter
                                            along past null-infinity and exit along future null-infinity. Where past
                                            null-inifinity is denoted as $\mathcal{I}^{-}$ and paramterized by
                                            $U=-\frac{\pi}{2}, V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ and
                                            future null-infinity is denoted as $\mathcal{I}^{+}$ and paramterized by
                                            $V=\frac{\pi}{2}, U \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$. Moving
                                            along any spacelike infinity, results in spacelike infinity. Where spacelike
                                            infinity is denoted as $i^{0}$ and paramterized by $(R,T)=(\pi,0)$. Image
                                            Source: https://tikz.net/relativity_penrose_diagram/ To continue. Penrose
                                            Diagram of the Einstein Static Universe Image Source: [2] Above is a Penrose
                                            diagram for Minkowski space that is represented as a patch of the Einstein
                                            static universe unwrapped, where the antipodal points are $i^{-},i^{+}$.
                                            Massless trajectories travel at $45^{\circ}$ to enter and exit at
                                            $\mathcal{I}^{ \pm}$ while the geodesics for massive particles enter at
                                            $i^{-}$ and exit at $i^{+}$. Theoretically, many penrose-patches
                                            mathematically establish the space for white holes, parallel universes,
                                            antiverses, and Einstein-Rose bridges, however, I digress. I will probably
                                            learn and then write about this in the future though! An example of a
                                            massless particles traveling at $45^{\circ}$ through the Penrose diagram for
                                            Minkowski space are photons and lightcones, where a lightcone is a
                                            subdivision of Minkowski $4$-dimensional spacetime with respect to an event
                                            that has four disjoint sets. This all tells us that the Penrose diagram is
                                            conformal. Geometry of Conformal Inversions Section Reference: Equating
                                            Extrapolate Dictionaries for Massless Scattering by Eivind Jørstad, Sabrina
                                            Pasterski and Atul Sharma from Perimeter Institute for Theoretical Physics,
                                            Dept. of Physics & Astronomy, University of Waterloo, Center for the
                                            Fundamental Laws of Nature & Black Hole Initiative, Harvard University
                                            $^{[1]}$ Four-dimensional Lorentzian space $\mathbb{R}^{4}$ is know as
                                            Minkowski space and is denoted as $\mathbb{M}=\mathbb{R}^{1,3}$, with the
                                            metric signature $(1,3)$ and the Cartesian coordinates $X^\mu, \mu=0,1,2,3$.
                                            For this Lorentzian Space to be Minkowski Space, it must possess a type of
                                            metric tensor taken to be $\eta_{\mu \nu}=\operatorname{diag}(-1,1,1,1)$,
                                            aptly named the Minkowski metric or Minkowski tensor. The light cone of the
                                            origin to null infinity is mapped using the conformal inversion map
                                            $\mathbb{M}$. The reason being is the authors mention embeded space
                                            formalism is eaiser to study conformal transormations with conformal
                                            compactification of $\mathbb{M}$. Poincaré Patch on the Einstein Cylinder
                                            Minkowski Spacetime and Lightcone Minkowski spacetime is $4$-dimensional
                                            Eucledian space, with each point representing an event in space time. Such
                                            that, the fourth coordinate is an imaginary fourth spacetime coordinate. A
                                            subdivision of Minkowski spacetime with respect to an event that has four
                                            disjoint sets is called a light cone, portrayed below. Image Source:
                                            https://byjus.com/physics/minkowski-space/ In this subdivision of Minkowski
                                            spacetime, there exists the absolute future, the absolute past, and
                                            elsewhere. For a $4$-vector $\mathbf{x}=(x_{0},x_{1},x_{2},x_{3})$ in
                                            Minkowski spacetime, variable $x_{0}$ is time, while variables
                                            $x_{1},x_{2},x_{3}$ are space variables. Furthermore, the time variable
                                            $x_{0}$ is sometimes denoted $t$, and when used in general relativity it can
                                            be denoted as $c$ or $i\;c\;t$. Where $c$ represents the speed of light and
                                            $i=\sqrt{-1}$ is an imaginary unit. Celestrial Quadric Let $Z^{I}$ with
                                            index $I=-1,0,...,4$ be coordinates on embedding space $\mathbb{R}^{2,4}$
                                            where the light cone of origin $\mathbb{R}^{2,4}$ is cut out by:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$
                                            such that the origin $Z^{I}=0$ is removed. The remaining light cone is
                                            positively rescaled with quotient $Z^I \sim t Z^I, t \in \mathbb{R}_{+}$,
                                            arriving at Penrose's conformal compactification of $\mathbb{M}$. Given now
                                            that the light point does not contain an origin, it no longer contains
                                            points at $Z^{-1}=Z^{0}=0$, meaning for the quoitent we can chose $Z^{I}$ to
                                            satisfy:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$$=1$
                                            To do so, the authors mention performing the positive rescaling
                                            $Z^{I}\rightarrow Z^{I}/\sqrt{(Z^{-1})^{2}+(Z^{0})^{2}}$, where the
                                            resulting space is topoglogically $S^{1}\times S^{3}$. This resulting
                                            topological space is a celestial quadric, such that the complement of the
                                            hyperplane in the space $Z^{-1}+Z^{4}=0$ consists of two Poincaré Patches
                                            that are each a copy of $\mathbb{M}$. The embeddings for the positive
                                            rescalings are defined as: $X^\mu \mapsto Z^I=\pm\left(\frac{1+|X|^2}{2},
                                            X^\mu, \frac{1-|X|^2}{2}\right)$ where $|X|^2=\eta_{\mu \nu} X^\mu X^\nu$ is
                                            the Lorentzian norm. To continue. Glossary AdS/CFT Correspondence Anti-de
                                            Sitter/conformal theory correspondence AdS/CFT in theoretical physics is a
                                            conjecture that describes the relationship between two kinds of physical
                                            theories. AdS used in quantum gravtiy and is formulated in terms of string
                                            theory of M-theory, while, CFT are quantum field theories that include
                                            theories such as Yang-Mills theories describing elementatry particles. Brane
                                            Brane, in string theory and related supergravity theories, is a physical
                                            object thats use is to generalize the notion of a zero-dimensional point
                                            particle, a one-dimensional string, or a two-dimensional membrane to
                                            higher-dimensional objects. Lorentzian Space Lorentzian $n$-space is the
                                            inner product space of $\mathbb{R}^{n}$ vector space with $n$-dimensional
                                            Lorentzian inner product. Where the vector space is a set that is closed
                                            under finite vector addition and scalar multiplication and inner product is
                                            defined as a vector space with an inner product on it. Minkowski Space The
                                            Minkowski Space is a particular type of Lorentzian space, specifically
                                            $4$-dimensional Lorentzian space, with a Minkowski metric or Minkowski
                                            tensor. Where the Minkowski metric is a type of metric tensor denoted as $d
                                            \tau^2$ with the form $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d
                                            x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of the
                                            study of spacetime within special relativity. Another relative feature of
                                            this space is that it unifies $\mathbb{R}^{3}$ plus time
                                            (the "fourth dimension" ) in Einstein's theory of special relativity.
                                            Minkowski Metric The Minkowski metric is defined as: $g_{\mu \nu} \approx
                                            \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0
                                            & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a metric of
                                            generally curved spacetime. One application of the Minkowski metric is
                                            modeling the cosmological constant term in Einstein's field equation with
                                            stress-energy from a vaccum or not. Metric Signature In theoretical physics,
                                            the metric signature counts the number of time-like or space-like characters
                                            are in the spacetime. For example, in the case Minkowski metric signature is
                                            $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time
                                            direction. If the eigenvalue is defined in three spatial directions $x,y,z$,
                                            then the metric signature is $(1,3,0)^{-}$ or $(-,+,+,+)$. Compatification
                                            and Massless Scattering in Anti-de Sitter SpaceIn theoretical physics,
                                            Minkowski Space is a particular type of $4$-dimensional Lorentzian space,
                                            with a Minkowski metric. Where the Minkowski metric is a metric tensor
                                            denoted as $d\tau^2$ with the form $-\left(d^0\right)^2+\left(d
                                            x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski
                                            space forms the basis of the study of spacetime within special relativity
                                            and is the primary space we will use throughout. Conformal Compactification
                                            of Asymptotically Flat Spacetimes We first define a flat metric on Minkowski
                                            space, resembling the standard Minkowski metric, given in the spherical
                                            coordinates as: $d s^2=-d t^2+d r^2+2 r^2 \gamma_{z \bar{z}} d z d \bar{z}$
                                            Here, $\gamma_{z \bar{z}}=\frac{2}{(1+z \bar{z})^2}$ is defined with polar
                                            coordinates $z=e^{i \phi} \tan \frac{\theta}{2}$ and the spherical
                                            coordinates are projective coordinates for the $S^{2}$ factor named the
                                            celestrial sphere $CS^{2}$. Next, let $u, v \in(-\infty, \infty)$, where the
                                            finite range $U,V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ covers a
                                            given manifold. Next, we define new coordinates $(T,R)$, using the double
                                            null coordinates $(U,V)$ such that $T=U+V$ and $R=V-U$. The Minkowski metric
                                            $ds^{2}$ relative to the coordinates $(T,R)$ is a conformal factor times the
                                            Lorentz metric on $S^{3}\times\mathbb{R}$ and can be written as:
                                            $\begin{aligned} ds^{2}=& \left(\frac{L^2}{4 \cos
                                            ^2\left(\frac{R-T}{2}\right) \cos ^2\left(\frac{R+T}{2}\right)}\right) \\ &
                                            -d T^2+d R^2+2 \sin ^2 R \gamma_{z \bar{z}} d z d \bar{z} \\=& \;\Omega^{-2}
                                            d \widetilde{s}^2 \end{aligned}$ Here, our conformal factor is denoted as
                                            $\Omega^{-2}$ and, as we can see from above, is defined as:
                                            $\Omega^{-2}=\frac{L^2}{4 \cos ^2\left(\frac{R-T}{2}\right) \cos
                                            ^2\left(\frac{R+T}{2}\right)}$ An important property of this conformal
                                            factor is that it is positive, thus, preserving the casual structure for the
                                            rescaled metric $d \widetilde{s}^2$. This is important, because preserving
                                            the casual structures in this compactification allows for curves that begin
                                            as timelike, null, or spacelike, to maintain as such with respect to the
                                            rescaled metric. Next, we will attach a boundary to this compactification so
                                            that we can better understand the behavior of inifinity as it relates to
                                            Minkowski space and asymptotically flat spacetimes. These boundaries are
                                            given by the author, and are as folllows: Massive particles following
                                            time-like trajectories will enter at past timelike infinity and will exit at
                                            future timelike infinity. Where past timelike infinity is denoted as $i^{-}$
                                            and paramterized by $(R,T)=(0,-\pi)$ and future timelike infinity is denoted
                                            as $i^{+}$ and parameterized by $(R,T)=(0,\pi)$. Massless particles enter
                                            along past null-infinity and exit along future null-infinity. Where past
                                            null-inifinity is denoted as $\mathcal{I}^{-}$ and paramterized by
                                            $U=-\frac{\pi}{2}, V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ and
                                            future null-infinity is denoted as $\mathcal{I}^{+}$ and paramterized by
                                            $V=\frac{\pi}{2}, U \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$. Moving
                                            along any spacelike infinity, results in spacelike infinity. Where spacelike
                                            infinity is denoted as $i^{0}$ and paramterized by $(R,T)=(\pi,0)$. Image
                                            Source: https://tikz.net/relativity_penrose_diagram/ To continue. Penrose
                                            Diagram of the Einstein Static Universe Image Source: [2] Above is a Penrose
                                            diagram for Minkowski space that is represented as a patch of the Einstein
                                            static universe unwrapped, where the antipodal points are $i^{-},i^{+}$.
                                            Massless trajectories travel at $45^{\circ}$ to enter and exit at
                                            $\mathcal{I}^{ \pm}$ while the geodesics for massive particles enter at
                                            $i^{-}$ and exit at $i^{+}$. Theoretically, many penrose-patches
                                            mathematically establish the space for white holes, parallel universes,
                                            antiverses, and Einstein-Rose bridges, however, I digress. I will probably
                                            learn and then write about this in the future though! An example of a
                                            massless particles traveling at $45^{\circ}$ through the Penrose diagram for
                                            Minkowski space are photons and lightcones, where a lightcone is a
                                            subdivision of Minkowski $4$-dimensional spacetime with respect to an event
                                            that has four disjoint sets. This all tells us that the Penrose diagram is
                                            conformal. Conformal Compactification of Asymptotically Flat SpacetimesWe
                                            first define a flat metric on Minkowski space, resembling the standard
                                            Minkowski metric, given in the spherical coordinates as: $d s^2=-d t^2+d
                                            r^2+2 r^2 \gamma_{z \bar{z}} d z d \bar{z}$ Here, $\gamma_{z
                                            \bar{z}}=\frac{2}{(1+z \bar{z})^2}$ is defined with polar coordinates
                                            $z=e^{i \phi} \tan \frac{\theta}{2}$ and the spherical coordinates are
                                            projective coordinates for the $S^{2}$ factor named the celestrial sphere
                                            $CS^{2}$. Next, let $u, v \in(-\infty, \infty)$, where the finite range $U,V
                                            \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ covers a given manifold.
                                            Next, we define new coordinates $(T,R)$, using the double null coordinates
                                            $(U,V)$ such that $T=U+V$ and $R=V-U$. The Minkowski metric $ds^{2}$
                                            relative to the coordinates $(T,R)$ is a conformal factor times the Lorentz
                                            metric on $S^{3}\times\mathbb{R}$ and can be written as: $\begin{aligned}
                                            ds^{2}=& \left(\frac{L^2}{4 \cos ^2\left(\frac{R-T}{2}\right) \cos
                                            ^2\left(\frac{R+T}{2}\right)}\right) \\ & -d T^2+d R^2+2 \sin ^2 R \gamma_{z
                                            \bar{z}} d z d \bar{z} \\=& \;\Omega^{-2} d \widetilde{s}^2 \end{aligned}$
                                            Here, our conformal factor is denoted as $\Omega^{-2}$ and, as we can see
                                            from above, is defined as: $\Omega^{-2}=\frac{L^2}{4 \cos
                                            ^2\left(\frac{R-T}{2}\right) \cos ^2\left(\frac{R+T}{2}\right)}$ An
                                            important property of this conformal factor is that it is positive, thus,
                                            preserving the casual structure for the rescaled metric $d \widetilde{s}^2$.
                                            This is important, because preserving the casual structures in this
                                            compactification allows for curves that begin as timelike, null, or
                                            spacelike, to maintain as such with respect to the rescaled metric. Next, we
                                            will attach a boundary to this compactification so that we can better
                                            understand the behavior of inifinity as it relates to Minkowski space and
                                            asymptotically flat spacetimes. These boundaries are given by the author,
                                            and are as folllows: Massive particles following time-like trajectories will
                                            enter at past timelike infinity and will exit at future timelike infinity.
                                            Where past timelike infinity is denoted as $i^{-}$ and paramterized by
                                            $(R,T)=(0,-\pi)$ and future timelike infinity is denoted as $i^{+}$ and
                                            parameterized by $(R,T)=(0,\pi)$. Massless particles enter along past
                                            null-infinity and exit along future null-infinity. Where past null-inifinity
                                            is denoted as $\mathcal{I}^{-}$ and paramterized by $U=-\frac{\pi}{2}, V
                                            \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ and future null-infinity is
                                            denoted as $\mathcal{I}^{+}$ and paramterized by $V=\frac{\pi}{2}, U
                                            \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$. Moving along any spacelike
                                            infinity, results in spacelike infinity. Where spacelike infinity is denoted
                                            as $i^{0}$ and paramterized by $(R,T)=(\pi,0)$. Massive particles following
                                            time-like trajectories will enter at past timelike infinity and will exit at
                                            future timelike infinity. Where past timelike infinity is denoted as $i^{-}$
                                            and paramterized by $(R,T)=(0,-\pi)$ and future timelike infinity is denoted
                                            as $i^{+}$ and parameterized by $(R,T)=(0,\pi)$. Massless particles enter
                                            along past null-infinity and exit along future null-infinity. Where past
                                            null-inifinity is denoted as $\mathcal{I}^{-}$ and paramterized by
                                            $U=-\frac{\pi}{2}, V \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$ and
                                            future null-infinity is denoted as $\mathcal{I}^{+}$ and paramterized by
                                            $V=\frac{\pi}{2}, U \in\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$. Moving
                                            along any spacelike infinity, results in spacelike infinity. Where spacelike
                                            infinity is denoted as $i^{0}$ and paramterized by $(R,T)=(\pi,0)$. Image
                                            Source: https://tikz.net/relativity_penrose_diagram/
                                            https://tikz.net/relativity_penrose_diagram/To continue. Penrose Diagram of
                                            the Einstein Static Universe Image Source: [2] Above is a Penrose diagram
                                            for Minkowski space that is represented as a patch of the Einstein static
                                            universe unwrapped, where the antipodal points are $i^{-},i^{+}$. Massless
                                            trajectories travel at $45^{\circ}$ to enter and exit at $\mathcal{I}^{
                                            \pm}$ while the geodesics for massive particles enter at $i^{-}$ and exit at
                                            $i^{+}$. Theoretically, many penrose-patches mathematically establish the
                                            space for white holes, parallel universes, antiverses, and Einstein-Rose
                                            bridges, however, I digress. I will probably learn and then write about this
                                            in the future though! An example of a massless particles traveling at
                                            $45^{\circ}$ through the Penrose diagram for Minkowski space are photons and
                                            lightcones, where a lightcone is a subdivision of Minkowski $4$-dimensional
                                            spacetime with respect to an event that has four disjoint sets. This all
                                            tells us that the Penrose diagram is conformal. Penrose Diagram of the
                                            Einstein Static UniverseImage Source: [2] [2]Above is a Penrose diagram for
                                            Minkowski space that is represented as a patch of the Einstein static
                                            universe unwrapped, where the antipodal points are $i^{-},i^{+}$. Massless
                                            trajectories travel at $45^{\circ}$ to enter and exit at $\mathcal{I}^{
                                            \pm}$ while the geodesics for massive particles enter at $i^{-}$ and exit at
                                            $i^{+}$. Theoretically, many penrose-patches mathematically establish the
                                            space for white holes, parallel universes, antiverses, and Einstein-Rose
                                            bridges, however, I digress. I will probably learn and then write about this
                                            in the future though! An example of a massless particles traveling at
                                            $45^{\circ}$ through the Penrose diagram for Minkowski space are photons and
                                            lightcones, where a lightcone is a subdivision of Minkowski $4$-dimensional
                                            spacetime with respect to an event that has four disjoint sets. This all
                                            tells us that the Penrose diagram is conformal. lightconesGeometry of
                                            Conformal Inversions Section Reference: Equating Extrapolate Dictionaries
                                            for Massless Scattering by Eivind Jørstad, Sabrina Pasterski and Atul Sharma
                                            from Perimeter Institute for Theoretical Physics, Dept. of Physics &
                                            Astronomy, University of Waterloo, Center for the Fundamental Laws of Nature
                                            & Black Hole Initiative, Harvard University $^{[1]}$ Four-dimensional
                                            Lorentzian space $\mathbb{R}^{4}$ is know as Minkowski space and is denoted
                                            as $\mathbb{M}=\mathbb{R}^{1,3}$, with the metric signature $(1,3)$ and the
                                            Cartesian coordinates $X^\mu, \mu=0,1,2,3$. For this Lorentzian Space to be
                                            Minkowski Space, it must possess a type of metric tensor taken to be
                                            $\eta_{\mu \nu}=\operatorname{diag}(-1,1,1,1)$, aptly named the Minkowski
                                            metric or Minkowski tensor. The light cone of the origin to null infinity is
                                            mapped using the conformal inversion map $\mathbb{M}$. The reason being is
                                            the authors mention embeded space formalism is eaiser to study conformal
                                            transormations with conformal compactification of $\mathbb{M}$. Poincaré
                                            Patch on the Einstein Cylinder Minkowski Spacetime and Lightcone Minkowski
                                            spacetime is $4$-dimensional Eucledian space, with each point representing
                                            an event in space time. Such that, the fourth coordinate is an imaginary
                                            fourth spacetime coordinate. A subdivision of Minkowski spacetime with
                                            respect to an event that has four disjoint sets is called a light cone,
                                            portrayed below. Image Source: https://byjus.com/physics/minkowski-space/ In
                                            this subdivision of Minkowski spacetime, there exists the absolute future,
                                            the absolute past, and elsewhere. For a $4$-vector
                                            $\mathbf{x}=(x_{0},x_{1},x_{2},x_{3})$ in Minkowski spacetime, variable
                                            $x_{0}$ is time, while variables $x_{1},x_{2},x_{3}$ are space variables.
                                            Furthermore, the time variable $x_{0}$ is sometimes denoted $t$, and when
                                            used in general relativity it can be denoted as $c$ or $i\;c\;t$. Where $c$
                                            represents the speed of light and $i=\sqrt{-1}$ is an imaginary unit.
                                            Celestrial Quadric Let $Z^{I}$ with index $I=-1,0,...,4$ be coordinates on
                                            embedding space $\mathbb{R}^{2,4}$ where the light cone of origin
                                            $\mathbb{R}^{2,4}$ is cut out by:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$
                                            such that the origin $Z^{I}=0$ is removed. The remaining light cone is
                                            positively rescaled with quotient $Z^I \sim t Z^I, t \in \mathbb{R}_{+}$,
                                            arriving at Penrose's conformal compactification of $\mathbb{M}$. Given now
                                            that the light point does not contain an origin, it no longer contains
                                            points at $Z^{-1}=Z^{0}=0$, meaning for the quoitent we can chose $Z^{I}$ to
                                            satisfy:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$$=1$
                                            To do so, the authors mention performing the positive rescaling
                                            $Z^{I}\rightarrow Z^{I}/\sqrt{(Z^{-1})^{2}+(Z^{0})^{2}}$, where the
                                            resulting space is topoglogically $S^{1}\times S^{3}$. This resulting
                                            topological space is a celestial quadric, such that the complement of the
                                            hyperplane in the space $Z^{-1}+Z^{4}=0$ consists of two Poincaré Patches
                                            that are each a copy of $\mathbb{M}$. The embeddings for the positive
                                            rescalings are defined as: $X^\mu \mapsto Z^I=\pm\left(\frac{1+|X|^2}{2},
                                            X^\mu, \frac{1-|X|^2}{2}\right)$ where $|X|^2=\eta_{\mu \nu} X^\mu X^\nu$ is
                                            the Lorentzian norm. To continue. Geometry of Conformal InversionsSection
                                            Reference: Equating Extrapolate Dictionaries for Massless Scattering by
                                            Eivind Jørstad, Sabrina Pasterski and Atul Sharma from Perimeter Institute
                                            for Theoretical Physics, Dept. of Physics & Astronomy, University of
                                            Waterloo, Center for the Fundamental Laws of Nature & Black Hole Initiative,
                                            Harvard University $^{[1]}$ Equating Extrapolate Dictionaries for Massless
                                            Scattering by Eivind Jørstad, Sabrina Pasterski and Atul Sharma from
                                            Perimeter Institute for Theoretical Physics, Dept. of Physics & Astronomy,
                                            University of Waterloo, Center for the Fundamental Laws of Nature & Black
                                            Hole Initiative, Harvard University $^{[1]}$ Equating Extrapolate
                                            Dictionaries for Massless Scattering by Eivind Jørstad, Sabrina Pasterski
                                            and Atul Sharma from Perimeter Institute for Theoretical Physics, Dept. of
                                            Physics & Astronomy, University of Waterloo, Center for the Fundamental Laws
                                            of Nature & Black Hole Initiative, Harvard University Four-dimensional
                                            Lorentzian space $\mathbb{R}^{4}$ is know as Minkowski space and is denoted
                                            as $\mathbb{M}=\mathbb{R}^{1,3}$, with the metric signature $(1,3)$ and the
                                            Cartesian coordinates $X^\mu, \mu=0,1,2,3$. For this Lorentzian Space to be
                                            Minkowski Space, it must possess a type of metric tensor taken to be
                                            $\eta_{\mu \nu}=\operatorname{diag}(-1,1,1,1)$, aptly named the Minkowski
                                            metric or Minkowski tensor. The light cone of the origin to null infinity is
                                            mapped using the conformal inversion map $\mathbb{M}$. The reason being is
                                            the authors mention embeded space formalism is eaiser to study conformal
                                            transormations with conformal compactification of $\mathbb{M}$. Lorentzian
                                            spacemetric signaturePoincaré Patch on the Einstein Cylinder Minkowski
                                            Spacetime and Lightcone Minkowski spacetime is $4$-dimensional Eucledian
                                            space, with each point representing an event in space time. Such that, the
                                            fourth coordinate is an imaginary fourth spacetime coordinate. A subdivision
                                            of Minkowski spacetime with respect to an event that has four disjoint sets
                                            is called a light cone, portrayed below. Image Source:
                                            https://byjus.com/physics/minkowski-space/ In this subdivision of Minkowski
                                            spacetime, there exists the absolute future, the absolute past, and
                                            elsewhere. For a $4$-vector $\mathbf{x}=(x_{0},x_{1},x_{2},x_{3})$ in
                                            Minkowski spacetime, variable $x_{0}$ is time, while variables
                                            $x_{1},x_{2},x_{3}$ are space variables. Furthermore, the time variable
                                            $x_{0}$ is sometimes denoted $t$, and when used in general relativity it can
                                            be denoted as $c$ or $i\;c\;t$. Where $c$ represents the speed of light and
                                            $i=\sqrt{-1}$ is an imaginary unit. Celestrial Quadric Let $Z^{I}$ with
                                            index $I=-1,0,...,4$ be coordinates on embedding space $\mathbb{R}^{2,4}$
                                            where the light cone of origin $\mathbb{R}^{2,4}$ is cut out by:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$
                                            such that the origin $Z^{I}=0$ is removed. The remaining light cone is
                                            positively rescaled with quotient $Z^I \sim t Z^I, t \in \mathbb{R}_{+}$,
                                            arriving at Penrose's conformal compactification of $\mathbb{M}$. Given now
                                            that the light point does not contain an origin, it no longer contains
                                            points at $Z^{-1}=Z^{0}=0$, meaning for the quoitent we can chose $Z^{I}$ to
                                            satisfy:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$$=1$
                                            To do so, the authors mention performing the positive rescaling
                                            $Z^{I}\rightarrow Z^{I}/\sqrt{(Z^{-1})^{2}+(Z^{0})^{2}}$, where the
                                            resulting space is topoglogically $S^{1}\times S^{3}$. This resulting
                                            topological space is a celestial quadric, such that the complement of the
                                            hyperplane in the space $Z^{-1}+Z^{4}=0$ consists of two Poincaré Patches
                                            that are each a copy of $\mathbb{M}$. The embeddings for the positive
                                            rescalings are defined as: $X^\mu \mapsto Z^I=\pm\left(\frac{1+|X|^2}{2},
                                            X^\mu, \frac{1-|X|^2}{2}\right)$ where $|X|^2=\eta_{\mu \nu} X^\mu X^\nu$ is
                                            the Lorentzian norm. To continue. Poincaré Patch on the Einstein Cylinder
                                            Minkowski Spacetime and Lightcone Minkowski spacetime is $4$-dimensional
                                            Eucledian space, with each point representing an event in space time. Such
                                            that, the fourth coordinate is an imaginary fourth spacetime coordinate. A
                                            subdivision of Minkowski spacetime with respect to an event that has four
                                            disjoint sets is called a light cone, portrayed below. Image Source:
                                            https://byjus.com/physics/minkowski-space/ In this subdivision of Minkowski
                                            spacetime, there exists the absolute future, the absolute past, and
                                            elsewhere. For a $4$-vector $\mathbf{x}=(x_{0},x_{1},x_{2},x_{3})$ in
                                            Minkowski spacetime, variable $x_{0}$ is time, while variables
                                            $x_{1},x_{2},x_{3}$ are space variables. Furthermore, the time variable
                                            $x_{0}$ is sometimes denoted $t$, and when used in general relativity it can
                                            be denoted as $c$ or $i\;c\;t$. Where $c$ represents the speed of light and
                                            $i=\sqrt{-1}$ is an imaginary unit. Minkowski Spacetime and Lightcone
                                            Minkowski spacetime is $4$-dimensional Eucledian space, with each point
                                            representing an event in space time. Such that, the fourth coordinate is an
                                            imaginary fourth spacetime coordinate. A subdivision of Minkowski spacetime
                                            with respect to an event that has four disjoint sets is called a light cone,
                                            portrayed below. Image Source: https://byjus.com/physics/minkowski-space/
                                            https://byjus.com/physics/minkowski-space/In this subdivision of Minkowski
                                            spacetime, there exists the absolute future, the absolute past, and
                                            elsewhere. For a $4$-vector $\mathbf{x}=(x_{0},x_{1},x_{2},x_{3})$ in
                                            Minkowski spacetime, variable $x_{0}$ is time, while variables
                                            $x_{1},x_{2},x_{3}$ are space variables. Furthermore, the time variable
                                            $x_{0}$ is sometimes denoted $t$, and when used in general relativity it can
                                            be denoted as $c$ or $i\;c\;t$. Where $c$ represents the speed of light and
                                            $i=\sqrt{-1}$ is an imaginary unit. Celestrial Quadric Let $Z^{I}$ with
                                            index $I=-1,0,...,4$ be coordinates on embedding space $\mathbb{R}^{2,4}$
                                            where the light cone of origin $\mathbb{R}^{2,4}$ is cut out by:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$
                                            such that the origin $Z^{I}=0$ is removed. The remaining light cone is
                                            positively rescaled with quotient $Z^I \sim t Z^I, t \in \mathbb{R}_{+}$,
                                            arriving at Penrose's conformal compactification of $\mathbb{M}$. Given now
                                            that the light point does not contain an origin, it no longer contains
                                            points at $Z^{-1}=Z^{0}=0$, meaning for the quoitent we can chose $Z^{I}$ to
                                            satisfy:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$$=1$
                                            To do so, the authors mention performing the positive rescaling
                                            $Z^{I}\rightarrow Z^{I}/\sqrt{(Z^{-1})^{2}+(Z^{0})^{2}}$, where the
                                            resulting space is topoglogically $S^{1}\times S^{3}$. This resulting
                                            topological space is a celestial quadric, such that the complement of the
                                            hyperplane in the space $Z^{-1}+Z^{4}=0$ consists of two Poincaré Patches
                                            that are each a copy of $\mathbb{M}$. The embeddings for the positive
                                            rescalings are defined as: $X^\mu \mapsto Z^I=\pm\left(\frac{1+|X|^2}{2},
                                            X^\mu, \frac{1-|X|^2}{2}\right)$ where $|X|^2=\eta_{\mu \nu} X^\mu X^\nu$ is
                                            the Lorentzian norm. To continue. Celestrial Quadric Let $Z^{I}$ with index
                                            $I=-1,0,...,4$ be coordinates on embedding space $\mathbb{R}^{2,4}$ where
                                            the light cone of origin $\mathbb{R}^{2,4}$ is cut out by:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$
                                            such that the origin $Z^{I}=0$ is removed. The remaining light cone is
                                            positively rescaled with quotient $Z^I \sim t Z^I, t \in \mathbb{R}_{+}$,
                                            arriving at Penrose's conformal compactification of $\mathbb{M}$. Given now
                                            that the light point does not contain an origin, it no longer contains
                                            points at $Z^{-1}=Z^{0}=0$, meaning for the quoitent we can chose $Z^{I}$ to
                                            satisfy:
                                            $\left(Z^{-1}\right)^2+\left(Z^0\right)^2$$=\left(Z^1\right)^2+\left(Z^2\right)^2+\left(Z^3\right)^2+\left(Z^4\right)^2$$=1$
                                            To do so, the authors mention performing the positive rescaling
                                            $Z^{I}\rightarrow Z^{I}/\sqrt{(Z^{-1})^{2}+(Z^{0})^{2}}$, where the
                                            resulting space is topoglogically $S^{1}\times S^{3}$. This resulting
                                            topological space is a celestial quadric, such that the complement of the
                                            hyperplane in the space $Z^{-1}+Z^{4}=0$ consists of two Poincaré Patches
                                            that are each a copy of $\mathbb{M}$. The embeddings for the positive
                                            rescalings are defined as: Poincaré Patches$X^\mu \mapsto
                                            Z^I=\pm\left(\frac{1+|X|^2}{2}, X^\mu, \frac{1-|X|^2}{2}\right)$ where
                                            $|X|^2=\eta_{\mu \nu} X^\mu X^\nu$ is the Lorentzian norm. To continue.
                                            Glossary AdS/CFT Correspondence Anti-de Sitter/conformal theory
                                            correspondence AdS/CFT in theoretical physics is a conjecture that describes
                                            the relationship between two kinds of physical theories. AdS used in quantum
                                            gravtiy and is formulated in terms of string theory of M-theory, while, CFT
                                            are quantum field theories that include theories such as Yang-Mills theories
                                            describing elementatry particles. GlossaryAdS/CFT CorrespondenceAnti-de
                                            Sitter/conformal theory correspondence AdS/CFT in theoretical physics is a
                                            conjecture that describes the relationship between two kinds of physical
                                            theories. AdS used in quantum gravtiy and is formulated in terms of string
                                            theory of M-theory, while, CFT are quantum field theories that include
                                            theories such as Yang-Mills theories describing elementatry particles. Brane
                                            Brane, in string theory and related supergravity theories, is a physical
                                            object thats use is to generalize the notion of a zero-dimensional point
                                            particle, a one-dimensional string, or a two-dimensional membrane to
                                            higher-dimensional objects. BraneBrane, in string theory and related
                                            supergravity theories, is a physical object thats use is to generalize the
                                            notion of a zero-dimensional point particle, a one-dimensional string, or a
                                            two-dimensional membrane to higher-dimensional objects. Lorentzian Space
                                            Lorentzian $n$-space is the inner product space of $\mathbb{R}^{n}$ vector
                                            space with $n$-dimensional Lorentzian inner product. Where the vector space
                                            is a set that is closed under finite vector addition and scalar
                                            multiplication and inner product is defined as a vector space with an inner
                                            product on it. Lorentzian SpaceLorentzian $n$-space is the inner product
                                            space of $\mathbb{R}^{n}$ vector space with $n$-dimensional Lorentzian inner
                                            product. Where the vector space is a set that is closed under finite vector
                                            addition and scalar multiplication and inner product is defined as a vector
                                            space with an inner product on it. Minkowski Space The Minkowski Space is a
                                            particular type of Lorentzian space, specifically $4$-dimensional Lorentzian
                                            space, with a Minkowski metric or Minkowski tensor. Where the Minkowski
                                            metric is a type of metric tensor denoted as $d \tau^2$ with the form
                                            $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity. Another relative feature of this space is that it
                                            unifies $\mathbb{R}^{3}$ plus time (the "fourth dimension" ) in Einstein's
                                            theory of special relativity. Minkowski SpaceThe Minkowski Space is a
                                            particular type of Lorentzian space, specifically $4$-dimensional Lorentzian
                                            space, with a Minkowski metric or Minkowski tensor. Where the Minkowski
                                            metric is a type of metric tensor denoted as $d \tau^2$ with the form
                                            $-\left(d x^0\right)^2+\left(d x^1\right)^2+\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity. Another relative feature of this space is that it
                                            unifies $\mathbb{R}^{3}$ plus time (the "fourth dimension" ) in Einstein's
                                            theory of special relativity. Minkowski Metric The Minkowski metric is
                                            defined as: $g_{\mu \nu} \approx \eta_{\mu \nu}=\left[\begin{array}{cccc}-1
                                            & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 &
                                            1\end{array}\right]$ Such that it is a metric of generally curved spacetime.
                                            One application of the Minkowski metric is modeling the cosmological
                                            constant term in Einstein's field equation with stress-energy from a vaccum
                                            or not. Minkowski MetricThe Minkowski metric is defined as: $g_{\mu \nu}
                                            \approx \eta_{\mu \nu}=\left[\begin{array}{cccc}-1 & 0 & 0 & 0 \\ 0 & 1 & 0
                                            & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{array}\right]$ Such that it is a
                                            metric of generally curved spacetime. One application of the Minkowski
                                            metric is modeling the cosmological constant term in Einstein's field
                                            equation with stress-energy from a vaccum or not. Metric Signature In
                                            theoretical physics, the metric signature counts the number of time-like or
                                            space-like characters are in the spacetime. For example, in the case
                                            Minkowski metric signature is $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time
                                            direction is defined in the time direction. If the eigenvalue is defined in
                                            three spatial directions $x,y,z$, then the metric signature is $(1,3,0)^{-}$
                                            or $(-,+,+,+)$. Metric SignatureIn theoretical physics, the metric signature
                                            counts the number of time-like or space-like characters are in the
                                            spacetime. For example, in the case Minkowski metric signature is
                                            $(1,3,0)^{+}$ or $(+,-,-,-)$ if the time direction is defined in the time
                                            direction. If the eigenvalue is defined in three spatial directions $x,y,z$,
                                            then the metric signature is $(1,3,0)^{-}$ or $(-,+,+,+)$.
                                            [https://www.contextswitching.org/my/resume] You are being redirected by
                                            your loyal guides 🐶🐱 You are being redirected by your loyal guides 🐶🐱
                                            You are being redirected by your loyal guides 🐶🐱
                                            [https://www.contextswitching.org/misc/epigeneticsandinheritance]
                                            Epigenetics and Inheritance - Context Switching Epigenetics and Inheritance
                                            Introduction In biology, epigenetics is the study of mitotically and/or
                                            meiotically heritable changes in gene function that cannot be explained by
                                            changes to the DNA sequence. Epigenetics normally involves change that is
                                            not erased by cell division and that also affects the regulation of gene
                                            expression. Epigenetics reflects our understanding that, despite the
                                            complement of DNA being fundamentally identical in all somatic cells of an
                                            organism, patterns in gene expression can vary greatly among different cell
                                            types. Additionally, these patterns have the potential for clonal
                                            inheritance. To continue. Role of DNA Mehtylation To continue. Role of
                                            Chromatin To continue. Epigenetics and Inheritance - Context Switching
                                            Epigenetics and Inheritance - Context SwitchingEpigenetics and Inheritance
                                            Introduction In biology, epigenetics is the study of mitotically and/or
                                            meiotically heritable changes in gene function that cannot be explained by
                                            changes to the DNA sequence. Epigenetics normally involves change that is
                                            not erased by cell division and that also affects the regulation of gene
                                            expression. Epigenetics reflects our understanding that, despite the
                                            complement of DNA being fundamentally identical in all somatic cells of an
                                            organism, patterns in gene expression can vary greatly among different cell
                                            types. Additionally, these patterns have the potential for clonal
                                            inheritance. To continue. Role of DNA Mehtylation To continue. Role of
                                            Chromatin To continue. Epigenetics and Inheritance Introduction In biology,
                                            epigenetics is the study of mitotically and/or meiotically heritable changes
                                            in gene function that cannot be explained by changes to the DNA sequence.
                                            Epigenetics normally involves change that is not erased by cell division and
                                            that also affects the regulation of gene expression. Epigenetics reflects
                                            our understanding that, despite the complement of DNA being fundamentally
                                            identical in all somatic cells of an organism, patterns in gene expression
                                            can vary greatly among different cell types. Additionally, these patterns
                                            have the potential for clonal inheritance. To continue. Role of DNA
                                            Mehtylation To continue. Role of Chromatin To continue. Epigenetics and
                                            Inheritance Introduction In biology, epigenetics is the study of mitotically
                                            and/or meiotically heritable changes in gene function that cannot be
                                            explained by changes to the DNA sequence. Epigenetics normally involves
                                            change that is not erased by cell division and that also affects the
                                            regulation of gene expression. Epigenetics reflects our understanding that,
                                            despite the complement of DNA being fundamentally identical in all somatic
                                            cells of an organism, patterns in gene expression can vary greatly among
                                            different cell types. Additionally, these patterns have the potential for
                                            clonal inheritance. To continue. Role of DNA Mehtylation To continue. Role
                                            of Chromatin To continue. Epigenetics and InheritanceIntroduction In
                                            biology, epigenetics is the study of mitotically and/or meiotically
                                            heritable changes in gene function that cannot be explained by changes to
                                            the DNA sequence. Epigenetics normally involves change that is not erased by
                                            cell division and that also affects the regulation of gene expression.
                                            Epigenetics reflects our understanding that, despite the complement of DNA
                                            being fundamentally identical in all somatic cells of an organism, patterns
                                            in gene expression can vary greatly among different cell types.
                                            Additionally, these patterns have the potential for clonal inheritance. To
                                            continue. Role of DNA Mehtylation To continue. Role of Chromatin To
                                            continue. Introduction In biology, epigenetics is the study of mitotically
                                            and/or meiotically heritable changes in gene function that cannot be
                                            explained by changes to the DNA sequence. Epigenetics normally involves
                                            change that is not erased by cell division and that also affects the
                                            regulation of gene expression. Epigenetics reflects our understanding that,
                                            despite the complement of DNA being fundamentally identical in all somatic
                                            cells of an organism, patterns in gene expression can vary greatly among
                                            different cell types. Additionally, these patterns have the potential for
                                            clonal inheritance. To continue. Role of DNA Mehtylation To continue. Role
                                            of DNA Mehtylation To continue. Role of Chromatin To continue. Role of
                                            Chromatin To continue.
                                            [https://www.contextswitching.org/neuro/multistorememory] Multi-Store Memory
                                            Model - Context Switching Multi-Store Memory Model Sensory Memory To
                                            continue. Working Model of Memory Central Executive The central executive is
                                            responsible for controlled processing and allocation of data to subsystems
                                            in working memory. Such subsystems include the visuospatial sketchpad and
                                            phonological loop, where the phonological loop can further be subdivided
                                            into the phonological store and the articulatory process. The primary
                                            functions of the central executive is dealing with cognitive tasks such as
                                            mental arithmetic and problem-solving. Phonological Loop The phonological
                                            loop is part of the working memory and consists of two main functions:
                                            dealing with spoken material and written material. To handle these two
                                            functions, the phonological loops consists of two parts which we will look
                                            at in the following sections. Phonological Store The first component we will
                                            talk about is the phonological store, which is linked to speech perception.
                                            Its function is to store both spoken and written words as speech-based
                                            encoded information for 1-2 seconds. Meaning, that while spoken words enter
                                            the store directly, written words are first encoded as articuarticulatory or
                                            spoken code, then the data can enter the phonological store. Articulatory
                                            Control Process To continue. Visuospatial Sketchpad To continue. Long-Term
                                            Memory To continue. Multi-Store Memory Model - Context Switching Multi-Store
                                            Memory Model - Context SwitchingMulti-Store Memory Model Sensory Memory To
                                            continue. Working Model of Memory Central Executive The central executive is
                                            responsible for controlled processing and allocation of data to subsystems
                                            in working memory. Such subsystems include the visuospatial sketchpad and
                                            phonological loop, where the phonological loop can further be subdivided
                                            into the phonological store and the articulatory process. The primary
                                            functions of the central executive is dealing with cognitive tasks such as
                                            mental arithmetic and problem-solving. Phonological Loop The phonological
                                            loop is part of the working memory and consists of two main functions:
                                            dealing with spoken material and written material. To handle these two
                                            functions, the phonological loops consists of two parts which we will look
                                            at in the following sections. Phonological Store The first component we will
                                            talk about is the phonological store, which is linked to speech perception.
                                            Its function is to store both spoken and written words as speech-based
                                            encoded information for 1-2 seconds. Meaning, that while spoken words enter
                                            the store directly, written words are first encoded as articuarticulatory or
                                            spoken code, then the data can enter the phonological store. Articulatory
                                            Control Process To continue. Visuospatial Sketchpad To continue. Long-Term
                                            Memory To continue. Multi-Store Memory Model Sensory Memory To continue.
                                            Working Model of Memory Central Executive The central executive is
                                            responsible for controlled processing and allocation of data to subsystems
                                            in working memory. Such subsystems include the visuospatial sketchpad and
                                            phonological loop, where the phonological loop can further be subdivided
                                            into the phonological store and the articulatory process. The primary
                                            functions of the central executive is dealing with cognitive tasks such as
                                            mental arithmetic and problem-solving. Phonological Loop The phonological
                                            loop is part of the working memory and consists of two main functions:
                                            dealing with spoken material and written material. To handle these two
                                            functions, the phonological loops consists of two parts which we will look
                                            at in the following sections. Phonological Store The first component we will
                                            talk about is the phonological store, which is linked to speech perception.
                                            Its function is to store both spoken and written words as speech-based
                                            encoded information for 1-2 seconds. Meaning, that while spoken words enter
                                            the store directly, written words are first encoded as articuarticulatory or
                                            spoken code, then the data can enter the phonological store. Articulatory
                                            Control Process To continue. Visuospatial Sketchpad To continue. Long-Term
                                            Memory To continue. Multi-Store Memory Model Sensory Memory To continue.
                                            Working Model of Memory Central Executive The central executive is
                                            responsible for controlled processing and allocation of data to subsystems
                                            in working memory. Such subsystems include the visuospatial sketchpad and
                                            phonological loop, where the phonological loop can further be subdivided
                                            into the phonological store and the articulatory process. The primary
                                            functions of the central executive is dealing with cognitive tasks such as
                                            mental arithmetic and problem-solving. Phonological Loop The phonological
                                            loop is part of the working memory and consists of two main functions:
                                            dealing with spoken material and written material. To handle these two
                                            functions, the phonological loops consists of two parts which we will look
                                            at in the following sections. Phonological Store The first component we will
                                            talk about is the phonological store, which is linked to speech perception.
                                            Its function is to store both spoken and written words as speech-based
                                            encoded information for 1-2 seconds. Meaning, that while spoken words enter
                                            the store directly, written words are first encoded as articuarticulatory or
                                            spoken code, then the data can enter the phonological store. Articulatory
                                            Control Process To continue. Visuospatial Sketchpad To continue. Long-Term
                                            Memory To continue. Multi-Store Memory ModelSensory Memory To continue.
                                            Sensory MemoryTo continue. Working Model of Memory Central Executive The
                                            central executive is responsible for controlled processing and allocation of
                                            data to subsystems in working memory. Such subsystems include the
                                            visuospatial sketchpad and phonological loop, where the phonological loop
                                            can further be subdivided into the phonological store and the articulatory
                                            process. The primary functions of the central executive is dealing with
                                            cognitive tasks such as mental arithmetic and problem-solving. Phonological
                                            Loop The phonological loop is part of the working memory and consists of two
                                            main functions: dealing with spoken material and written material. To handle
                                            these two functions, the phonological loops consists of two parts which we
                                            will look at in the following sections. Phonological Store The first
                                            component we will talk about is the phonological store, which is linked to
                                            speech perception. Its function is to store both spoken and written words as
                                            speech-based encoded information for 1-2 seconds. Meaning, that while spoken
                                            words enter the store directly, written words are first encoded as
                                            articuarticulatory or spoken code, then the data can enter the phonological
                                            store. Articulatory Control Process To continue. Visuospatial Sketchpad To
                                            continue. Working Model of MemoryCentral Executive The central executive is
                                            responsible for controlled processing and allocation of data to subsystems
                                            in working memory. Such subsystems include the visuospatial sketchpad and
                                            phonological loop, where the phonological loop can further be subdivided
                                            into the phonological store and the articulatory process. The primary
                                            functions of the central executive is dealing with cognitive tasks such as
                                            mental arithmetic and problem-solving. Phonological Loop The phonological
                                            loop is part of the working memory and consists of two main functions:
                                            dealing with spoken material and written material. To handle these two
                                            functions, the phonological loops consists of two parts which we will look
                                            at in the following sections. Phonological Store The first component we will
                                            talk about is the phonological store, which is linked to speech perception.
                                            Its function is to store both spoken and written words as speech-based
                                            encoded information for 1-2 seconds. Meaning, that while spoken words enter
                                            the store directly, written words are first encoded as articuarticulatory or
                                            spoken code, then the data can enter the phonological store. Articulatory
                                            Control Process To continue. Visuospatial Sketchpad To continue. Central
                                            ExecutiveThe central executive is responsible for controlled processing and
                                            allocation of data to subsystems in working memory. Such subsystems include
                                            the visuospatial sketchpad and phonological loop, where the phonological
                                            loop can further be subdivided into the phonological store and the
                                            articulatory process. The primary functions of the central executive is
                                            dealing with cognitive tasks such as mental arithmetic and problem-solving.
                                            Phonological Loop The phonological loop is part of the working memory and
                                            consists of two main functions: dealing with spoken material and written
                                            material. To handle these two functions, the phonological loops consists of
                                            two parts which we will look at in the following sections. Phonological
                                            Store The first component we will talk about is the phonological store,
                                            which is linked to speech perception. Its function is to store both spoken
                                            and written words as speech-based encoded information for 1-2 seconds.
                                            Meaning, that while spoken words enter the store directly, written words are
                                            first encoded as articuarticulatory or spoken code, then the data can enter
                                            the phonological store. Articulatory Control Process To continue.
                                            Phonological LoopThe phonological loop is part of the working memory and
                                            consists of two main functions: dealing with spoken material and written
                                            material. To handle these two functions, the phonological loops consists of
                                            two parts which we will look at in the following sections. Phonological
                                            Store The first component we will talk about is the phonological store,
                                            which is linked to speech perception. Its function is to store both spoken
                                            and written words as speech-based encoded information for 1-2 seconds.
                                            Meaning, that while spoken words enter the store directly, written words are
                                            first encoded as articuarticulatory or spoken code, then the data can enter
                                            the phonological store. Phonological StoreThe first component we will talk
                                            about is the phonological store, which is linked to speech perception. Its
                                            function is to store both spoken and written words as speech-based encoded
                                            information for 1-2 seconds. Meaning, that while spoken words enter the
                                            store directly, written words are first encoded as articuarticulatory or
                                            spoken code, then the data can enter the phonological store. Articulatory
                                            Control Process To continue. Articulatory Control ProcessTo continue.
                                            Visuospatial Sketchpad To continue. Visuospatial SketchpadTo continue.
                                            Long-Term Memory To continue. Long-Term MemoryTo continue.
                                            [https://www.contextswitching.org/tcs/quantumcnn] Quantum Convolutional
                                            Neural Network - Context Switching Quantum Convolutional Neural Network
                                            Introduction We will look at how Quantum Convolutional Neural Networks QCNN
                                            can be used for deep neural networks, potentially paving the way for
                                            groundbreaking advancements in machine learning image recognition. Quantum
                                            computing can improve the time complexity of performing larger operations,
                                            such as convolution matrix multiplication, therefore, allowing for an
                                            increased number or size of convolution kernel, as well as exploring larger
                                            or more complex input structures. To continue, we define and analyze each
                                            procedure of the QCNN, such as quantum forward propagation, quantum back
                                            propagation, and more, and see how quantum computing techniques can be
                                            applied to improve the operation. Let's begin! Quantum Forward Propagation
                                            Section Reference: Quantum Algorithms for Deep Convolutional Neural Networks
                                            by: Iordanis Kerenidis, Jonas Landman, Anupam Prakash. CNRS, IRIF,
                                            Universite Paris Diderot, Paris, France (Dated: November 5, 2019) $^{[1]}$
                                            Quantum Convolutional Layer The input for the quantum convolution layer is a
                                            $3$D tensor input $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the weights layer, filter layer, or kernel layer is a $4$D
                                            tensor $K^{\ell} \in \mathbb{R}^{H \times W \times D^{\ell} \times
                                            D^{\ell+1}}$, where the input and kernel layer are both stored in QRAM.
                                            Given precision parameters $\epsilon$, $\Delta>0$, there exists a quantum
                                            algorithm that computes a quantum state that is $\Delta$ close to
                                            $|f(\bar{X}^{\ell+1})\rangle$ where $X^{\ell+1}=X^{\ell} * K^{\ell}$, and
                                            $f: \mathbb{R} \mapsto[0, C]$ is a non-linear activation function. Note,
                                            this function could be a sigmoidal, hyperbolic tangent, ReLU, softmax, or
                                            any other activation function and is rather dependent mainly on the task the
                                            activation function needs to complete. To continue, a classical appoximation
                                            for the computer quantum state exists where:
                                            $\left\|f\big(\bar{X}^{\ell+1}\big)-f\big(X^{\ell+1}\big)\right\|_{\infty}
                                            \leq \epsilon$ The time complexity of the quantum algorithm that computes
                                            the output quantum state $|f(\bar{X}^{\ell+1})\rangle$ is $\widetilde{O}(M /
                                            \epsilon)$. In this equation, $M$ denotes the maximum norm of the product
                                            state between one of the kernels and one of the tensor input regions in
                                            $X^{\ell}$, where the size is $HW D^{\ell}$. It is important to note, that
                                            $\widetilde{O}$ is made to hide factors poly-logarithmic in $\Delta$ with
                                            respect to the size of $X^{\ell}$ and $K^{\ell}$. Given that a convolution
                                            product is equivalant to a matrix-matrix multiplication and the
                                            convolutional product between $X^{\ell}$ and $K^{\ell}$ is: $X_{i^{\ell+1},
                                            j^{\ell+1}, d^{\ell+1}}^{\ell+1}=$ $\sum_{i=0}^H \sum_{j=0}^W
                                            \sum_{d=0}^{D^{\ell}} K_{i, j, d, d^{\ell+1}}^{\ell} X_{i^{\ell+1}+i,
                                            j^{\ell+1}+j, d}^{\ell}$ it is possible to reformulate this convolution
                                            product equation as a matrix product. A diagram of this reshaping of the
                                            input and kernel is given by the authors and depicited below: Image Source:
                                            $[1]$ To express this reformulation mathematically, we must do the
                                            following. First, the original $3$D tensor input input for the quantum
                                            convolution layer $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the $4$D tensor kernel layer $K^{\ell} \in \mathbb{R}^{H
                                            \times W \times D^{\ell} \times D^{\ell+1}}$ needs to be reshaped to
                                            matrices. First, $X^{\ell}$ needs to be expanded into a matrix $A^{\ell}
                                            \in$ $\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times\left(H W
                                            D^{\ell}\right)}$, such that each row of $A^{\ell}$ is a vectorized version
                                            of a subregion of $X^{\ell}$. Next, the original kernel tensor $K^{\ell}$ is
                                            reshaped into a matrix $F^{\ell} \in$ $\mathbb{R}^{\left(H W D^{\ell}\right)
                                            \times D^{\ell+1}}$, such that each column of $F^{\ell}$ is a vectrozied
                                            version of one of the $D^{\ell+1}$ kernels. This matrix expression of the
                                            tensor input and kernel is needed for the convolution product $X^{\ell} *
                                            K^{\ell}=X^{\ell+1}$ to be written as a matrix multiplication, such that
                                            each column of the output matrix $Y^{\ell+1} \in
                                            \mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times D^{\ell+1}}$ is a
                                            first vectorized form of one of the $D^{\ell+1}$ channels of $X^{\ell+1}$.
                                            Later, we will dicuss how quantum computing, specifically quantum
                                            parallelism can not only be applied, but improve the time complexity of this
                                            step. Lastly, quantum states proportional to the rows of input $A^{\ell}$
                                            and $F^{\ell}$ are used, denoted $|A_p^{\ell}\rangle$ and
                                            $|F_q^{\ell}\rangle$ respectively. These quantum states are defined as: $
                                            \left|A_p^{\ell}\right\rangle=$ $\frac{1}{\left\|A_p^{\ell}\right\|}
                                            \sum_{r=0}^{H W D^{\ell}-1} A_{p r}^{\ell}|r\rangle$
                                            $\left|F_q^{\ell}\right\rangle=$ $\frac{1}{\left\|F_q^{\ell}\right\|}
                                            \sum_{s=0}^{D^{\ell+1}-1} F_{s q}^{\ell}|s\rangle$ and will continue to be
                                            used throughout this section. Quantum Convolution The authors describe
                                            procedure for the quantum convolutional in the following four sequential
                                            steps: inner product estimation, non linearity, quantum sampling, and then
                                            quantum random access memory update and pooling. Inner Product Estimation
                                            First we load input row vector $A_p^{\ell}$ and kernel vector $F_q^{\ell}$
                                            into quantum states by quering QRAM in the following manner:
                                            $\left\{\begin{aligned} |p\rangle|0\rangle &
                                            \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\ |q\rangle|0\rangle &
                                            \mapsto|q\rangle\left|F_q^{\ell}\right\rangle \end{aligned}\right.$ This is
                                            done so that following mapping can be perfromed with the two vectors:
                                            $\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto $ $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle$ Here
                                            $\bar{P}_{p q}$ is the inner product estimation of $A_p^{\ell}$ and
                                            $F_q^{\ell}$. The "true" value of the inner product is calculated as $P_{p
                                            q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}$, such
                                            that the inner product estimation differs by a chosen constant $\epsilon$.
                                            Here, the normalization factor is $K=\sqrt{H^{\ell+1} W^{\ell+1}
                                            D^{\ell+1}}$ and $|g_{pq}\rangle$ is some garbage state. Non Linearity Our
                                            obtained approximated convolution output $\bar{P}_{p q}$ or
                                            $\bar{Y}^{\ell+1}$ differs from the "true" convolution output $Y_{p,
                                            q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)$ by $\epsilon$. Now, we
                                            apply a non-linear function $f$ as a boolean circut giving the quantum
                                            state: $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum Sampling Conditional Rotation and Amplitude
                                            Amplification To procure the state below, states are conditionally rotated
                                            and the probabilistic amplitudes are amplified, such that we arrive at the
                                            state: $\frac{1}{K} \sum_{p, q} \alpha_{p
                                            q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum tomography is performed with precision $\eta$ so
                                            that all values and positions $(p, q, f(\bar{Y}_{p q}^{\ell+1}))$ are
                                            obtained with a high probability. Values above $\eta$ are known exactly,
                                            while values that are less than or equal to $\eta$ are set to $0$. QRAM
                                            Update and Pooling Next, QRAM needs to updated with the value for the next
                                            layer, which is $A^{\ell+1}$, while sampling. Pooling needs to be
                                            implemented in this step as well, either through a specific update or by
                                            using a QRAM data data structure. We will review quantum pooling more in
                                            depth later. Quantum Pooling At the end of layer $\ell$, the pooling
                                            operation of size $P$ is performed on the convolution layer output
                                            $f(X^{\ell+1})$, yielding the tensor after pooling $\tilde{X}^{\ell+1}$.
                                            Below, thee authors provide a figure shows a $2\times 2$ tensor pooling such
                                            that different pooling regions having seperate colors: Image Source: $[1]$
                                            Here, for some point at position $(i^{\ell+1}, j^{\ell+1}, d^{\ell+1})$ in
                                            $f(X^{\ell+1})$, the pooling region it corresponds to is at postion
                                            $(\tilde{i}^{\ell+1}, \tilde{j}^{\ell+1}, \tilde{d}^{\ell+1})$ in
                                            $\tilde{X}^{\ell+1}$, such that: $\left\{\begin{array}{l}
                                            \tilde{d}^{\ell+1}=d^{\ell+1} \\
                                            \tilde{j}^{\ell+1}=\left\lfloor\frac{j^{\ell+1}}{P}\right\rfloor \\
                                            \tilde{i}^{\ell+1}=\left\lfloor\frac{i^{\ell+1}}{P}\right\rfloor
                                            \end{array}\right.$ This pooling operation occurs at the end of the layer
                                            during the QRAM update and pooling operation, such that the sampled values
                                            are stored according to the pooling layers. Note, the authors state this
                                            kind of pooling can be efficiently applied to their QCNN structure, which we
                                            will look at to continue. Let's start by denoting output of layer $\ell$
                                            after quantum tomography $\mathcal{X}^{\ell+1}$. Next, quantum pooling is
                                            applied to the yielding $\tilde{\mathcal{X}}^{\ell+1}$, which has dimensions
                                            $\frac{H^{\ell+1}}{P} \times \frac{W^{\ell+1}}{P} \times D^{\ell+1}$.
                                            $\tilde{\mathcal{X}}^{\ell+1}$ will be used as the input for layer $\ell +
                                            1$ and the values for $\tilde{\mathcal{X}}^{\ell+1}$ are stored in QRAM. The
                                            valyes are use to create trees $\tilde{T}_{p^{\prime}}^{\ell+1}$, such that
                                            they relate to the matrix expansion $\tilde{A}^{\ell+1}$ and that we will
                                            talk about later. It is important to note that $\mathcal{X}^{\ell+1}$ cannot
                                            be know before quantum tomography is over. Thus, QRAM updating must be
                                            changed to pool in an online fashion each time a sample from
                                            $|f(\bar{X}^{\ell+1})\rangle$ is drawn. To continue. Quantum Time Complexity
                                            The quantum time complexity of one forward pass through the convolution
                                            layer $\ell$, where $\widetilde{O}$ hides the polylogarithmic factors, is:
                                            $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right)\right)}}\right)$
                                            which can be written also as: $\widetilde{O}\left(\sigma H^{\ell+1}
                                            W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right)$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right)$ To continue. Quantum Back Propagation To
                                            continue. Quantum Computing Preliminaries You can read about quantum
                                            computing preliminaries on my website here. Quantum Convolutional Neural
                                            Network - Context Switching Quantum Convolutional Neural Network - Context
                                            SwitchingQuantum Convolutional Neural Network Introduction We will look at
                                            how Quantum Convolutional Neural Networks QCNN can be used for deep neural
                                            networks, potentially paving the way for groundbreaking advancements in
                                            machine learning image recognition. Quantum computing can improve the time
                                            complexity of performing larger operations, such as convolution matrix
                                            multiplication, therefore, allowing for an increased number or size of
                                            convolution kernel, as well as exploring larger or more complex input
                                            structures. To continue, we define and analyze each procedure of the QCNN,
                                            such as quantum forward propagation, quantum back propagation, and more, and
                                            see how quantum computing techniques can be applied to improve the
                                            operation. Let's begin! Quantum Forward Propagation Section Reference:
                                            Quantum Algorithms for Deep Convolutional Neural Networks by: Iordanis
                                            Kerenidis, Jonas Landman, Anupam Prakash. CNRS, IRIF, Universite Paris
                                            Diderot, Paris, France (Dated: November 5, 2019) $^{[1]}$ Quantum
                                            Convolutional Layer The input for the quantum convolution layer is a $3$D
                                            tensor input $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the weights layer, filter layer, or kernel layer is a $4$D
                                            tensor $K^{\ell} \in \mathbb{R}^{H \times W \times D^{\ell} \times
                                            D^{\ell+1}}$, where the input and kernel layer are both stored in QRAM.
                                            Given precision parameters $\epsilon$, $\Delta>0$, there exists a quantum
                                            algorithm that computes a quantum state that is $\Delta$ close to
                                            $|f(\bar{X}^{\ell+1})\rangle$ where $X^{\ell+1}=X^{\ell} * K^{\ell}$, and
                                            $f: \mathbb{R} \mapsto[0, C]$ is a non-linear activation function. Note,
                                            this function could be a sigmoidal, hyperbolic tangent, ReLU, softmax, or
                                            any other activation function and is rather dependent mainly on the task the
                                            activation function needs to complete. To continue, a classical appoximation
                                            for the computer quantum state exists where:
                                            $\left\|f\big(\bar{X}^{\ell+1}\big)-f\big(X^{\ell+1}\big)\right\|_{\infty}
                                            \leq \epsilon$ The time complexity of the quantum algorithm that computes
                                            the output quantum state $|f(\bar{X}^{\ell+1})\rangle$ is $\widetilde{O}(M /
                                            \epsilon)$. In this equation, $M$ denotes the maximum norm of the product
                                            state between one of the kernels and one of the tensor input regions in
                                            $X^{\ell}$, where the size is $HW D^{\ell}$. It is important to note, that
                                            $\widetilde{O}$ is made to hide factors poly-logarithmic in $\Delta$ with
                                            respect to the size of $X^{\ell}$ and $K^{\ell}$. Given that a convolution
                                            product is equivalant to a matrix-matrix multiplication and the
                                            convolutional product between $X^{\ell}$ and $K^{\ell}$ is: $X_{i^{\ell+1},
                                            j^{\ell+1}, d^{\ell+1}}^{\ell+1}=$ $\sum_{i=0}^H \sum_{j=0}^W
                                            \sum_{d=0}^{D^{\ell}} K_{i, j, d, d^{\ell+1}}^{\ell} X_{i^{\ell+1}+i,
                                            j^{\ell+1}+j, d}^{\ell}$ it is possible to reformulate this convolution
                                            product equation as a matrix product. A diagram of this reshaping of the
                                            input and kernel is given by the authors and depicited below: Image Source:
                                            $[1]$ To express this reformulation mathematically, we must do the
                                            following. First, the original $3$D tensor input input for the quantum
                                            convolution layer $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the $4$D tensor kernel layer $K^{\ell} \in \mathbb{R}^{H
                                            \times W \times D^{\ell} \times D^{\ell+1}}$ needs to be reshaped to
                                            matrices. First, $X^{\ell}$ needs to be expanded into a matrix $A^{\ell}
                                            \in$ $\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times\left(H W
                                            D^{\ell}\right)}$, such that each row of $A^{\ell}$ is a vectorized version
                                            of a subregion of $X^{\ell}$. Next, the original kernel tensor $K^{\ell}$ is
                                            reshaped into a matrix $F^{\ell} \in$ $\mathbb{R}^{\left(H W D^{\ell}\right)
                                            \times D^{\ell+1}}$, such that each column of $F^{\ell}$ is a vectrozied
                                            version of one of the $D^{\ell+1}$ kernels. This matrix expression of the
                                            tensor input and kernel is needed for the convolution product $X^{\ell} *
                                            K^{\ell}=X^{\ell+1}$ to be written as a matrix multiplication, such that
                                            each column of the output matrix $Y^{\ell+1} \in
                                            \mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times D^{\ell+1}}$ is a
                                            first vectorized form of one of the $D^{\ell+1}$ channels of $X^{\ell+1}$.
                                            Later, we will dicuss how quantum computing, specifically quantum
                                            parallelism can not only be applied, but improve the time complexity of this
                                            step. Lastly, quantum states proportional to the rows of input $A^{\ell}$
                                            and $F^{\ell}$ are used, denoted $|A_p^{\ell}\rangle$ and
                                            $|F_q^{\ell}\rangle$ respectively. These quantum states are defined as: $
                                            \left|A_p^{\ell}\right\rangle=$ $\frac{1}{\left\|A_p^{\ell}\right\|}
                                            \sum_{r=0}^{H W D^{\ell}-1} A_{p r}^{\ell}|r\rangle$
                                            $\left|F_q^{\ell}\right\rangle=$ $\frac{1}{\left\|F_q^{\ell}\right\|}
                                            \sum_{s=0}^{D^{\ell+1}-1} F_{s q}^{\ell}|s\rangle$ and will continue to be
                                            used throughout this section. Quantum Convolution The authors describe
                                            procedure for the quantum convolutional in the following four sequential
                                            steps: inner product estimation, non linearity, quantum sampling, and then
                                            quantum random access memory update and pooling. Inner Product Estimation
                                            First we load input row vector $A_p^{\ell}$ and kernel vector $F_q^{\ell}$
                                            into quantum states by quering QRAM in the following manner:
                                            $\left\{\begin{aligned} |p\rangle|0\rangle &
                                            \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\ |q\rangle|0\rangle &
                                            \mapsto|q\rangle\left|F_q^{\ell}\right\rangle \end{aligned}\right.$ This is
                                            done so that following mapping can be perfromed with the two vectors:
                                            $\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto $ $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle$ Here
                                            $\bar{P}_{p q}$ is the inner product estimation of $A_p^{\ell}$ and
                                            $F_q^{\ell}$. The "true" value of the inner product is calculated as $P_{p
                                            q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}$, such
                                            that the inner product estimation differs by a chosen constant $\epsilon$.
                                            Here, the normalization factor is $K=\sqrt{H^{\ell+1} W^{\ell+1}
                                            D^{\ell+1}}$ and $|g_{pq}\rangle$ is some garbage state. Non Linearity Our
                                            obtained approximated convolution output $\bar{P}_{p q}$ or
                                            $\bar{Y}^{\ell+1}$ differs from the "true" convolution output $Y_{p,
                                            q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)$ by $\epsilon$. Now, we
                                            apply a non-linear function $f$ as a boolean circut giving the quantum
                                            state: $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum Sampling Conditional Rotation and Amplitude
                                            Amplification To procure the state below, states are conditionally rotated
                                            and the probabilistic amplitudes are amplified, such that we arrive at the
                                            state: $\frac{1}{K} \sum_{p, q} \alpha_{p
                                            q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum tomography is performed with precision $\eta$ so
                                            that all values and positions $(p, q, f(\bar{Y}_{p q}^{\ell+1}))$ are
                                            obtained with a high probability. Values above $\eta$ are known exactly,
                                            while values that are less than or equal to $\eta$ are set to $0$. QRAM
                                            Update and Pooling Next, QRAM needs to updated with the value for the next
                                            layer, which is $A^{\ell+1}$, while sampling. Pooling needs to be
                                            implemented in this step as well, either through a specific update or by
                                            using a QRAM data data structure. We will review quantum pooling more in
                                            depth later. Quantum Pooling At the end of layer $\ell$, the pooling
                                            operation of size $P$ is performed on the convolution layer output
                                            $f(X^{\ell+1})$, yielding the tensor after pooling $\tilde{X}^{\ell+1}$.
                                            Below, thee authors provide a figure shows a $2\times 2$ tensor pooling such
                                            that different pooling regions having seperate colors: Image Source: $[1]$
                                            Here, for some point at position $(i^{\ell+1}, j^{\ell+1}, d^{\ell+1})$ in
                                            $f(X^{\ell+1})$, the pooling region it corresponds to is at postion
                                            $(\tilde{i}^{\ell+1}, \tilde{j}^{\ell+1}, \tilde{d}^{\ell+1})$ in
                                            $\tilde{X}^{\ell+1}$, such that: $\left\{\begin{array}{l}
                                            \tilde{d}^{\ell+1}=d^{\ell+1} \\
                                            \tilde{j}^{\ell+1}=\left\lfloor\frac{j^{\ell+1}}{P}\right\rfloor \\
                                            \tilde{i}^{\ell+1}=\left\lfloor\frac{i^{\ell+1}}{P}\right\rfloor
                                            \end{array}\right.$ This pooling operation occurs at the end of the layer
                                            during the QRAM update and pooling operation, such that the sampled values
                                            are stored according to the pooling layers. Note, the authors state this
                                            kind of pooling can be efficiently applied to their QCNN structure, which we
                                            will look at to continue. Let's start by denoting output of layer $\ell$
                                            after quantum tomography $\mathcal{X}^{\ell+1}$. Next, quantum pooling is
                                            applied to the yielding $\tilde{\mathcal{X}}^{\ell+1}$, which has dimensions
                                            $\frac{H^{\ell+1}}{P} \times \frac{W^{\ell+1}}{P} \times D^{\ell+1}$.
                                            $\tilde{\mathcal{X}}^{\ell+1}$ will be used as the input for layer $\ell +
                                            1$ and the values for $\tilde{\mathcal{X}}^{\ell+1}$ are stored in QRAM. The
                                            valyes are use to create trees $\tilde{T}_{p^{\prime}}^{\ell+1}$, such that
                                            they relate to the matrix expansion $\tilde{A}^{\ell+1}$ and that we will
                                            talk about later. It is important to note that $\mathcal{X}^{\ell+1}$ cannot
                                            be know before quantum tomography is over. Thus, QRAM updating must be
                                            changed to pool in an online fashion each time a sample from
                                            $|f(\bar{X}^{\ell+1})\rangle$ is drawn. To continue. Quantum Time Complexity
                                            The quantum time complexity of one forward pass through the convolution
                                            layer $\ell$, where $\widetilde{O}$ hides the polylogarithmic factors, is:
                                            $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right)\right)}}\right)$
                                            which can be written also as: $\widetilde{O}\left(\sigma H^{\ell+1}
                                            W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right)$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right)$ To continue. Quantum Back Propagation To
                                            continue. Quantum Computing Preliminaries You can read about quantum
                                            computing preliminaries on my website here. Quantum Convolutional Neural
                                            Network Introduction We will look at how Quantum Convolutional Neural
                                            Networks QCNN can be used for deep neural networks, potentially paving the
                                            way for groundbreaking advancements in machine learning image recognition.
                                            Quantum computing can improve the time complexity of performing larger
                                            operations, such as convolution matrix multiplication, therefore, allowing
                                            for an increased number or size of convolution kernel, as well as exploring
                                            larger or more complex input structures. To continue, we define and analyze
                                            each procedure of the QCNN, such as quantum forward propagation, quantum
                                            back propagation, and more, and see how quantum computing techniques can be
                                            applied to improve the operation. Let's begin! Quantum Forward Propagation
                                            Section Reference: Quantum Algorithms for Deep Convolutional Neural Networks
                                            by: Iordanis Kerenidis, Jonas Landman, Anupam Prakash. CNRS, IRIF,
                                            Universite Paris Diderot, Paris, France (Dated: November 5, 2019) $^{[1]}$
                                            Quantum Convolutional Layer The input for the quantum convolution layer is a
                                            $3$D tensor input $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the weights layer, filter layer, or kernel layer is a $4$D
                                            tensor $K^{\ell} \in \mathbb{R}^{H \times W \times D^{\ell} \times
                                            D^{\ell+1}}$, where the input and kernel layer are both stored in QRAM.
                                            Given precision parameters $\epsilon$, $\Delta>0$, there exists a quantum
                                            algorithm that computes a quantum state that is $\Delta$ close to
                                            $|f(\bar{X}^{\ell+1})\rangle$ where $X^{\ell+1}=X^{\ell} * K^{\ell}$, and
                                            $f: \mathbb{R} \mapsto[0, C]$ is a non-linear activation function. Note,
                                            this function could be a sigmoidal, hyperbolic tangent, ReLU, softmax, or
                                            any other activation function and is rather dependent mainly on the task the
                                            activation function needs to complete. To continue, a classical appoximation
                                            for the computer quantum state exists where:
                                            $\left\|f\big(\bar{X}^{\ell+1}\big)-f\big(X^{\ell+1}\big)\right\|_{\infty}
                                            \leq \epsilon$ The time complexity of the quantum algorithm that computes
                                            the output quantum state $|f(\bar{X}^{\ell+1})\rangle$ is $\widetilde{O}(M /
                                            \epsilon)$. In this equation, $M$ denotes the maximum norm of the product
                                            state between one of the kernels and one of the tensor input regions in
                                            $X^{\ell}$, where the size is $HW D^{\ell}$. It is important to note, that
                                            $\widetilde{O}$ is made to hide factors poly-logarithmic in $\Delta$ with
                                            respect to the size of $X^{\ell}$ and $K^{\ell}$. Given that a convolution
                                            product is equivalant to a matrix-matrix multiplication and the
                                            convolutional product between $X^{\ell}$ and $K^{\ell}$ is: $X_{i^{\ell+1},
                                            j^{\ell+1}, d^{\ell+1}}^{\ell+1}=$ $\sum_{i=0}^H \sum_{j=0}^W
                                            \sum_{d=0}^{D^{\ell}} K_{i, j, d, d^{\ell+1}}^{\ell} X_{i^{\ell+1}+i,
                                            j^{\ell+1}+j, d}^{\ell}$ it is possible to reformulate this convolution
                                            product equation as a matrix product. A diagram of this reshaping of the
                                            input and kernel is given by the authors and depicited below: Image Source:
                                            $[1]$ To express this reformulation mathematically, we must do the
                                            following. First, the original $3$D tensor input input for the quantum
                                            convolution layer $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the $4$D tensor kernel layer $K^{\ell} \in \mathbb{R}^{H
                                            \times W \times D^{\ell} \times D^{\ell+1}}$ needs to be reshaped to
                                            matrices. First, $X^{\ell}$ needs to be expanded into a matrix $A^{\ell}
                                            \in$ $\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times\left(H W
                                            D^{\ell}\right)}$, such that each row of $A^{\ell}$ is a vectorized version
                                            of a subregion of $X^{\ell}$. Next, the original kernel tensor $K^{\ell}$ is
                                            reshaped into a matrix $F^{\ell} \in$ $\mathbb{R}^{\left(H W D^{\ell}\right)
                                            \times D^{\ell+1}}$, such that each column of $F^{\ell}$ is a vectrozied
                                            version of one of the $D^{\ell+1}$ kernels. This matrix expression of the
                                            tensor input and kernel is needed for the convolution product $X^{\ell} *
                                            K^{\ell}=X^{\ell+1}$ to be written as a matrix multiplication, such that
                                            each column of the output matrix $Y^{\ell+1} \in
                                            \mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times D^{\ell+1}}$ is a
                                            first vectorized form of one of the $D^{\ell+1}$ channels of $X^{\ell+1}$.
                                            Later, we will dicuss how quantum computing, specifically quantum
                                            parallelism can not only be applied, but improve the time complexity of this
                                            step. Lastly, quantum states proportional to the rows of input $A^{\ell}$
                                            and $F^{\ell}$ are used, denoted $|A_p^{\ell}\rangle$ and
                                            $|F_q^{\ell}\rangle$ respectively. These quantum states are defined as: $
                                            \left|A_p^{\ell}\right\rangle=$ $\frac{1}{\left\|A_p^{\ell}\right\|}
                                            \sum_{r=0}^{H W D^{\ell}-1} A_{p r}^{\ell}|r\rangle$
                                            $\left|F_q^{\ell}\right\rangle=$ $\frac{1}{\left\|F_q^{\ell}\right\|}
                                            \sum_{s=0}^{D^{\ell+1}-1} F_{s q}^{\ell}|s\rangle$ and will continue to be
                                            used throughout this section. Quantum Convolution The authors describe
                                            procedure for the quantum convolutional in the following four sequential
                                            steps: inner product estimation, non linearity, quantum sampling, and then
                                            quantum random access memory update and pooling. Inner Product Estimation
                                            First we load input row vector $A_p^{\ell}$ and kernel vector $F_q^{\ell}$
                                            into quantum states by quering QRAM in the following manner:
                                            $\left\{\begin{aligned} |p\rangle|0\rangle &
                                            \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\ |q\rangle|0\rangle &
                                            \mapsto|q\rangle\left|F_q^{\ell}\right\rangle \end{aligned}\right.$ This is
                                            done so that following mapping can be perfromed with the two vectors:
                                            $\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto $ $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle$ Here
                                            $\bar{P}_{p q}$ is the inner product estimation of $A_p^{\ell}$ and
                                            $F_q^{\ell}$. The "true" value of the inner product is calculated as $P_{p
                                            q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}$, such
                                            that the inner product estimation differs by a chosen constant $\epsilon$.
                                            Here, the normalization factor is $K=\sqrt{H^{\ell+1} W^{\ell+1}
                                            D^{\ell+1}}$ and $|g_{pq}\rangle$ is some garbage state. Non Linearity Our
                                            obtained approximated convolution output $\bar{P}_{p q}$ or
                                            $\bar{Y}^{\ell+1}$ differs from the "true" convolution output $Y_{p,
                                            q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)$ by $\epsilon$. Now, we
                                            apply a non-linear function $f$ as a boolean circut giving the quantum
                                            state: $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum Sampling Conditional Rotation and Amplitude
                                            Amplification To procure the state below, states are conditionally rotated
                                            and the probabilistic amplitudes are amplified, such that we arrive at the
                                            state: $\frac{1}{K} \sum_{p, q} \alpha_{p
                                            q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum tomography is performed with precision $\eta$ so
                                            that all values and positions $(p, q, f(\bar{Y}_{p q}^{\ell+1}))$ are
                                            obtained with a high probability. Values above $\eta$ are known exactly,
                                            while values that are less than or equal to $\eta$ are set to $0$. QRAM
                                            Update and Pooling Next, QRAM needs to updated with the value for the next
                                            layer, which is $A^{\ell+1}$, while sampling. Pooling needs to be
                                            implemented in this step as well, either through a specific update or by
                                            using a QRAM data data structure. We will review quantum pooling more in
                                            depth later. Quantum Pooling At the end of layer $\ell$, the pooling
                                            operation of size $P$ is performed on the convolution layer output
                                            $f(X^{\ell+1})$, yielding the tensor after pooling $\tilde{X}^{\ell+1}$.
                                            Below, thee authors provide a figure shows a $2\times 2$ tensor pooling such
                                            that different pooling regions having seperate colors: Image Source: $[1]$
                                            Here, for some point at position $(i^{\ell+1}, j^{\ell+1}, d^{\ell+1})$ in
                                            $f(X^{\ell+1})$, the pooling region it corresponds to is at postion
                                            $(\tilde{i}^{\ell+1}, \tilde{j}^{\ell+1}, \tilde{d}^{\ell+1})$ in
                                            $\tilde{X}^{\ell+1}$, such that: $\left\{\begin{array}{l}
                                            \tilde{d}^{\ell+1}=d^{\ell+1} \\
                                            \tilde{j}^{\ell+1}=\left\lfloor\frac{j^{\ell+1}}{P}\right\rfloor \\
                                            \tilde{i}^{\ell+1}=\left\lfloor\frac{i^{\ell+1}}{P}\right\rfloor
                                            \end{array}\right.$ This pooling operation occurs at the end of the layer
                                            during the QRAM update and pooling operation, such that the sampled values
                                            are stored according to the pooling layers. Note, the authors state this
                                            kind of pooling can be efficiently applied to their QCNN structure, which we
                                            will look at to continue. Let's start by denoting output of layer $\ell$
                                            after quantum tomography $\mathcal{X}^{\ell+1}$. Next, quantum pooling is
                                            applied to the yielding $\tilde{\mathcal{X}}^{\ell+1}$, which has dimensions
                                            $\frac{H^{\ell+1}}{P} \times \frac{W^{\ell+1}}{P} \times D^{\ell+1}$.
                                            $\tilde{\mathcal{X}}^{\ell+1}$ will be used as the input for layer $\ell +
                                            1$ and the values for $\tilde{\mathcal{X}}^{\ell+1}$ are stored in QRAM. The
                                            valyes are use to create trees $\tilde{T}_{p^{\prime}}^{\ell+1}$, such that
                                            they relate to the matrix expansion $\tilde{A}^{\ell+1}$ and that we will
                                            talk about later. It is important to note that $\mathcal{X}^{\ell+1}$ cannot
                                            be know before quantum tomography is over. Thus, QRAM updating must be
                                            changed to pool in an online fashion each time a sample from
                                            $|f(\bar{X}^{\ell+1})\rangle$ is drawn. To continue. Quantum Time Complexity
                                            The quantum time complexity of one forward pass through the convolution
                                            layer $\ell$, where $\widetilde{O}$ hides the polylogarithmic factors, is:
                                            $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right)\right)}}\right)$
                                            which can be written also as: $\widetilde{O}\left(\sigma H^{\ell+1}
                                            W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right)$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right)$ To continue. Quantum Back Propagation To
                                            continue. Quantum Computing Preliminaries You can read about quantum
                                            computing preliminaries on my website here. Quantum Convolutional Neural
                                            Network Introduction We will look at how Quantum Convolutional Neural
                                            Networks QCNN can be used for deep neural networks, potentially paving the
                                            way for groundbreaking advancements in machine learning image recognition.
                                            Quantum computing can improve the time complexity of performing larger
                                            operations, such as convolution matrix multiplication, therefore, allowing
                                            for an increased number or size of convolution kernel, as well as exploring
                                            larger or more complex input structures. To continue, we define and analyze
                                            each procedure of the QCNN, such as quantum forward propagation, quantum
                                            back propagation, and more, and see how quantum computing techniques can be
                                            applied to improve the operation. Let's begin! Quantum Forward Propagation
                                            Section Reference: Quantum Algorithms for Deep Convolutional Neural Networks
                                            by: Iordanis Kerenidis, Jonas Landman, Anupam Prakash. CNRS, IRIF,
                                            Universite Paris Diderot, Paris, France (Dated: November 5, 2019) $^{[1]}$
                                            Quantum Convolutional Layer The input for the quantum convolution layer is a
                                            $3$D tensor input $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the weights layer, filter layer, or kernel layer is a $4$D
                                            tensor $K^{\ell} \in \mathbb{R}^{H \times W \times D^{\ell} \times
                                            D^{\ell+1}}$, where the input and kernel layer are both stored in QRAM.
                                            Given precision parameters $\epsilon$, $\Delta>0$, there exists a quantum
                                            algorithm that computes a quantum state that is $\Delta$ close to
                                            $|f(\bar{X}^{\ell+1})\rangle$ where $X^{\ell+1}=X^{\ell} * K^{\ell}$, and
                                            $f: \mathbb{R} \mapsto[0, C]$ is a non-linear activation function. Note,
                                            this function could be a sigmoidal, hyperbolic tangent, ReLU, softmax, or
                                            any other activation function and is rather dependent mainly on the task the
                                            activation function needs to complete. To continue, a classical appoximation
                                            for the computer quantum state exists where:
                                            $\left\|f\big(\bar{X}^{\ell+1}\big)-f\big(X^{\ell+1}\big)\right\|_{\infty}
                                            \leq \epsilon$ The time complexity of the quantum algorithm that computes
                                            the output quantum state $|f(\bar{X}^{\ell+1})\rangle$ is $\widetilde{O}(M /
                                            \epsilon)$. In this equation, $M$ denotes the maximum norm of the product
                                            state between one of the kernels and one of the tensor input regions in
                                            $X^{\ell}$, where the size is $HW D^{\ell}$. It is important to note, that
                                            $\widetilde{O}$ is made to hide factors poly-logarithmic in $\Delta$ with
                                            respect to the size of $X^{\ell}$ and $K^{\ell}$. Given that a convolution
                                            product is equivalant to a matrix-matrix multiplication and the
                                            convolutional product between $X^{\ell}$ and $K^{\ell}$ is: $X_{i^{\ell+1},
                                            j^{\ell+1}, d^{\ell+1}}^{\ell+1}=$ $\sum_{i=0}^H \sum_{j=0}^W
                                            \sum_{d=0}^{D^{\ell}} K_{i, j, d, d^{\ell+1}}^{\ell} X_{i^{\ell+1}+i,
                                            j^{\ell+1}+j, d}^{\ell}$ it is possible to reformulate this convolution
                                            product equation as a matrix product. A diagram of this reshaping of the
                                            input and kernel is given by the authors and depicited below: Image Source:
                                            $[1]$ To express this reformulation mathematically, we must do the
                                            following. First, the original $3$D tensor input input for the quantum
                                            convolution layer $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the $4$D tensor kernel layer $K^{\ell} \in \mathbb{R}^{H
                                            \times W \times D^{\ell} \times D^{\ell+1}}$ needs to be reshaped to
                                            matrices. First, $X^{\ell}$ needs to be expanded into a matrix $A^{\ell}
                                            \in$ $\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times\left(H W
                                            D^{\ell}\right)}$, such that each row of $A^{\ell}$ is a vectorized version
                                            of a subregion of $X^{\ell}$. Next, the original kernel tensor $K^{\ell}$ is
                                            reshaped into a matrix $F^{\ell} \in$ $\mathbb{R}^{\left(H W D^{\ell}\right)
                                            \times D^{\ell+1}}$, such that each column of $F^{\ell}$ is a vectrozied
                                            version of one of the $D^{\ell+1}$ kernels. This matrix expression of the
                                            tensor input and kernel is needed for the convolution product $X^{\ell} *
                                            K^{\ell}=X^{\ell+1}$ to be written as a matrix multiplication, such that
                                            each column of the output matrix $Y^{\ell+1} \in
                                            \mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times D^{\ell+1}}$ is a
                                            first vectorized form of one of the $D^{\ell+1}$ channels of $X^{\ell+1}$.
                                            Later, we will dicuss how quantum computing, specifically quantum
                                            parallelism can not only be applied, but improve the time complexity of this
                                            step. Lastly, quantum states proportional to the rows of input $A^{\ell}$
                                            and $F^{\ell}$ are used, denoted $|A_p^{\ell}\rangle$ and
                                            $|F_q^{\ell}\rangle$ respectively. These quantum states are defined as: $
                                            \left|A_p^{\ell}\right\rangle=$ $\frac{1}{\left\|A_p^{\ell}\right\|}
                                            \sum_{r=0}^{H W D^{\ell}-1} A_{p r}^{\ell}|r\rangle$
                                            $\left|F_q^{\ell}\right\rangle=$ $\frac{1}{\left\|F_q^{\ell}\right\|}
                                            \sum_{s=0}^{D^{\ell+1}-1} F_{s q}^{\ell}|s\rangle$ and will continue to be
                                            used throughout this section. Quantum Convolution The authors describe
                                            procedure for the quantum convolutional in the following four sequential
                                            steps: inner product estimation, non linearity, quantum sampling, and then
                                            quantum random access memory update and pooling. Inner Product Estimation
                                            First we load input row vector $A_p^{\ell}$ and kernel vector $F_q^{\ell}$
                                            into quantum states by quering QRAM in the following manner:
                                            $\left\{\begin{aligned} |p\rangle|0\rangle &
                                            \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\ |q\rangle|0\rangle &
                                            \mapsto|q\rangle\left|F_q^{\ell}\right\rangle \end{aligned}\right.$ This is
                                            done so that following mapping can be perfromed with the two vectors:
                                            $\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto $ $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle$ Here
                                            $\bar{P}_{p q}$ is the inner product estimation of $A_p^{\ell}$ and
                                            $F_q^{\ell}$. The "true" value of the inner product is calculated as $P_{p
                                            q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}$, such
                                            that the inner product estimation differs by a chosen constant $\epsilon$.
                                            Here, the normalization factor is $K=\sqrt{H^{\ell+1} W^{\ell+1}
                                            D^{\ell+1}}$ and $|g_{pq}\rangle$ is some garbage state. Non Linearity Our
                                            obtained approximated convolution output $\bar{P}_{p q}$ or
                                            $\bar{Y}^{\ell+1}$ differs from the "true" convolution output $Y_{p,
                                            q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)$ by $\epsilon$. Now, we
                                            apply a non-linear function $f$ as a boolean circut giving the quantum
                                            state: $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum Sampling Conditional Rotation and Amplitude
                                            Amplification To procure the state below, states are conditionally rotated
                                            and the probabilistic amplitudes are amplified, such that we arrive at the
                                            state: $\frac{1}{K} \sum_{p, q} \alpha_{p
                                            q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum tomography is performed with precision $\eta$ so
                                            that all values and positions $(p, q, f(\bar{Y}_{p q}^{\ell+1}))$ are
                                            obtained with a high probability. Values above $\eta$ are known exactly,
                                            while values that are less than or equal to $\eta$ are set to $0$. QRAM
                                            Update and Pooling Next, QRAM needs to updated with the value for the next
                                            layer, which is $A^{\ell+1}$, while sampling. Pooling needs to be
                                            implemented in this step as well, either through a specific update or by
                                            using a QRAM data data structure. We will review quantum pooling more in
                                            depth later. Quantum Pooling At the end of layer $\ell$, the pooling
                                            operation of size $P$ is performed on the convolution layer output
                                            $f(X^{\ell+1})$, yielding the tensor after pooling $\tilde{X}^{\ell+1}$.
                                            Below, thee authors provide a figure shows a $2\times 2$ tensor pooling such
                                            that different pooling regions having seperate colors: Image Source: $[1]$
                                            Here, for some point at position $(i^{\ell+1}, j^{\ell+1}, d^{\ell+1})$ in
                                            $f(X^{\ell+1})$, the pooling region it corresponds to is at postion
                                            $(\tilde{i}^{\ell+1}, \tilde{j}^{\ell+1}, \tilde{d}^{\ell+1})$ in
                                            $\tilde{X}^{\ell+1}$, such that: $\left\{\begin{array}{l}
                                            \tilde{d}^{\ell+1}=d^{\ell+1} \\
                                            \tilde{j}^{\ell+1}=\left\lfloor\frac{j^{\ell+1}}{P}\right\rfloor \\
                                            \tilde{i}^{\ell+1}=\left\lfloor\frac{i^{\ell+1}}{P}\right\rfloor
                                            \end{array}\right.$ This pooling operation occurs at the end of the layer
                                            during the QRAM update and pooling operation, such that the sampled values
                                            are stored according to the pooling layers. Note, the authors state this
                                            kind of pooling can be efficiently applied to their QCNN structure, which we
                                            will look at to continue. Let's start by denoting output of layer $\ell$
                                            after quantum tomography $\mathcal{X}^{\ell+1}$. Next, quantum pooling is
                                            applied to the yielding $\tilde{\mathcal{X}}^{\ell+1}$, which has dimensions
                                            $\frac{H^{\ell+1}}{P} \times \frac{W^{\ell+1}}{P} \times D^{\ell+1}$.
                                            $\tilde{\mathcal{X}}^{\ell+1}$ will be used as the input for layer $\ell +
                                            1$ and the values for $\tilde{\mathcal{X}}^{\ell+1}$ are stored in QRAM. The
                                            valyes are use to create trees $\tilde{T}_{p^{\prime}}^{\ell+1}$, such that
                                            they relate to the matrix expansion $\tilde{A}^{\ell+1}$ and that we will
                                            talk about later. It is important to note that $\mathcal{X}^{\ell+1}$ cannot
                                            be know before quantum tomography is over. Thus, QRAM updating must be
                                            changed to pool in an online fashion each time a sample from
                                            $|f(\bar{X}^{\ell+1})\rangle$ is drawn. To continue. Quantum Time Complexity
                                            The quantum time complexity of one forward pass through the convolution
                                            layer $\ell$, where $\widetilde{O}$ hides the polylogarithmic factors, is:
                                            $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right)\right)}}\right)$
                                            which can be written also as: $\widetilde{O}\left(\sigma H^{\ell+1}
                                            W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right)$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right)$ To continue. Quantum Back Propagation To
                                            continue. Quantum Computing Preliminaries You can read about quantum
                                            computing preliminaries on my website here. Quantum Convolutional Neural
                                            NetworkIntroduction We will look at how Quantum Convolutional Neural
                                            Networks QCNN can be used for deep neural networks, potentially paving the
                                            way for groundbreaking advancements in machine learning image recognition.
                                            Quantum computing can improve the time complexity of performing larger
                                            operations, such as convolution matrix multiplication, therefore, allowing
                                            for an increased number or size of convolution kernel, as well as exploring
                                            larger or more complex input structures. To continue, we define and analyze
                                            each procedure of the QCNN, such as quantum forward propagation, quantum
                                            back propagation, and more, and see how quantum computing techniques can be
                                            applied to improve the operation. Let's begin! IntroductionWe will look at
                                            how Quantum Convolutional Neural Networks QCNN can be used for deep neural
                                            networks, potentially paving the way for groundbreaking advancements in
                                            machine learning image recognition. Quantum computing can improve the time
                                            complexity of performing larger operations, such as convolution matrix
                                            multiplication, therefore, allowing for an increased number or size of
                                            convolution kernel, as well as exploring larger or more complex input
                                            structures. Quantum computingtime complexityTo continue, we define and
                                            analyze each procedure of the QCNN, such as quantum forward propagation,
                                            quantum back propagation, and more, and see how quantum computing techniques
                                            can be applied to improve the operation. Let's begin! quantum forward
                                            propagationquantum back propagationQuantum Forward Propagation Section
                                            Reference: Quantum Algorithms for Deep Convolutional Neural Networks by:
                                            Iordanis Kerenidis, Jonas Landman, Anupam Prakash. CNRS, IRIF, Universite
                                            Paris Diderot, Paris, France (Dated: November 5, 2019) $^{[1]}$ Quantum
                                            Convolutional Layer The input for the quantum convolution layer is a $3$D
                                            tensor input $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the weights layer, filter layer, or kernel layer is a $4$D
                                            tensor $K^{\ell} \in \mathbb{R}^{H \times W \times D^{\ell} \times
                                            D^{\ell+1}}$, where the input and kernel layer are both stored in QRAM.
                                            Given precision parameters $\epsilon$, $\Delta>0$, there exists a quantum
                                            algorithm that computes a quantum state that is $\Delta$ close to
                                            $|f(\bar{X}^{\ell+1})\rangle$ where $X^{\ell+1}=X^{\ell} * K^{\ell}$, and
                                            $f: \mathbb{R} \mapsto[0, C]$ is a non-linear activation function. Note,
                                            this function could be a sigmoidal, hyperbolic tangent, ReLU, softmax, or
                                            any other activation function and is rather dependent mainly on the task the
                                            activation function needs to complete. To continue, a classical appoximation
                                            for the computer quantum state exists where:
                                            $\left\|f\big(\bar{X}^{\ell+1}\big)-f\big(X^{\ell+1}\big)\right\|_{\infty}
                                            \leq \epsilon$ The time complexity of the quantum algorithm that computes
                                            the output quantum state $|f(\bar{X}^{\ell+1})\rangle$ is $\widetilde{O}(M /
                                            \epsilon)$. In this equation, $M$ denotes the maximum norm of the product
                                            state between one of the kernels and one of the tensor input regions in
                                            $X^{\ell}$, where the size is $HW D^{\ell}$. It is important to note, that
                                            $\widetilde{O}$ is made to hide factors poly-logarithmic in $\Delta$ with
                                            respect to the size of $X^{\ell}$ and $K^{\ell}$. Given that a convolution
                                            product is equivalant to a matrix-matrix multiplication and the
                                            convolutional product between $X^{\ell}$ and $K^{\ell}$ is: $X_{i^{\ell+1},
                                            j^{\ell+1}, d^{\ell+1}}^{\ell+1}=$ $\sum_{i=0}^H \sum_{j=0}^W
                                            \sum_{d=0}^{D^{\ell}} K_{i, j, d, d^{\ell+1}}^{\ell} X_{i^{\ell+1}+i,
                                            j^{\ell+1}+j, d}^{\ell}$ it is possible to reformulate this convolution
                                            product equation as a matrix product. A diagram of this reshaping of the
                                            input and kernel is given by the authors and depicited below: Image Source:
                                            $[1]$ To express this reformulation mathematically, we must do the
                                            following. First, the original $3$D tensor input input for the quantum
                                            convolution layer $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the $4$D tensor kernel layer $K^{\ell} \in \mathbb{R}^{H
                                            \times W \times D^{\ell} \times D^{\ell+1}}$ needs to be reshaped to
                                            matrices. First, $X^{\ell}$ needs to be expanded into a matrix $A^{\ell}
                                            \in$ $\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times\left(H W
                                            D^{\ell}\right)}$, such that each row of $A^{\ell}$ is a vectorized version
                                            of a subregion of $X^{\ell}$. Next, the original kernel tensor $K^{\ell}$ is
                                            reshaped into a matrix $F^{\ell} \in$ $\mathbb{R}^{\left(H W D^{\ell}\right)
                                            \times D^{\ell+1}}$, such that each column of $F^{\ell}$ is a vectrozied
                                            version of one of the $D^{\ell+1}$ kernels. This matrix expression of the
                                            tensor input and kernel is needed for the convolution product $X^{\ell} *
                                            K^{\ell}=X^{\ell+1}$ to be written as a matrix multiplication, such that
                                            each column of the output matrix $Y^{\ell+1} \in
                                            \mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times D^{\ell+1}}$ is a
                                            first vectorized form of one of the $D^{\ell+1}$ channels of $X^{\ell+1}$.
                                            Later, we will dicuss how quantum computing, specifically quantum
                                            parallelism can not only be applied, but improve the time complexity of this
                                            step. Lastly, quantum states proportional to the rows of input $A^{\ell}$
                                            and $F^{\ell}$ are used, denoted $|A_p^{\ell}\rangle$ and
                                            $|F_q^{\ell}\rangle$ respectively. These quantum states are defined as: $
                                            \left|A_p^{\ell}\right\rangle=$ $\frac{1}{\left\|A_p^{\ell}\right\|}
                                            \sum_{r=0}^{H W D^{\ell}-1} A_{p r}^{\ell}|r\rangle$
                                            $\left|F_q^{\ell}\right\rangle=$ $\frac{1}{\left\|F_q^{\ell}\right\|}
                                            \sum_{s=0}^{D^{\ell+1}-1} F_{s q}^{\ell}|s\rangle$ and will continue to be
                                            used throughout this section. Quantum Convolution The authors describe
                                            procedure for the quantum convolutional in the following four sequential
                                            steps: inner product estimation, non linearity, quantum sampling, and then
                                            quantum random access memory update and pooling. Inner Product Estimation
                                            First we load input row vector $A_p^{\ell}$ and kernel vector $F_q^{\ell}$
                                            into quantum states by quering QRAM in the following manner:
                                            $\left\{\begin{aligned} |p\rangle|0\rangle &
                                            \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\ |q\rangle|0\rangle &
                                            \mapsto|q\rangle\left|F_q^{\ell}\right\rangle \end{aligned}\right.$ This is
                                            done so that following mapping can be perfromed with the two vectors:
                                            $\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto $ $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle$ Here
                                            $\bar{P}_{p q}$ is the inner product estimation of $A_p^{\ell}$ and
                                            $F_q^{\ell}$. The "true" value of the inner product is calculated as $P_{p
                                            q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}$, such
                                            that the inner product estimation differs by a chosen constant $\epsilon$.
                                            Here, the normalization factor is $K=\sqrt{H^{\ell+1} W^{\ell+1}
                                            D^{\ell+1}}$ and $|g_{pq}\rangle$ is some garbage state. Non Linearity Our
                                            obtained approximated convolution output $\bar{P}_{p q}$ or
                                            $\bar{Y}^{\ell+1}$ differs from the "true" convolution output $Y_{p,
                                            q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)$ by $\epsilon$. Now, we
                                            apply a non-linear function $f$ as a boolean circut giving the quantum
                                            state: $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum Sampling Conditional Rotation and Amplitude
                                            Amplification To procure the state below, states are conditionally rotated
                                            and the probabilistic amplitudes are amplified, such that we arrive at the
                                            state: $\frac{1}{K} \sum_{p, q} \alpha_{p
                                            q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum tomography is performed with precision $\eta$ so
                                            that all values and positions $(p, q, f(\bar{Y}_{p q}^{\ell+1}))$ are
                                            obtained with a high probability. Values above $\eta$ are known exactly,
                                            while values that are less than or equal to $\eta$ are set to $0$. QRAM
                                            Update and Pooling Next, QRAM needs to updated with the value for the next
                                            layer, which is $A^{\ell+1}$, while sampling. Pooling needs to be
                                            implemented in this step as well, either through a specific update or by
                                            using a QRAM data data structure. We will review quantum pooling more in
                                            depth later. Quantum Pooling At the end of layer $\ell$, the pooling
                                            operation of size $P$ is performed on the convolution layer output
                                            $f(X^{\ell+1})$, yielding the tensor after pooling $\tilde{X}^{\ell+1}$.
                                            Below, thee authors provide a figure shows a $2\times 2$ tensor pooling such
                                            that different pooling regions having seperate colors: Image Source: $[1]$
                                            Here, for some point at position $(i^{\ell+1}, j^{\ell+1}, d^{\ell+1})$ in
                                            $f(X^{\ell+1})$, the pooling region it corresponds to is at postion
                                            $(\tilde{i}^{\ell+1}, \tilde{j}^{\ell+1}, \tilde{d}^{\ell+1})$ in
                                            $\tilde{X}^{\ell+1}$, such that: $\left\{\begin{array}{l}
                                            \tilde{d}^{\ell+1}=d^{\ell+1} \\
                                            \tilde{j}^{\ell+1}=\left\lfloor\frac{j^{\ell+1}}{P}\right\rfloor \\
                                            \tilde{i}^{\ell+1}=\left\lfloor\frac{i^{\ell+1}}{P}\right\rfloor
                                            \end{array}\right.$ This pooling operation occurs at the end of the layer
                                            during the QRAM update and pooling operation, such that the sampled values
                                            are stored according to the pooling layers. Note, the authors state this
                                            kind of pooling can be efficiently applied to their QCNN structure, which we
                                            will look at to continue. Let's start by denoting output of layer $\ell$
                                            after quantum tomography $\mathcal{X}^{\ell+1}$. Next, quantum pooling is
                                            applied to the yielding $\tilde{\mathcal{X}}^{\ell+1}$, which has dimensions
                                            $\frac{H^{\ell+1}}{P} \times \frac{W^{\ell+1}}{P} \times D^{\ell+1}$.
                                            $\tilde{\mathcal{X}}^{\ell+1}$ will be used as the input for layer $\ell +
                                            1$ and the values for $\tilde{\mathcal{X}}^{\ell+1}$ are stored in QRAM. The
                                            valyes are use to create trees $\tilde{T}_{p^{\prime}}^{\ell+1}$, such that
                                            they relate to the matrix expansion $\tilde{A}^{\ell+1}$ and that we will
                                            talk about later. It is important to note that $\mathcal{X}^{\ell+1}$ cannot
                                            be know before quantum tomography is over. Thus, QRAM updating must be
                                            changed to pool in an online fashion each time a sample from
                                            $|f(\bar{X}^{\ell+1})\rangle$ is drawn. To continue. Quantum Time Complexity
                                            The quantum time complexity of one forward pass through the convolution
                                            layer $\ell$, where $\widetilde{O}$ hides the polylogarithmic factors, is:
                                            $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right)\right)}}\right)$
                                            which can be written also as: $\widetilde{O}\left(\sigma H^{\ell+1}
                                            W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right)$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right)$ To continue. Quantum Forward PropagationSection
                                            Reference: Quantum Algorithms for Deep Convolutional Neural Networks by:
                                            Iordanis Kerenidis, Jonas Landman, Anupam Prakash. CNRS, IRIF, Universite
                                            Paris Diderot, Paris, France (Dated: November 5, 2019) $^{[1]}$ Quantum
                                            Algorithms for Deep Convolutional Neural Networks by: Iordanis Kerenidis,
                                            Jonas Landman, Anupam Prakash. CNRS, IRIF, Universite Paris Diderot, Paris,
                                            France (Dated: November 5, 2019) $^{[1]}$ Quantum Algorithms for Deep
                                            Convolutional Neural Networks by: Iordanis Kerenidis, Jonas Landman, Anupam
                                            Prakash. CNRS, IRIF, Universite Paris Diderot, Paris, France (Dated:
                                            November 5, 2019) Quantum Convolutional Layer The input for the quantum
                                            convolution layer is a $3$D tensor input $X^{\ell} \in \mathbb{R}^{H^{\ell}
                                            \times W^{\ell} \times D^{\ell}}$ and the weights layer, filter layer, or
                                            kernel layer is a $4$D tensor $K^{\ell} \in \mathbb{R}^{H \times W \times
                                            D^{\ell} \times D^{\ell+1}}$, where the input and kernel layer are both
                                            stored in QRAM. Given precision parameters $\epsilon$, $\Delta>0$, there
                                            exists a quantum algorithm that computes a quantum state that is $\Delta$
                                            close to $|f(\bar{X}^{\ell+1})\rangle$ where $X^{\ell+1}=X^{\ell} *
                                            K^{\ell}$, and $f: \mathbb{R} \mapsto[0, C]$ is a non-linear activation
                                            function. Note, this function could be a sigmoidal, hyperbolic tangent,
                                            ReLU, softmax, or any other activation function and is rather dependent
                                            mainly on the task the activation function needs to complete. To continue, a
                                            classical appoximation for the computer quantum state exists where:
                                            $\left\|f\big(\bar{X}^{\ell+1}\big)-f\big(X^{\ell+1}\big)\right\|_{\infty}
                                            \leq \epsilon$ The time complexity of the quantum algorithm that computes
                                            the output quantum state $|f(\bar{X}^{\ell+1})\rangle$ is $\widetilde{O}(M /
                                            \epsilon)$. In this equation, $M$ denotes the maximum norm of the product
                                            state between one of the kernels and one of the tensor input regions in
                                            $X^{\ell}$, where the size is $HW D^{\ell}$. It is important to note, that
                                            $\widetilde{O}$ is made to hide factors poly-logarithmic in $\Delta$ with
                                            respect to the size of $X^{\ell}$ and $K^{\ell}$. Given that a convolution
                                            product is equivalant to a matrix-matrix multiplication and the
                                            convolutional product between $X^{\ell}$ and $K^{\ell}$ is: $X_{i^{\ell+1},
                                            j^{\ell+1}, d^{\ell+1}}^{\ell+1}=$ $\sum_{i=0}^H \sum_{j=0}^W
                                            \sum_{d=0}^{D^{\ell}} K_{i, j, d, d^{\ell+1}}^{\ell} X_{i^{\ell+1}+i,
                                            j^{\ell+1}+j, d}^{\ell}$ it is possible to reformulate this convolution
                                            product equation as a matrix product. A diagram of this reshaping of the
                                            input and kernel is given by the authors and depicited below: Image Source:
                                            $[1]$ To express this reformulation mathematically, we must do the
                                            following. First, the original $3$D tensor input input for the quantum
                                            convolution layer $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the $4$D tensor kernel layer $K^{\ell} \in \mathbb{R}^{H
                                            \times W \times D^{\ell} \times D^{\ell+1}}$ needs to be reshaped to
                                            matrices. First, $X^{\ell}$ needs to be expanded into a matrix $A^{\ell}
                                            \in$ $\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times\left(H W
                                            D^{\ell}\right)}$, such that each row of $A^{\ell}$ is a vectorized version
                                            of a subregion of $X^{\ell}$. Next, the original kernel tensor $K^{\ell}$ is
                                            reshaped into a matrix $F^{\ell} \in$ $\mathbb{R}^{\left(H W D^{\ell}\right)
                                            \times D^{\ell+1}}$, such that each column of $F^{\ell}$ is a vectrozied
                                            version of one of the $D^{\ell+1}$ kernels. This matrix expression of the
                                            tensor input and kernel is needed for the convolution product $X^{\ell} *
                                            K^{\ell}=X^{\ell+1}$ to be written as a matrix multiplication, such that
                                            each column of the output matrix $Y^{\ell+1} \in
                                            \mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times D^{\ell+1}}$ is a
                                            first vectorized form of one of the $D^{\ell+1}$ channels of $X^{\ell+1}$.
                                            Later, we will dicuss how quantum computing, specifically quantum
                                            parallelism can not only be applied, but improve the time complexity of this
                                            step. Lastly, quantum states proportional to the rows of input $A^{\ell}$
                                            and $F^{\ell}$ are used, denoted $|A_p^{\ell}\rangle$ and
                                            $|F_q^{\ell}\rangle$ respectively. These quantum states are defined as: $
                                            \left|A_p^{\ell}\right\rangle=$ $\frac{1}{\left\|A_p^{\ell}\right\|}
                                            \sum_{r=0}^{H W D^{\ell}-1} A_{p r}^{\ell}|r\rangle$
                                            $\left|F_q^{\ell}\right\rangle=$ $\frac{1}{\left\|F_q^{\ell}\right\|}
                                            \sum_{s=0}^{D^{\ell+1}-1} F_{s q}^{\ell}|s\rangle$ and will continue to be
                                            used throughout this section. Quantum Convolution The authors describe
                                            procedure for the quantum convolutional in the following four sequential
                                            steps: inner product estimation, non linearity, quantum sampling, and then
                                            quantum random access memory update and pooling. Inner Product Estimation
                                            First we load input row vector $A_p^{\ell}$ and kernel vector $F_q^{\ell}$
                                            into quantum states by quering QRAM in the following manner:
                                            $\left\{\begin{aligned} |p\rangle|0\rangle &
                                            \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\ |q\rangle|0\rangle &
                                            \mapsto|q\rangle\left|F_q^{\ell}\right\rangle \end{aligned}\right.$ This is
                                            done so that following mapping can be perfromed with the two vectors:
                                            $\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto $ $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle$ Here
                                            $\bar{P}_{p q}$ is the inner product estimation of $A_p^{\ell}$ and
                                            $F_q^{\ell}$. The "true" value of the inner product is calculated as $P_{p
                                            q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}$, such
                                            that the inner product estimation differs by a chosen constant $\epsilon$.
                                            Here, the normalization factor is $K=\sqrt{H^{\ell+1} W^{\ell+1}
                                            D^{\ell+1}}$ and $|g_{pq}\rangle$ is some garbage state. Non Linearity Our
                                            obtained approximated convolution output $\bar{P}_{p q}$ or
                                            $\bar{Y}^{\ell+1}$ differs from the "true" convolution output $Y_{p,
                                            q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)$ by $\epsilon$. Now, we
                                            apply a non-linear function $f$ as a boolean circut giving the quantum
                                            state: $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum Sampling Conditional Rotation and Amplitude
                                            Amplification To procure the state below, states are conditionally rotated
                                            and the probabilistic amplitudes are amplified, such that we arrive at the
                                            state: $\frac{1}{K} \sum_{p, q} \alpha_{p
                                            q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum tomography is performed with precision $\eta$ so
                                            that all values and positions $(p, q, f(\bar{Y}_{p q}^{\ell+1}))$ are
                                            obtained with a high probability. Values above $\eta$ are known exactly,
                                            while values that are less than or equal to $\eta$ are set to $0$. QRAM
                                            Update and Pooling Next, QRAM needs to updated with the value for the next
                                            layer, which is $A^{\ell+1}$, while sampling. Pooling needs to be
                                            implemented in this step as well, either through a specific update or by
                                            using a QRAM data data structure. We will review quantum pooling more in
                                            depth later. Quantum Convolutional Layer The input for the quantum
                                            convolution layer is a $3$D tensor input $X^{\ell} \in \mathbb{R}^{H^{\ell}
                                            \times W^{\ell} \times D^{\ell}}$ and the weights layer, filter layer, or
                                            kernel layer is a $4$D tensor $K^{\ell} \in \mathbb{R}^{H \times W \times
                                            D^{\ell} \times D^{\ell+1}}$, where the input and kernel layer are both
                                            stored in QRAM. Given precision parameters $\epsilon$, $\Delta>0$, there
                                            exists a quantum algorithm that computes a quantum state that is $\Delta$
                                            close to $|f(\bar{X}^{\ell+1})\rangle$ where $X^{\ell+1}=X^{\ell} *
                                            K^{\ell}$, and $f: \mathbb{R} \mapsto[0, C]$ is a non-linear activation
                                            function. Note, this function could be a sigmoidal, hyperbolic tangent,
                                            ReLU, softmax, or any other activation function and is rather dependent
                                            mainly on the task the activation function needs to complete. To continue, a
                                            classical appoximation for the computer quantum state exists where:
                                            $\left\|f\big(\bar{X}^{\ell+1}\big)-f\big(X^{\ell+1}\big)\right\|_{\infty}
                                            \leq \epsilon$ The time complexity of the quantum algorithm that computes
                                            the output quantum state $|f(\bar{X}^{\ell+1})\rangle$ is $\widetilde{O}(M /
                                            \epsilon)$. In this equation, $M$ denotes the maximum norm of the product
                                            state between one of the kernels and one of the tensor input regions in
                                            $X^{\ell}$, where the size is $HW D^{\ell}$. It is important to note, that
                                            $\widetilde{O}$ is made to hide factors poly-logarithmic in $\Delta$ with
                                            respect to the size of $X^{\ell}$ and $K^{\ell}$. Given that a convolution
                                            product is equivalant to a matrix-matrix multiplication and the
                                            convolutional product between $X^{\ell}$ and $K^{\ell}$ is: $X_{i^{\ell+1},
                                            j^{\ell+1}, d^{\ell+1}}^{\ell+1}=$ $\sum_{i=0}^H \sum_{j=0}^W
                                            \sum_{d=0}^{D^{\ell}} K_{i, j, d, d^{\ell+1}}^{\ell} X_{i^{\ell+1}+i,
                                            j^{\ell+1}+j, d}^{\ell}$ it is possible to reformulate this convolution
                                            product equation as a matrix product. A diagram of this reshaping of the
                                            input and kernel is given by the authors and depicited below: Image Source:
                                            $[1]$ $[1]$ To express this reformulation mathematically, we must do the
                                            following. First, the original $3$D tensor input input for the quantum
                                            convolution layer $X^{\ell} \in \mathbb{R}^{H^{\ell} \times W^{\ell} \times
                                            D^{\ell}}$ and the $4$D tensor kernel layer $K^{\ell} \in \mathbb{R}^{H
                                            \times W \times D^{\ell} \times D^{\ell+1}}$ needs to be reshaped to
                                            matrices. First, $X^{\ell}$ needs to be expanded into a matrix $A^{\ell}
                                            \in$ $\mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times\left(H W
                                            D^{\ell}\right)}$, such that each row of $A^{\ell}$ is a vectorized version
                                            of a subregion of $X^{\ell}$. Next, the original kernel tensor $K^{\ell}$ is
                                            reshaped into a matrix $F^{\ell} \in$ $\mathbb{R}^{\left(H W D^{\ell}\right)
                                            \times D^{\ell+1}}$, such that each column of $F^{\ell}$ is a vectrozied
                                            version of one of the $D^{\ell+1}$ kernels. This matrix expression of the
                                            tensor input and kernel is needed for the convolution product $X^{\ell} *
                                            K^{\ell}=X^{\ell+1}$ to be written as a matrix multiplication, such that
                                            each column of the output matrix $Y^{\ell+1} \in
                                            \mathbb{R}^{\left(H^{\ell+1} W^{\ell+1}\right) \times D^{\ell+1}}$ is a
                                            first vectorized form of one of the $D^{\ell+1}$ channels of $X^{\ell+1}$.
                                            Later, we will dicuss how quantum computing, specifically quantum
                                            parallelism can not only be applied, but improve the time complexity of this
                                            step. Lastly, quantum states proportional to the rows of input $A^{\ell}$
                                            and $F^{\ell}$ are used, denoted $|A_p^{\ell}\rangle$ and
                                            $|F_q^{\ell}\rangle$ respectively. These quantum states are defined as: $
                                            \left|A_p^{\ell}\right\rangle=$ $\frac{1}{\left\|A_p^{\ell}\right\|}
                                            \sum_{r=0}^{H W D^{\ell}-1} A_{p r}^{\ell}|r\rangle$
                                            $\left|F_q^{\ell}\right\rangle=$ $\frac{1}{\left\|F_q^{\ell}\right\|}
                                            \sum_{s=0}^{D^{\ell+1}-1} F_{s q}^{\ell}|s\rangle$ and will continue to be
                                            used throughout this section. Quantum Convolution The authors describe
                                            procedure for the quantum convolutional in the following four sequential
                                            steps: inner product estimation, non linearity, quantum sampling, and then
                                            quantum random access memory update and pooling. Inner Product Estimation
                                            First we load input row vector $A_p^{\ell}$ and kernel vector $F_q^{\ell}$
                                            into quantum states by quering QRAM in the following manner:
                                            $\left\{\begin{aligned} |p\rangle|0\rangle &
                                            \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\ |q\rangle|0\rangle &
                                            \mapsto|q\rangle\left|F_q^{\ell}\right\rangle \end{aligned}\right.$ This is
                                            done so that following mapping can be perfromed with the two vectors:
                                            $\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto $ $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle$ Here
                                            $\bar{P}_{p q}$ is the inner product estimation of $A_p^{\ell}$ and
                                            $F_q^{\ell}$. The "true" value of the inner product is calculated as $P_{p
                                            q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}$, such
                                            that the inner product estimation differs by a chosen constant $\epsilon$.
                                            Here, the normalization factor is $K=\sqrt{H^{\ell+1} W^{\ell+1}
                                            D^{\ell+1}}$ and $|g_{pq}\rangle$ is some garbage state. Non Linearity Our
                                            obtained approximated convolution output $\bar{P}_{p q}$ or
                                            $\bar{Y}^{\ell+1}$ differs from the "true" convolution output $Y_{p,
                                            q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)$ by $\epsilon$. Now, we
                                            apply a non-linear function $f$ as a boolean circut giving the quantum
                                            state: $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum ConvolutionThe authors describe procedure for the
                                            quantum convolutional in the following four sequential steps: inner product
                                            estimation, non linearity, quantum sampling, and then quantum random access
                                            memory update and pooling. inner product estimationnon linearityquantum
                                            samplingquantum random access memory update and poolingInner Product
                                            Estimation Inner Product EstimationFirst we load input row vector
                                            $A_p^{\ell}$ and kernel vector $F_q^{\ell}$ into quantum states by quering
                                            QRAM in the following manner: $\left\{\begin{aligned} |p\rangle|0\rangle &
                                            \mapsto|p\rangle\left|A_p^{\ell}\right\rangle \\ |q\rangle|0\rangle &
                                            \mapsto|q\rangle\left|F_q^{\ell}\right\rangle \end{aligned}\right.$ This is
                                            done so that following mapping can be perfromed with the two vectors:
                                            $\frac{1}{K} \sum_{p, q}|p\rangle|q\rangle \mapsto $ $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|\bar{P}_{p q}\rangle\left|g_{p q}\right\rangle$ Here
                                            $\bar{P}_{p q}$ is the inner product estimation of $A_p^{\ell}$ and
                                            $F_q^{\ell}$. The "true" value of the inner product is calculated as $P_{p
                                            q}=\frac{1+\left\langle A_p^{\ell} \mid F_q^{\ell}\right\rangle}{2}$, such
                                            that the inner product estimation differs by a chosen constant $\epsilon$.
                                            Here, the normalization factor is $K=\sqrt{H^{\ell+1} W^{\ell+1}
                                            D^{\ell+1}}$ and $|g_{pq}\rangle$ is some garbage state. Non Linearity Non
                                            LinearityOur obtained approximated convolution output $\bar{P}_{p q}$ or
                                            $\bar{Y}^{\ell+1}$ differs from the "true" convolution output $Y_{p,
                                            q}^{\ell+1}=\left(A_p^{\ell}, F_q^{\ell}\right)$ by $\epsilon$. Now, we
                                            apply a non-linear function $f$ as a boolean circut giving the quantum
                                            state: $\frac{1}{K} \sum_{p,
                                            q}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum Sampling Conditional Rotation and Amplitude
                                            Amplification To procure the state below, states are conditionally rotated
                                            and the probabilistic amplitudes are amplified, such that we arrive at the
                                            state: $\frac{1}{K} \sum_{p, q} \alpha_{p
                                            q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum tomography is performed with precision $\eta$ so
                                            that all values and positions $(p, q, f(\bar{Y}_{p q}^{\ell+1}))$ are
                                            obtained with a high probability. Values above $\eta$ are known exactly,
                                            while values that are less than or equal to $\eta$ are set to $0$. Quantum
                                            SamplingConditional Rotation and Amplitude Amplification Conditional
                                            Rotation and Amplitude AmplificationTo procure the state below, states are
                                            conditionally rotated and the probabilistic amplitudes are amplified, such
                                            that we arrive at the state: $\frac{1}{K} \sum_{p, q} \alpha_{p
                                            q}^{\prime}|p\rangle|q\rangle|f(\bar{Y}_{p,q}^{\ell+1})\rangle\left|g_{p
                                            q}\right\rangle$ Quantum tomography is performed with precision $\eta$ so
                                            that all values and positions $(p, q, f(\bar{Y}_{p q}^{\ell+1}))$ are
                                            obtained with a high probability. Values above $\eta$ are known exactly,
                                            while values that are less than or equal to $\eta$ are set to $0$. QRAM
                                            Update and Pooling Next, QRAM needs to updated with the value for the next
                                            layer, which is $A^{\ell+1}$, while sampling. Pooling needs to be
                                            implemented in this step as well, either through a specific update or by
                                            using a QRAM data data structure. We will review quantum pooling more in
                                            depth later. QRAM Update and PoolingNext, QRAM needs to updated with the
                                            value for the next layer, which is $A^{\ell+1}$, while sampling. Pooling
                                            needs to be implemented in this step as well, either through a specific
                                            update or by using a QRAM data data structure. We will review quantum
                                            pooling more in depth later. quantum poolingQuantum Pooling At the end of
                                            layer $\ell$, the pooling operation of size $P$ is performed on the
                                            convolution layer output $f(X^{\ell+1})$, yielding the tensor after pooling
                                            $\tilde{X}^{\ell+1}$. Below, thee authors provide a figure shows a $2\times
                                            2$ tensor pooling such that different pooling regions having seperate
                                            colors: Image Source: $[1]$ Here, for some point at position $(i^{\ell+1},
                                            j^{\ell+1}, d^{\ell+1})$ in $f(X^{\ell+1})$, the pooling region it
                                            corresponds to is at postion $(\tilde{i}^{\ell+1}, \tilde{j}^{\ell+1},
                                            \tilde{d}^{\ell+1})$ in $\tilde{X}^{\ell+1}$, such that:
                                            $\left\{\begin{array}{l} \tilde{d}^{\ell+1}=d^{\ell+1} \\
                                            \tilde{j}^{\ell+1}=\left\lfloor\frac{j^{\ell+1}}{P}\right\rfloor \\
                                            \tilde{i}^{\ell+1}=\left\lfloor\frac{i^{\ell+1}}{P}\right\rfloor
                                            \end{array}\right.$ This pooling operation occurs at the end of the layer
                                            during the QRAM update and pooling operation, such that the sampled values
                                            are stored according to the pooling layers. Note, the authors state this
                                            kind of pooling can be efficiently applied to their QCNN structure, which we
                                            will look at to continue. Let's start by denoting output of layer $\ell$
                                            after quantum tomography $\mathcal{X}^{\ell+1}$. Next, quantum pooling is
                                            applied to the yielding $\tilde{\mathcal{X}}^{\ell+1}$, which has dimensions
                                            $\frac{H^{\ell+1}}{P} \times \frac{W^{\ell+1}}{P} \times D^{\ell+1}$.
                                            $\tilde{\mathcal{X}}^{\ell+1}$ will be used as the input for layer $\ell +
                                            1$ and the values for $\tilde{\mathcal{X}}^{\ell+1}$ are stored in QRAM. The
                                            valyes are use to create trees $\tilde{T}_{p^{\prime}}^{\ell+1}$, such that
                                            they relate to the matrix expansion $\tilde{A}^{\ell+1}$ and that we will
                                            talk about later. It is important to note that $\mathcal{X}^{\ell+1}$ cannot
                                            be know before quantum tomography is over. Thus, QRAM updating must be
                                            changed to pool in an online fashion each time a sample from
                                            $|f(\bar{X}^{\ell+1})\rangle$ is drawn. To continue. Quantum PoolingAt the
                                            end of layer $\ell$, the pooling operation of size $P$ is performed on the
                                            convolution layer output $f(X^{\ell+1})$, yielding the tensor after pooling
                                            $\tilde{X}^{\ell+1}$. Below, thee authors provide a figure shows a $2\times
                                            2$ tensor pooling such that different pooling regions having seperate
                                            colors: Image Source: $[1]$ $[1]$ Here, for some point at position
                                            $(i^{\ell+1}, j^{\ell+1}, d^{\ell+1})$ in $f(X^{\ell+1})$, the pooling
                                            region it corresponds to is at postion $(\tilde{i}^{\ell+1},
                                            \tilde{j}^{\ell+1}, \tilde{d}^{\ell+1})$ in $\tilde{X}^{\ell+1}$, such that:
                                            $\left\{\begin{array}{l} \tilde{d}^{\ell+1}=d^{\ell+1} \\
                                            \tilde{j}^{\ell+1}=\left\lfloor\frac{j^{\ell+1}}{P}\right\rfloor \\
                                            \tilde{i}^{\ell+1}=\left\lfloor\frac{i^{\ell+1}}{P}\right\rfloor
                                            \end{array}\right.$ This pooling operation occurs at the end of the layer
                                            during the QRAM update and pooling operation, such that the sampled values
                                            are stored according to the pooling layers. Note, the authors state this
                                            kind of pooling can be efficiently applied to their QCNN structure, which we
                                            will look at to continue. QRAM update and poolingLet's start by denoting
                                            output of layer $\ell$ after quantum tomography $\mathcal{X}^{\ell+1}$.
                                            Next, quantum pooling is applied to the yielding
                                            $\tilde{\mathcal{X}}^{\ell+1}$, which has dimensions $\frac{H^{\ell+1}}{P}
                                            \times \frac{W^{\ell+1}}{P} \times D^{\ell+1}$.
                                            $\tilde{\mathcal{X}}^{\ell+1}$ will be used as the input for layer $\ell +
                                            1$ and the values for $\tilde{\mathcal{X}}^{\ell+1}$ are stored in QRAM. The
                                            valyes are use to create trees $\tilde{T}_{p^{\prime}}^{\ell+1}$, such that
                                            they relate to the matrix expansion $\tilde{A}^{\ell+1}$ and that we will
                                            talk about later. It is important to note that $\mathcal{X}^{\ell+1}$ cannot
                                            be know before quantum tomography is over. Thus, QRAM updating must be
                                            changed to pool in an online fashion each time a sample from
                                            $|f(\bar{X}^{\ell+1})\rangle$ is drawn. To continue. Quantum Time Complexity
                                            The quantum time complexity of one forward pass through the convolution
                                            layer $\ell$, where $\widetilde{O}$ hides the polylogarithmic factors, is:
                                            $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right)\right)}}\right)$
                                            which can be written also as: $\widetilde{O}\left(\sigma H^{\ell+1}
                                            W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right)$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right)$ To continue. Quantum Time ComplexityThe quantum
                                            time complexity of one forward pass through the convolution layer $\ell$,
                                            where $\widetilde{O}$ hides the polylogarithmic factors, is:
                                            $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right)\right)}}\right)$
                                            which can be written also as: $\widetilde{O}\left(\sigma H^{\ell+1}
                                            W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right)$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right)$ To continue. Quantum Back Propagation To
                                            continue. Quantum Back PropagationTo continue. Quantum Computing
                                            Preliminaries You can read about quantum computing preliminaries on my
                                            website here. Quantum Computing PreliminariesYou can read about quantum
                                            computing preliminaries on my website here. here
                                            [https://www.contextswitching.org/neuro/topologicalneuroscience]
                                            Topological Neuroscience - Context Switching Topological Neuroscience Graph
                                            theory is one main theoretical framework used to model, estimate, and
                                            simulate brain networks in complex network science. A graph is a composition
                                            of interconnected elements of vertices and edges, where vertices can
                                            represent neural structures and edges represent the functional connectivity
                                            between pairs of vertices. To reconstruct the brain network imaging
                                            modalities used are mainly the resting-state functional MRI rsfMRI. This
                                            imaging indirectly measures brain activity while a subject is at rest. Graph
                                            Theory Preliminaries Degree The vertex degree quantifies the total numbers
                                            of vertex connections that exist in an uniderected binary network. The
                                            vertex degree for an undirected weighted network which represents the sum of
                                            all edges of the vertex and is equivalent to its degree centraility. We can
                                            look at the vertex degree anaglously to the vertex strength and is useful to
                                            give an idea of how densely individual vertices are connected. It is
                                            computed as: $C_{D}=s_{i}=\sum_{j\neq i}^{}w_{ij}$ where $w_{ij}$ is the
                                            respective weight of the edge that links vertex $i$ and vertex $j$.
                                            Clustering Coefficent
                                            $Cl=\frac{2}{s_{i}(s_{i}-1)}\sum_{j,h}^{}(\hat{w}_{ij}\hat{w}_{jh}\hat{w}_{hi})^{\frac{1}{3}}$
                                            where $s_{i}$ is the degree of vertex $i$ and the edge wegihts are
                                            normalised by the maximum weight in the network. We can represent this
                                            normilization as: $\hat{w}_{ij}=\frac{w_{ij}}{max(w)}$ Centrailities In
                                            order to measure a vertex's importance in a netwrok and consider its
                                            neightbour's influence we ascertain its eigenvector centrality which is
                                            degree-based. This is so we can consider both the vertices quantity and
                                            quality its connections. We can compute the eigenvector centraility by
                                            computing the spectra of the adjacency matrix $Ax=\lambda x$. Where $A$
                                            represent the adjacency matrix and $x$ is the eigenvector with a value of
                                            $\lambda$ of $A$. We can then define the eigenvector conetrality of some
                                            vertex $i$ as: $C_{E}(i)=\frac{1}{\lambda_{1}}\sum_{j=1}^{N}A_{ij}x_{j}$
                                            Hypergraphs Generally, hypergraphs is a graph where edges can join any
                                            number of vertices. An undirected hypergraph $H$ can be defined as a pair
                                            $H=(V,HE)$. Such that $V$ represents the set of vertices and $HE$ represents
                                            the set of hyperedges. More Graph Theory Fun This concludes most of the
                                            information neccesary for this page. However, if you are having fun with
                                            graph theory, feel free to read more on my Graph Theory page. Multivariate
                                            Information Preliminary Shannon Entropy You can read about Shannon entropy
                                            in my Entropy and Average Code Length section. However, the important part
                                            to know is how to calculate the amount of uncertainty or suprise present in
                                            the probability distribution of a single random variable $X$ or the Shannon
                                            entropy of $X$. The Shannon Entropy, or just entropy, on $X$ is denoted as
                                            $H(X)$, given with a probabilty distribution of $p(x)$, is defined as:
                                            $H(X)=-k\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$
                                            $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ where $n$ represents the
                                            cartesian length of $\left[N_{x}\right]=\left\{1,...N_{x}\right\}$ which is
                                            the codomain or alphabet of $X$. High-Order Functional Hubs in the Human
                                            Brain Section Reference: Emergence of High-Order Functional Hubs in the
                                            Human Brain by: Fernando A.N. Santos, Prejaas K.B. Tewarie, Pierre Baudot,
                                            et al. $^{[1]}$ High-Order Neural Networks Currently, neuroscience research
                                            predominantly focuses on a microscopic examination of neuronal architecture
                                            rooted in pairwise interactions. Consequently, mesoscopic and macroscopic
                                            studies inherit this dyadic perspective, which potentially provides an
                                            incomplete understanding of complex systems. Important observations like the
                                            regulatory role of astrocytes in both structural and synaptic plasticity.
                                            The authors cite that astrocytes regulate hetero-synaptic interactions as
                                            well as the heterogeneity of synaptic strength. These crucial regulatory
                                            astrocytes enable higher-order interactions, thus, forcing a shift in
                                            perspective of current neural interaction theories to incorporate
                                            higher-order structures. Image Source: See Reference [1] Let's more cloesley
                                            examining why higher-order structures in brain networks is important for
                                            gaining a more informed neuroscientific understanding. The figure above
                                            illustrates the neccesity for the incorporation of such higher-order
                                            strcutures. This is because the illustration shows how interactions in
                                            higher-order complex systems, especially those in functional brain netwroks,
                                            are misrepresented as the approximations of the sum of pairwise
                                            interactions. However, the authors suggest, with a sutiable high-order
                                            connectivity rule, we can define higher-order hubs in a network, that will
                                            circumvent the combinitorial complexity assoicated with enumerating only the
                                            important high-order edges in known priority hyperedge networks. Hypergraph
                                            Adjacency Matrix Algorithm Image Source: See Reference [1] The
                                            authors'$^{[1]}$ algorithm for representing statistical structures of time
                                            series as hypergraphs is given in the five steps below and depicted in the
                                            figure above. Let the input be an $N$ time series. First, we must build the
                                            biological neural network hypergraph. Assign one node per time series. For
                                            all corresponding time series in the rsfMRI, each node represents a
                                            different brain region. Next, choose an order $k\in \left\{3,...,N\right\}$.
                                            Then, between all possible different groups of $k$ nodes with a high-order
                                            interdependency metric, calculate the $\binom{N}{k}$ $k$-order associated
                                            terms. The reason hypergraphs are chosen to represent pairwise connectivity,
                                            is because different multivariate similarities can be measured and
                                            considered. The authors focus on two infomraiton-theortic measures:
                                            Interaction Information and Total Correlation. Interactive information
                                            quantifies a single tuple for statisical dependencies. Total Correlation
                                            quantifies the sum of all statisitcal dependencies over all tuple subsets.
                                            For hyperedge selection, the number of $k$-order iteractions in $N$ nodes
                                            grows as $N^{k}$. Hypergprah, where significat network edges is choosen
                                            because of there representation of high-order connectivity. The procedure
                                            used is to encode data relating to the encoding $k$-uniform hypergraph is
                                            termed a "hyper-adjacency matrix." A benefit of the hyper-adjacency matrix
                                            encoding is the representation of a low adjacency matrix representation for
                                            simplicial complexes in uniform hypergraphs. This encoding type can also be
                                            adapted to develop vector centralities in hypergraphs as well. Which is a
                                            good use case in topological neuroscience. Formally, two hyperedges of $k$
                                            dimensions are connected if they share a $k-1$-hyperedge. The authors
                                            mentioned the final step in their algorithm, after multivariate signal
                                            processing and specification of the hyper-adjacency matrix, the topological
                                            features from network science can be applied. Here, given Eigenvector
                                            centrality, an extension of this is used to investigate high-order hubs in
                                            the human brain. Where, given hubs as triplets with higher eigenvector
                                            centrality, representing hyper-adjacency matrices the calculation of
                                            modularity and betweenness centrality in different neurological contexts. To
                                            continue. Total Correlation and the Brain's Visual System To continue.
                                            Glossary Topolgical Space You can read about topological space on my website
                                            here. Resting State Functional Magnetic Resonance Imaging Resting-state fMRI
                                            examines the brain's functional architecture by assessing spontaneous
                                            low-frequency fluctuations in the BOLD signal. Topological Neuroscience -
                                            Context Switching Topological Neuroscience - Context SwitchingTopological
                                            Neuroscience Graph theory is one main theoretical framework used to model,
                                            estimate, and simulate brain networks in complex network science. A graph is
                                            a composition of interconnected elements of vertices and edges, where
                                            vertices can represent neural structures and edges represent the functional
                                            connectivity between pairs of vertices. To reconstruct the brain network
                                            imaging modalities used are mainly the resting-state functional MRI rsfMRI.
                                            This imaging indirectly measures brain activity while a subject is at rest.
                                            Graph Theory Preliminaries Degree The vertex degree quantifies the total
                                            numbers of vertex connections that exist in an uniderected binary network.
                                            The vertex degree for an undirected weighted network which represents the
                                            sum of all edges of the vertex and is equivalent to its degree centraility.
                                            We can look at the vertex degree anaglously to the vertex strength and is
                                            useful to give an idea of how densely individual vertices are connected. It
                                            is computed as: $C_{D}=s_{i}=\sum_{j\neq i}^{}w_{ij}$ where $w_{ij}$ is the
                                            respective weight of the edge that links vertex $i$ and vertex $j$.
                                            Clustering Coefficent
                                            $Cl=\frac{2}{s_{i}(s_{i}-1)}\sum_{j,h}^{}(\hat{w}_{ij}\hat{w}_{jh}\hat{w}_{hi})^{\frac{1}{3}}$
                                            where $s_{i}$ is the degree of vertex $i$ and the edge wegihts are
                                            normalised by the maximum weight in the network. We can represent this
                                            normilization as: $\hat{w}_{ij}=\frac{w_{ij}}{max(w)}$ Centrailities In
                                            order to measure a vertex's importance in a netwrok and consider its
                                            neightbour's influence we ascertain its eigenvector centrality which is
                                            degree-based. This is so we can consider both the vertices quantity and
                                            quality its connections. We can compute the eigenvector centraility by
                                            computing the spectra of the adjacency matrix $Ax=\lambda x$. Where $A$
                                            represent the adjacency matrix and $x$ is the eigenvector with a value of
                                            $\lambda$ of $A$. We can then define the eigenvector conetrality of some
                                            vertex $i$ as: $C_{E}(i)=\frac{1}{\lambda_{1}}\sum_{j=1}^{N}A_{ij}x_{j}$
                                            Hypergraphs Generally, hypergraphs is a graph where edges can join any
                                            number of vertices. An undirected hypergraph $H$ can be defined as a pair
                                            $H=(V,HE)$. Such that $V$ represents the set of vertices and $HE$ represents
                                            the set of hyperedges. More Graph Theory Fun This concludes most of the
                                            information neccesary for this page. However, if you are having fun with
                                            graph theory, feel free to read more on my Graph Theory page. Multivariate
                                            Information Preliminary Shannon Entropy You can read about Shannon entropy
                                            in my Entropy and Average Code Length section. However, the important part
                                            to know is how to calculate the amount of uncertainty or suprise present in
                                            the probability distribution of a single random variable $X$ or the Shannon
                                            entropy of $X$. The Shannon Entropy, or just entropy, on $X$ is denoted as
                                            $H(X)$, given with a probabilty distribution of $p(x)$, is defined as:
                                            $H(X)=-k\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$
                                            $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ where $n$ represents the
                                            cartesian length of $\left[N_{x}\right]=\left\{1,...N_{x}\right\}$ which is
                                            the codomain or alphabet of $X$. High-Order Functional Hubs in the Human
                                            Brain Section Reference: Emergence of High-Order Functional Hubs in the
                                            Human Brain by: Fernando A.N. Santos, Prejaas K.B. Tewarie, Pierre Baudot,
                                            et al. $^{[1]}$ High-Order Neural Networks Currently, neuroscience research
                                            predominantly focuses on a microscopic examination of neuronal architecture
                                            rooted in pairwise interactions. Consequently, mesoscopic and macroscopic
                                            studies inherit this dyadic perspective, which potentially provides an
                                            incomplete understanding of complex systems. Important observations like the
                                            regulatory role of astrocytes in both structural and synaptic plasticity.
                                            The authors cite that astrocytes regulate hetero-synaptic interactions as
                                            well as the heterogeneity of synaptic strength. These crucial regulatory
                                            astrocytes enable higher-order interactions, thus, forcing a shift in
                                            perspective of current neural interaction theories to incorporate
                                            higher-order structures. Image Source: See Reference [1] Let's more cloesley
                                            examining why higher-order structures in brain networks is important for
                                            gaining a more informed neuroscientific understanding. The figure above
                                            illustrates the neccesity for the incorporation of such higher-order
                                            strcutures. This is because the illustration shows how interactions in
                                            higher-order complex systems, especially those in functional brain netwroks,
                                            are misrepresented as the approximations of the sum of pairwise
                                            interactions. However, the authors suggest, with a sutiable high-order
                                            connectivity rule, we can define higher-order hubs in a network, that will
                                            circumvent the combinitorial complexity assoicated with enumerating only the
                                            important high-order edges in known priority hyperedge networks. Hypergraph
                                            Adjacency Matrix Algorithm Image Source: See Reference [1] The
                                            authors'$^{[1]}$ algorithm for representing statistical structures of time
                                            series as hypergraphs is given in the five steps below and depicted in the
                                            figure above. Let the input be an $N$ time series. First, we must build the
                                            biological neural network hypergraph. Assign one node per time series. For
                                            all corresponding time series in the rsfMRI, each node represents a
                                            different brain region. Next, choose an order $k\in \left\{3,...,N\right\}$.
                                            Then, between all possible different groups of $k$ nodes with a high-order
                                            interdependency metric, calculate the $\binom{N}{k}$ $k$-order associated
                                            terms. The reason hypergraphs are chosen to represent pairwise connectivity,
                                            is because different multivariate similarities can be measured and
                                            considered. The authors focus on two infomraiton-theortic measures:
                                            Interaction Information and Total Correlation. Interactive information
                                            quantifies a single tuple for statisical dependencies. Total Correlation
                                            quantifies the sum of all statisitcal dependencies over all tuple subsets.
                                            For hyperedge selection, the number of $k$-order iteractions in $N$ nodes
                                            grows as $N^{k}$. Hypergprah, where significat network edges is choosen
                                            because of there representation of high-order connectivity. The procedure
                                            used is to encode data relating to the encoding $k$-uniform hypergraph is
                                            termed a "hyper-adjacency matrix." A benefit of the hyper-adjacency matrix
                                            encoding is the representation of a low adjacency matrix representation for
                                            simplicial complexes in uniform hypergraphs. This encoding type can also be
                                            adapted to develop vector centralities in hypergraphs as well. Which is a
                                            good use case in topological neuroscience. Formally, two hyperedges of $k$
                                            dimensions are connected if they share a $k-1$-hyperedge. The authors
                                            mentioned the final step in their algorithm, after multivariate signal
                                            processing and specification of the hyper-adjacency matrix, the topological
                                            features from network science can be applied. Here, given Eigenvector
                                            centrality, an extension of this is used to investigate high-order hubs in
                                            the human brain. Where, given hubs as triplets with higher eigenvector
                                            centrality, representing hyper-adjacency matrices the calculation of
                                            modularity and betweenness centrality in different neurological contexts. To
                                            continue. Total Correlation and the Brain's Visual System To continue.
                                            Glossary Topolgical Space You can read about topological space on my website
                                            here. Resting State Functional Magnetic Resonance Imaging Resting-state fMRI
                                            examines the brain's functional architecture by assessing spontaneous
                                            low-frequency fluctuations in the BOLD signal. Topological Neuroscience
                                            Graph theory is one main theoretical framework used to model, estimate, and
                                            simulate brain networks in complex network science. A graph is a composition
                                            of interconnected elements of vertices and edges, where vertices can
                                            represent neural structures and edges represent the functional connectivity
                                            between pairs of vertices. To reconstruct the brain network imaging
                                            modalities used are mainly the resting-state functional MRI rsfMRI. This
                                            imaging indirectly measures brain activity while a subject is at rest. Graph
                                            Theory Preliminaries Degree The vertex degree quantifies the total numbers
                                            of vertex connections that exist in an uniderected binary network. The
                                            vertex degree for an undirected weighted network which represents the sum of
                                            all edges of the vertex and is equivalent to its degree centraility. We can
                                            look at the vertex degree anaglously to the vertex strength and is useful to
                                            give an idea of how densely individual vertices are connected. It is
                                            computed as: $C_{D}=s_{i}=\sum_{j\neq i}^{}w_{ij}$ where $w_{ij}$ is the
                                            respective weight of the edge that links vertex $i$ and vertex $j$.
                                            Clustering Coefficent
                                            $Cl=\frac{2}{s_{i}(s_{i}-1)}\sum_{j,h}^{}(\hat{w}_{ij}\hat{w}_{jh}\hat{w}_{hi})^{\frac{1}{3}}$
                                            where $s_{i}$ is the degree of vertex $i$ and the edge wegihts are
                                            normalised by the maximum weight in the network. We can represent this
                                            normilization as: $\hat{w}_{ij}=\frac{w_{ij}}{max(w)}$ Centrailities In
                                            order to measure a vertex's importance in a netwrok and consider its
                                            neightbour's influence we ascertain its eigenvector centrality which is
                                            degree-based. This is so we can consider both the vertices quantity and
                                            quality its connections. We can compute the eigenvector centraility by
                                            computing the spectra of the adjacency matrix $Ax=\lambda x$. Where $A$
                                            represent the adjacency matrix and $x$ is the eigenvector with a value of
                                            $\lambda$ of $A$. We can then define the eigenvector conetrality of some
                                            vertex $i$ as: $C_{E}(i)=\frac{1}{\lambda_{1}}\sum_{j=1}^{N}A_{ij}x_{j}$
                                            Hypergraphs Generally, hypergraphs is a graph where edges can join any
                                            number of vertices. An undirected hypergraph $H$ can be defined as a pair
                                            $H=(V,HE)$. Such that $V$ represents the set of vertices and $HE$ represents
                                            the set of hyperedges. More Graph Theory Fun This concludes most of the
                                            information neccesary for this page. However, if you are having fun with
                                            graph theory, feel free to read more on my Graph Theory page. Multivariate
                                            Information Preliminary Shannon Entropy You can read about Shannon entropy
                                            in my Entropy and Average Code Length section. However, the important part
                                            to know is how to calculate the amount of uncertainty or suprise present in
                                            the probability distribution of a single random variable $X$ or the Shannon
                                            entropy of $X$. The Shannon Entropy, or just entropy, on $X$ is denoted as
                                            $H(X)$, given with a probabilty distribution of $p(x)$, is defined as:
                                            $H(X)=-k\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$
                                            $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ where $n$ represents the
                                            cartesian length of $\left[N_{x}\right]=\left\{1,...N_{x}\right\}$ which is
                                            the codomain or alphabet of $X$. High-Order Functional Hubs in the Human
                                            Brain Section Reference: Emergence of High-Order Functional Hubs in the
                                            Human Brain by: Fernando A.N. Santos, Prejaas K.B. Tewarie, Pierre Baudot,
                                            et al. $^{[1]}$ High-Order Neural Networks Currently, neuroscience research
                                            predominantly focuses on a microscopic examination of neuronal architecture
                                            rooted in pairwise interactions. Consequently, mesoscopic and macroscopic
                                            studies inherit this dyadic perspective, which potentially provides an
                                            incomplete understanding of complex systems. Important observations like the
                                            regulatory role of astrocytes in both structural and synaptic plasticity.
                                            The authors cite that astrocytes regulate hetero-synaptic interactions as
                                            well as the heterogeneity of synaptic strength. These crucial regulatory
                                            astrocytes enable higher-order interactions, thus, forcing a shift in
                                            perspective of current neural interaction theories to incorporate
                                            higher-order structures. Image Source: See Reference [1] Let's more cloesley
                                            examining why higher-order structures in brain networks is important for
                                            gaining a more informed neuroscientific understanding. The figure above
                                            illustrates the neccesity for the incorporation of such higher-order
                                            strcutures. This is because the illustration shows how interactions in
                                            higher-order complex systems, especially those in functional brain netwroks,
                                            are misrepresented as the approximations of the sum of pairwise
                                            interactions. However, the authors suggest, with a sutiable high-order
                                            connectivity rule, we can define higher-order hubs in a network, that will
                                            circumvent the combinitorial complexity assoicated with enumerating only the
                                            important high-order edges in known priority hyperedge networks. Hypergraph
                                            Adjacency Matrix Algorithm Image Source: See Reference [1] The
                                            authors'$^{[1]}$ algorithm for representing statistical structures of time
                                            series as hypergraphs is given in the five steps below and depicted in the
                                            figure above. Let the input be an $N$ time series. First, we must build the
                                            biological neural network hypergraph. Assign one node per time series. For
                                            all corresponding time series in the rsfMRI, each node represents a
                                            different brain region. Next, choose an order $k\in \left\{3,...,N\right\}$.
                                            Then, between all possible different groups of $k$ nodes with a high-order
                                            interdependency metric, calculate the $\binom{N}{k}$ $k$-order associated
                                            terms. The reason hypergraphs are chosen to represent pairwise connectivity,
                                            is because different multivariate similarities can be measured and
                                            considered. The authors focus on two infomraiton-theortic measures:
                                            Interaction Information and Total Correlation. Interactive information
                                            quantifies a single tuple for statisical dependencies. Total Correlation
                                            quantifies the sum of all statisitcal dependencies over all tuple subsets.
                                            For hyperedge selection, the number of $k$-order iteractions in $N$ nodes
                                            grows as $N^{k}$. Hypergprah, where significat network edges is choosen
                                            because of there representation of high-order connectivity. The procedure
                                            used is to encode data relating to the encoding $k$-uniform hypergraph is
                                            termed a "hyper-adjacency matrix." A benefit of the hyper-adjacency matrix
                                            encoding is the representation of a low adjacency matrix representation for
                                            simplicial complexes in uniform hypergraphs. This encoding type can also be
                                            adapted to develop vector centralities in hypergraphs as well. Which is a
                                            good use case in topological neuroscience. Formally, two hyperedges of $k$
                                            dimensions are connected if they share a $k-1$-hyperedge. The authors
                                            mentioned the final step in their algorithm, after multivariate signal
                                            processing and specification of the hyper-adjacency matrix, the topological
                                            features from network science can be applied. Here, given Eigenvector
                                            centrality, an extension of this is used to investigate high-order hubs in
                                            the human brain. Where, given hubs as triplets with higher eigenvector
                                            centrality, representing hyper-adjacency matrices the calculation of
                                            modularity and betweenness centrality in different neurological contexts. To
                                            continue. Total Correlation and the Brain's Visual System To continue.
                                            Glossary Topolgical Space You can read about topological space on my website
                                            here. Resting State Functional Magnetic Resonance Imaging Resting-state fMRI
                                            examines the brain's functional architecture by assessing spontaneous
                                            low-frequency fluctuations in the BOLD signal. Topological Neuroscience
                                            Graph theory is one main theoretical framework used to model, estimate, and
                                            simulate brain networks in complex network science. A graph is a composition
                                            of interconnected elements of vertices and edges, where vertices can
                                            represent neural structures and edges represent the functional connectivity
                                            between pairs of vertices. To reconstruct the brain network imaging
                                            modalities used are mainly the resting-state functional MRI rsfMRI. This
                                            imaging indirectly measures brain activity while a subject is at rest. Graph
                                            Theory Preliminaries Degree The vertex degree quantifies the total numbers
                                            of vertex connections that exist in an uniderected binary network. The
                                            vertex degree for an undirected weighted network which represents the sum of
                                            all edges of the vertex and is equivalent to its degree centraility. We can
                                            look at the vertex degree anaglously to the vertex strength and is useful to
                                            give an idea of how densely individual vertices are connected. It is
                                            computed as: $C_{D}=s_{i}=\sum_{j\neq i}^{}w_{ij}$ where $w_{ij}$ is the
                                            respective weight of the edge that links vertex $i$ and vertex $j$.
                                            Clustering Coefficent
                                            $Cl=\frac{2}{s_{i}(s_{i}-1)}\sum_{j,h}^{}(\hat{w}_{ij}\hat{w}_{jh}\hat{w}_{hi})^{\frac{1}{3}}$
                                            where $s_{i}$ is the degree of vertex $i$ and the edge wegihts are
                                            normalised by the maximum weight in the network. We can represent this
                                            normilization as: $\hat{w}_{ij}=\frac{w_{ij}}{max(w)}$ Centrailities In
                                            order to measure a vertex's importance in a netwrok and consider its
                                            neightbour's influence we ascertain its eigenvector centrality which is
                                            degree-based. This is so we can consider both the vertices quantity and
                                            quality its connections. We can compute the eigenvector centraility by
                                            computing the spectra of the adjacency matrix $Ax=\lambda x$. Where $A$
                                            represent the adjacency matrix and $x$ is the eigenvector with a value of
                                            $\lambda$ of $A$. We can then define the eigenvector conetrality of some
                                            vertex $i$ as: $C_{E}(i)=\frac{1}{\lambda_{1}}\sum_{j=1}^{N}A_{ij}x_{j}$
                                            Hypergraphs Generally, hypergraphs is a graph where edges can join any
                                            number of vertices. An undirected hypergraph $H$ can be defined as a pair
                                            $H=(V,HE)$. Such that $V$ represents the set of vertices and $HE$ represents
                                            the set of hyperedges. More Graph Theory Fun This concludes most of the
                                            information neccesary for this page. However, if you are having fun with
                                            graph theory, feel free to read more on my Graph Theory page. Topological
                                            NeuroscienceGraph theory is one main theoretical framework used to model,
                                            estimate, and simulate brain networks in complex network science. A graph is
                                            a composition of interconnected elements of vertices and edges, where
                                            vertices can represent neural structures and edges represent the functional
                                            connectivity between pairs of vertices. To reconstruct the brain network
                                            imaging modalities used are mainly the resting-state functional MRI rsfMRI.
                                            This imaging indirectly measures brain activity while a subject is at rest.
                                            Graph Theory Preliminaries Degree The vertex degree quantifies the total
                                            numbers of vertex connections that exist in an uniderected binary network.
                                            The vertex degree for an undirected weighted network which represents the
                                            sum of all edges of the vertex and is equivalent to its degree centraility.
                                            We can look at the vertex degree anaglously to the vertex strength and is
                                            useful to give an idea of how densely individual vertices are connected. It
                                            is computed as: $C_{D}=s_{i}=\sum_{j\neq i}^{}w_{ij}$ where $w_{ij}$ is the
                                            respective weight of the edge that links vertex $i$ and vertex $j$.
                                            Clustering Coefficent
                                            $Cl=\frac{2}{s_{i}(s_{i}-1)}\sum_{j,h}^{}(\hat{w}_{ij}\hat{w}_{jh}\hat{w}_{hi})^{\frac{1}{3}}$
                                            where $s_{i}$ is the degree of vertex $i$ and the edge wegihts are
                                            normalised by the maximum weight in the network. We can represent this
                                            normilization as: $\hat{w}_{ij}=\frac{w_{ij}}{max(w)}$ Centrailities In
                                            order to measure a vertex's importance in a netwrok and consider its
                                            neightbour's influence we ascertain its eigenvector centrality which is
                                            degree-based. This is so we can consider both the vertices quantity and
                                            quality its connections. We can compute the eigenvector centraility by
                                            computing the spectra of the adjacency matrix $Ax=\lambda x$. Where $A$
                                            represent the adjacency matrix and $x$ is the eigenvector with a value of
                                            $\lambda$ of $A$. We can then define the eigenvector conetrality of some
                                            vertex $i$ as: $C_{E}(i)=\frac{1}{\lambda_{1}}\sum_{j=1}^{N}A_{ij}x_{j}$
                                            Hypergraphs Generally, hypergraphs is a graph where edges can join any
                                            number of vertices. An undirected hypergraph $H$ can be defined as a pair
                                            $H=(V,HE)$. Such that $V$ represents the set of vertices and $HE$ represents
                                            the set of hyperedges. More Graph Theory Fun This concludes most of the
                                            information neccesary for this page. However, if you are having fun with
                                            graph theory, feel free to read more on my Graph Theory page. Graph Theory
                                            PreliminariesDegree The vertex degree quantifies the total numbers of vertex
                                            connections that exist in an uniderected binary network. The vertex degree
                                            for an undirected weighted network which represents the sum of all edges of
                                            the vertex and is equivalent to its degree centraility. We can look at the
                                            vertex degree anaglously to the vertex strength and is useful to give an
                                            idea of how densely individual vertices are connected. It is computed as:
                                            $C_{D}=s_{i}=\sum_{j\neq i}^{}w_{ij}$ where $w_{ij}$ is the respective
                                            weight of the edge that links vertex $i$ and vertex $j$. DegreeThe vertex
                                            degree quantifies the total numbers of vertex connections that exist in an
                                            uniderected binary network. The vertex degree for an undirected weighted
                                            network which represents the sum of all edges of the vertex and is
                                            equivalent to its degree centraility. We can look at the vertex degree
                                            anaglously to the vertex strength and is useful to give an idea of how
                                            densely individual vertices are connected. It is computed as:
                                            $C_{D}=s_{i}=\sum_{j\neq i}^{}w_{ij}$ where $w_{ij}$ is the respective
                                            weight of the edge that links vertex $i$ and vertex $j$. Clustering
                                            Coefficent
                                            $Cl=\frac{2}{s_{i}(s_{i}-1)}\sum_{j,h}^{}(\hat{w}_{ij}\hat{w}_{jh}\hat{w}_{hi})^{\frac{1}{3}}$
                                            where $s_{i}$ is the degree of vertex $i$ and the edge wegihts are
                                            normalised by the maximum weight in the network. We can represent this
                                            normilization as: $\hat{w}_{ij}=\frac{w_{ij}}{max(w)}$ Clustering
                                            Coefficent$Cl=\frac{2}{s_{i}(s_{i}-1)}\sum_{j,h}^{}(\hat{w}_{ij}\hat{w}_{jh}\hat{w}_{hi})^{\frac{1}{3}}$
                                            where $s_{i}$ is the degree of vertex $i$ and the edge wegihts are
                                            normalised by the maximum weight in the network. We can represent this
                                            normilization as: $\hat{w}_{ij}=\frac{w_{ij}}{max(w)}$ Centrailities In
                                            order to measure a vertex's importance in a netwrok and consider its
                                            neightbour's influence we ascertain its eigenvector centrality which is
                                            degree-based. This is so we can consider both the vertices quantity and
                                            quality its connections. We can compute the eigenvector centraility by
                                            computing the spectra of the adjacency matrix $Ax=\lambda x$. Where $A$
                                            represent the adjacency matrix and $x$ is the eigenvector with a value of
                                            $\lambda$ of $A$. We can then define the eigenvector conetrality of some
                                            vertex $i$ as: $C_{E}(i)=\frac{1}{\lambda_{1}}\sum_{j=1}^{N}A_{ij}x_{j}$
                                            CentrailitiesIn order to measure a vertex's importance in a netwrok and
                                            consider its neightbour's influence we ascertain its eigenvector centrality
                                            which is degree-based. This is so we can consider both the vertices quantity
                                            and quality its connections. We can compute the eigenvector centraility by
                                            computing the spectra of the adjacency matrix $Ax=\lambda x$. Where $A$
                                            represent the adjacency matrix and $x$ is the eigenvector with a value of
                                            $\lambda$ of $A$. We can then define the eigenvector conetrality of some
                                            vertex $i$ as: $C_{E}(i)=\frac{1}{\lambda_{1}}\sum_{j=1}^{N}A_{ij}x_{j}$
                                            Hypergraphs Generally, hypergraphs is a graph where edges can join any
                                            number of vertices. An undirected hypergraph $H$ can be defined as a pair
                                            $H=(V,HE)$. Such that $V$ represents the set of vertices and $HE$ represents
                                            the set of hyperedges. HypergraphsGenerally, hypergraphs is a graph where
                                            edges can join any number of vertices. An undirected hypergraph $H$ can be
                                            defined as a pair $H=(V,HE)$. Such that $V$ represents the set of vertices
                                            and $HE$ represents the set of hyperedges. More Graph Theory Fun This
                                            concludes most of the information neccesary for this page. However, if you
                                            are having fun with graph theory, feel free to read more on my Graph Theory
                                            page. More Graph Theory FunThis concludes most of the information neccesary
                                            for this page. However, if you are having fun with graph theory, feel free
                                            to read more on my Graph Theory page. Graph TheoryMultivariate Information
                                            Preliminary Shannon Entropy You can read about Shannon entropy in my Entropy
                                            and Average Code Length section. However, the important part to know is how
                                            to calculate the amount of uncertainty or suprise present in the probability
                                            distribution of a single random variable $X$ or the Shannon entropy of $X$.
                                            The Shannon Entropy, or just entropy, on $X$ is denoted as $H(X)$, given
                                            with a probabilty distribution of $p(x)$, is defined as:
                                            $H(X)=-k\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$
                                            $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ where $n$ represents the
                                            cartesian length of $\left[N_{x}\right]=\left\{1,...N_{x}\right\}$ which is
                                            the codomain or alphabet of $X$. Multivariate Information PreliminaryShannon
                                            Entropy You can read about Shannon entropy in my Entropy and Average Code
                                            Length section. However, the important part to know is how to calculate the
                                            amount of uncertainty or suprise present in the probability distribution of
                                            a single random variable $X$ or the Shannon entropy of $X$. The Shannon
                                            Entropy, or just entropy, on $X$ is denoted as $H(X)$, given with a
                                            probabilty distribution of $p(x)$, is defined as:
                                            $H(X)=-k\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$
                                            $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ where $n$ represents the
                                            cartesian length of $\left[N_{x}\right]=\left\{1,...N_{x}\right\}$ which is
                                            the codomain or alphabet of $X$. Shannon EntropyYou can read about Shannon
                                            entropy in my Entropy and Average Code Length section. However, the
                                            important part to know is how to calculate the amount of uncertainty or
                                            suprise present in the probability distribution of a single random variable
                                            $X$ or the Shannon entropy of $X$. The Shannon Entropy, or just entropy, on
                                            $X$ is denoted as $H(X)$, given with a probabilty distribution of $p(x)$, is
                                            defined as: Entropy and Average Code
                                            Length$H(X)=-k\sum_{i=1}^{n}p_{i}\log_{2}(p_{i})$
                                            $=\sum_{i=1}^{n}p_{i}\log_{2}\frac{1}{p_{i}}$ where $n$ represents the
                                            cartesian length of $\left[N_{x}\right]=\left\{1,...N_{x}\right\}$ which is
                                            the codomain or alphabet of $X$. High-Order Functional Hubs in the Human
                                            Brain Section Reference: Emergence of High-Order Functional Hubs in the
                                            Human Brain by: Fernando A.N. Santos, Prejaas K.B. Tewarie, Pierre Baudot,
                                            et al. $^{[1]}$ High-Order Neural Networks Currently, neuroscience research
                                            predominantly focuses on a microscopic examination of neuronal architecture
                                            rooted in pairwise interactions. Consequently, mesoscopic and macroscopic
                                            studies inherit this dyadic perspective, which potentially provides an
                                            incomplete understanding of complex systems. Important observations like the
                                            regulatory role of astrocytes in both structural and synaptic plasticity.
                                            The authors cite that astrocytes regulate hetero-synaptic interactions as
                                            well as the heterogeneity of synaptic strength. These crucial regulatory
                                            astrocytes enable higher-order interactions, thus, forcing a shift in
                                            perspective of current neural interaction theories to incorporate
                                            higher-order structures. Image Source: See Reference [1] Let's more cloesley
                                            examining why higher-order structures in brain networks is important for
                                            gaining a more informed neuroscientific understanding. The figure above
                                            illustrates the neccesity for the incorporation of such higher-order
                                            strcutures. This is because the illustration shows how interactions in
                                            higher-order complex systems, especially those in functional brain netwroks,
                                            are misrepresented as the approximations of the sum of pairwise
                                            interactions. However, the authors suggest, with a sutiable high-order
                                            connectivity rule, we can define higher-order hubs in a network, that will
                                            circumvent the combinitorial complexity assoicated with enumerating only the
                                            important high-order edges in known priority hyperedge networks. Hypergraph
                                            Adjacency Matrix Algorithm Image Source: See Reference [1] The
                                            authors'$^{[1]}$ algorithm for representing statistical structures of time
                                            series as hypergraphs is given in the five steps below and depicted in the
                                            figure above. Let the input be an $N$ time series. First, we must build the
                                            biological neural network hypergraph. Assign one node per time series. For
                                            all corresponding time series in the rsfMRI, each node represents a
                                            different brain region. Next, choose an order $k\in \left\{3,...,N\right\}$.
                                            Then, between all possible different groups of $k$ nodes with a high-order
                                            interdependency metric, calculate the $\binom{N}{k}$ $k$-order associated
                                            terms. The reason hypergraphs are chosen to represent pairwise connectivity,
                                            is because different multivariate similarities can be measured and
                                            considered. The authors focus on two infomraiton-theortic measures:
                                            Interaction Information and Total Correlation. Interactive information
                                            quantifies a single tuple for statisical dependencies. Total Correlation
                                            quantifies the sum of all statisitcal dependencies over all tuple subsets.
                                            For hyperedge selection, the number of $k$-order iteractions in $N$ nodes
                                            grows as $N^{k}$. Hypergprah, where significat network edges is choosen
                                            because of there representation of high-order connectivity. The procedure
                                            used is to encode data relating to the encoding $k$-uniform hypergraph is
                                            termed a "hyper-adjacency matrix." A benefit of the hyper-adjacency matrix
                                            encoding is the representation of a low adjacency matrix representation for
                                            simplicial complexes in uniform hypergraphs. This encoding type can also be
                                            adapted to develop vector centralities in hypergraphs as well. Which is a
                                            good use case in topological neuroscience. Formally, two hyperedges of $k$
                                            dimensions are connected if they share a $k-1$-hyperedge. The authors
                                            mentioned the final step in their algorithm, after multivariate signal
                                            processing and specification of the hyper-adjacency matrix, the topological
                                            features from network science can be applied. Here, given Eigenvector
                                            centrality, an extension of this is used to investigate high-order hubs in
                                            the human brain. Where, given hubs as triplets with higher eigenvector
                                            centrality, representing hyper-adjacency matrices the calculation of
                                            modularity and betweenness centrality in different neurological contexts. To
                                            continue. High-Order Functional Hubs in the Human Brain Section Reference:
                                            Emergence of High-Order Functional Hubs in the Human Brain by: Fernando A.N.
                                            Santos, Prejaas K.B. Tewarie, Pierre Baudot, et al. $^{[1]}$ Emergence of
                                            High-Order Functional Hubs in the Human Brain by: Fernando A.N. Santos,
                                            Prejaas K.B. Tewarie, Pierre Baudot, et al. $^{[1]}$ Emergence of High-Order
                                            Functional Hubs in the Human Brain by: Fernando A.N. Santos, Prejaas K.B.
                                            Tewarie, Pierre Baudot, et al. High-Order Neural Networks Currently,
                                            neuroscience research predominantly focuses on a microscopic examination of
                                            neuronal architecture rooted in pairwise interactions. Consequently,
                                            mesoscopic and macroscopic studies inherit this dyadic perspective, which
                                            potentially provides an incomplete understanding of complex systems.
                                            Important observations like the regulatory role of astrocytes in both
                                            structural and synaptic plasticity. The authors cite that astrocytes
                                            regulate hetero-synaptic interactions as well as the heterogeneity of
                                            synaptic strength. These crucial regulatory astrocytes enable higher-order
                                            interactions, thus, forcing a shift in perspective of current neural
                                            interaction theories to incorporate higher-order structures. Image Source:
                                            See Reference [1] Let's more cloesley examining why higher-order structures
                                            in brain networks is important for gaining a more informed neuroscientific
                                            understanding. The figure above illustrates the neccesity for the
                                            incorporation of such higher-order strcutures. This is because the
                                            illustration shows how interactions in higher-order complex systems,
                                            especially those in functional brain netwroks, are misrepresented as the
                                            approximations of the sum of pairwise interactions. However, the authors
                                            suggest, with a sutiable high-order connectivity rule, we can define
                                            higher-order hubs in a network, that will circumvent the combinitorial
                                            complexity assoicated with enumerating only the important high-order edges
                                            in known priority hyperedge networks. High-Order Neural Networks Currently,
                                            neuroscience research predominantly focuses on a microscopic examination of
                                            neuronal architecture rooted in pairwise interactions. Consequently,
                                            mesoscopic and macroscopic studies inherit this dyadic perspective, which
                                            potentially provides an incomplete understanding of complex systems.
                                            Important observations like the regulatory role of astrocytes in both
                                            structural and synaptic plasticity. The authors cite that astrocytes
                                            regulate hetero-synaptic interactions as well as the heterogeneity of
                                            synaptic strength. These crucial regulatory astrocytes enable higher-order
                                            interactions, thus, forcing a shift in perspective of current neural
                                            interaction theories to incorporate higher-order structures. Image Source:
                                            See Reference [1] See Reference [1]Let's more cloesley examining why
                                            higher-order structures in brain networks is important for gaining a more
                                            informed neuroscientific understanding. The figure above illustrates the
                                            neccesity for the incorporation of such higher-order strcutures. This is
                                            because the illustration shows how interactions in higher-order complex
                                            systems, especially those in functional brain netwroks, are misrepresented
                                            as the approximations of the sum of pairwise interactions. However, the
                                            authors suggest, with a sutiable high-order connectivity rule, we can define
                                            higher-order hubs in a network, that will circumvent the combinitorial
                                            complexity assoicated with enumerating only the important high-order edges
                                            in known priority hyperedge networks. Hypergraph Adjacency Matrix Algorithm
                                            Image Source: See Reference [1] The authors'$^{[1]}$ algorithm for
                                            representing statistical structures of time series as hypergraphs is given
                                            in the five steps below and depicted in the figure above. Let the input be
                                            an $N$ time series. First, we must build the biological neural network
                                            hypergraph. Assign one node per time series. For all corresponding time
                                            series in the rsfMRI, each node represents a different brain region. Next,
                                            choose an order $k\in \left\{3,...,N\right\}$. Then, between all possible
                                            different groups of $k$ nodes with a high-order interdependency metric,
                                            calculate the $\binom{N}{k}$ $k$-order associated terms. The reason
                                            hypergraphs are chosen to represent pairwise connectivity, is because
                                            different multivariate similarities can be measured and considered. The
                                            authors focus on two infomraiton-theortic measures: Interaction Information
                                            and Total Correlation. Interactive information quantifies a single tuple for
                                            statisical dependencies. Total Correlation quantifies the sum of all
                                            statisitcal dependencies over all tuple subsets. For hyperedge selection,
                                            the number of $k$-order iteractions in $N$ nodes grows as $N^{k}$.
                                            Hypergprah, where significat network edges is choosen because of there
                                            representation of high-order connectivity. The procedure used is to encode
                                            data relating to the encoding $k$-uniform hypergraph is termed a
                                            "hyper-adjacency matrix." A benefit of the hyper-adjacency matrix encoding
                                            is the representation of a low adjacency matrix representation for
                                            simplicial complexes in uniform hypergraphs. This encoding type can also be
                                            adapted to develop vector centralities in hypergraphs as well. Which is a
                                            good use case in topological neuroscience. Formally, two hyperedges of $k$
                                            dimensions are connected if they share a $k-1$-hyperedge. The authors
                                            mentioned the final step in their algorithm, after multivariate signal
                                            processing and specification of the hyper-adjacency matrix, the topological
                                            features from network science can be applied. Here, given Eigenvector
                                            centrality, an extension of this is used to investigate high-order hubs in
                                            the human brain. Where, given hubs as triplets with higher eigenvector
                                            centrality, representing hyper-adjacency matrices the calculation of
                                            modularity and betweenness centrality in different neurological contexts. To
                                            continue. Hypergraph Adjacency Matrix Algorithm Image Source: See Reference
                                            [1] See Reference [1]The authors'$^{[1]}$ algorithm for representing
                                            statistical structures of time series as hypergraphs is given in the five
                                            steps below and depicted in the figure above. authors'$^{[1]}$Let the input
                                            be an $N$ time series. First, we must build the biological neural network
                                            hypergraph. Assign one node per time series. For all corresponding time
                                            series in the rsfMRI, each node represents a different brain region. Next,
                                            choose an order $k\in \left\{3,...,N\right\}$. Then, between all possible
                                            different groups of $k$ nodes with a high-order interdependency metric,
                                            calculate the $\binom{N}{k}$ $k$-order associated terms. The reason
                                            hypergraphs are chosen to represent pairwise connectivity, is because
                                            different multivariate similarities can be measured and considered. The
                                            authors focus on two infomraiton-theortic measures: Interaction Information
                                            and Total Correlation. Interactive information quantifies a single tuple for
                                            statisical dependencies. Total Correlation quantifies the sum of all
                                            statisitcal dependencies over all tuple subsets. For hyperedge selection,
                                            the number of $k$-order iteractions in $N$ nodes grows as $N^{k}$.
                                            Hypergprah, where significat network edges is choosen because of there
                                            representation of high-order connectivity. The procedure used is to encode
                                            data relating to the encoding $k$-uniform hypergraph is termed a
                                            "hyper-adjacency matrix." A benefit of the hyper-adjacency matrix encoding
                                            is the representation of a low adjacency matrix representation for
                                            simplicial complexes in uniform hypergraphs. This encoding type can also be
                                            adapted to develop vector centralities in hypergraphs as well. Which is a
                                            good use case in topological neuroscience. Formally, two hyperedges of $k$
                                            dimensions are connected if they share a $k-1$-hyperedge. The authors
                                            mentioned the final step in their algorithm, after multivariate signal
                                            processing and specification of the hyper-adjacency matrix, the topological
                                            features from network science can be applied. Here, given Eigenvector
                                            centrality, an extension of this is used to investigate high-order hubs in
                                            the human brain. Where, given hubs as triplets with higher eigenvector
                                            centrality, representing hyper-adjacency matrices the calculation of
                                            modularity and betweenness centrality in different neurological contexts.
                                            Let the input be an $N$ time series. First, we must build the biological
                                            neural network hypergraph. Assign one node per time series. For all
                                            corresponding time series in the rsfMRI, each node represents a different
                                            brain region. Let the input be an $N$ time series. First, we must build the
                                            biological neural network hypergraph. Assign one node per time series. For
                                            all corresponding time series in the rsfMRI, each node represents a
                                            different brain region. Next, choose an order $k\in \left\{3,...,N\right\}$.
                                            Then, between all possible different groups of $k$ nodes with a high-order
                                            interdependency metric, calculate the $\binom{N}{k}$ $k$-order associated
                                            terms. The reason hypergraphs are chosen to represent pairwise connectivity,
                                            is because different multivariate similarities can be measured and
                                            considered. The authors focus on two infomraiton-theortic measures:
                                            Interaction Information and Total Correlation. Interactive information
                                            quantifies a single tuple for statisical dependencies. Total Correlation
                                            quantifies the sum of all statisitcal dependencies over all tuple subsets.
                                            Next, choose an order $k\in \left\{3,...,N\right\}$. Then, between all
                                            possible different groups of $k$ nodes with a high-order interdependency
                                            metric, calculate the $\binom{N}{k}$ $k$-order associated terms. The reason
                                            hypergraphs are chosen to represent pairwise connectivity, is because
                                            different multivariate similarities can be measured and considered. The
                                            authors focus on two infomraiton-theortic measures: Interaction Information
                                            and Total Correlation. Interactive information quantifies a single tuple for
                                            statisical dependencies. Total Correlation quantifies the sum of all
                                            statisitcal dependencies over all tuple subsets. For hyperedge selection,
                                            the number of $k$-order iteractions in $N$ nodes grows as $N^{k}$.
                                            Hypergprah, where significat network edges is choosen because of there
                                            representation of high-order connectivity. For hyperedge selection, the
                                            number of $k$-order iteractions in $N$ nodes grows as $N^{k}$. Hypergprah,
                                            where significat network edges is choosen because of there representation of
                                            high-order connectivity. The procedure used is to encode data relating to
                                            the encoding $k$-uniform hypergraph is termed a "hyper-adjacency matrix." A
                                            benefit of the hyper-adjacency matrix encoding is the representation of a
                                            low adjacency matrix representation for simplicial complexes in uniform
                                            hypergraphs. This encoding type can also be adapted to develop vector
                                            centralities in hypergraphs as well. Which is a good use case in topological
                                            neuroscience. Formally, two hyperedges of $k$ dimensions are connected if
                                            they share a $k-1$-hyperedge. The procedure used is to encode data relating
                                            to the encoding $k$-uniform hypergraph is termed a "hyper-adjacency matrix."
                                            A benefit of the hyper-adjacency matrix encoding is the representation of a
                                            low adjacency matrix representation for simplicial complexes in uniform
                                            hypergraphs. This encoding type can also be adapted to develop vector
                                            centralities in hypergraphs as well. Which is a good use case in topological
                                            neuroscience. Formally, two hyperedges of $k$ dimensions are connected if
                                            they share a $k-1$-hyperedge. The authors mentioned the final step in their
                                            algorithm, after multivariate signal processing and specification of the
                                            hyper-adjacency matrix, the topological features from network science can be
                                            applied. Here, given Eigenvector centrality, an extension of this is used to
                                            investigate high-order hubs in the human brain. Where, given hubs as
                                            triplets with higher eigenvector centrality, representing hyper-adjacency
                                            matrices the calculation of modularity and betweenness centrality in
                                            different neurological contexts. The authors mentioned the final step in
                                            their algorithm, after multivariate signal processing and specification of
                                            the hyper-adjacency matrix, the topological features from network science
                                            can be applied. Here, given Eigenvector centrality, an extension of this is
                                            used to investigate high-order hubs in the human brain. Where, given hubs as
                                            triplets with higher eigenvector centrality, representing hyper-adjacency
                                            matrices the calculation of modularity and betweenness centrality in
                                            different neurological contexts. To continue. Total Correlation and the
                                            Brain's Visual System To continue. Total Correlation and the Brain's Visual
                                            SystemTo continue. Glossary Topolgical Space You can read about topological
                                            space on my website here. Resting State Functional Magnetic Resonance
                                            Imaging Resting-state fMRI examines the brain's functional architecture by
                                            assessing spontaneous low-frequency fluctuations in the BOLD signal.
                                            GlossaryTopolgical Space You can read about topological space on my website
                                            here. Resting State Functional Magnetic Resonance Imaging Resting-state fMRI
                                            examines the brain's functional architecture by assessing spontaneous
                                            low-frequency fluctuations in the BOLD signal. Topolgical Space You can read
                                            about topological space on my website here. Topolgical SpaceYou can read
                                            about topological space on my website here. hereResting State Functional
                                            Magnetic Resonance Imaging Resting-state fMRI examines the brain's
                                            functional architecture by assessing spontaneous low-frequency fluctuations
                                            in the BOLD signal. Resting State Functional Magnetic Resonance Imaging
                                            [https://www.contextswitching.org]
                                            Context Switching Context Switching by Alejandro Rosales For PK and Lucy, my
                                            best boy and best girl. Whose constant warmth I could never repay, even with
                                            a lifetime of dog treats and scratches. I coded this website from scratch to
                                            provide a place to share what I enjoy learning. I hope you enjoy it too!
                                            Jump to: Pinned New Explore  Pinned Quantum Support Vector Machine Using
                                            quantum computing, the authors exploit quantum mechanics for the algorithmic
                                            complexity optimization of a Support Vector Machine with high-dimensional
                                            feature space. Where the high-dimensional classical data is mapped
                                            non-linearly to Hilbert Space and a hyperplane in quantum space is used to
                                            separate and label the data. By using the... read more Molecular Bases of
                                            Memory Formation Molecular neuroscience is an area of chemical neuroscience
                                            that studies the molecular basis of intercellular activity applied to
                                            animals' nervous systems. This area of research covers molecular
                                            neuroanatomy, mechanisms of molecular signaling in the nervous system, and
                                            the molecular basis of neuroplasticity and neurodegenerative disease, which
                                            we will focus on... read more Compatification and Massless Scattering in
                                            Anti-de Sitter Space In theoretical physics, Minkowski Space is a particular
                                            type of $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                            Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                            $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d x^2\right)^2+\left(d
                                            x^3\right)^2$. Minkowski space forms the basis of the study of spacetime
                                            within special relativity and is... read more  New Riemannian Manifold A
                                            Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                            denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$ assigns
                                            a vector space $T_{p}M$, termed the tangent space of $M$ at $p$. The
                                            Riemanniam metric defines a positive-definite inner product: $g_p: T_p M
                                            \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                            \rightarrow \mathbb{R}$ defined as: $|v|_p=\sqrt{g_p(v, v)}$... read more
                                            Quantum [Forward Propagation] Time Complexity The quantum time complexity of
                                            one forward pass through the convolution layer $\ell$, where $\widetilde{O}$
                                            hides the polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                            \eta^2} \cdot \frac{M
                                            \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                            )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                            H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                            \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$ is
                                            the fraction of sampled elements, where the number of elements is size
                                            $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time of the
                                            classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1} D^{\ell+1}
                                            \cdot H W D^{\ell}\right )$... read more Thalamic Nuclei The thalamic nuclei
                                            are paired structures of the thalamus divided into three main groups: the
                                            lateral nuclear, medial nuclear, and anterior nuclear groups. The internal
                                            medullary lamina, a Y-shaped structure that splits these groups, is present
                                            on each side of the thalamus. A midline, thin thalamic nuclei, adjacent to
                                            the... read more The Maldacena Conjecture Given $AdS_{n+1}$ space of
                                            constant negative curvature, a Hyperboloid in $n+2$ dimensional flat
                                            spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric
                                            $\eta_{a b}=$ $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the
                                            constant: $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we
                                            introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$
                                            $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and
                                            $X_{n+1}=$ $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n \Omega_i^2=$
                                            $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time
                                                variable is $\tau$ is compact, we... read more  Explore All Articles
                                                Computer Science Mathematics Neuroscience Physics Miscellaneous My
                                                Research and Work Context Switching Context SwitchingContext Switching
                                                by Alejandro Rosales For PK and Lucy, my best boy and best girl. Whose
                                                constant warmth I could never repay, even with a lifetime of dog treats
                                                and scratches. I coded this website from scratch to provide a place to
                                                share what I enjoy learning. I hope you enjoy it too! Jump to: Pinned
                                                New Explore  Pinned Quantum Support Vector Machine Using quantum
                                                computing, the authors exploit quantum mechanics for the algorithmic
                                                complexity optimization of a Support Vector Machine with
                                                high-dimensional feature space. Where the high-dimensional classical
                                                data is mapped non-linearly to Hilbert Space and a hyperplane in quantum
                                                space is used to separate and label the data. By using the... read more
                                                Molecular Bases of Memory Formation Molecular neuroscience is an area of
                                                chemical neuroscience that studies the molecular basis of intercellular
                                                activity applied to animals' nervous systems. This area of research
                                                covers molecular neuroanatomy, mechanisms of molecular signaling in the
                                                nervous system, and the molecular basis of neuroplasticity and
                                                neurodegenerative disease, which we will focus on... read more
                                                Compatification and Massless Scattering in Anti-de Sitter Space In
                                                theoretical physics, Minkowski Space is a particular type of
                                                $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                                Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                                $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d
                                                x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of
                                                the study of spacetime within special relativity and is... read more
                                                 New Riemannian Manifold A Riemannian manifold is a smooth manifold $M$
                                                with a Riemannian metric denoted $(M,g)$. For every point $p\in M$ the
                                                tangent bundle of $M$ assigns a vector space $T_{p}M$, termed the
                                                tangent space of $M$ at $p$. The Riemanniam metric defines a
                                                positive-definite inner product: $g_p: T_p M \times T_p M \rightarrow
                                                \mathbb{R}$ with a norm $|\cdot|_p: T_p M \rightarrow \mathbb{R}$
                                                defined as: $|v|_p=\sqrt{g_p(v, v)}$... read more Quantum [Forward
                                                Propagation] Time Complexity The quantum time complexity of one forward
                                                pass through the convolution layer $\ell$, where $\widetilde{O}$ hides
                                                the polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                                \eta^2} \cdot \frac{M
                                                \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                                )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                                H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                                \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$
                                                is the fraction of sampled elements, where the number of elements is
                                                size $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time
                                                of the classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1}
                                                D^{\ell+1} \cdot H W D^{\ell}\right )$... read more Thalamic Nuclei The
                                                thalamic nuclei are paired structures of the thalamus divided into three
                                                main groups: the lateral nuclear, medial nuclear, and anterior nuclear
                                                groups. The internal medullary lamina, a Y-shaped structure that splits
                                                these groups, is present on each side of the thalamus. A midline, thin
                                                thalamic nuclei, adjacent to the... read more The Maldacena Conjecture
                                                Given $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid in
                                                $n+2$ dimensional flat spacetime with coordinates $(X^0, X^1, \ldots,$ $
                                                X^n, X^{n+1})$ and metric $\eta_{a b}=$
                                                $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant:
                                                $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce
                                                global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$ $\Lambda
                                                \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and
                                                $X_{n+1}=$ $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n
                                                \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since
                                                the time variable is $\tau$ is compact, we... read more  Explore All
                                                Articles Computer Science Mathematics Neuroscience Physics Miscellaneous
                                                My Research and Work Context Switching by Alejandro Rosales Context
                                                Switching by Alejandro Rosales Context SwitchingbyAlejandro
                                                RosalesAlejandro Rosales For PK and Lucy, my best boy and best girl.
                                                Whose constant warmth I could never repay, even with a lifetime of dog
                                                treats and scratches. I coded this website from scratch to provide a
                                                place to share what I enjoy learning. I hope you enjoy it too! For PK
                                                and Lucy, my best boy and best girl. Whose constant warmth I could never
                                                repay, even with a lifetime of dog treats and scratches. I coded this
                                                website from scratch to provide a place to share what I enjoy learning.
                                                I hope you enjoy it too! PK and LucyJump to: Pinned New Explore  Pinned
                                                Quantum Support Vector Machine Using quantum computing, the authors
                                                exploit quantum mechanics for the algorithmic complexity optimization of
                                                a Support Vector Machine with high-dimensional feature space. Where the
                                                high-dimensional classical data is mapped non-linearly to Hilbert Space
                                                and a hyperplane in quantum space is used to separate and label the
                                                data. By using the... read more Molecular Bases of Memory Formation
                                                Molecular neuroscience is an area of chemical neuroscience that studies
                                                the molecular basis of intercellular activity applied to animals'
                                                nervous systems. This area of research covers molecular neuroanatomy,
                                                mechanisms of molecular signaling in the nervous system, and the
                                                molecular basis of neuroplasticity and neurodegenerative disease, which
                                                we will focus on... read more Compatification and Massless Scattering in
                                                Anti-de Sitter Space In theoretical physics, Minkowski Space is a
                                                particular type of $4$-dimensional Lorentzian space, with a Minkowski
                                                metric. Where the Minkowski metric is a metric tensor denoted as
                                                $d\tau^2$ with the form $-\left(d^0\right)^2+\left(d x^1\right)^2$
                                                $+\;\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms
                                                the basis of the study of spacetime within special relativity and is...
                                                read more  New Riemannian Manifold A Riemannian manifold is a smooth
                                                manifold $M$ with a Riemannian metric denoted $(M,g)$. For every point
                                                $p\in M$ the tangent bundle of $M$ assigns a vector space $T_{p}M$,
                                                termed the tangent space of $M$ at $p$. The Riemanniam metric defines a
                                                positive-definite inner product: $g_p: T_p M \times T_p M \rightarrow
                                                \mathbb{R}$ with a norm $|\cdot|_p: T_p M \rightarrow \mathbb{R}$
                                                defined as: $|v|_p=\sqrt{g_p(v, v)}$... read more Quantum [Forward
                                                Propagation] Time Complexity The quantum time complexity of one forward
                                                pass through the convolution layer $\ell$, where $\widetilde{O}$ hides
                                                the polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                                \eta^2} \cdot \frac{M
                                                \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                                )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                                H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                                \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$
                                                is the fraction of sampled elements, where the number of elements is
                                                size $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time
                                                of the classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1}
                                                D^{\ell+1} \cdot H W D^{\ell}\right )$... read more Thalamic Nuclei The
                                                thalamic nuclei are paired structures of the thalamus divided into three
                                                main groups: the lateral nuclear, medial nuclear, and anterior nuclear
                                                groups. The internal medullary lamina, a Y-shaped structure that splits
                                                these groups, is present on each side of the thalamus. A midline, thin
                                                thalamic nuclei, adjacent to the... read more The Maldacena Conjecture
                                                Given $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid in
                                                $n+2$ dimensional flat spacetime with coordinates $(X^0, X^1, \ldots,$ $
                                                X^n, X^{n+1})$ and metric $\eta_{a b}=$
                                                $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant:
                                                $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce
                                                global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$ $\Lambda
                                                \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and
                                                $X_{n+1}=$ $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n
                                                \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since
                                                the time variable is $\tau$ is compact, we... read more  Explore All
                                                Articles Computer Science Mathematics Neuroscience Physics Miscellaneous
                                                My Research and Work Jump to: Pinned New Explore Jump to:Pinned New
                                                Explore Pinned PinnedNew NewExplore Explore Pinned Quantum Support
                                                Vector Machine Using quantum computing, the authors exploit quantum
                                                mechanics for the algorithmic complexity optimization of a Support
                                                Vector Machine with high-dimensional feature space. Where the
                                                high-dimensional classical data is mapped non-linearly to Hilbert Space
                                                and a hyperplane in quantum space is used to separate and label the
                                                data. By using the... read more Molecular Bases of Memory Formation
                                                Molecular neuroscience is an area of chemical neuroscience that studies
                                                the molecular basis of intercellular activity applied to animals'
                                                nervous systems. This area of research covers molecular neuroanatomy,
                                                mechanisms of molecular signaling in the nervous system, and the
                                                molecular basis of neuroplasticity and neurodegenerative disease, which
                                                we will focus on... read more Compatification and Massless Scattering in
                                                Anti-de Sitter Space In theoretical physics, Minkowski Space is a
                                                particular type of $4$-dimensional Lorentzian space, with a Minkowski
                                                metric. Where the Minkowski metric is a metric tensor denoted as
                                                $d\tau^2$ with the form $-\left(d^0\right)^2+\left(d x^1\right)^2$
                                                $+\;\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms
                                                the basis of the study of spacetime within special relativity and is...
                                                read more  Pinned Quantum Support Vector Machine Quantum Support Vector
                                                Machine Quantum Support Vector Machine Using quantum computing, the
                                                authors exploit quantum mechanics for the algorithmic complexity
                                                optimization of a Support Vector Machine with high-dimensional feature
                                                space. Where the high-dimensional classical data is mapped non-linearly
                                                to Hilbert Space and a hyperplane in quantum space is used to separate
                                                and label the data. By using the... read more Using quantum computing,
                                                the authors exploit quantum mechanics for the algorithmic complexity
                                                optimization of a Support Vector Machine with high-dimensional feature
                                                space. Where the high-dimensional classical data is mapped non-linearly
                                                to Hilbert Space and a hyperplane in quantum space is used to separate
                                                and label the data. By using the... read more read more Molecular Bases
                                                of Memory Formation Molecular Bases of Memory Formation Molecular Bases
                                                of Memory Formation Molecular neuroscience is an area of chemical
                                                neuroscience that studies the molecular basis of intercellular activity
                                                applied to animals' nervous systems. This area of research covers
                                                molecular neuroanatomy, mechanisms of molecular signaling in the nervous
                                                system, and the molecular basis of neuroplasticity and neurodegenerative
                                                disease, which we will focus on... read more Molecular neuroscience is
                                                an area of chemical neuroscience that studies the molecular basis of
                                                intercellular activity applied to animals' nervous systems. This area of
                                                research covers molecular neuroanatomy, mechanisms of molecular
                                                signaling in the nervous system, and the molecular basis of
                                                neuroplasticity and neurodegenerative disease, which we will focus on...
                                                read more Molecular neuroscience is an area of chemical neuroscience
                                                that studies the molecular basis of intercellular activity applied to
                                                animals' nervous systems. This area of research covers molecular
                                                neuroanatomy, mechanisms of molecular signaling in the nervous system,
                                                and the molecular basis of neuroplasticity and neurodegenerative
                                                disease, which we will focus on... read more read more Compatification
                                                and Massless Scattering in Anti-de Sitter Space Compatification and
                                                Massless Scattering in Anti-de Sitter Space Compatification and Massless
                                                Scattering in Anti-de Sitter Space In theoretical physics, Minkowski
                                                Space is a particular type of $4$-dimensional Lorentzian space, with a
                                                Minkowski metric. Where the Minkowski metric is a metric tensor denoted
                                                as $d\tau^2$ with the form $-\left(d^0\right)^2+\left(d x^1\right)^2$
                                                $+\;\left(d x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms
                                                the basis of the study of spacetime within special relativity and is...
                                                read more In theoretical physics, Minkowski Space is a particular type
                                                of $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                                Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                                $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d
                                                x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of
                                                the study of spacetime within special relativity and is... read more
                                                read more New Riemannian Manifold A Riemannian manifold is a smooth
                                                manifold $M$ with a Riemannian metric denoted $(M,g)$. For every point
                                                $p\in M$ the tangent bundle of $M$ assigns a vector space $T_{p}M$,
                                                termed the tangent space of $M$ at $p$. The Riemanniam metric defines a
                                                positive-definite inner product: $g_p: T_p M \times T_p M \rightarrow
                                                \mathbb{R}$ with a norm $|\cdot|_p: T_p M \rightarrow \mathbb{R}$
                                                defined as: $|v|_p=\sqrt{g_p(v, v)}$... read more Quantum [Forward
                                                Propagation] Time Complexity The quantum time complexity of one forward
                                                pass through the convolution layer $\ell$, where $\widetilde{O}$ hides
                                                the polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                                \eta^2} \cdot \frac{M
                                                \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                                )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                                H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                                \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$
                                                is the fraction of sampled elements, where the number of elements is
                                                size $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time
                                                of the classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1}
                                                D^{\ell+1} \cdot H W D^{\ell}\right )$... read more Thalamic Nuclei The
                                                thalamic nuclei are paired structures of the thalamus divided into three
                                                main groups: the lateral nuclear, medial nuclear, and anterior nuclear
                                                groups. The internal medullary lamina, a Y-shaped structure that splits
                                                these groups, is present on each side of the thalamus. A midline, thin
                                                thalamic nuclei, adjacent to the... read more The Maldacena Conjecture
                                                Given $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid in
                                                $n+2$ dimensional flat spacetime with coordinates $(X^0, X^1, \ldots,$ $
                                                X^n, X^{n+1})$ and metric $\eta_{a b}=$
                                                $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the constant:
                                                $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce
                                                global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$ $\Lambda
                                                \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and
                                                $X_{n+1}=$ $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n
                                                \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since
                                                the time variable is $\tau$ is compact, we... read more  New Riemannian
                                                Manifold Riemannian Manifold Riemannian Manifold A Riemannian manifold
                                                is a smooth manifold $M$ with a Riemannian metric denoted $(M,g)$. For
                                                every point $p\in M$ the tangent bundle of $M$ assigns a vector space
                                                $T_{p}M$, termed the tangent space of $M$ at $p$. The Riemanniam metric
                                                defines a positive-definite inner product: $g_p: T_p M \times T_p M
                                                \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M \rightarrow
                                                \mathbb{R}$ defined as: $|v|_p=\sqrt{g_p(v, v)}$... read more A
                                                Riemannian manifold is a smooth manifold $M$ with a Riemannian metric
                                                denoted $(M,g)$. For every point $p\in M$ the tangent bundle of $M$
                                                assigns a vector space $T_{p}M$, termed the tangent space of $M$ at $p$.
                                                The Riemanniam metric defines a positive-definite inner product: $g_p:
                                                T_p M \times T_p M \rightarrow \mathbb{R}$ with a norm $|\cdot|_p: T_p M
                                                \rightarrow \mathbb{R}$ defined as: $|v|_p=\sqrt{g_p(v, v)}$... read
                                                more read more Quantum [Forward Propagation] Time Complexity Quantum
                                                [Forward Propagation] Time Complexity Quantum [Forward Propagation] Time
                                                Complexity The quantum time complexity of one forward pass through the
                                                convolution layer $\ell$, where $\widetilde{O}$ hides the
                                                polylogarithmic factors, is: $\widetilde{O}\left(\frac{1}{\epsilon
                                                \eta^2} \cdot \frac{M
                                                \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                                )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                                H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                                \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$
                                                is the fraction of sampled elements, where the number of elements is
                                                size $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time
                                                of the classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1}
                                                D^{\ell+1} \cdot H W D^{\ell}\right )$... read more The quantum time
                                                complexity of one forward pass through the convolution layer $\ell$,
                                                where $\widetilde{O}$ hides the polylogarithmic factors, is:
                                                $\widetilde{O}\left(\frac{1}{\epsilon \eta^2} \cdot \frac{M
                                                \sqrt{C}}{\sqrt{\mathbb{E}\left(f\left(\bar{X}^{\ell+1}\right )\right
                                                )}}\right )$ which can be written also as: $\widetilde{O}\left(\sigma
                                                H^{\ell+1} W^{\ell+1} D^{\ell+1} \cdot \frac{M \sqrt{C}}{\epsilon
                                                \sqrt{\mathbb{E}(f(\bar{X}^{\ell+1}))}}\right )$ Here, $\sigma\in[0,1]$
                                                is the fraction of sampled elements, where the number of elements is
                                                size $H^{\ell+1} W^{\ell+1} D^{\ell+1}$. Note, that the the running time
                                                of the classical CNN layer is: $\widetilde{O}\left(H^{\ell+1} W^{\ell+1}
                                                D^{\ell+1} \cdot H W D^{\ell}\right )$... read more read more Thalamic
                                                Nuclei Thalamic Nuclei Thalamic Nuclei The thalamic nuclei are paired
                                                structures of the thalamus divided into three main groups: the lateral
                                                nuclear, medial nuclear, and anterior nuclear groups. The internal
                                                medullary lamina, a Y-shaped structure that splits these groups, is
                                                present on each side of the thalamus. A midline, thin thalamic nuclei,
                                                adjacent to the... read more The thalamic nuclei are paired structures
                                                of the thalamus divided into three main groups: the lateral nuclear,
                                                medial nuclear, and anterior nuclear groups. The internal medullary
                                                lamina, a Y-shaped structure that splits these groups, is present on
                                                each side of the thalamus. A midline, thin thalamic nuclei, adjacent to
                                                the... read more read more The Maldacena Conjecture The Maldacena
                                                Conjecture The Maldacena Conjecture Given $AdS_{n+1}$ space of constant
                                                negative curvature, a Hyperboloid in $n+2$ dimensional flat spacetime
                                                with coordinates $(X^0, X^1, \ldots,$ $ X^n, X^{n+1})$ and metric
                                                $\eta_{a b}=$ $\operatorname{diag}(+,-,-,\ldots,$ $-,+)$, we get the
                                                constant: $\Lambda^2=$ $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we
                                                introduce global coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$
                                                $\Lambda \sec \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and
                                                $X_{n+1}=$ $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n
                                                \Omega_i^2=$ $1,0 \leq \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since
                                                the time variable is $\tau$ is compact, we... read more Given
                                                $AdS_{n+1}$ space of constant negative curvature, a Hyperboloid in $n+2$
                                                dimensional flat spacetime with coordinates $(X^0, X^1, \ldots,$ $ X^n,
                                                X^{n+1})$ and metric $\eta_{a b}=$ $\operatorname{diag}(+,-,-,\ldots,$
                                                $-,+)$, we get the constant: $\Lambda^2=$
                                                $(X_0)_2+(X_{n+1})^2-\sum_{i=1}^n(X_i)^2$ Now, we introduce global
                                                coordinates in $AdS$ $\rho, \tau, \Omega_i$. Let $X_0=$ $\Lambda \sec
                                                \rho \cos \tau$, $X_i=$ $\Lambda \tan \rho\;\Omega_i$, and $X_{n+1}=$
                                                $\Lambda \sec \rho \sin \tau$ where $\sum_{i=1}^n \Omega_i^2=$ $1,0 \leq
                                                \rho < \pi / 2,0 $ $\leq \tau<2 \pi$. Note, since the time variable is
                                                $\tau$ is compact, we... read more read more Explore All Articles
                                                Computer Science Mathematics Neuroscience Physics Miscellaneous My
                                                Research and Work  Explore All Articles All Articles All Articles
                                                Computer Science Computer Science Computer Science Mathematics
                                                Mathematics Mathematics Neuroscience Neuroscience Neuroscience Physics
                                                Physics Physics Miscellaneous Miscellaneous Miscellaneous My Research
                                                and Work My Research and Work My Research and Work
                                                [https://www.contextswitching.org/math/abstractalgebra] Abstract Algebra
                                                - Context Switching Abstract Algebra Abstract Algebra or modern algebra
                                                can be defined as the theory of algebraic structures. For the most part,
                                                abstract algebra deals with four algebraic structures: groups, rings,
                                                fields, and vector spaces. We will look at and examine these four
                                                algebraic strucutres in this page. Group Theory Groups, Subgroups and
                                                Isomorphisms The three most commonly studied algebraic structures are
                                                groups, rings, and fields. Groups can be found in many areas of
                                                mathematics such as geometry, analysis, topology, and other fields as
                                                well. We will continue with formal definitions. A group is a set $G$
                                                with one binary operation that adheres to the following (using
                                                multiplication or juxtaposition for this binary operation): The
                                                operation is associative. So, for all $g_{1}, g_{2}, g_{3} \in G$,
                                                $(g_{1}g_{2})g_{3}=g_{1}(g_{2}g_{3})$. There exists an identity for the
                                                operation. So, for all $g \in G$, an element $1 \in G$ such that $1g=g$
                                                and $g1=g.$ Every $g \in G$ has an inverse for the operation. So, for
                                                all $g \in G$ there exists $g^{-1}$ with the property that $gg^{-1}=1$
                                                and $g^{-1}g=1$ If the operation is commutative along with 1, 2, and 3
                                                above, then the group is an abelian group. The order of group $G$ is the
                                                number of elements in $G$, denoted as $|G|$. If $|G|> \infty$, then $G$
                                                is an infinite group if, othewise $G$ is a finite group. The integers
                                                $\mathbb{Z}$, rationals $\mathbb{Q}$, and reals $\mathbb{R}$ all form
                                                groups under addition and are all albenian groups. The elements of the
                                                set of rational numbers $\mathbb{Q}$ are not a group under
                                                multiplication because $0$ does not have a multiplicative inverse. For a
                                                set to be closed under multiplication, any element $x$ must have a
                                                multiplicative inverse $-x$. The elemenet $0 \in \mathbb{Q}$ under
                                                multpication $\times$ does not have an inverse. However, the set of
                                                nonzero elements of $\mathbb{Q}$ under multiplication is an abelian
                                                group. This is because the nonzero elements of $\mathbb{Q}$ with one
                                                binary operation satisfies 1, 2, 3 above and is commutative under
                                                multiplication. Rings and the Integers Rings and the Rings of Integers A
                                                ring, formally defined, is a set $R$ with two binary operations defined
                                                in the set. The two binary operations are normally called addition $+$
                                                and multiplication denoted as $\cdot$ or by juxtaposition. The binary
                                                operations satisfy the following six axioms: (Note: the operations
                                                "addition" and "multiplication" may be very different from the usual
                                                addition and mutiplication of numbers) Addition is commutative. Meaning,
                                                $a+b=b+a$ for every pair $a,b\in R$. Addition is associative. Meaning,
                                                $a+(b+c)=(a+b)+c$ for $a,b,c\in R$. There exists an additive identity,
                                                denoted by 0. Meaning, that $a+b=a$ for every $a\in R$. For every $a\in
                                                R$ there exsists some additive inverse denoted by $-a$ such that
                                                $a+(-a)=0$. Multiplication is associative. Meaning, $a(bc)=(ab)c$ for
                                                $a;b;c\in R$. Multiplication is left and right distributive and right
                                                distributive over addition. Meaning, $a(b+c)=ab+ac$ and $(b+c)a=ba+ca$
                                                for for $a;b;c\in R$. Further, if the binary operations on set $R$ also
                                                satisfy the following axiom: Multiplication is commutative then $R$ is a
                                                commutative ring. Meaning, $ab=ba$ for every pair $a,b\in R$. Further,
                                                if for the binary operations on set $R$: There exsists a multiplicative
                                                identity denoted by $1$ such that $a\cdot 1=a$ and $1\cdot a=a$ for
                                                every $a\in R$ then $R$ is a ring with identity. If $R$ satisfies axioms
                                                1 through 6 and 8 but not 7, then $R$ is a ring with unity. Satify
                                                axioms 1 through 8 then $R$ is a commutative ring with an identity or a
                                                unity. Recall that a set $G$ with one operation, say $+$, defined on it
                                                and satisfies axioms 1 through 4 is called an abelian group. This means
                                                each ring is an abelian group, since, relative to the operration $+$
                                                axioms 1 through 4 for a ring must be satisified by definition. Looking
                                                now at the basic number number systems $\mathbb{Z}$, $\mathbb{Q}$, and
                                                $\mathbb{R}$, are examples of rings, both commutative and
                                                noncommutative. We will see that the ring algebraic structure is
                                                prevalent in mathematics. For some definitions, a ring $R$ with only one
                                                elmenet is called trivial. The ring is trivial if and only if $0=1$
                                                according to the binary oprations in $R$. A finite ring is a ring $R$
                                                with a finite number of elements, otherwise $R$ is an infinite ring.
                                                Looking back at our basic number systems, $\mathbb{Z}$, $\mathbb{Q}$,
                                                and $\mathbb{R}$ are all infinite rings. To continue. Algebraic Linear
                                                Algebra Vector Spaces Over a Field Eucledian $n$-Space We will look at
                                                extending the concept of two and three dimensional vectors to an
                                                arbitary $n$ dimensions. To do this, we will take certain properties of
                                                $\mathbb{R}^{3}$ an use them as definitions in higher dimensions. First,
                                                consider the set
                                                $\mathbb{R}^{n}=\left\{(x_{1},...,x_{n}):x_{i}\in\mathbb{R}\right\}$. To
                                                represent a vector in $\mathbb{R}^{n}$ we use an $n$-tuple
                                                $\vec{v}=(x_{1},...,x_{n})$ and to represent the component or coordinate
                                                of the vector $\vec{v}$ we use $x_{i}$. If $\vec{u}=(x_{1},...,x_{n}),
                                                \vec{v}=(y_{1},...,y_{n})$ then: $\vec{u} + \vec{v}=(x_{1} +
                                                y_{1},...,x_{n} + y_{n})$, $k\vec{u}=(k x_{1},...,k x_{n})$ for
                                                $k\in\mathbb{R}$ The vector $\vec{0}=(0,...,0)$ is called the zero
                                                vector. To continue. Glossary Binary Operations Binary Operations take
                                                two elments of a set and produce a new element that is a part of the
                                                same set. Binary Operations are fundemetal to Algebrai structures. Let
                                                $S$ be a nonempty set. A binary operation on set $S$ is a function from
                                                $S \times S$ to $S$. We can denote the image of the ordered pair $(a, b)
                                                \in S \times S$. We can poorly denote the binary opration by this image
                                                as $ab \in S$. Put differently, a binary operation on a set $S$ is given
                                                when every ordered pair ($a,b)$ of elements of set $S$ is an associated
                                                unique element $c \in S$. A binary operation on on set $S$ must be
                                                closed if $c \in S$. Abstract Algebra - Context Switching Abstract
                                                Algebra - Context SwitchingAbstract Algebra Abstract Algebra or modern
                                                algebra can be defined as the theory of algebraic structures. For the
                                                most part, abstract algebra deals with four algebraic structures:
                                                groups, rings, fields, and vector spaces. We will look at and examine
                                                these four algebraic strucutres in this page. Group Theory Groups,
                                                Subgroups and Isomorphisms The three most commonly studied algebraic
                                                structures are groups, rings, and fields. Groups can be found in many
                                                areas of mathematics such as geometry, analysis, topology, and other
                                                fields as well. We will continue with formal definitions. A group is a
                                                set $G$ with one binary operation that adheres to the following (using
                                                multiplication or juxtaposition for this binary operation): The
                                                operation is associative. So, for all $g_{1}, g_{2}, g_{3} \in G$,
                                                $(g_{1}g_{2})g_{3} = g_{1}(g_{2}g_{3})$. There exists an identity for
                                                the operation. So, for all $g \in G$, an element $1 \in G$ such that $1g
                                                = g$ and $g1 = g.$ Every $g \in G$ has an inverse for the operation. So,
                                                for all $g \in G$ there exists $g^{-1}$ with the property that
                                                $gg^{-1}=1$ and $g^{-1}g=1$ If the operation is commutative along with
                                                1, 2, and 3 above, then the group is an abelian group. The order of
                                                group $G$ is the number of elements in $G$, denoted as $|G|$. If $|G| >
                                                \infty$, then $G$ is an infinite group if, othewise $G$ is a finite
                                                group. The integers $\mathbb{Z}$, rationals $\mathbb{Q}$, and reals
                                                $\mathbb{R}$ all form groups under addition and are all albenian groups.
                                                The elements of the set of rational numbers $\mathbb{Q}$ are not a group
                                                under multiplication because $0$ does not have a multiplicative inverse.
                                                For a set to be closed under multiplication, any element $x$ must have a
                                                multiplicative inverse $-x$. The elemenet $0 \in \mathbb{Q}$ under
                                                multpication $\times$ does not have an inverse. However, the set of
                                                nonzero elements of $\mathbb{Q}$ under multiplication is an abelian
                                                group. This is because the nonzero elements of $\mathbb{Q}$ with one
                                                binary operation satisfies 1, 2, 3 above and is commutative under
                                                multiplication. Rings and the Integers Rings and the Rings of Integers A
                                                ring, formally defined, is a set $R$ with two binary operations defined
                                                in the set. The two binary operations are normally called addition $+$
                                                and multiplication denoted as $\cdot$ or by juxtaposition. The binary
                                                operations satisfy the following six axioms: (Note: the operations
                                                "addition" and "multiplication" may be very different from the usual
                                                addition and mutiplication of numbers) Addition is commutative. Meaning,
                                                $a+b=b+a$ for every pair $a,b\in R$. Addition is associative. Meaning,
                                                $a+(b+c)=(a+b)+c$ for $a,b,c\in R$. There exists an additive identity,
                                                denoted by 0. Meaning, that $a+b=a$ for every $a\in R$. For every $a\in
                                                R$ there exsists some additive inverse denoted by $-a$ such that
                                                $a+(-a)=0$. Multiplication is associative. Meaning, $a(bc)=(ab)c$ for
                                                $a;b;c\in R$. Multiplication is left and right distributive and right
                                                distributive over addition. Meaning, $a(b+c)=ab+ac$ and $(b+c)a=ba+ca$
                                                for for $a;b;c\in R$. Further, if the binary operations on set $R$ also
                                                satisfy the following axiom: Multiplication is commutative then $R$ is a
                                                commutative ring. Meaning, $ab=ba$ for every pair $a,b\in R$. Further,
                                                if for the binary operations on set $R$: There exsists a multiplicative
                                                identity denoted by $1$ such that $a\cdot 1=a$ and $1\cdot a=a$ for
                                                every $a\in R$ then $R$ is a ring with identity. If $R$ satisfies axioms
                                                1 through 6 and 8 but not 7, then $R$ is a ring with unity. Satify
                                                axioms 1 through 8 then $R$ is a commutative ring with an identity or a
                                                unity. Recall that a set $G$ with one operation, say $+$, defined on it
                                                and satisfies axioms 1 through 4 is called an abelian group. This means
                                                each ring is an abelian group, since, relative to the operration $+$
                                                axioms 1 through 4 for a ring must be satisified by definition. Looking
                                                now at the basic number number systems $\mathbb{Z}$, $\mathbb{Q}$, and
                                                $\mathbb{R}$, are examples of rings, both commutative and
                                                noncommutative. We will see that the ring algebraic structure is
                                                prevalent in mathematics. For some definitions, a ring $R$ with only one
                                                elmenet is called trivial. The ring is trivial if and only if $0=1$
                                                according to the binary oprations in $R$. A finite ring is a ring $R$
                                                with a finite number of elements, otherwise $R$ is an infinite ring.
                                                Looking back at our basic number systems, $\mathbb{Z}$, $\mathbb{Q}$,
                                                and $\mathbb{R}$ are all infinite rings. To continue. Algebraic Linear
                                                Algebra Vector Spaces Over a Field Eucledian $n$-Space We will look at
                                                extending the concept of two and three dimensional vectors to an
                                                arbitary $n$ dimensions. To do this, we will take certain properties of
                                                $\mathbb{R}^{3}$ an use them as definitions in higher dimensions. First,
                                                consider the set
                                                $\mathbb{R}^{n}=\left\{(x_{1},...,x_{n}):x_{i}\in\mathbb{R}\right\}$. To
                                                represent a vector in $\mathbb{R}^{n}$ we use an $n$-tuple
                                                $\vec{v}=(x_{1},...,x_{n})$ and to represent the component or coordinate
                                                of the vector $\vec{v}$ we use $x_{i}$. If $\vec{u}=(x_{1},...,x_{n}),
                                                \vec{v}=(y_{1},...,y_{n})$ then: $\vec{u} + \vec{v}=(x_{1} +
                                                y_{1},...,x_{n} + y_{n})$, $k\vec{u}=(k x_{1},...,k x_{n})$ for
                                                $k\in\mathbb{R}$ The vector $\vec{0}=(0,...,0)$ is called the zero
                                                vector. To continue. Glossary Binary Operations Binary Operations take
                                                two elments of a set and produce a new element that is a part of the
                                                same set. Binary Operations are fundemetal to Algebrai structures. Let
                                                $S$ be a nonempty set. A binary operation on set $S$ is a function from
                                                $S \times S$ to $S$. We can denote the image of the ordered pair $(a, b)
                                                \in S \times S$. We can poorly denote the binary opration by this image
                                                as $ab \in S$. Put differently, a binary operation on a set $S$ is given
                                                when every ordered pair ($a,b)$ of elements of set $S$ is an associated
                                                unique element $c \in S$. A binary operation on on set $S$ must be
                                                closed if $c \in S$. Abstract Algebra Abstract Algebra or modern algebra
                                                can be defined as the theory of algebraic structures. For the most part,
                                                abstract algebra deals with four algebraic structures: groups, rings,
                                                fields, and vector spaces. We will look at and examine these four
                                                algebraic strucutres in this page. Group Theory Groups, Subgroups and
                                                Isomorphisms The three most commonly studied algebraic structures are
                                                groups, rings, and fields. Groups can be found in many areas of
                                                mathematics such as geometry, analysis, topology, and other fields as
                                                well. We will continue with formal definitions. A group is a set $G$
                                                with one binary operation that adheres to the following (using
                                                multiplication or juxtaposition for this binary operation): The
                                                operation is associative. So, for all $g_{1}, g_{2}, g_{3} \in G$,
                                                $(g_{1}g_{2})g_{3} = g_{1}(g_{2}g_{3})$. There exists an identity for
                                                the operation. So, for all $g \in G$, an element $1 \in G$ such that $1g
                                                = g$ and $g1 = g.$ Every $g \in G$ has an inverse for the operation. So,
                                                for all $g \in G$ there exists $g^{-1}$ with the property that
                                                $gg^{-1}=1$ and $g^{-1}g=1$ If the operation is commutative along with
                                                1, 2, and 3 above, then the group is an abelian group. The order of
                                                group $G$ is the number of elements in $G$, denoted as $|G|$. If $|G| >
                                                \infty$, then $G$ is an infinite group if, othewise $G$ is a finite
                                                group. The integers $\mathbb{Z}$, rationals $\mathbb{Q}$, and reals
                                                $\mathbb{R}$ all form groups under addition and are all albenian groups.
                                                The elements of the set of rational numbers $\mathbb{Q}$ are not a group
                                                under multiplication because $0$ does not have a multiplicative inverse.
                                                For a set to be closed under multiplication, any element $x$ must have a
                                                multiplicative inverse $-x$. The elemenet $0 \in \mathbb{Q}$ under
                                                multpication $\times$ does not have an inverse. However, the set of
                                                nonzero elements of $\mathbb{Q}$ under multiplication is an abelian
                                                group. This is because the nonzero elements of $\mathbb{Q}$ with one
                                                binary operation satisfies 1, 2, 3 above and is commutative under
                                                multiplication. Rings and the Integers Rings and the Rings of Integers A
                                                ring, formally defined, is a set $R$ with two binary operations defined
                                                in the set. The two binary operations are normally called addition $+$
                                                and multiplication denoted as $\cdot$ or by juxtaposition. The binary
                                                operations satisfy the following six axioms: (Note: the operations
                                                "addition" and "multiplication" may be very different from the usual
                                                addition and mutiplication of numbers) Addition is commutative. Meaning,
                                                $a+b=b+a$ for every pair $a,b\in R$. Addition is associative. Meaning,
                                                $a+(b+c)=(a+b)+c$ for $a,b,c\in R$. There exists an additive identity,
                                                denoted by 0. Meaning, that $a+b=a$ for every $a\in R$. For every $a\in
                                                R$ there exsists some additive inverse denoted by $-a$ such that
                                                $a+(-a)=0$. Multiplication is associative. Meaning, $a(bc)=(ab)c$ for
                                                $a;b;c\in R$. Multiplication is left and right distributive and right
                                                distributive over addition. Meaning, $a(b+c)=ab+ac$ and $(b+c)a=ba+ca$
                                                for for $a;b;c\in R$. Further, if the binary operations on set $R$ also
                                                satisfy the following axiom: Multiplication is commutative then $R$ is a
                                                commutative ring. Meaning, $ab=ba$ for every pair $a,b\in R$. Further,
                                                if for the binary operations on set $R$: There exsists a multiplicative
                                                identity denoted by $1$ such that $a\cdot 1=a$ and $1\cdot a=a$ for
                                                every $a\in R$ then $R$ is a ring with identity. If $R$ satisfies axioms
                                                1 through 6 and 8 but not 7, then $R$ is a ring with unity. Satify
                                                axioms 1 through 8 then $R$ is a commutative ring with an identity or a
                                                unity. Recall that a set $G$ with one operation, say $+$, defined on it
                                                and satisfies axioms 1 through 4 is called an abelian group. This means
                                                each ring is an abelian group, since, relative to the operration $+$
                                                axioms 1 through 4 for a ring must be satisified by definition. Looking
                                                now at the basic number number systems $\mathbb{Z}$, $\mathbb{Q}$, and
                                                $\mathbb{R}$, are examples of rings, both commutative and
                                                noncommutative. We will see that the ring algebraic structure is
                                                prevalent in mathematics. For some definitions, a ring $R$ with only one
                                                elmenet is called trivial. The ring is trivial if and only if $0=1$
                                                according to the binary oprations in $R$. A finite ring is a ring $R$
                                                with a finite number of elements, otherwise $R$ is an infinite ring.
                                                Looking back at our basic number systems, $\mathbb{Z}$, $\mathbb{Q}$,
                                                and $\mathbb{R}$ are all infinite rings. To continue. Algebraic Linear
                                                Algebra Vector Spaces Over a Field Eucledian $n$-Space We will look at
                                                extending the concept of two and three dimensional vectors to an
                                                arbitary $n$ dimensions. To do this, we will take certain properties of
                                                $\mathbb{R}^{3}$ an use them as definitions in higher dimensions. First,
                                                consider the set
                                                $\mathbb{R}^{n}=\left\{(x_{1},...,x_{n}):x_{i}\in\mathbb{R}\right\}$. To
                                                represent a vector in $\mathbb{R}^{n}$ we use an $n$-tuple
                                                $\vec{v}=(x_{1},...,x_{n})$ and to represent the component or coordinate
                                                of the vector $\vec{v}$ we use $x_{i}$. If $\vec{u}=(x_{1},...,x_{n}),
                                                \vec{v}=(y_{1},...,y_{n})$ then: $\vec{u} + \vec{v}=(x_{1} +
                                                y_{1},...,x_{n} + y_{n})$, $k\vec{u}=(k x_{1},...,k x_{n})$ for
                                                $k\in\mathbb{R}$ The vector $\vec{0}=(0,...,0)$ is called the zero
                                                vector. To continue. Glossary Binary Operations Binary Operations take
                                                two elments of a set and produce a new element that is a part of the
                                                same set. Binary Operations are fundemetal to Algebrai structures. Let
                                                $S$ be a nonempty set. A binary operation on set $S$ is a function from
                                                $S \times S$ to $S$. We can denote the image of the ordered pair $(a, b)
                                                \in S \times S$. We can poorly denote the binary opration by this image
                                                as $ab \in S$. Put differently, a binary operation on a set $S$ is given
                                                when every ordered pair ($a,b)$ of elements of set $S$ is an associated
                                                unique element $c \in S$. A binary operation on on set $S$ must be
                                                closed if $c \in S$. Abstract Algebra Abstract Algebra or modern algebra
                                                can be defined as the theory of algebraic structures. For the most part,
                                                abstract algebra deals with four algebraic structures: groups, rings,
                                                fields, and vector spaces. We will look at and examine these four
                                                algebraic strucutres in this page. Group Theory Groups, Subgroups and
                                                Isomorphisms The three most commonly studied algebraic structures are
                                                groups, rings, and fields. Groups can be found in many areas of
                                                mathematics such as geometry, analysis, topology, and other fields as
                                                well. We will continue with formal definitions. A group is a set $G$
                                                with one binary operation that adheres to the following (using
                                                multiplication or juxtaposition for this binary operation): The
                                                operation is associative. So, for all $g_{1}, g_{2}, g_{3} \in G$,
                                                $(g_{1}g_{2})g_{3} = g_{1}(g_{2}g_{3})$. There exists an identity for
                                                the operation. So, for all $g \in G$, an element $1 \in G$ such that $1g
                                                = g$ and $g1 = g.$ Every $g \in G$ has an inverse for the operation. So,
                                                for all $g \in G$ there exists $g^{-1}$ with the property that
                                                $gg^{-1}=1$ and $g^{-1}g=1$ If the operation is commutative along with
                                                1, 2, and 3 above, then the group is an abelian group. The order of
                                                group $G$ is the number of elements in $G$, denoted as $|G|$. If $|G| >
                                                \infty$, then $G$ is an infinite group if, othewise $G$ is a finite
                                                group. The integers $\mathbb{Z}$, rationals $\mathbb{Q}$, and reals
                                                $\mathbb{R}$ all form groups under addition and are all albenian groups.
                                                The elements of the set of rational numbers $\mathbb{Q}$ are not a group
                                                under multiplication because $0$ does not have a multiplicative inverse.
                                                For a set to be closed under multiplication, any element $x$ must have a
                                                multiplicative inverse $-x$. The elemenet $0 \in \mathbb{Q}$ under
                                                multpication $\times$ does not have an inverse. However, the set of
                                                nonzero elements of $\mathbb{Q}$ under multiplication is an abelian
                                                group. This is because the nonzero elements of $\mathbb{Q}$ with one
                                                binary operation satisfies 1, 2, 3 above and is commutative under
                                                multiplication. Rings and the Integers Rings and the Rings of Integers A
                                                ring, formally defined, is a set $R$ with two binary operations defined
                                                in the set. The two binary operations are normally called addition $+$
                                                and multiplication denoted as $\cdot$ or by juxtaposition. The binary
                                                operations satisfy the following six axioms: (Note: the operations
                                                "addition" and "multiplication" may be very different from the usual
                                                addition and mutiplication of numbers) Addition is commutative. Meaning,
                                                $a+b=b+a$ for every pair $a,b\in R$. Addition is associative. Meaning,
                                                $a+(b+c)=(a+b)+c$ for $a,b,c\in R$. There exists an additive identity,
                                                denoted by 0. Meaning, that $a+b=a$ for every $a\in R$. For every $a\in
                                                R$ there exsists some additive inverse denoted by $-a$ such that
                                                $a+(-a)=0$. Multiplication is associative. Meaning, $a(bc)=(ab)c$ for
                                                $a;b;c\in R$. Multiplication is left and right distributive and right
                                                distributive over addition. Meaning, $a(b+c)=ab+ac$ and $(b+c)a=ba+ca$
                                                for for $a;b;c\in R$. Further, if the binary operations on set $R$ also
                                                satisfy the following axiom: Multiplication is commutative then $R$ is a
                                                commutative ring. Meaning, $ab=ba$ for every pair $a,b\in R$. Further,
                                                if for the binary operations on set $R$: There exsists a multiplicative
                                                identity denoted by $1$ such that $a\cdot 1=a$ and $1\cdot a=a$ for
                                                every $a\in R$ then $R$ is a ring with identity. If $R$ satisfies axioms
                                                1 through 6 and 8 but not 7, then $R$ is a ring with unity. Satify
                                                axioms 1 through 8 then $R$ is a commutative ring with an identity or a
                                                unity. Recall that a set $G$ with one operation, say $+$, defined on it
                                                and satisfies axioms 1 through 4 is called an abelian group. This means
                                                each ring is an abelian group, since, relative to the operration $+$
                                                axioms 1 through 4 for a ring must be satisified by definition. Looking
                                                now at the basic number number systems $\mathbb{Z}$, $\mathbb{Q}$, and
                                                $\mathbb{R}$, are examples of rings, both commutative and
                                                noncommutative. We will see that the ring algebraic structure is
                                                prevalent in mathematics. For some definitions, a ring $R$ with only one
                                                elmenet is called trivial. The ring is trivial if and only if $0=1$
                                                according to the binary oprations in $R$. A finite ring is a ring $R$
                                                with a finite number of elements, otherwise $R$ is an infinite ring.
                                                Looking back at our basic number systems, $\mathbb{Z}$, $\mathbb{Q}$,
                                                and $\mathbb{R}$ are all infinite rings. To continue. Algebraic Linear
                                                Algebra Vector Spaces Over a Field Eucledian $n$-Space We will look at
                                                extending the concept of two and three dimensional vectors to an
                                                arbitary $n$ dimensions. To do this, we will take certain properties of
                                                $\mathbb{R}^{3}$ an use them as definitions in higher dimensions. First,
                                                consider the set
                                                $\mathbb{R}^{n}=\left\{(x_{1},...,x_{n}):x_{i}\in\mathbb{R}\right\}$. To
                                                represent a vector in $\mathbb{R}^{n}$ we use an $n$-tuple
                                                $\vec{v}=(x_{1},...,x_{n})$ and to represent the component or coordinate
                                                of the vector $\vec{v}$ we use $x_{i}$. If $\vec{u}=(x_{1},...,x_{n}),
                                                \vec{v}=(y_{1},...,y_{n})$ then: $\vec{u} + \vec{v}=(x_{1} +
                                                y_{1},...,x_{n} + y_{n})$, $k\vec{u}=(k x_{1},...,k x_{n})$ for
                                                $k\in\mathbb{R}$ The vector $\vec{0}=(0,...,0)$ is called the zero
                                                vector. To continue. Glossary Binary Operations Binary Operations take
                                                two elments of a set and produce a new element that is a part of the
                                                same set. Binary Operations are fundemetal to Algebrai structures. Let
                                                $S$ be a nonempty set. A binary operation on set $S$ is a function from
                                                $S \times S$ to $S$. We can denote the image of the ordered pair $(a, b)
                                                \in S \times S$. We can poorly denote the binary opration by this image
                                                as $ab \in S$. Put differently, a binary operation on a set $S$ is given
                                                when every ordered pair ($a,b)$ of elements of set $S$ is an associated
                                                unique element $c \in S$. A binary operation on on set $S$ must be
                                                closed if $c \in S$. Abstract AlgebraAbstract Algebra or modern algebra
                                                can be defined as the theory of algebraic structures. For the most part,
                                                abstract algebra deals with four algebraic structures: groups, rings,
                                                fields, and vector spaces. We will look at and examine these four
                                                algebraic strucutres in this page. Group Theory Groups, Subgroups and
                                                Isomorphisms The three most commonly studied algebraic structures are
                                                groups, rings, and fields. Groups can be found in many areas of
                                                mathematics such as geometry, analysis, topology, and other fields as
                                                well. We will continue with formal definitions. A group is a set $G$
                                                with one binary operation that adheres to the following (using
                                                multiplication or juxtaposition for this binary operation): The
                                                operation is associative. So, for all $g_{1}, g_{2}, g_{3} \in G$,
                                                $(g_{1}g_{2})g_{3} = g_{1}(g_{2}g_{3})$. There exists an identity for
                                                the operation. So, for all $g \in G$, an element $1 \in G$ such that $1g
                                                = g$ and $g1 = g.$ Every $g \in G$ has an inverse for the operation. So,
                                                for all $g \in G$ there exists $g^{-1}$ with the property that
                                                $gg^{-1}=1$ and $g^{-1}g=1$ If the operation is commutative along with
                                                1, 2, and 3 above, then the group is an abelian group. The order of
                                                group $G$ is the number of elements in $G$, denoted as $|G|$. If $|G| >
                                                \infty$, then $G$ is an infinite group if, othewise $G$ is a finite
                                                group. The integers $\mathbb{Z}$, rationals $\mathbb{Q}$, and reals
                                                $\mathbb{R}$ all form groups under addition and are all albenian groups.
                                                The elements of the set of rational numbers $\mathbb{Q}$ are not a group
                                                under multiplication because $0$ does not have a multiplicative inverse.
                                                For a set to be closed under multiplication, any element $x$ must have a
                                                multiplicative inverse $-x$. The elemenet $0 \in \mathbb{Q}$ under
                                                multpication $\times$ does not have an inverse. However, the set of
                                                nonzero elements of $\mathbb{Q}$ under multiplication is an abelian
                                                group. This is because the nonzero elements of $\mathbb{Q}$ with one
                                                binary operation satisfies 1, 2, 3 above and is commutative under
                                                multiplication. Group TheoryGroups, Subgroups and Isomorphisms The three
                                                most commonly studied algebraic structures are groups, rings, and
                                                fields. Groups can be found in many areas of mathematics such as
                                                geometry, analysis, topology, and other fields as well. We will continue
                                                with formal definitions. A group is a set $G$ with one binary operation
                                                that adheres to the following (using multiplication or juxtaposition for
                                                this binary operation): The operation is associative. So, for all
                                                $g_{1}, g_{2}, g_{3} \in G$, $(g_{1}g_{2})g_{3} = g_{1}(g_{2}g_{3})$.
                                                There exists an identity for the operation. So, for all $g \in G$, an
                                                element $1 \in G$ such that $1g = g$ and $g1 = g.$ Every $g \in G$ has
                                                an inverse for the operation. So, for all $g \in G$ there exists
                                                $g^{-1}$ with the property that $gg^{-1}=1$ and $g^{-1}g=1$ If the
                                                operation is commutative along with 1, 2, and 3 above, then the group is
                                                an abelian group. The order of group $G$ is the number of elements in
                                                $G$, denoted as $|G|$. If $|G| > \infty$, then $G$ is an infinite group
                                                if, othewise $G$ is a finite group. The integers $\mathbb{Z}$, rationals
                                                $\mathbb{Q}$, and reals $\mathbb{R}$ all form groups under addition and
                                                are all albenian groups. The elements of the set of rational numbers
                                                $\mathbb{Q}$ are not a group under multiplication because $0$ does not
                                                have a multiplicative inverse. For a set to be closed under
                                                multiplication, any element $x$ must have a multiplicative inverse $-x$.
                                                The elemenet $0 \in \mathbb{Q}$ under multpication $\times$ does not
                                                have an inverse. However, the set of nonzero elements of $\mathbb{Q}$
                                                under multiplication is an abelian group. This is because the nonzero
                                                elements of $\mathbb{Q}$ with one binary operation satisfies 1, 2, 3
                                                above and is commutative under multiplication. Groups, Subgroups and
                                                IsomorphismsThe three most commonly studied algebraic structures are
                                                groups, rings, and fields. Groups can be found in many areas of
                                                mathematics such as geometry, analysis, topology, and other fields as
                                                well. We will continue with formal definitions. A group is a set $G$
                                                with one binary operation that adheres to the following (using
                                                multiplication or juxtaposition for this binary operation): binary
                                                operationThe operation is associative. So, for all $g_{1}, g_{2}, g_{3}
                                                \in G$, $(g_{1}g_{2})g_{3} = g_{1}(g_{2}g_{3})$. There exists an
                                                identity for the operation. So, for all $g \in G$, an element $1 \in G$
                                                such that $1g = g$ and $g1 = g.$ Every $g \in G$ has an inverse for the
                                                operation. So, for all $g \in G$ there exists $g^{-1}$ with the property
                                                that $gg^{-1}=1$ and $g^{-1}g=1$ The operation is associative. So, for
                                                all $g_{1}, g_{2}, g_{3} \in G$, $(g_{1}g_{2})g_{3} =
                                                g_{1}(g_{2}g_{3})$. There exists an identity for the operation. So, for
                                                all $g \in G$, an element $1 \in G$ such that $1g = g$ and $g1 = g.$
                                                Every $g \in G$ has an inverse for the operation. So, for all $g \in G$
                                                there exists $g^{-1}$ with the property that $gg^{-1}=1$ and $g^{-1}g=1$
                                                The operation is associative. So, for all $g_{1}, g_{2}, g_{3} \in G$,
                                                $(g_{1}g_{2})g_{3} = g_{1}(g_{2}g_{3})$.There exists an identity for the
                                                operation. So, for all $g \in G$, an element $1 \in G$ such that $1g =
                                                g$ and $g1 = g.$Every $g \in G$ has an inverse for the operation. So,
                                                for all $g \in G$ there exists $g^{-1}$ with the property that
                                                $gg^{-1}=1$ and $g^{-1}g=1$If the operation is commutative along with 1,
                                                2, and 3 above, then the group is an abelian group. The order of group
                                                $G$ is the number of elements in $G$, denoted as $|G|$. If $|G| >
                                                \infty$, then $G$ is an infinite group if, othewise $G$ is a finite
                                                group. The integers $\mathbb{Z}$, rationals $\mathbb{Q}$, and reals
                                                $\mathbb{R}$ all form groups under addition and are all albenian groups.
                                                The elements of the set of rational numbers $\mathbb{Q}$ are not a group
                                                under multiplication because $0$ does not have a multiplicative inverse.
                                                For a set to be closed under multiplication, any element $x$ must have a
                                                multiplicative inverse $-x$. The elemenet $0 \in \mathbb{Q}$ under
                                                multpication $\times$ does not have an inverse. However, the set of
                                                nonzero elements of $\mathbb{Q}$ under multiplication is an abelian
                                                group. This is because the nonzero elements of $\mathbb{Q}$ with one
                                                binary operation satisfies 1, 2, 3 above and is commutative under
                                                multiplication. Rings and the Integers Rings and the Rings of Integers A
                                                ring, formally defined, is a set $R$ with two binary operations defined
                                                in the set. The two binary operations are normally called addition $+$
                                                and multiplication denoted as $\cdot$ or by juxtaposition. The binary
                                                operations satisfy the following six axioms: (Note: the operations
                                                "addition" and "multiplication" may be very different from the usual
                                                addition and mutiplication of numbers) Addition is commutative. Meaning,
                                                $a+b=b+a$ for every pair $a,b\in R$. Addition is associative. Meaning,
                                                $a+(b+c)=(a+b)+c$ for $a,b,c\in R$. There exists an additive identity,
                                                denoted by 0. Meaning, that $a+b=a$ for every $a\in R$. For every $a\in
                                                R$ there exsists some additive inverse denoted by $-a$ such that
                                                $a+(-a)=0$. Multiplication is associative. Meaning, $a(bc)=(ab)c$ for
                                                $a;b;c\in R$. Multiplication is left and right distributive and right
                                                distributive over addition. Meaning, $a(b+c)=ab+ac$ and $(b+c)a=ba+ca$
                                                for for $a;b;c\in R$. Further, if the binary operations on set $R$ also
                                                satisfy the following axiom: Multiplication is commutative then $R$ is a
                                                commutative ring. Meaning, $ab=ba$ for every pair $a,b\in R$. Further,
                                                if for the binary operations on set $R$: There exsists a multiplicative
                                                identity denoted by $1$ such that $a\cdot 1=a$ and $1\cdot a=a$ for
                                                every $a\in R$ then $R$ is a ring with identity. If $R$ satisfies axioms
                                                1 through 6 and 8 but not 7, then $R$ is a ring with unity. Satify
                                                axioms 1 through 8 then $R$ is a commutative ring with an identity or a
                                                unity. Recall that a set $G$ with one operation, say $+$, defined on it
                                                and satisfies axioms 1 through 4 is called an abelian group. This means
                                                each ring is an abelian group, since, relative to the operration $+$
                                                axioms 1 through 4 for a ring must be satisified by definition. Looking
                                                now at the basic number number systems $\mathbb{Z}$, $\mathbb{Q}$, and
                                                $\mathbb{R}$, are examples of rings, both commutative and
                                                noncommutative. We will see that the ring algebraic structure is
                                                prevalent in mathematics. For some definitions, a ring $R$ with only one
                                                elmenet is called trivial. The ring is trivial if and only if $0=1$
                                                according to the binary oprations in $R$. A finite ring is a ring $R$
                                                with a finite number of elements, otherwise $R$ is an infinite ring.
                                                Looking back at our basic number systems, $\mathbb{Z}$, $\mathbb{Q}$,
                                                and $\mathbb{R}$ are all infinite rings. To continue. Rings and the
                                                IntegersRings and the Rings of Integers A ring, formally defined, is a
                                                set $R$ with two binary operations defined in the set. The two binary
                                                operations are normally called addition $+$ and multiplication denoted
                                                as $\cdot$ or by juxtaposition. The binary operations satisfy the
                                                following six axioms: (Note: the operations "addition" and
                                                "multiplication" may be very different from the usual addition and
                                                mutiplication of numbers) Addition is commutative. Meaning, $a+b=b+a$
                                                for every pair $a,b\in R$. Addition is associative. Meaning,
                                                $a+(b+c)=(a+b)+c$ for $a,b,c\in R$. There exists an additive identity,
                                                denoted by 0. Meaning, that $a+b=a$ for every $a\in R$. For every $a\in
                                                R$ there exsists some additive inverse denoted by $-a$ such that
                                                $a+(-a)=0$. Multiplication is associative. Meaning, $a(bc)=(ab)c$ for
                                                $a;b;c\in R$. Multiplication is left and right distributive and right
                                                distributive over addition. Meaning, $a(b+c)=ab+ac$ and $(b+c)a=ba+ca$
                                                for for $a;b;c\in R$. Further, if the binary operations on set $R$ also
                                                satisfy the following axiom: Multiplication is commutative then $R$ is a
                                                commutative ring. Meaning, $ab=ba$ for every pair $a,b\in R$. Further,
                                                if for the binary operations on set $R$: There exsists a multiplicative
                                                identity denoted by $1$ such that $a\cdot 1=a$ and $1\cdot a=a$ for
                                                every $a\in R$ then $R$ is a ring with identity. If $R$ satisfies axioms
                                                1 through 6 and 8 but not 7, then $R$ is a ring with unity. Satify
                                                axioms 1 through 8 then $R$ is a commutative ring with an identity or a
                                                unity. Recall that a set $G$ with one operation, say $+$, defined on it
                                                and satisfies axioms 1 through 4 is called an abelian group. This means
                                                each ring is an abelian group, since, relative to the operration $+$
                                                axioms 1 through 4 for a ring must be satisified by definition. Looking
                                                now at the basic number number systems $\mathbb{Z}$, $\mathbb{Q}$, and
                                                $\mathbb{R}$, are examples of rings, both commutative and
                                                noncommutative. We will see that the ring algebraic structure is
                                                prevalent in mathematics. For some definitions, a ring $R$ with only one
                                                elmenet is called trivial. The ring is trivial if and only if $0=1$
                                                according to the binary oprations in $R$. A finite ring is a ring $R$
                                                with a finite number of elements, otherwise $R$ is an infinite ring.
                                                Looking back at our basic number systems, $\mathbb{Z}$, $\mathbb{Q}$,
                                                and $\mathbb{R}$ are all infinite rings. To continue. Rings and the
                                                Rings of Integers A ring, formally defined, is a set $R$ with two binary
                                                operations defined in the set. The two binary operations are normally
                                                called addition $+$ and multiplication denoted as $\cdot$ or by
                                                juxtaposition. The binary operations satisfy the following six axioms:
                                                (Note: the operations "addition" and "multiplication" may be very
                                                different from the usual addition and mutiplication of numbers) Addition
                                                is commutative. Meaning, $a+b=b+a$ for every pair $a,b\in R$. Addition
                                                is associative. Meaning, $a+(b+c)=(a+b)+c$ for $a,b,c\in R$. There
                                                exists an additive identity, denoted by 0. Meaning, that $a+b=a$ for
                                                every $a\in R$. For every $a\in R$ there exsists some additive inverse
                                                denoted by $-a$ such that $a+(-a)=0$. Multiplication is associative.
                                                Meaning, $a(bc)=(ab)c$ for $a;b;c\in R$. Multiplication is left and
                                                right distributive and right distributive over addition. Meaning,
                                                $a(b+c)=ab+ac$ and $(b+c)a=ba+ca$ for for $a;b;c\in R$. Further, if the
                                                binary operations on set $R$ also satisfy the following axiom:
                                                Multiplication is commutative then $R$ is a commutative ring. Meaning,
                                                $ab=ba$ for every pair $a,b\in R$. Further, if for the binary operations
                                                on set $R$: There exsists a multiplicative identity denoted by $1$ such
                                                that $a\cdot 1=a$ and $1\cdot a=a$ for every $a\in R$ then $R$ is a ring
                                                with identity. If $R$ satisfies axioms 1 through 6 and 8 but not 7, then
                                                $R$ is a ring with unity. Satify axioms 1 through 8 then $R$ is a
                                                commutative ring with an identity or a unity. Addition is commutative.
                                                Meaning, $a+b=b+a$ for every pair $a,b\in R$. Addition is associative.
                                                Meaning, $a+(b+c)=(a+b)+c$ for $a,b,c\in R$. There exists an additive
                                                identity, denoted by 0. Meaning, that $a+b=a$ for every $a\in R$. For
                                                every $a\in R$ there exsists some additive inverse denoted by $-a$ such
                                                that $a+(-a)=0$. Multiplication is associative. Meaning, $a(bc)=(ab)c$
                                                for $a;b;c\in R$. Multiplication is left and right distributive and
                                                right distributive over addition. Meaning, $a(b+c)=ab+ac$ and
                                                $(b+c)a=ba+ca$ for for $a;b;c\in R$. Further, if the binary operations
                                                on set $R$ also satisfy the following axiom: Multiplication is
                                                commutative then $R$ is a commutative ring. Meaning, $ab=ba$ for every
                                                pair $a,b\in R$. Further, if for the binary operations on set $R$: There
                                                exsists a multiplicative identity denoted by $1$ such that $a\cdot 1=a$
                                                and $1\cdot a=a$ for every $a\in R$ then $R$ is a ring with identity. If
                                                $R$ satisfies axioms 1 through 6 and 8 but not 7, then $R$ is a ring
                                                with unity. Satify axioms 1 through 8 then $R$ is a commutative ring
                                                with an identity or a unity. Addition is commutative. Meaning, $a+b=b+a$
                                                for every pair $a,b\in R$.Addition is associative. Meaning,
                                                $a+(b+c)=(a+b)+c$ for $a,b,c\in R$.There exists an additive identity,
                                                denoted by 0. Meaning, that $a+b=a$ for every $a\in R$.For every $a\in
                                                R$ there exsists some additive inverse denoted by $-a$ such that
                                                $a+(-a)=0$.Multiplication is associative. Meaning, $a(bc)=(ab)c$ for
                                                $a;b;c\in R$.Multiplication is left and right distributive and right
                                                distributive over addition. Meaning, $a(b+c)=ab+ac$ and $(b+c)a=ba+ca$
                                                for for $a;b;c\in R$.Further, if the binary operations on set $R$ also
                                                satisfy the following axiom:Multiplication is commutative then $R$ is a
                                                commutative ring. Meaning, $ab=ba$ for every pair $a,b\in R$.Further, if
                                                for the binary operations on set $R$:There exsists a multiplicative
                                                identity denoted by $1$ such that $a\cdot 1=a$ and $1\cdot a=a$ for
                                                every $a\in R$ then $R$ is a ring with identity. If $R$ satisfies axioms
                                                1 through 6 and 8 but not 7, then $R$ is a ring with unity.Satify axioms
                                                1 through 8 then $R$ is a commutative ring with an identity or a
                                                unity.Recall that a set $G$ with one operation, say $+$, defined on it
                                                and satisfies axioms 1 through 4 is called an abelian group. This means
                                                each ring is an abelian group, since, relative to the operration $+$
                                                axioms 1 through 4 for a ring must be satisified by definition. Looking
                                                now at the basic number number systems $\mathbb{Z}$, $\mathbb{Q}$, and
                                                $\mathbb{R}$, are examples of rings, both commutative and
                                                noncommutative. We will see that the ring algebraic structure is
                                                prevalent in mathematics. For some definitions, a ring $R$ with only one
                                                elmenet is called trivial. The ring is trivial if and only if $0=1$
                                                according to the binary oprations in $R$. A finite ring is a ring $R$
                                                with a finite number of elements, otherwise $R$ is an infinite ring.
                                                Looking back at our basic number systems, $\mathbb{Z}$, $\mathbb{Q}$,
                                                and $\mathbb{R}$ are all infinite rings. To continue. Algebraic Linear
                                                Algebra Vector Spaces Over a Field Eucledian $n$-Space We will look at
                                                extending the concept of two and three dimensional vectors to an
                                                arbitary $n$ dimensions. To do this, we will take certain properties of
                                                $\mathbb{R}^{3}$ an use them as definitions in higher dimensions. First,
                                                consider the set
                                                $\mathbb{R}^{n}=\left\{(x_{1},...,x_{n}):x_{i}\in\mathbb{R}\right\}$. To
                                                represent a vector in $\mathbb{R}^{n}$ we use an $n$-tuple
                                                $\vec{v}=(x_{1},...,x_{n})$ and to represent the component or coordinate
                                                of the vector $\vec{v}$ we use $x_{i}$. If $\vec{u}=(x_{1},...,x_{n}),
                                                \vec{v}=(y_{1},...,y_{n})$ then: $\vec{u} + \vec{v}=(x_{1} +
                                                y_{1},...,x_{n} + y_{n})$, $k\vec{u}=(k x_{1},...,k x_{n})$ for
                                                $k\in\mathbb{R}$ The vector $\vec{0}=(0,...,0)$ is called the zero
                                                vector. To continue. Algebraic Linear AlgebraVector Spaces Over a Field
                                                Vector Spaces Over a FieldEucledian $n$-Space We will look at extending
                                                the concept of two and three dimensional vectors to an arbitary $n$
                                                dimensions. To do this, we will take certain properties of
                                                $\mathbb{R}^{3}$ an use them as definitions in higher dimensions. First,
                                                consider the set
                                                $\mathbb{R}^{n}=\left\{(x_{1},...,x_{n}):x_{i}\in\mathbb{R}\right\}$. To
                                                represent a vector in $\mathbb{R}^{n}$ we use an $n$-tuple
                                                $\vec{v}=(x_{1},...,x_{n})$ and to represent the component or coordinate
                                                of the vector $\vec{v}$ we use $x_{i}$. If $\vec{u}=(x_{1},...,x_{n}),
                                                \vec{v}=(y_{1},...,y_{n})$ then: $\vec{u} + \vec{v}=(x_{1} +
                                                y_{1},...,x_{n} + y_{n})$, $k\vec{u}=(k x_{1},...,k x_{n})$ for
                                                $k\in\mathbb{R}$ The vector $\vec{0}=(0,...,0)$ is called the zero
                                                vector. To continue. Eucledian $n$-SpaceWe will look at extending the
                                                concept of two and three dimensional vectors to an arbitary $n$
                                                dimensions. To do this, we will take certain properties of
                                                $\mathbb{R}^{3}$ an use them as definitions in higher dimensions. First,
                                                consider the set
                                                $\mathbb{R}^{n}=\left\{(x_{1},...,x_{n}):x_{i}\in\mathbb{R}\right\}$. To
                                                represent a vector in $\mathbb{R}^{n}$ we use an $n$-tuple
                                                $\vec{v}=(x_{1},...,x_{n})$ and to represent the component or coordinate
                                                of the vector $\vec{v}$ we use $x_{i}$. If $\vec{u}=(x_{1},...,x_{n}),
                                                \vec{v}=(y_{1},...,y_{n})$ then: $\vec{u} + \vec{v}=(x_{1} +
                                                y_{1},...,x_{n} + y_{n})$, $k\vec{u}=(k x_{1},...,k x_{n})$ for
                                                $k\in\mathbb{R}$ The vector $\vec{0}=(0,...,0)$ is called the zero
                                                vector. To continue. Glossary Binary Operations Binary Operations take
                                                two elments of a set and produce a new element that is a part of the
                                                same set. Binary Operations are fundemetal to Algebrai structures. Let
                                                $S$ be a nonempty set. A binary operation on set $S$ is a function from
                                                $S \times S$ to $S$. We can denote the image of the ordered pair $(a, b)
                                                \in S \times S$. We can poorly denote the binary opration by this image
                                                as $ab \in S$. Put differently, a binary operation on a set $S$ is given
                                                when every ordered pair ($a,b)$ of elements of set $S$ is an associated
                                                unique element $c \in S$. A binary operation on on set $S$ must be
                                                closed if $c \in S$. GlossaryBinary Operations Binary Operations take
                                                two elments of a set and produce a new element that is a part of the
                                                same set. Binary Operations are fundemetal to Algebrai structures. Let
                                                $S$ be a nonempty set. A binary operation on set $S$ is a function from
                                                $S \times S$ to $S$. We can denote the image of the ordered pair $(a, b)
                                                \in S \times S$. We can poorly denote the binary opration by this image
                                                as $ab \in S$. Put differently, a binary operation on a set $S$ is given
                                                when every ordered pair ($a,b)$ of elements of set $S$ is an associated
                                                unique element $c \in S$. A binary operation on on set $S$ must be
                                                closed if $c \in S$. Binary OperationsBinary Operations take two elments
                                                of a set and produce a new element that is a part of the same set.
                                                Binary Operations are fundemetal to Algebrai structures. Let $S$ be a
                                                nonempty set. A binary operation on set $S$ is a function from $S \times
                                                S$ to $S$. We can denote the image of the ordered pair $(a, b) \in S
                                                \times S$. We can poorly denote the binary opration by this image as $ab
                                                \in S$. Put differently, a binary operation on a set $S$ is given when
                                                every ordered pair ($a,b)$ of elements of set $S$ is an associated
                                                unique element $c \in S$. A binary operation on on set $S$ must be
                                                closed if $c \in S$.
                                                [https://www.contextswitching.org/misc/bioinformaticsfunctionalgenomics]
                                                Bioinformatics and Functional Genomics - Context Switching
                                                Bioinformatics and Functional Genomics Introduction Bioinformatics is a
                                                growing revolution in the field of molecular biology and computers.
                                                Here, our emphasis will be on employing bioinformatics tools and
                                                biological databases to address challenges from current issues in
                                                biological, biotechnological, and biomedical research. Such as looking
                                                at computational algorithms and computer databases to analyze proteins,
                                                genes, and the complete collection of deoxyribonucleic acid DNA that
                                                comprises an organism or genome. A major challenge in biology currently
                                                is to analyze and ascertain insights from large quantiites of data
                                                garnered via genome-sequencing, proteomics, and other biolgical projects
                                                that yield sequencing and structural data. Therefore, there is a need
                                                for tools of bioinformatics to help reveal the fundemental, underlying
                                                strucutre and function of biological problems relating to
                                                macromolecules, biochemical pathways, disease processes, and evolution,
                                                to name a few. Pairwise Sequence Alignment Introduction Two proteins
                                                relating at the sequence levels suggest they are homologous and is an
                                                important question for genes or proteins. If two genes are homologous,
                                                then they evolved from a common ancestor and suggests that they share
                                                common functionality. Thus, the motivation of developing tools to
                                                analyze many DNA and protein sequences is to identify domains or motifs
                                                that are shared among common molecules, which can be accomplished by
                                                aligning sequencing. This has many uses, one of which is to use our
                                                understanding of proteins and gene relatedness within and between
                                                organisms, to imrpove our fundemental understanding of life. Protein
                                                Alignment Basic Local Alignment Search Tool BLAST Algorithm The Basic
                                                Local Alignment Search Tool BLAST Algorithm reveals what related
                                                sequences are present in the same and other organisms. At a large scale
                                                view, BLAST preforms a pairwise sequence alignment for one sequence,
                                                termed the query, and the entire database, termed the target. This
                                                results in tens of millions of sequences evaluations where the return
                                                data are matches that most closley relate. A benefit of BLAST compared
                                                to other global or local alignment algorithms is that it offers a local
                                                alignment strategy having both speed and sensitivity. In this section we
                                                will analyze the procedure for the BLAST algorithm. The procedure for
                                                BLAST is described in the following four sequential steps: sequence of
                                                interest, BLAST selected implementation, database selection, and then
                                                parameter selection. Sequence of Interest BLAST Selected Implementation
                                                Database Selection Parameter Selection To continue. Bioinformatics and
                                                Functional Genomics - Context Switching Bioinformatics and Functional
                                                Genomics - Context SwitchingBioinformatics and Functional Genomics
                                                Introduction Bioinformatics is a growing revolution in the field of
                                                molecular biology and computers. Here, our emphasis will be on employing
                                                bioinformatics tools and biological databases to address challenges from
                                                current issues in biological, biotechnological, and biomedical research.
                                                Such as looking at computational algorithms and computer databases to
                                                analyze proteins, genes, and the complete collection of deoxyribonucleic
                                                acid DNA that comprises an organism or genome. A major challenge in
                                                biology currently is to analyze and ascertain insights from large
                                                quantiites of data garnered via genome-sequencing, proteomics, and other
                                                biolgical projects that yield sequencing and structural data. Therefore,
                                                there is a need for tools of bioinformatics to help reveal the
                                                fundemental, underlying strucutre and function of biological problems
                                                relating to macromolecules, biochemical pathways, disease processes, and
                                                evolution, to name a few. Pairwise Sequence Alignment Introduction Two
                                                proteins relating at the sequence levels suggest they are homologous and
                                                is an important question for genes or proteins. If two genes are
                                                homologous, then they evolved from a common ancestor and suggests that
                                                they share common functionality. Thus, the motivation of developing
                                                tools to analyze many DNA and protein sequences is to identify domains
                                                or motifs that are shared among common molecules, which can be
                                                accomplished by aligning sequencing. This has many uses, one of which is
                                                to use our understanding of proteins and gene relatedness within and
                                                between organisms, to imrpove our fundemental understanding of life.
                                                Protein Alignment Basic Local Alignment Search Tool BLAST Algorithm The
                                                Basic Local Alignment Search Tool BLAST Algorithm reveals what related
                                                sequences are present in the same and other organisms. At a large scale
                                                view, BLAST preforms a pairwise sequence alignment for one sequence,
                                                termed the query, and the entire database, termed the target. This
                                                results in tens of millions of sequences evaluations where the return
                                                data are matches that most closley relate. A benefit of BLAST compared
                                                to other global or local alignment algorithms is that it offers a local
                                                alignment strategy having both speed and sensitivity. In this section we
                                                will analyze the procedure for the BLAST algorithm. The procedure for
                                                BLAST is described in the following four sequential steps: sequence of
                                                interest, BLAST selected implementation, database selection, and then
                                                parameter selection. Sequence of Interest BLAST Selected Implementation
                                                Database Selection Parameter Selection To continue. Bioinformatics and
                                                Functional Genomics Introduction Bioinformatics is a growing revolution
                                                in the field of molecular biology and computers. Here, our emphasis will
                                                be on employing bioinformatics tools and biological databases to address
                                                challenges from current issues in biological, biotechnological, and
                                                biomedical research. Such as looking at computational algorithms and
                                                computer databases to analyze proteins, genes, and the complete
                                                collection of deoxyribonucleic acid DNA that comprises an organism or
                                                genome. A major challenge in biology currently is to analyze and
                                                ascertain insights from large quantiites of data garnered via
                                                genome-sequencing, proteomics, and other biolgical projects that yield
                                                sequencing and structural data. Therefore, there is a need for tools of
                                                bioinformatics to help reveal the fundemental, underlying strucutre and
                                                function of biological problems relating to macromolecules, biochemical
                                                pathways, disease processes, and evolution, to name a few. Pairwise
                                                Sequence Alignment Introduction Two proteins relating at the sequence
                                                levels suggest they are homologous and is an important question for
                                                genes or proteins. If two genes are homologous, then they evolved from a
                                                common ancestor and suggests that they share common functionality. Thus,
                                                the motivation of developing tools to analyze many DNA and protein
                                                sequences is to identify domains or motifs that are shared among common
                                                molecules, which can be accomplished by aligning sequencing. This has
                                                many uses, one of which is to use our understanding of proteins and gene
                                                relatedness within and between organisms, to imrpove our fundemental
                                                understanding of life. Protein Alignment Basic Local Alignment Search
                                                Tool BLAST Algorithm The Basic Local Alignment Search Tool BLAST
                                                Algorithm reveals what related sequences are present in the same and
                                                other organisms. At a large scale view, BLAST preforms a pairwise
                                                sequence alignment for one sequence, termed the query, and the entire
                                                database, termed the target. This results in tens of millions of
                                                sequences evaluations where the return data are matches that most
                                                closley relate. A benefit of BLAST compared to other global or local
                                                alignment algorithms is that it offers a local alignment strategy having
                                                both speed and sensitivity. In this section we will analyze the
                                                procedure for the BLAST algorithm. The procedure for BLAST is described
                                                in the following four sequential steps: sequence of interest, BLAST
                                                selected implementation, database selection, and then parameter
                                                selection. Sequence of Interest BLAST Selected Implementation Database
                                                Selection Parameter Selection To continue. Bioinformatics and Functional
                                                Genomics Introduction Bioinformatics is a growing revolution in the
                                                field of molecular biology and computers. Here, our emphasis will be on
                                                employing bioinformatics tools and biological databases to address
                                                challenges from current issues in biological, biotechnological, and
                                                biomedical research. Such as looking at computational algorithms and
                                                computer databases to analyze proteins, genes, and the complete
                                                collection of deoxyribonucleic acid DNA that comprises an organism or
                                                genome. A major challenge in biology currently is to analyze and
                                                ascertain insights from large quantiites of data garnered via
                                                genome-sequencing, proteomics, and other biolgical projects that yield
                                                sequencing and structural data. Therefore, there is a need for tools of
                                                bioinformatics to help reveal the fundemental, underlying strucutre and
                                                function of biological problems relating to macromolecules, biochemical
                                                pathways, disease processes, and evolution, to name a few. Pairwise
                                                Sequence Alignment Introduction Two proteins relating at the sequence
                                                levels suggest they are homologous and is an important question for
                                                genes or proteins. If two genes are homologous, then they evolved from a
                                                common ancestor and suggests that they share common functionality. Thus,
                                                the motivation of developing tools to analyze many DNA and protein
                                                sequences is to identify domains or motifs that are shared among common
                                                molecules, which can be accomplished by aligning sequencing. This has
                                                many uses, one of which is to use our understanding of proteins and gene
                                                relatedness within and between organisms, to imrpove our fundemental
                                                understanding of life. Protein Alignment Basic Local Alignment Search
                                                Tool BLAST Algorithm The Basic Local Alignment Search Tool BLAST
                                                Algorithm reveals what related sequences are present in the same and
                                                other organisms. At a large scale view, BLAST preforms a pairwise
                                                sequence alignment for one sequence, termed the query, and the entire
                                                database, termed the target. This results in tens of millions of
                                                sequences evaluations where the return data are matches that most
                                                closley relate. A benefit of BLAST compared to other global or local
                                                alignment algorithms is that it offers a local alignment strategy having
                                                both speed and sensitivity. In this section we will analyze the
                                                procedure for the BLAST algorithm. The procedure for BLAST is described
                                                in the following four sequential steps: sequence of interest, BLAST
                                                selected implementation, database selection, and then parameter
                                                selection. Sequence of Interest BLAST Selected Implementation Database
                                                Selection Parameter Selection To continue. Bioinformatics and Functional
                                                GenomicsIntroduction Bioinformatics is a growing revolution in the field
                                                of molecular biology and computers. Here, our emphasis will be on
                                                employing bioinformatics tools and biological databases to address
                                                challenges from current issues in biological, biotechnological, and
                                                biomedical research. Such as looking at computational algorithms and
                                                computer databases to analyze proteins, genes, and the complete
                                                collection of deoxyribonucleic acid DNA that comprises an organism or
                                                genome. A major challenge in biology currently is to analyze and
                                                ascertain insights from large quantiites of data garnered via
                                                genome-sequencing, proteomics, and other biolgical projects that yield
                                                sequencing and structural data. Therefore, there is a need for tools of
                                                bioinformatics to help reveal the fundemental, underlying strucutre and
                                                function of biological problems relating to macromolecules, biochemical
                                                pathways, disease processes, and evolution, to name a few. Introduction
                                                Bioinformatics is a growing revolution in the field of molecular biology
                                                and computers. Here, our emphasis will be on employing bioinformatics
                                                tools and biological databases to address challenges from current issues
                                                in biological, biotechnological, and biomedical research. Such as
                                                looking at computational algorithms and computer databases to analyze
                                                proteins, genes, and the complete collection of deoxyribonucleic acid
                                                DNA that comprises an organism or genome. A major challenge in biology
                                                currently is to analyze and ascertain insights from large quantiites of
                                                data garnered via genome-sequencing, proteomics, and other biolgical
                                                projects that yield sequencing and structural data. Therefore, there is
                                                a need for tools of bioinformatics to help reveal the fundemental,
                                                underlying strucutre and function of biological problems relating to
                                                macromolecules, biochemical pathways, disease processes, and evolution,
                                                to name a few. Pairwise Sequence Alignment Introduction Two proteins
                                                relating at the sequence levels suggest they are homologous and is an
                                                important question for genes or proteins. If two genes are homologous,
                                                then they evolved from a common ancestor and suggests that they share
                                                common functionality. Thus, the motivation of developing tools to
                                                analyze many DNA and protein sequences is to identify domains or motifs
                                                that are shared among common molecules, which can be accomplished by
                                                aligning sequencing. This has many uses, one of which is to use our
                                                understanding of proteins and gene relatedness within and between
                                                organisms, to imrpove our fundemental understanding of life. Protein
                                                Alignment Basic Local Alignment Search Tool BLAST Algorithm The Basic
                                                Local Alignment Search Tool BLAST Algorithm reveals what related
                                                sequences are present in the same and other organisms. At a large scale
                                                view, BLAST preforms a pairwise sequence alignment for one sequence,
                                                termed the query, and the entire database, termed the target. This
                                                results in tens of millions of sequences evaluations where the return
                                                data are matches that most closley relate. A benefit of BLAST compared
                                                to other global or local alignment algorithms is that it offers a local
                                                alignment strategy having both speed and sensitivity. In this section we
                                                will analyze the procedure for the BLAST algorithm. The procedure for
                                                BLAST is described in the following four sequential steps: sequence of
                                                interest, BLAST selected implementation, database selection, and then
                                                parameter selection. Sequence of Interest BLAST Selected Implementation
                                                Database Selection Parameter Selection To continue. Pairwise Sequence
                                                AlignmentIntroduction Two proteins relating at the sequence levels
                                                suggest they are homologous and is an important question for genes or
                                                proteins. If two genes are homologous, then they evolved from a common
                                                ancestor and suggests that they share common functionality. Thus, the
                                                motivation of developing tools to analyze many DNA and protein sequences
                                                is to identify domains or motifs that are shared among common molecules,
                                                which can be accomplished by aligning sequencing. This has many uses,
                                                one of which is to use our understanding of proteins and gene
                                                relatedness within and between organisms, to imrpove our fundemental
                                                understanding of life. Introduction Two proteins relating at the
                                                sequence levels suggest they are homologous and is an important question
                                                for genes or proteins. If two genes are homologous, then they evolved
                                                from a common ancestor and suggests that they share common
                                                functionality. Thus, the motivation of developing tools to analyze many
                                                DNA and protein sequences is to identify domains or motifs that are
                                                shared among common molecules, which can be accomplished by aligning
                                                sequencing. This has many uses, one of which is to use our understanding
                                                of proteins and gene relatedness within and between organisms, to
                                                imrpove our fundemental understanding of life. Protein Alignment Basic
                                                Local Alignment Search Tool BLAST Algorithm The Basic Local Alignment
                                                Search Tool BLAST Algorithm reveals what related sequences are present
                                                in the same and other organisms. At a large scale view, BLAST preforms a
                                                pairwise sequence alignment for one sequence, termed the query, and the
                                                entire database, termed the target. This results in tens of millions of
                                                sequences evaluations where the return data are matches that most
                                                closley relate. A benefit of BLAST compared to other global or local
                                                alignment algorithms is that it offers a local alignment strategy having
                                                both speed and sensitivity. In this section we will analyze the
                                                procedure for the BLAST algorithm. The procedure for BLAST is described
                                                in the following four sequential steps: sequence of interest, BLAST
                                                selected implementation, database selection, and then parameter
                                                selection. Sequence of Interest BLAST Selected Implementation Database
                                                Selection Parameter Selection To continue. Protein AlignmentBasic Local
                                                Alignment Search Tool BLAST Algorithm The Basic Local Alignment Search
                                                Tool BLAST Algorithm reveals what related sequences are present in the
                                                same and other organisms. At a large scale view, BLAST preforms a
                                                pairwise sequence alignment for one sequence, termed the query, and the
                                                entire database, termed the target. This results in tens of millions of
                                                sequences evaluations where the return data are matches that most
                                                closley relate. A benefit of BLAST compared to other global or local
                                                alignment algorithms is that it offers a local alignment strategy having
                                                both speed and sensitivity. In this section we will analyze the
                                                procedure for the BLAST algorithm. The procedure for BLAST is described
                                                in the following four sequential steps: sequence of interest, BLAST
                                                selected implementation, database selection, and then parameter
                                                selection. Sequence of Interest BLAST Selected Implementation Database
                                                Selection Parameter Selection To continue. Basic Local Alignment Search
                                                Tool BLAST Algorithm The Basic Local Alignment Search Tool BLAST
                                                Algorithm reveals what related sequences are present in the same and
                                                other organisms. At a large scale view, BLAST preforms a pairwise
                                                sequence alignment for one sequence, termed the query, and the entire
                                                database, termed the target. This results in tens of millions of
                                                sequences evaluations where the return data are matches that most
                                                closley relate. A benefit of BLAST compared to other global or local
                                                alignment algorithms is that it offers a local alignment strategy having
                                                both speed and sensitivity. In this section we will analyze the
                                                procedure for the BLAST algorithm. The procedure for BLAST is described
                                                in the following four sequential steps: sequence of interest, BLAST
                                                selected implementation, database selection, and then parameter
                                                selection. sequence of interestBLAST selected implementation database
                                                selection parameter selectionSequence of InterestSequence of
                                                InterestBLAST Selected ImplementationBLAST Selected
                                                ImplementationDatabase SelectionDatabase SelectionParameter
                                                SelectionParameter Selection To continue.
                                                [https://www.contextswitching.org/my/papers/qcnn/]
                                                You are being redirected by your loyal guides 🐶🐱 You are being
                                                redirected by your loyal guides 🐶🐱 You are being redirected by your
                                                loyal guides 🐶🐱
                                                [https://www.contextswitching.org/neuro/connectionsinhumanstructuralconnectome]
                                                Connections in the Human Structural Connectome - Context Switching
                                                Connections in the Human Structural Connectome Page Reference: Cliques
                                                and cavities in the human connectome by Ann E. Sizemore, Chad Giusti,
                                                Ari Kahn, Jean M. Vettel, Richard F. Betzel, and Danielle S. Bassett
                                                $^{[1]}$ Introduction From the human structural connectome, the
                                                authors'$^{[1]}$ attempt to extract architectural features using
                                                diffusion spectrum imaging DSI and encode the data required into
                                                triplcate as undirected, weighted network. They do so to capture as much
                                                information about possible paths which transmit as human process and
                                                preform complex behaviors. Persistent Homology in Neural Networks Our
                                                interest is in isolating mesoscale structural features, specifically
                                                complex minimal cycles in weighted neural networks. First, we will
                                                identify cliques or brain regions that rapidly and effectively share
                                                information. Formally, let a $(k+1)$-clique of graph $G$ be a set of
                                                $(k+1)$ pairwise edges in $G$. A subgraph of a clique must be a clique
                                                itself of a lower degree, which is called a face. To continue. Local
                                                Neighborhoods in Strucutral Brain Networks Clique Features of Local
                                                Neighborhoods Clique Feature Representation Neurological data can be
                                                summairzed as a network of corresponding node brain regions with
                                                weighted edges that is represented as the density of white matter
                                                steamlines reconstructed between corresepodning nodes. Image Source: See
                                                Reference [1] In the above figure, the authors present a group-averaged
                                                network that has an edge density $\rho=0.25$. For values examined as
                                                null-models with minimally Euclidean invasive wired networks, weights
                                                assigning edge weights inverse between brain regions can be assigned.
                                                Glossary Diffusion Specturm Imaging DSI Diffusion Spectrum Imaging (DSI)
                                                is an advanced magnetic resonance imaging (MRI) technique used in
                                                neuroimaging to study complex tissue microstructures, particularly in
                                                the brain. Connections in the Human Structural Connectome - Context
                                                Switching Connections in the Human Structural Connectome - Context
                                                SwitchingConnections in the Human Structural Connectome Page Reference:
                                                Cliques and cavities in the human connectome by Ann E. Sizemore, Chad
                                                Giusti, Ari Kahn, Jean M. Vettel, Richard F. Betzel, and Danielle S.
                                                Bassett $^{[1]}$ Introduction From the human structural connectome, the
                                                authors'$^{[1]}$ attempt to extract architectural features using
                                                diffusion spectrum imaging DSI and encode the data required into
                                                triplcate as undirected, weighted network. They do so to capture as much
                                                information about possible paths which transmit as human process and
                                                preform complex behaviors. Persistent Homology in Neural Networks Our
                                                interest is in isolating mesoscale structural features, specifically
                                                complex minimal cycles in weighted neural networks. First, we will
                                                identify cliques or brain regions that rapidly and effectively share
                                                information. Formally, let a $(k+1)$-clique of graph $G$ be a set of
                                                $(k+1)$ pairwise edges in $G$. A subgraph of a clique must be a clique
                                                itself of a lower degree, which is called a face. To continue. Local
                                                Neighborhoods in Strucutral Brain Networks Clique Features of Local
                                                Neighborhoods Clique Feature Representation Neurological data can be
                                                summairzed as a network of corresponding node brain regions with
                                                weighted edges that is represented as the density of white matter
                                                steamlines reconstructed between corresepodning nodes. Image Source: See
                                                Reference [1] In the above figure, the authors present a group-averaged
                                                network that has an edge density $\rho=0.25$. For values examined as
                                                null-models with minimally Euclidean invasive wired networks, weights
                                                assigning edge weights inverse between brain regions can be assigned.
                                                Glossary Diffusion Specturm Imaging DSI Diffusion Spectrum Imaging (DSI)
                                                is an advanced magnetic resonance imaging (MRI) technique used in
                                                neuroimaging to study complex tissue microstructures, particularly in
                                                the brain. Connections in the Human Structural Connectome Page
                                                Reference: Cliques and cavities in the human connectome by Ann E.
                                                Sizemore, Chad Giusti, Ari Kahn, Jean M. Vettel, Richard F. Betzel, and
                                                Danielle S. Bassett $^{[1]}$ Introduction From the human structural
                                                connectome, the authors'$^{[1]}$ attempt to extract architectural
                                                features using diffusion spectrum imaging DSI and encode the data
                                                required into triplcate as undirected, weighted network. They do so to
                                                capture as much information about possible paths which transmit as human
                                                process and preform complex behaviors. Persistent Homology in Neural
                                                Networks Our interest is in isolating mesoscale structural features,
                                                specifically complex minimal cycles in weighted neural networks. First,
                                                we will identify cliques or brain regions that rapidly and effectively
                                                share information. Formally, let a $(k+1)$-clique of graph $G$ be a set
                                                of $(k+1)$ pairwise edges in $G$. A subgraph of a clique must be a
                                                clique itself of a lower degree, which is called a face. To continue.
                                                Local Neighborhoods in Strucutral Brain Networks Clique Features of
                                                Local Neighborhoods Clique Feature Representation Neurological data can
                                                be summairzed as a network of corresponding node brain regions with
                                                weighted edges that is represented as the density of white matter
                                                steamlines reconstructed between corresepodning nodes. Image Source: See
                                                Reference [1] In the above figure, the authors present a group-averaged
                                                network that has an edge density $\rho=0.25$. For values examined as
                                                null-models with minimally Euclidean invasive wired networks, weights
                                                assigning edge weights inverse between brain regions can be assigned.
                                                Glossary Diffusion Specturm Imaging DSI Diffusion Spectrum Imaging (DSI)
                                                is an advanced magnetic resonance imaging (MRI) technique used in
                                                neuroimaging to study complex tissue microstructures, particularly in
                                                the brain. Connections in the Human Structural Connectome Page
                                                Reference: Cliques and cavities in the human connectome by Ann E.
                                                Sizemore, Chad Giusti, Ari Kahn, Jean M. Vettel, Richard F. Betzel, and
                                                Danielle S. Bassett $^{[1]}$ Introduction From the human structural
                                                connectome, the authors'$^{[1]}$ attempt to extract architectural
                                                features using diffusion spectrum imaging DSI and encode the data
                                                required into triplcate as undirected, weighted network. They do so to
                                                capture as much information about possible paths which transmit as human
                                                process and preform complex behaviors. Persistent Homology in Neural
                                                Networks Our interest is in isolating mesoscale structural features,
                                                specifically complex minimal cycles in weighted neural networks. First,
                                                we will identify cliques or brain regions that rapidly and effectively
                                                share information. Formally, let a $(k+1)$-clique of graph $G$ be a set
                                                of $(k+1)$ pairwise edges in $G$. A subgraph of a clique must be a
                                                clique itself of a lower degree, which is called a face. To continue.
                                                Local Neighborhoods in Strucutral Brain Networks Clique Features of
                                                Local Neighborhoods Clique Feature Representation Neurological data can
                                                be summairzed as a network of corresponding node brain regions with
                                                weighted edges that is represented as the density of white matter
                                                steamlines reconstructed between corresepodning nodes. Image Source: See
                                                Reference [1] In the above figure, the authors present a group-averaged
                                                network that has an edge density $\rho=0.25$. For values examined as
                                                null-models with minimally Euclidean invasive wired networks, weights
                                                assigning edge weights inverse between brain regions can be assigned.
                                                Connections in the Human Structural ConnectomePage Reference: Cliques
                                                and cavities in the human connectome by Ann E. Sizemore, Chad Giusti,
                                                Ari Kahn, Jean M. Vettel, Richard F. Betzel, and Danielle S. Bassett
                                                $^{[1]}$ Cliques and cavities in the human connectome by Ann E.
                                                Sizemore, Chad Giusti, Ari Kahn, Jean M. Vettel, Richard F. Betzel, and
                                                Danielle S. Bassett $^{[1]}$ Cliques and cavities in the human
                                                connectome by Ann E. Sizemore, Chad Giusti, Ari Kahn, Jean M. Vettel,
                                                Richard F. Betzel, and Danielle S. Bassett Introduction From the human
                                                structural connectome, the authors'$^{[1]}$ attempt to extract
                                                architectural features using diffusion spectrum imaging DSI and encode
                                                the data required into triplcate as undirected, weighted network. They
                                                do so to capture as much information about possible paths which transmit
                                                as human process and preform complex behaviors. Introduction From the
                                                human structural connectome, the authors'$^{[1]}$ attempt to extract
                                                architectural features using diffusion spectrum imaging DSI and encode
                                                the data required into triplcate as undirected, weighted network. They
                                                do so to capture as much information about possible paths which transmit
                                                as human process and preform complex behaviors.
                                                authors'$^{[1]}$diffusion spectrum imaging DSIPersistent Homology in
                                                Neural Networks Our interest is in isolating mesoscale structural
                                                features, specifically complex minimal cycles in weighted neural
                                                networks. First, we will identify cliques or brain regions that rapidly
                                                and effectively share information. Formally, let a $(k+1)$-clique of
                                                graph $G$ be a set of $(k+1)$ pairwise edges in $G$. A subgraph of a
                                                clique must be a clique itself of a lower degree, which is called a
                                                face. To continue. Persistent Homology in Neural NetworksOur interest is
                                                in isolating mesoscale structural features, specifically complex minimal
                                                cycles in weighted neural networks. First, we will identify cliques or
                                                brain regions that rapidly and effectively share information. Formally,
                                                let a $(k+1)$-clique of graph $G$ be a set of $(k+1)$ pairwise edges in
                                                $G$. A subgraph of a clique must be a clique itself of a lower degree,
                                                which is called a face. To continue. Local Neighborhoods in Strucutral
                                                Brain Networks Clique Features of Local Neighborhoods Clique Feature
                                                Representation Neurological data can be summairzed as a network of
                                                corresponding node brain regions with weighted edges that is represented
                                                as the density of white matter steamlines reconstructed between
                                                corresepodning nodes. Image Source: See Reference [1] In the above
                                                figure, the authors present a group-averaged network that has an edge
                                                density $\rho=0.25$. For values examined as null-models with minimally
                                                Euclidean invasive wired networks, weights assigning edge weights
                                                inverse between brain regions can be assigned. Local Neighborhoods in
                                                Strucutral Brain Networks Clique Features of Local Neighborhoods Clique
                                                Feature Representation Neurological data can be summairzed as a network
                                                of corresponding node brain regions with weighted edges that is
                                                represented as the density of white matter steamlines reconstructed
                                                between corresepodning nodes. Clique Features of Local Neighborhoods
                                                Clique Feature Representation Neurological data can be summairzed as a
                                                network of corresponding node brain regions with weighted edges that is
                                                represented as the density of white matter steamlines reconstructed
                                                between corresepodning nodes. Clique Feature Representation Neurological
                                                data can be summairzed as a network of corresponding node brain regions
                                                with weighted edges that is represented as the density of white matter
                                                steamlines reconstructed between corresepodning nodes. Image Source: See
                                                Reference [1] See Reference [1]In the above figure, the authors present
                                                a group-averaged network that has an edge density $\rho=0.25$. For
                                                values examined as null-models with minimally Euclidean invasive wired
                                                networks, weights assigning edge weights inverse between brain regions
                                                can be assigned.Glossary Diffusion Specturm Imaging DSI Diffusion
                                                Spectrum Imaging (DSI) is an advanced magnetic resonance imaging (MRI)
                                                technique used in neuroimaging to study complex tissue microstructures,
                                                particularly in the brain. GlossaryDiffusion Specturm Imaging DSI
                                                Diffusion Spectrum Imaging (DSI) is an advanced magnetic resonance
                                                imaging (MRI) technique used in neuroimaging to study complex tissue
                                                microstructures, particularly in the brain. Diffusion Specturm Imaging
                                                DSIDiffusion Spectrum Imaging (DSI) is an advanced magnetic resonance
                                                imaging (MRI) technique used in neuroimaging to study complex tissue
                                                microstructures, particularly in the brain.
                                                [https://www.contextswitching.org/tcs/theoryofcomputation]
                                                Theory of Computation - Context Switching Theory of Computation
                                                Deterministic Automatons and Languages Regular Languages Deterministic
                                                Finite Automaton DFA A deterministic finite automaton DFA is a 5-tuple:
                                                $(Q, \Sigma, \delta, q_{0}, F)$ where: $Q$ is a finite set of states
                                                $\Sigma$ is an alphabet $\delta$ is a transition function described as
                                                $\delta : Q \times \Sigma \rightarrow Q$ $q_{0} \in Q$ is the initial
                                                state $F \subseteq Q$ is a subset of states called accept states Context
                                                Free Languages Languages Pushdown Automaton PDA A pushdown automata PFA
                                                is a 6-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, F)$ where: $Q$ is a
                                                finite set of states $\Sigma$ is an alphabet $\Gamma$ is the stack
                                                alphabet $\delta$ is a transition function described as $\delta : Q
                                                \times \Sigma \times \Gamma \rightarrow P(Q \times \Gamma_{\epsilon})$
                                                $q_{0} \in Q$ is the initial state $F \subseteq Q$ is a subset of states
                                                called accept states Turing Complete Languages Basic Turing Machine A
                                                Basic Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                                q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets
                                                and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                                blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in
                                                \Gamma$ and $\Sigma \subseteq \Gamma$ $\delta$ is a transition function
                                                described as $\delta : Q \times \Sigma \rightarrow Q \times \Gamma
                                                \times \left\{L,\;R\right\}$ $q_{0} \in Q$ is the initial state
                                                $q_{accept} \in Q$ is the accept state $q_{reject} \in Q$ is the reject
                                                state, where $q_{reject} \neq q_{accept}$ Context-Sensitve Lanuages
                                                Linear-Bound Automaton (LBA) To continue. Nondeterministic Automatons
                                                and Languages Nondeterministic Finite Automaton A deterministic finite
                                                automaton DFA is a 5-tuple: $(Q, \Sigma, \delta, q_{0}, F)$ where: $Q$
                                                is a finite set of states $\Sigma$ is an alphabet $\delta$ is a
                                                transition function described as $\delta : Q \times \Sigma_{\epsilon}
                                                \rightarrow \mathcal{P}(Q)$ $q_{0} \in Q$ is the initial state $F
                                                \subseteq Q$ is a subset of states called accept states Nondeterministic
                                                Turing Machine To continue. Theory of Computation - Context Switching
                                                Theory of Computation - Context SwitchingTheory of Computation
                                                Deterministic Automatons and Languages Regular Languages Deterministic
                                                Finite Automaton DFA A deterministic finite automaton DFA is a 5-tuple:
                                                $(Q, \Sigma, \delta, q_{0}, F)$ where: $Q$ is a finite set of states
                                                $\Sigma$ is an alphabet $\delta$ is a transition function described as
                                                $\delta : Q \times \Sigma \rightarrow Q$ $q_{0} \in Q$ is the initial
                                                state $F \subseteq Q$ is a subset of states called accept states Context
                                                Free Languages Languages Pushdown Automaton PDA A pushdown automata PFA
                                                is a 6-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, F)$ where: $Q$ is a
                                                finite set of states $\Sigma$ is an alphabet $\Gamma$ is the stack
                                                alphabet $\delta$ is a transition function described as $\delta : Q
                                                \times \Sigma \times \Gamma \rightarrow P(Q \times \Gamma_{\epsilon})$
                                                $q_{0} \in Q$ is the initial state $F \subseteq Q$ is a subset of states
                                                called accept states Turing Complete Languages Basic Turing Machine A
                                                Basic Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                                q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets
                                                and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                                blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in
                                                \Gamma$ and $\Sigma \subseteq \Gamma$ $\delta$ is a transition function
                                                described as $\delta : Q \times \Sigma \rightarrow Q \times \Gamma
                                                \times \left\{L,\;R\right\}$ $q_{0} \in Q$ is the initial state
                                                $q_{accept} \in Q$ is the accept state $q_{reject} \in Q$ is the reject
                                                state, where $q_{reject} \neq q_{accept}$ Context-Sensitve Lanuages
                                                Linear-Bound Automaton (LBA) To continue. Nondeterministic Automatons
                                                and Languages Nondeterministic Finite Automaton A deterministic finite
                                                automaton DFA is a 5-tuple: $(Q, \Sigma, \delta, q_{0}, F)$ where: $Q$
                                                is a finite set of states $\Sigma$ is an alphabet $\delta$ is a
                                                transition function described as $\delta : Q \times \Sigma_{\epsilon}
                                                \rightarrow \mathcal{P}(Q)$ $q_{0} \in Q$ is the initial state $F
                                                \subseteq Q$ is a subset of states called accept states Nondeterministic
                                                Turing Machine To continue. Theory of Computation Deterministic
                                                Automatons and Languages Regular Languages Deterministic Finite
                                                Automaton DFA A deterministic finite automaton DFA is a 5-tuple: $(Q,
                                                \Sigma, \delta, q_{0}, F)$ where: $Q$ is a finite set of states $\Sigma$
                                                is an alphabet $\delta$ is a transition function described as $\delta :
                                                Q \times \Sigma \rightarrow Q$ $q_{0} \in Q$ is the initial state $F
                                                \subseteq Q$ is a subset of states called accept states Context Free
                                                Languages Languages Pushdown Automaton PDA A pushdown automata PFA is a
                                                6-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, F)$ where: $Q$ is a finite
                                                set of states $\Sigma$ is an alphabet $\Gamma$ is the stack alphabet
                                                $\delta$ is a transition function described as $\delta : Q \times \Sigma
                                                \times \Gamma \rightarrow P(Q \times \Gamma_{\epsilon})$ $q_{0} \in Q$
                                                is the initial state $F \subseteq Q$ is a subset of states called accept
                                                states Turing Complete Languages Basic Turing Machine A Basic Turing
                                                Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{accept},
                                                q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets and: $Q$ is a
                                                set of states $\Sigma$ is an alphabet not containing the blank symbol
                                                $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$ and
                                                $\Sigma \subseteq \Gamma$ $\delta$ is a transition function described as
                                                $\delta : Q \times \Sigma \rightarrow Q \times \Gamma \times
                                                \left\{L,\;R\right\}$ $q_{0} \in Q$ is the initial state $q_{accept} \in
                                                Q$ is the accept state $q_{reject} \in Q$ is the reject state, where
                                                $q_{reject} \neq q_{accept}$ Context-Sensitve Lanuages Linear-Bound
                                                Automaton (LBA) To continue. Nondeterministic Automatons and Languages
                                                Nondeterministic Finite Automaton A deterministic finite automaton DFA
                                                is a 5-tuple: $(Q, \Sigma, \delta, q_{0}, F)$ where: $Q$ is a finite set
                                                of states $\Sigma$ is an alphabet $\delta$ is a transition function
                                                described as $\delta : Q \times \Sigma_{\epsilon} \rightarrow
                                                \mathcal{P}(Q)$ $q_{0} \in Q$ is the initial state $F \subseteq Q$ is a
                                                subset of states called accept states Nondeterministic Turing Machine To
                                                continue. Theory of Computation Deterministic Automatons and Languages
                                                Regular Languages Deterministic Finite Automaton DFA A deterministic
                                                finite automaton DFA is a 5-tuple: $(Q, \Sigma, \delta, q_{0}, F)$
                                                where: $Q$ is a finite set of states $\Sigma$ is an alphabet $\delta$ is
                                                a transition function described as $\delta : Q \times \Sigma \rightarrow
                                                Q$ $q_{0} \in Q$ is the initial state $F \subseteq Q$ is a subset of
                                                states called accept states Context Free Languages Languages Pushdown
                                                Automaton PDA A pushdown automata PFA is a 6-tuple: $(Q, \Sigma, \Gamma,
                                                \delta, q_{0}, F)$ where: $Q$ is a finite set of states $\Sigma$ is an
                                                alphabet $\Gamma$ is the stack alphabet $\delta$ is a transition
                                                function described as $\delta : Q \times \Sigma \times \Gamma
                                                \rightarrow P(Q \times \Gamma_{\epsilon})$ $q_{0} \in Q$ is the initial
                                                state $F \subseteq Q$ is a subset of states called accept states Turing
                                                Complete Languages Basic Turing Machine A Basic Turing Machine is a
                                                7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{accept}, q_{reject})$
                                                where $Q, \Sigma, \Gamma$ are all finite sets and: $Q$ is a set of
                                                states $\Sigma$ is an alphabet not containing the blank symbol $\sqcup$
                                                $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$ and $\Sigma
                                                \subseteq \Gamma$ $\delta$ is a transition function described as $\delta
                                                : Q \times \Sigma \rightarrow Q \times \Gamma \times
                                                \left\{L,\;R\right\}$ $q_{0} \in Q$ is the initial state $q_{accept} \in
                                                Q$ is the accept state $q_{reject} \in Q$ is the reject state, where
                                                $q_{reject} \neq q_{accept}$ Context-Sensitve Lanuages Linear-Bound
                                                Automaton (LBA) To continue. Nondeterministic Automatons and Languages
                                                Nondeterministic Finite Automaton A deterministic finite automaton DFA
                                                is a 5-tuple: $(Q, \Sigma, \delta, q_{0}, F)$ where: $Q$ is a finite set
                                                of states $\Sigma$ is an alphabet $\delta$ is a transition function
                                                described as $\delta : Q \times \Sigma_{\epsilon} \rightarrow
                                                \mathcal{P}(Q)$ $q_{0} \in Q$ is the initial state $F \subseteq Q$ is a
                                                subset of states called accept states Nondeterministic Turing Machine To
                                                continue. Theory of ComputationDeterministic Automatons and Languages
                                                Regular Languages Deterministic Finite Automaton DFA A deterministic
                                                finite automaton DFA is a 5-tuple: $(Q, \Sigma, \delta, q_{0}, F)$
                                                where: $Q$ is a finite set of states $\Sigma$ is an alphabet $\delta$ is
                                                a transition function described as $\delta : Q \times \Sigma \rightarrow
                                                Q$ $q_{0} \in Q$ is the initial state $F \subseteq Q$ is a subset of
                                                states called accept states Context Free Languages Languages Pushdown
                                                Automaton PDA A pushdown automata PFA is a 6-tuple: $(Q, \Sigma, \Gamma,
                                                \delta, q_{0}, F)$ where: $Q$ is a finite set of states $\Sigma$ is an
                                                alphabet $\Gamma$ is the stack alphabet $\delta$ is a transition
                                                function described as $\delta : Q \times \Sigma \times \Gamma
                                                \rightarrow P(Q \times \Gamma_{\epsilon})$ $q_{0} \in Q$ is the initial
                                                state $F \subseteq Q$ is a subset of states called accept states Turing
                                                Complete Languages Basic Turing Machine A Basic Turing Machine is a
                                                7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{accept}, q_{reject})$
                                                where $Q, \Sigma, \Gamma$ are all finite sets and: $Q$ is a set of
                                                states $\Sigma$ is an alphabet not containing the blank symbol $\sqcup$
                                                $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$ and $\Sigma
                                                \subseteq \Gamma$ $\delta$ is a transition function described as $\delta
                                                : Q \times \Sigma \rightarrow Q \times \Gamma \times
                                                \left\{L,\;R\right\}$ $q_{0} \in Q$ is the initial state $q_{accept} \in
                                                Q$ is the accept state $q_{reject} \in Q$ is the reject state, where
                                                $q_{reject} \neq q_{accept}$ Context-Sensitve Lanuages Linear-Bound
                                                Automaton (LBA) To continue. Deterministic Automatons and Languages
                                                Regular Languages Deterministic Finite Automaton DFA A deterministic
                                                finite automaton DFA is a 5-tuple: $(Q, \Sigma, \delta, q_{0}, F)$
                                                where: $Q$ is a finite set of states $\Sigma$ is an alphabet $\delta$ is
                                                a transition function described as $\delta : Q \times \Sigma \rightarrow
                                                Q$ $q_{0} \in Q$ is the initial state $F \subseteq Q$ is a subset of
                                                states called accept states Regular LanguagesDeterministic Finite
                                                Automaton DFA A deterministic finite automaton DFA is a 5-tuple: $(Q,
                                                \Sigma, \delta, q_{0}, F)$ where: $Q$ is a finite set of states $\Sigma$
                                                is an alphabet $\delta$ is a transition function described as $\delta :
                                                Q \times \Sigma \rightarrow Q$ $q_{0} \in Q$ is the initial state $F
                                                \subseteq Q$ is a subset of states called accept states Deterministic
                                                Finite Automaton DFAA deterministic finite automaton DFA is a 5-tuple:
                                                $(Q, \Sigma, \delta, q_{0}, F)$ where: $Q$ is a finite set of states
                                                $\Sigma$ is an alphabet $\delta$ is a transition function described as
                                                $\delta : Q \times \Sigma \rightarrow Q$ $q_{0} \in Q$ is the initial
                                                state $F \subseteq Q$ is a subset of states called accept states $Q$ is
                                                a finite set of states $\Sigma$ is an alphabet $\delta$ is a transition
                                                function described as $\delta : Q \times \Sigma \rightarrow Q$ $q_{0}
                                                \in Q$ is the initial state $F \subseteq Q$ is a subset of states called
                                                accept states $Q$ is a finite set of states$\Sigma$ is an
                                                alphabet$\delta$ is a transition function described as $\delta : Q
                                                \times \Sigma \rightarrow Q$$q_{0} \in Q$ is the initial state$F
                                                \subseteq Q$ is a subset of states called accept statesContext Free
                                                Languages Languages Pushdown Automaton PDA A pushdown automata PFA is a
                                                6-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, F)$ where: $Q$ is a finite
                                                set of states $\Sigma$ is an alphabet $\Gamma$ is the stack alphabet
                                                $\delta$ is a transition function described as $\delta : Q \times \Sigma
                                                \times \Gamma \rightarrow P(Q \times \Gamma_{\epsilon})$ $q_{0} \in Q$
                                                is the initial state $F \subseteq Q$ is a subset of states called accept
                                                states Context Free Languages LanguagesPushdown Automaton PDA A pushdown
                                                automata PFA is a 6-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, F)$
                                                where: $Q$ is a finite set of states $\Sigma$ is an alphabet $\Gamma$ is
                                                the stack alphabet $\delta$ is a transition function described as
                                                $\delta : Q \times \Sigma \times \Gamma \rightarrow P(Q \times
                                                \Gamma_{\epsilon})$ $q_{0} \in Q$ is the initial state $F \subseteq Q$
                                                is a subset of states called accept states Pushdown Automaton PDAA
                                                pushdown automata PFA is a 6-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                                F)$ where: $Q$ is a finite set of states $\Sigma$ is an alphabet
                                                $\Gamma$ is the stack alphabet $\delta$ is a transition function
                                                described as $\delta : Q \times \Sigma \times \Gamma \rightarrow P(Q
                                                \times \Gamma_{\epsilon})$ $q_{0} \in Q$ is the initial state $F
                                                \subseteq Q$ is a subset of states called accept states $Q$ is a finite
                                                set of states $\Sigma$ is an alphabet $\Gamma$ is the stack alphabet
                                                $\delta$ is a transition function described as $\delta : Q \times \Sigma
                                                \times \Gamma \rightarrow P(Q \times \Gamma_{\epsilon})$ $q_{0} \in Q$
                                                is the initial state $F \subseteq Q$ is a subset of states called accept
                                                states $Q$ is a finite set of states$\Sigma$ is an alphabet$\Gamma$ is
                                                the stack alphabet$\delta$ is a transition function described as $\delta
                                                : Q \times \Sigma \times \Gamma \rightarrow P(Q \times
                                                \Gamma_{\epsilon})$$q_{0} \in Q$ is the initial state$F \subseteq Q$ is
                                                a subset of states called accept statesTuring Complete Languages Basic
                                                Turing Machine A Basic Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma,
                                                \delta, q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are
                                                all finite sets and: $Q$ is a set of states $\Sigma$ is an alphabet not
                                                containing the blank symbol $\sqcup$ $\Gamma$ is the tape alphabet,
                                                where $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$ $\delta$ is a
                                                transition function described as $\delta : Q \times \Sigma \rightarrow Q
                                                \times \Gamma \times \left\{L,\;R\right\}$ $q_{0} \in Q$ is the initial
                                                state $q_{accept} \in Q$ is the accept state $q_{reject} \in Q$ is the
                                                reject state, where $q_{reject} \neq q_{accept}$ Turing Complete
                                                LanguagesBasic Turing Machine A Basic Turing Machine is a 7-tuple: $(Q,
                                                \Sigma, \Gamma, \delta, q_{0}, q_{accept}, q_{reject})$ where $Q,
                                                \Sigma, \Gamma$ are all finite sets and: $Q$ is a set of states $\Sigma$
                                                is an alphabet not containing the blank symbol $\sqcup$ $\Gamma$ is the
                                                tape alphabet, where $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$
                                                $\delta$ is a transition function described as $\delta : Q \times \Sigma
                                                \rightarrow Q \times \Gamma \times \left\{L,\;R\right\}$ $q_{0} \in Q$
                                                is the initial state $q_{accept} \in Q$ is the accept state $q_{reject}
                                                \in Q$ is the reject state, where $q_{reject} \neq q_{accept}$ Basic
                                                Turing MachineA Basic Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma,
                                                \delta, q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are
                                                all finite sets and: $Q$ is a set of states $\Sigma$ is an alphabet not
                                                containing the blank symbol $\sqcup$ $\Gamma$ is the tape alphabet,
                                                where $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$ $\delta$ is a
                                                transition function described as $\delta : Q \times \Sigma \rightarrow Q
                                                \times \Gamma \times \left\{L,\;R\right\}$ $q_{0} \in Q$ is the initial
                                                state $q_{accept} \in Q$ is the accept state $q_{reject} \in Q$ is the
                                                reject state, where $q_{reject} \neq q_{accept}$ $Q$ is a set of states
                                                $\Sigma$ is an alphabet not containing the blank symbol $\sqcup$
                                                $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$ and $\Sigma
                                                \subseteq \Gamma$ $\delta$ is a transition function described as $\delta
                                                : Q \times \Sigma \rightarrow Q \times \Gamma \times
                                                \left\{L,\;R\right\}$ $q_{0} \in Q$ is the initial state $q_{accept} \in
                                                Q$ is the accept state $q_{reject} \in Q$ is the reject state, where
                                                $q_{reject} \neq q_{accept}$ $Q$ is a set of states$\Sigma$ is an
                                                alphabet not containing the blank symbol $\sqcup$$\Gamma$ is the tape
                                                alphabet, where $\sqcup \in \Gamma$ and $\Sigma \subseteq
                                                \Gamma$$\delta$ is a transition function described as $\delta : Q \times
                                                \Sigma \rightarrow Q \times \Gamma \times \left\{L,\;R\right\}$$q_{0}
                                                \in Q$ is the initial state$q_{accept} \in Q$ is the accept
                                                state$q_{reject} \in Q$ is the reject state, where $q_{reject} \neq
                                                q_{accept}$Context-Sensitve Lanuages Linear-Bound Automaton (LBA) To
                                                continue. Context-Sensitve LanuagesLinear-Bound Automaton (LBA) To
                                                continue. Linear-Bound Automaton (LBA)To continue. Nondeterministic
                                                Automatons and Languages Nondeterministic Finite Automaton A
                                                deterministic finite automaton DFA is a 5-tuple: $(Q, \Sigma, \delta,
                                                q_{0}, F)$ where: $Q$ is a finite set of states $\Sigma$ is an alphabet
                                                $\delta$ is a transition function described as $\delta : Q \times
                                                \Sigma_{\epsilon} \rightarrow \mathcal{P}(Q)$ $q_{0} \in Q$ is the
                                                initial state $F \subseteq Q$ is a subset of states called accept states
                                                Nondeterministic Turing Machine To continue. Nondeterministic Automatons
                                                and Languages Nondeterministic Finite Automaton A deterministic finite
                                                automaton DFA is a 5-tuple: $(Q, \Sigma, \delta, q_{0}, F)$ where: $Q$
                                                is a finite set of states $\Sigma$ is an alphabet $\delta$ is a
                                                transition function described as $\delta : Q \times \Sigma_{\epsilon}
                                                \rightarrow \mathcal{P}(Q)$ $q_{0} \in Q$ is the initial state $F
                                                \subseteq Q$ is a subset of states called accept states Nondeterministic
                                                Finite AutomatonA deterministic finite automaton DFA is a 5-tuple: $(Q,
                                                \Sigma, \delta, q_{0}, F)$ where: $Q$ is a finite set of states $\Sigma$
                                                is an alphabet $\delta$ is a transition function described as $\delta :
                                                Q \times \Sigma_{\epsilon} \rightarrow \mathcal{P}(Q)$ $q_{0} \in Q$ is
                                                the initial state $F \subseteq Q$ is a subset of states called accept
                                                states $Q$ is a finite set of states $\Sigma$ is an alphabet $\delta$ is
                                                a transition function described as $\delta : Q \times \Sigma_{\epsilon}
                                                \rightarrow \mathcal{P}(Q)$ $q_{0} \in Q$ is the initial state $F
                                                \subseteq Q$ is a subset of states called accept states $Q$ is a finite
                                                set of states$\Sigma$ is an alphabet$\delta$ is a transition function
                                                described as $\delta : Q \times \Sigma_{\epsilon} \rightarrow
                                                \mathcal{P}(Q)$$q_{0} \in Q$ is the initial state$F \subseteq Q$ is a
                                                subset of states called accept statesNondeterministic Turing Machine To
                                                continue. Nondeterministic Turing MachineTo continue.
                                                [https://www.contextswitching.org/tcs/graphtheory]
                                                Graph Theory - Context Switching Graph Theory What is a Graph
                                                Mathematical Unordered Pairs Set Notation $G = (V, E)$ $V$ is a set of
                                                vertices $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in V\;and\;x
                                                \neq y\right\}$ A simple undirected graph $G$ is an ordered pair or
                                                tuple $(V, E)$ where $V$ and $E$ are finite sets. $E \subseteq
                                                \left\{\left\{x, y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ states
                                                that the elements that are apart of set of edges $E$ are unordered
                                                pairs. Where the elemtents that make up our ordered pair for our edge
                                                are elements from our set $V$. Meaning, in order to define an edge from
                                                $E$, its elements or vertices must came frome set the $V$. Our two
                                                elements of our edge are our head and tail vertice. These two vertices
                                                define an edge or the elements of $E$. It is important to note that in
                                                our defintion that defines our edges in our simple graph, the edges that
                                                make up our set $E$ are unordered pairs or tuples. This means that it
                                                does not matter which vertice is the head or tail vertice. i.e. which
                                                vertice comes first and which vertice comes last. Meaning we do not have
                                                a start and end vertice for our edges, and do not care about the
                                                direction of our vertice. Rather, all that matters is we define the
                                                vertices that make up our edges and that they are apart of the vertices
                                                in our graph $V$. Unlike in a directed graph where an ordered pair
                                                defines each edge and direction of the edge matters. The reason I
                                                introduced the simple undirected graph is because it allows us to get
                                                familiar with the mathematical notation that we will build upon for
                                                other graphs and is one of the most common type of graphs. Graph Types
                                                We will now talk about a couple of different graphs that exist, more
                                                specifically ones that are commonly used. Simple Undirected Graph $G =
                                                (V, E)$ $V$ is a set of vertices $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ See What is a Graph
                                                section. Simple Directed Graph Ordered Pair $n$-ary Cartesian Product $G
                                                = (V, E)$ $V$ is a set of vertices $E \subseteq \left\{(x, y)\;|\;(x, y)
                                                \in V^{2}\;and\;x \neq y\right\}$ Notice in this one the difference is
                                                that the elements in set $E$, or the set of Edges, are tuples or ordered
                                                pairs not unordered pairs like with a directed graph. Why do the edges
                                                in the Edges set, $E$ need to be defined as an ordered pair? Well,
                                                because since the elements in our ordered pair are vertices in the
                                                graph, this means mathematically, $(x, y) \neq (y, x)$. i.e, the order
                                                matters. Thus, in the ordered pair of elements or vertices, which
                                                vertice or element in the tuple is the head and which one is the tail
                                                matters. In an unordered pair, or a set, it does not matter which one is
                                                first or last, it is more so which vertices make up the edge. So, for an
                                                edge in in the set of edges $E$ in a directedd graph, we define the
                                                collection as an ordered pair or by using a tuple. For the vertices that
                                                make up our ordered pair of edges, it does not suffice to give the rule
                                                that the vertices $x, y \in V$ or that our vertices are elements in $V$.
                                                This rule and the procedding rule in the set builder notation for $E$ in
                                                a directed graph does not produce an ordered pair. Rather, we must
                                                change it to $(x, y) \in V^{2}$. Reason being is because $V^{2}$ is
                                                equivalant as writing $V \times V$, or the cartesian product between set
                                                $V$ and set $V$. The cartesian product between sets forms one set whose
                                                elements are ordered pairs of the original sets. The general name for
                                                raising a set to $n$, is called an $n$-ary cartesian product. For
                                                example, if we had some set $X$, the $n$-ary cartesian product of set
                                                $X$ would be written as $X^{n}$. Another thing to note, is that raising
                                                a set to $2$, like in our case, it is called the cartesian square. So
                                                now, with that being said, with our updated rule we can take an element
                                                from our set $V^{2}$ are get an ordered pair, like is necessary for a
                                                directed graph. When before, with an element coming from just $V$, we
                                                were simply taking one element from the set of vertices. Loop Permitting
                                                Graphs Undirected simple graph permitting loops: $G = (V, E)$ $V$ is a
                                                set of vertices $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in
                                                V\right\}$ Notice how our set of edges $E$ is defined similarily to the
                                                Edge set of our previous non-loop permitting directed simple graph $E
                                                \subseteq \left\{(x, y)\;|\;(x, y) \in V^{2}\;and\;x \neq y\right\}$,
                                                however, in the definition for $E$ in our loop permitting graph the
                                                vertices, $x$ and $y$, for each edge can be the same vertice. This is
                                                because in our rule for loop permitting graphs in our definition for $E$
                                                it does not have the rule $x \neq y$. i.e, our updated definition for
                                                for our directed graph that allows for edges to have the same start and
                                                end vertice, or allows for the head and tail vertice to be the same
                                                vertice. Multigraphs Directed multigraph: $G = (V, E, \phi)$ $V$ is a
                                                set of vertices $E$ is a set of edges $\phi : E \rightarrow \left\{(x,
                                                y)\;|\;(x, y) \in V^{2}\; and \;x \neq y\right\}$ Weighted Graph To
                                                continue. Graph Theory - Context Switching Graph Theory - Context
                                                SwitchingGraph Theory What is a Graph Mathematical Unordered Pairs Set
                                                Notation $G = (V, E)$ $V$ is a set of vertices $E \subseteq
                                                \left\{\left\{x, y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ A
                                                simple undirected graph $G$ is an ordered pair or tuple $(V, E)$ where
                                                $V$ and $E$ are finite sets. $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ states that the
                                                elements that are apart of set of edges $E$ are unordered pairs. Where
                                                the elemtents that make up our ordered pair for our edge are elements
                                                from our set $V$. Meaning, in order to define an edge from $E$, its
                                                elements or vertices must came frome set the $V$. Our two elements of
                                                our edge are our head and tail vertice. These two vertices define an
                                                edge or the elements of $E$. It is important to note that in our
                                                defintion that defines our edges in our simple graph, the edges that
                                                make up our set $E$ are unordered pairs or tuples. This means that it
                                                does not matter which vertice is the head or tail vertice. i.e. which
                                                vertice comes first and which vertice comes last. Meaning we do not have
                                                a start and end vertice for our edges, and do not care about the
                                                direction of our vertice. Rather, all that matters is we define the
                                                vertices that make up our edges and that they are apart of the vertices
                                                in our graph $V$. Unlike in a directed graph where an ordered pair
                                                defines each edge and direction of the edge matters. The reason I
                                                introduced the simple undirected graph is because it allows us to get
                                                familiar with the mathematical notation that we will build upon for
                                                other graphs and is one of the most common type of graphs. Graph Types
                                                We will now talk about a couple of different graphs that exist, more
                                                specifically ones that are commonly used. Simple Undirected Graph $G =
                                                (V, E)$ $V$ is a set of vertices $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ See What is a Graph
                                                section. Simple Directed Graph Ordered Pair $n$-ary Cartesian Product $G
                                                = (V, E)$ $V$ is a set of vertices $E \subseteq \left\{(x, y)\;|\;(x, y)
                                                \in V^{2}\;and\;x \neq y\right\}$ Notice in this one the difference is
                                                that the elements in set $E$, or the set of Edges, are tuples or ordered
                                                pairs not unordered pairs like with a directed graph. Why do the edges
                                                in the Edges set, $E$ need to be defined as an ordered pair? Well,
                                                because since the elements in our ordered pair are vertices in the
                                                graph, this means mathematically, $(x, y) \neq (y, x)$. i.e, the order
                                                matters. Thus, in the ordered pair of elements or vertices, which
                                                vertice or element in the tuple is the head and which one is the tail
                                                matters. In an unordered pair, or a set, it does not matter which one is
                                                first or last, it is more so which vertices make up the edge. So, for an
                                                edge in in the set of edges $E$ in a directedd graph, we define the
                                                collection as an ordered pair or by using a tuple. For the vertices that
                                                make up our ordered pair of edges, it does not suffice to give the rule
                                                that the vertices $x, y \in V$ or that our vertices are elements in $V$.
                                                This rule and the procedding rule in the set builder notation for $E$ in
                                                a directed graph does not produce an ordered pair. Rather, we must
                                                change it to $(x, y) \in V^{2}$. Reason being is because $V^{2}$ is
                                                equivalant as writing $V \times V$, or the cartesian product between set
                                                $V$ and set $V$. The cartesian product between sets forms one set whose
                                                elements are ordered pairs of the original sets. The general name for
                                                raising a set to $n$, is called an $n$-ary cartesian product. For
                                                example, if we had some set $X$, the $n$-ary cartesian product of set
                                                $X$ would be written as $X^{n}$. Another thing to note, is that raising
                                                a set to $2$, like in our case, it is called the cartesian square. So
                                                now, with that being said, with our updated rule we can take an element
                                                from our set $V^{2}$ are get an ordered pair, like is necessary for a
                                                directed graph. When before, with an element coming from just $V$, we
                                                were simply taking one element from the set of vertices. Loop Permitting
                                                Graphs Undirected simple graph permitting loops: $G = (V, E)$ $V$ is a
                                                set of vertices $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in
                                                V\right\}$ Notice how our set of edges $E$ is defined similarily to the
                                                Edge set of our previous non-loop permitting directed simple graph $E
                                                \subseteq \left\{(x, y)\;|\;(x, y) \in V^{2}\;and\;x \neq y\right\}$,
                                                however, in the definition for $E$ in our loop permitting graph the
                                                vertices, $x$ and $y$, for each edge can be the same vertice. This is
                                                because in our rule for loop permitting graphs in our definition for $E$
                                                it does not have the rule $x \neq y$. i.e, our updated definition for
                                                for our directed graph that allows for edges to have the same start and
                                                end vertice, or allows for the head and tail vertice to be the same
                                                vertice. Multigraphs Directed multigraph: $G = (V, E, \phi)$ $V$ is a
                                                set of vertices $E$ is a set of edges $\phi : E \rightarrow \left\{(x,
                                                y)\;|\;(x, y) \in V^{2}\; and \;x \neq y\right\}$ Weighted Graph To
                                                continue. Graph Theory What is a Graph Mathematical Unordered Pairs Set
                                                Notation $G = (V, E)$ $V$ is a set of vertices $E \subseteq
                                                \left\{\left\{x, y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ A
                                                simple undirected graph $G$ is an ordered pair or tuple $(V, E)$ where
                                                $V$ and $E$ are finite sets. $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ states that the
                                                elements that are apart of set of edges $E$ are unordered pairs. Where
                                                the elemtents that make up our ordered pair for our edge are elements
                                                from our set $V$. Meaning, in order to define an edge from $E$, its
                                                elements or vertices must came frome set the $V$. Our two elements of
                                                our edge are our head and tail vertice. These two vertices define an
                                                edge or the elements of $E$. It is important to note that in our
                                                defintion that defines our edges in our simple graph, the edges that
                                                make up our set $E$ are unordered pairs or tuples. This means that it
                                                does not matter which vertice is the head or tail vertice. i.e. which
                                                vertice comes first and which vertice comes last. Meaning we do not have
                                                a start and end vertice for our edges, and do not care about the
                                                direction of our vertice. Rather, all that matters is we define the
                                                vertices that make up our edges and that they are apart of the vertices
                                                in our graph $V$. Unlike in a directed graph where an ordered pair
                                                defines each edge and direction of the edge matters. The reason I
                                                introduced the simple undirected graph is because it allows us to get
                                                familiar with the mathematical notation that we will build upon for
                                                other graphs and is one of the most common type of graphs. Graph Types
                                                We will now talk about a couple of different graphs that exist, more
                                                specifically ones that are commonly used. Simple Undirected Graph $G =
                                                (V, E)$ $V$ is a set of vertices $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ See What is a Graph
                                                section. Simple Directed Graph Ordered Pair $n$-ary Cartesian Product $G
                                                = (V, E)$ $V$ is a set of vertices $E \subseteq \left\{(x, y)\;|\;(x, y)
                                                \in V^{2}\;and\;x \neq y\right\}$ Notice in this one the difference is
                                                that the elements in set $E$, or the set of Edges, are tuples or ordered
                                                pairs not unordered pairs like with a directed graph. Why do the edges
                                                in the Edges set, $E$ need to be defined as an ordered pair? Well,
                                                because since the elements in our ordered pair are vertices in the
                                                graph, this means mathematically, $(x, y) \neq (y, x)$. i.e, the order
                                                matters. Thus, in the ordered pair of elements or vertices, which
                                                vertice or element in the tuple is the head and which one is the tail
                                                matters. In an unordered pair, or a set, it does not matter which one is
                                                first or last, it is more so which vertices make up the edge. So, for an
                                                edge in in the set of edges $E$ in a directedd graph, we define the
                                                collection as an ordered pair or by using a tuple. For the vertices that
                                                make up our ordered pair of edges, it does not suffice to give the rule
                                                that the vertices $x, y \in V$ or that our vertices are elements in $V$.
                                                This rule and the procedding rule in the set builder notation for $E$ in
                                                a directed graph does not produce an ordered pair. Rather, we must
                                                change it to $(x, y) \in V^{2}$. Reason being is because $V^{2}$ is
                                                equivalant as writing $V \times V$, or the cartesian product between set
                                                $V$ and set $V$. The cartesian product between sets forms one set whose
                                                elements are ordered pairs of the original sets. The general name for
                                                raising a set to $n$, is called an $n$-ary cartesian product. For
                                                example, if we had some set $X$, the $n$-ary cartesian product of set
                                                $X$ would be written as $X^{n}$. Another thing to note, is that raising
                                                a set to $2$, like in our case, it is called the cartesian square. So
                                                now, with that being said, with our updated rule we can take an element
                                                from our set $V^{2}$ are get an ordered pair, like is necessary for a
                                                directed graph. When before, with an element coming from just $V$, we
                                                were simply taking one element from the set of vertices. Loop Permitting
                                                Graphs Undirected simple graph permitting loops: $G = (V, E)$ $V$ is a
                                                set of vertices $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in
                                                V\right\}$ Notice how our set of edges $E$ is defined similarily to the
                                                Edge set of our previous non-loop permitting directed simple graph $E
                                                \subseteq \left\{(x, y)\;|\;(x, y) \in V^{2}\;and\;x \neq y\right\}$,
                                                however, in the definition for $E$ in our loop permitting graph the
                                                vertices, $x$ and $y$, for each edge can be the same vertice. This is
                                                because in our rule for loop permitting graphs in our definition for $E$
                                                it does not have the rule $x \neq y$. i.e, our updated definition for
                                                for our directed graph that allows for edges to have the same start and
                                                end vertice, or allows for the head and tail vertice to be the same
                                                vertice. Multigraphs Directed multigraph: $G = (V, E, \phi)$ $V$ is a
                                                set of vertices $E$ is a set of edges $\phi : E \rightarrow \left\{(x,
                                                y)\;|\;(x, y) \in V^{2}\; and \;x \neq y\right\}$ Weighted Graph To
                                                continue. Graph Theory What is a Graph Mathematical Unordered Pairs Set
                                                Notation $G = (V, E)$ $V$ is a set of vertices $E \subseteq
                                                \left\{\left\{x, y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ A
                                                simple undirected graph $G$ is an ordered pair or tuple $(V, E)$ where
                                                $V$ and $E$ are finite sets. $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ states that the
                                                elements that are apart of set of edges $E$ are unordered pairs. Where
                                                the elemtents that make up our ordered pair for our edge are elements
                                                from our set $V$. Meaning, in order to define an edge from $E$, its
                                                elements or vertices must came frome set the $V$. Our two elements of
                                                our edge are our head and tail vertice. These two vertices define an
                                                edge or the elements of $E$. It is important to note that in our
                                                defintion that defines our edges in our simple graph, the edges that
                                                make up our set $E$ are unordered pairs or tuples. This means that it
                                                does not matter which vertice is the head or tail vertice. i.e. which
                                                vertice comes first and which vertice comes last. Meaning we do not have
                                                a start and end vertice for our edges, and do not care about the
                                                direction of our vertice. Rather, all that matters is we define the
                                                vertices that make up our edges and that they are apart of the vertices
                                                in our graph $V$. Unlike in a directed graph where an ordered pair
                                                defines each edge and direction of the edge matters. The reason I
                                                introduced the simple undirected graph is because it allows us to get
                                                familiar with the mathematical notation that we will build upon for
                                                other graphs and is one of the most common type of graphs. Graph Types
                                                We will now talk about a couple of different graphs that exist, more
                                                specifically ones that are commonly used. Simple Undirected Graph $G =
                                                (V, E)$ $V$ is a set of vertices $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ See What is a Graph
                                                section. Simple Directed Graph Ordered Pair $n$-ary Cartesian Product $G
                                                = (V, E)$ $V$ is a set of vertices $E \subseteq \left\{(x, y)\;|\;(x, y)
                                                \in V^{2}\;and\;x \neq y\right\}$ Notice in this one the difference is
                                                that the elements in set $E$, or the set of Edges, are tuples or ordered
                                                pairs not unordered pairs like with a directed graph. Why do the edges
                                                in the Edges set, $E$ need to be defined as an ordered pair? Well,
                                                because since the elements in our ordered pair are vertices in the
                                                graph, this means mathematically, $(x, y) \neq (y, x)$. i.e, the order
                                                matters. Thus, in the ordered pair of elements or vertices, which
                                                vertice or element in the tuple is the head and which one is the tail
                                                matters. In an unordered pair, or a set, it does not matter which one is
                                                first or last, it is more so which vertices make up the edge. So, for an
                                                edge in in the set of edges $E$ in a directedd graph, we define the
                                                collection as an ordered pair or by using a tuple. For the vertices that
                                                make up our ordered pair of edges, it does not suffice to give the rule
                                                that the vertices $x, y \in V$ or that our vertices are elements in $V$.
                                                This rule and the procedding rule in the set builder notation for $E$ in
                                                a directed graph does not produce an ordered pair. Rather, we must
                                                change it to $(x, y) \in V^{2}$. Reason being is because $V^{2}$ is
                                                equivalant as writing $V \times V$, or the cartesian product between set
                                                $V$ and set $V$. The cartesian product between sets forms one set whose
                                                elements are ordered pairs of the original sets. The general name for
                                                raising a set to $n$, is called an $n$-ary cartesian product. For
                                                example, if we had some set $X$, the $n$-ary cartesian product of set
                                                $X$ would be written as $X^{n}$. Another thing to note, is that raising
                                                a set to $2$, like in our case, it is called the cartesian square. So
                                                now, with that being said, with our updated rule we can take an element
                                                from our set $V^{2}$ are get an ordered pair, like is necessary for a
                                                directed graph. When before, with an element coming from just $V$, we
                                                were simply taking one element from the set of vertices. Loop Permitting
                                                Graphs Undirected simple graph permitting loops: $G = (V, E)$ $V$ is a
                                                set of vertices $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in
                                                V\right\}$ Notice how our set of edges $E$ is defined similarily to the
                                                Edge set of our previous non-loop permitting directed simple graph $E
                                                \subseteq \left\{(x, y)\;|\;(x, y) \in V^{2}\;and\;x \neq y\right\}$,
                                                however, in the definition for $E$ in our loop permitting graph the
                                                vertices, $x$ and $y$, for each edge can be the same vertice. This is
                                                because in our rule for loop permitting graphs in our definition for $E$
                                                it does not have the rule $x \neq y$. i.e, our updated definition for
                                                for our directed graph that allows for edges to have the same start and
                                                end vertice, or allows for the head and tail vertice to be the same
                                                vertice. Multigraphs Directed multigraph: $G = (V, E, \phi)$ $V$ is a
                                                set of vertices $E$ is a set of edges $\phi : E \rightarrow \left\{(x,
                                                y)\;|\;(x, y) \in V^{2}\; and \;x \neq y\right\}$ Weighted Graph To
                                                continue. Graph TheoryWhat is a Graph Mathematical Unordered Pairs Set
                                                Notation $G = (V, E)$ $V$ is a set of vertices $E \subseteq
                                                \left\{\left\{x, y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ A
                                                simple undirected graph $G$ is an ordered pair or tuple $(V, E)$ where
                                                $V$ and $E$ are finite sets. $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ states that the
                                                elements that are apart of set of edges $E$ are unordered pairs. Where
                                                the elemtents that make up our ordered pair for our edge are elements
                                                from our set $V$. Meaning, in order to define an edge from $E$, its
                                                elements or vertices must came frome set the $V$. Our two elements of
                                                our edge are our head and tail vertice. These two vertices define an
                                                edge or the elements of $E$. It is important to note that in our
                                                defintion that defines our edges in our simple graph, the edges that
                                                make up our set $E$ are unordered pairs or tuples. This means that it
                                                does not matter which vertice is the head or tail vertice. i.e. which
                                                vertice comes first and which vertice comes last. Meaning we do not have
                                                a start and end vertice for our edges, and do not care about the
                                                direction of our vertice. Rather, all that matters is we define the
                                                vertices that make up our edges and that they are apart of the vertices
                                                in our graph $V$. Unlike in a directed graph where an ordered pair
                                                defines each edge and direction of the edge matters. The reason I
                                                introduced the simple undirected graph is because it allows us to get
                                                familiar with the mathematical notation that we will build upon for
                                                other graphs and is one of the most common type of graphs. What is a
                                                GraphMathematical Unordered Pairs Set Notation $G = (V, E)$ $V$ is a set
                                                of vertices $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in
                                                V\;and\;x \neq y\right\}$ A simple undirected graph $G$ is an ordered
                                                pair or tuple $(V, E)$ where $V$ and $E$ are finite sets. $E \subseteq
                                                \left\{\left\{x, y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ states
                                                that the elements that are apart of set of edges $E$ are unordered
                                                pairs. Where the elemtents that make up our ordered pair for our edge
                                                are elements from our set $V$. Meaning, in order to define an edge from
                                                $E$, its elements or vertices must came frome set the $V$. Our two
                                                elements of our edge are our head and tail vertice. These two vertices
                                                define an edge or the elements of $E$. It is important to note that in
                                                our defintion that defines our edges in our simple graph, the edges that
                                                make up our set $E$ are unordered pairs or tuples. This means that it
                                                does not matter which vertice is the head or tail vertice. i.e. which
                                                vertice comes first and which vertice comes last. Meaning we do not have
                                                a start and end vertice for our edges, and do not care about the
                                                direction of our vertice. Rather, all that matters is we define the
                                                vertices that make up our edges and that they are apart of the vertices
                                                in our graph $V$. Unlike in a directed graph where an ordered pair
                                                defines each edge and direction of the edge matters. The reason I
                                                introduced the simple undirected graph is because it allows us to get
                                                familiar with the mathematical notation that we will build upon for
                                                other graphs and is one of the most common type of graphs.
                                                MathematicalUnordered Pairs Set Notation Unordered PairsSet Notation$G =
                                                (V, E)$ $V$ is a set of vertices $E \subseteq \left\{\left\{x,
                                                y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ A simple undirected
                                                graph $G$ is an ordered pair or tuple $(V, E)$ where $V$ and $E$ are
                                                finite sets. $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in
                                                V\;and\;x \neq y\right\}$ states that the elements that are apart of set
                                                of edges $E$ are unordered pairs. Where the elemtents that make up our
                                                ordered pair for our edge are elements from our set $V$. Meaning, in
                                                order to define an edge from $E$, its elements or vertices must came
                                                frome set the $V$. Our two elements of our edge are our head and tail
                                                vertice. These two vertices define an edge or the elements of $E$. It is
                                                important to note that in our defintion that defines our edges in our
                                                simple graph, the edges that make up our set $E$ are unordered pairs or
                                                tuples. This means that it does not matter which vertice is the head or
                                                tail vertice. i.e. which vertice comes first and which vertice comes
                                                last. Meaning we do not have a start and end vertice for our edges, and
                                                do not care about the direction of our vertice. Rather, all that matters
                                                is we define the vertices that make up our edges and that they are apart
                                                of the vertices in our graph $V$. Unlike in a directed graph where an
                                                ordered pair defines each edge and direction of the edge matters.
                                                unordered pairstuples.which directedThe reason I introduced the simple
                                                undirected graph is because it allows us to get familiar with the
                                                mathematical notation that we will build upon for other graphs and is
                                                one of the most common type of graphs. Graph Types We will now talk
                                                about a couple of different graphs that exist, more specifically ones
                                                that are commonly used. Simple Undirected Graph $G = (V, E)$ $V$ is a
                                                set of vertices $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in
                                                V\;and\;x \neq y\right\}$ See What is a Graph section. Simple Directed
                                                Graph Ordered Pair $n$-ary Cartesian Product $G = (V, E)$ $V$ is a set
                                                of vertices $E \subseteq \left\{(x, y)\;|\;(x, y) \in V^{2}\;and\;x \neq
                                                y\right\}$ Notice in this one the difference is that the elements in set
                                                $E$, or the set of Edges, are tuples or ordered pairs not unordered
                                                pairs like with a directed graph. Why do the edges in the Edges set, $E$
                                                need to be defined as an ordered pair? Well, because since the elements
                                                in our ordered pair are vertices in the graph, this means
                                                mathematically, $(x, y) \neq (y, x)$. i.e, the order matters. Thus, in
                                                the ordered pair of elements or vertices, which vertice or element in
                                                the tuple is the head and which one is the tail matters. In an unordered
                                                pair, or a set, it does not matter which one is first or last, it is
                                                more so which vertices make up the edge. So, for an edge in in the set
                                                of edges $E$ in a directedd graph, we define the collection as an
                                                ordered pair or by using a tuple. For the vertices that make up our
                                                ordered pair of edges, it does not suffice to give the rule that the
                                                vertices $x, y \in V$ or that our vertices are elements in $V$. This
                                                rule and the procedding rule in the set builder notation for $E$ in a
                                                directed graph does not produce an ordered pair. Rather, we must change
                                                it to $(x, y) \in V^{2}$. Reason being is because $V^{2}$ is equivalant
                                                as writing $V \times V$, or the cartesian product between set $V$ and
                                                set $V$. The cartesian product between sets forms one set whose elements
                                                are ordered pairs of the original sets. The general name for raising a
                                                set to $n$, is called an $n$-ary cartesian product. For example, if we
                                                had some set $X$, the $n$-ary cartesian product of set $X$ would be
                                                written as $X^{n}$. Another thing to note, is that raising a set to $2$,
                                                like in our case, it is called the cartesian square. So now, with that
                                                being said, with our updated rule we can take an element from our set
                                                $V^{2}$ are get an ordered pair, like is necessary for a directed graph.
                                                When before, with an element coming from just $V$, we were simply taking
                                                one element from the set of vertices. Loop Permitting Graphs Undirected
                                                simple graph permitting loops: $G = (V, E)$ $V$ is a set of vertices $E
                                                \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in V\right\}$ Notice how
                                                our set of edges $E$ is defined similarily to the Edge set of our
                                                previous non-loop permitting directed simple graph $E \subseteq
                                                \left\{(x, y)\;|\;(x, y) \in V^{2}\;and\;x \neq y\right\}$, however, in
                                                the definition for $E$ in our loop permitting graph the vertices, $x$
                                                and $y$, for each edge can be the same vertice. This is because in our
                                                rule for loop permitting graphs in our definition for $E$ it does not
                                                have the rule $x \neq y$. i.e, our updated definition for for our
                                                directed graph that allows for edges to have the same start and end
                                                vertice, or allows for the head and tail vertice to be the same vertice.
                                                Multigraphs Directed multigraph: $G = (V, E, \phi)$ $V$ is a set of
                                                vertices $E$ is a set of edges $\phi : E \rightarrow \left\{(x,
                                                y)\;|\;(x, y) \in V^{2}\; and \;x \neq y\right\}$ Weighted Graph To
                                                continue. Graph TypesWe will now talk about a couple of different graphs
                                                that exist, more specifically ones that are commonly used. Simple
                                                Undirected Graph $G = (V, E)$ $V$ is a set of vertices $E \subseteq
                                                \left\{\left\{x, y\right\}\;|\;x, y \in V\;and\;x \neq y\right\}$ See
                                                What is a Graph section. Simple Undirected Graph$G = (V, E)$ $V$ is a
                                                set of vertices $E \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in
                                                V\;and\;x \neq y\right\}$ See What is a Graph section. What is a
                                                GraphSimple Directed Graph Ordered Pair $n$-ary Cartesian Product $G =
                                                (V, E)$ $V$ is a set of vertices $E \subseteq \left\{(x, y)\;|\;(x, y)
                                                \in V^{2}\;and\;x \neq y\right\}$ Notice in this one the difference is
                                                that the elements in set $E$, or the set of Edges, are tuples or ordered
                                                pairs not unordered pairs like with a directed graph. Why do the edges
                                                in the Edges set, $E$ need to be defined as an ordered pair? Well,
                                                because since the elements in our ordered pair are vertices in the
                                                graph, this means mathematically, $(x, y) \neq (y, x)$. i.e, the order
                                                matters. Thus, in the ordered pair of elements or vertices, which
                                                vertice or element in the tuple is the head and which one is the tail
                                                matters. In an unordered pair, or a set, it does not matter which one is
                                                first or last, it is more so which vertices make up the edge. So, for an
                                                edge in in the set of edges $E$ in a directedd graph, we define the
                                                collection as an ordered pair or by using a tuple. For the vertices that
                                                make up our ordered pair of edges, it does not suffice to give the rule
                                                that the vertices $x, y \in V$ or that our vertices are elements in $V$.
                                                This rule and the procedding rule in the set builder notation for $E$ in
                                                a directed graph does not produce an ordered pair. Rather, we must
                                                change it to $(x, y) \in V^{2}$. Reason being is because $V^{2}$ is
                                                equivalant as writing $V \times V$, or the cartesian product between set
                                                $V$ and set $V$. The cartesian product between sets forms one set whose
                                                elements are ordered pairs of the original sets. The general name for
                                                raising a set to $n$, is called an $n$-ary cartesian product. For
                                                example, if we had some set $X$, the $n$-ary cartesian product of set
                                                $X$ would be written as $X^{n}$. Another thing to note, is that raising
                                                a set to $2$, like in our case, it is called the cartesian square. So
                                                now, with that being said, with our updated rule we can take an element
                                                from our set $V^{2}$ are get an ordered pair, like is necessary for a
                                                directed graph. When before, with an element coming from just $V$, we
                                                were simply taking one element from the set of vertices. Simple Directed
                                                GraphOrdered Pair $n$-ary Cartesian Product Ordered Pair$n$-ary
                                                Cartesian Product$G = (V, E)$ $V$ is a set of vertices $E \subseteq
                                                \left\{(x, y)\;|\;(x, y) \in V^{2}\;and\;x \neq y\right\}$ Notice in
                                                this one the difference is that the elements in set $E$, or the set of
                                                Edges, are tuples or ordered pairs not unordered pairs like with a
                                                directed graph. Why do the edges in the Edges set, $E$ need to be
                                                defined as an ordered pair? Well, because since the elements in our
                                                ordered pair are vertices in the graph, this means mathematically, $(x,
                                                y) \neq (y, x)$. i.e, the order matters. Thus, in the ordered pair of
                                                elements or vertices, which vertice or element in the tuple is the head
                                                and which one is the tail matters. In an unordered pair, or a set, it
                                                does not matter which one is first or last, it is more so which vertices
                                                make up the edge. So, for an edge in in the set of edges $E$ in a
                                                directedd graph, we define the collection as an ordered pair or by using
                                                a tuple. For the vertices that make up our ordered pair of edges, it
                                                does not suffice to give the rule that the vertices $x, y \in V$ or that
                                                our vertices are elements in $V$. This rule and the procedding rule in
                                                the set builder notation for $E$ in a directed graph does not produce an
                                                ordered pair. Rather, we must change it to $(x, y) \in V^{2}$. Reason
                                                being is because $V^{2}$ is equivalant as writing $V \times V$, or the
                                                cartesian product between set $V$ and set $V$. The cartesian product
                                                between sets forms one set whose elements are ordered pairs of the
                                                original sets. The general name for raising a set to $n$, is called an
                                                $n$-ary cartesian product. For example, if we had some set $X$, the
                                                $n$-ary cartesian product of set $X$ would be written as $X^{n}$.
                                                Another thing to note, is that raising a set to $2$, like in our case,
                                                it is called the cartesian square. So now, with that being said, with
                                                our updated rule we can take an element from our set $V^{2}$ are get an
                                                ordered pair, like is necessary for a directed graph. When before, with
                                                an element coming from just $V$, we were simply taking one element from
                                                the set of vertices. Loop Permitting Graphs Undirected simple graph
                                                permitting loops: $G = (V, E)$ $V$ is a set of vertices $E \subseteq
                                                \left\{\left\{x, y\right\}\;|\;x, y \in V\right\}$ Notice how our set of
                                                edges $E$ is defined similarily to the Edge set of our previous non-loop
                                                permitting directed simple graph $E \subseteq \left\{(x, y)\;|\;(x, y)
                                                \in V^{2}\;and\;x \neq y\right\}$, however, in the definition for $E$ in
                                                our loop permitting graph the vertices, $x$ and $y$, for each edge can
                                                be the same vertice. This is because in our rule for loop permitting
                                                graphs in our definition for $E$ it does not have the rule $x \neq y$.
                                                i.e, our updated definition for for our directed graph that allows for
                                                edges to have the same start and end vertice, or allows for the head and
                                                tail vertice to be the same vertice. Loop Permitting GraphsUndirected
                                                simple graph permitting loops: $G = (V, E)$ $V$ is a set of vertices $E
                                                \subseteq \left\{\left\{x, y\right\}\;|\;x, y \in V\right\}$
                                                UndirectedNotice how our set of edges $E$ is defined similarily to the
                                                Edge set of our previous non-loop permitting directed simple graph $E
                                                \subseteq \left\{(x, y)\;|\;(x, y) \in V^{2}\;and\;x \neq y\right\}$,
                                                however, in the definition for $E$ in our loop permitting graph the
                                                vertices, $x$ and $y$, for each edge can be the same vertice. This is
                                                because in our rule for loop permitting graphs in our definition for $E$
                                                it does not have the rule $x \neq y$. i.e, our updated definition for
                                                for our directed graph that allows for edges to have the same start and
                                                end vertice, or allows for the head and tail vertice to be the same
                                                vertice. Multigraphs Directed multigraph: $G = (V, E, \phi)$ $V$ is a
                                                set of vertices $E$ is a set of edges $\phi : E \rightarrow \left\{(x,
                                                y)\;|\;(x, y) \in V^{2}\; and \;x \neq y\right\}$ MultigraphsDirected
                                                multigraph: $G = (V, E, \phi)$ $V$ is a set of vertices $E$ is a set of
                                                edges $\phi : E \rightarrow \left\{(x, y)\;|\;(x, y) \in V^{2}\; and \;x
                                                \neq y\right\}$ DirectedWeighted Graph To continue. Weighted GraphTo
                                                continue.
                                                [https://www.contextswitching.org/tcs/quantumcomputingtheory]
                                                Quantum Computing Theory - Context Switching Quantum Computing Theory
                                                Introduction Quantum Computing Theory is a field of computer science
                                                that uses the principles of quantum mechanics, mathematics, and computer
                                                science. By borrowing concepts from each field scientists can rigorously
                                                define both a broad and narrow theoretical model of a quantum computer,
                                                and later apply it to the real world. These theoretical models, such as
                                                the result of a quantum system manipulating subatomic particles, the
                                                theoretical circuits quantum computers implement to perform larger
                                                operations, and how to optimize the resource complexity for quantum
                                                systems, are just a few of the fundamental concepts in quantum computing
                                                theory. 1-Qubit Qubit A qubit, short for quantum bit, is a two-level
                                                quantum system and is a part of two-dimensional Hilbert space $H_{2}$,
                                                where Hilbert space $H$ is nondenumerable infinite complex vector space.
                                                The two-dimensional complex vector space $H_{2}$ comes with a fixed
                                                orthonormal basis states $B=\left\{|0\rangle,|1\rangle\right\}$ when the
                                                base measurment is in the $Z$ basis. States $|0\rangle$ and $|1\rangle$
                                                are the basis states, denoted with Dirac notation. The states of the
                                                quantum system or qubit can be denoted as a vector like:
                                                $\alpha|0\rangle + \beta|1\rangle$ This vector has a unit length of $1$,
                                                so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and $|\beta|^{2}$ are
                                                the probabilities of the system being in the representative states.
                                                Meaning that the probabilities that when the qubit is measured will give
                                                a state $0$ or $1$ in this two-level quantum system. We will go more in
                                                depth into probabilites here soon. First, however, we will look at
                                                formal definition of the inner dot product of some given some qubit
                                                $\theta$. For $|\theta\rangle$ the unit length is equivalant to its
                                                inner product. Where, for ket $|\theta\rangle$ and bra
                                                $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex numbers and both
                                                have two real numbers. The inner dot product is
                                                $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                                formulated as:
                                                $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                                For some quantum state for the qubit $\psi$ can be defined as:
                                                $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$
                                                $=\langle v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where,
                                                squaring our projections, $\langle v|\psi\rangle$ and $\langle
                                                h|\psi\rangle$, onto axis an axis gives us our respective probabilites
                                                for $|v\rangle$ and $|h\rangle$ respectfully. An example of a qubit is
                                                the spin of an electron. The two levels of this quibit are spin up or
                                                spin down. What differs from a classical system is that quantum
                                                mechanics allows for the qubit to be in a coherent superposition of both
                                                states simultaneously. Measuring a qubit in a basis gives a projective
                                                measurement of a qubit of state $\phi$ in its computational basis can be
                                                expressed as a linear combination of state vectors, such as:
                                                $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured
                                                in a basis, collapses the qubit to either the quantum state $|0\rangle$
                                                or $|1\rangle$ given by the respective norm-square of the probability
                                                amplitudes $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch
                                                Sphere We can use a bloch ball or sphere to help us visualize sping down
                                                $0$ and spin up $1$ of a single qubit. The bloch spehere has a radius of
                                                1, meaning that $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and
                                                $|1\rangle$ corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$
                                                value shows a spin up or sping down. Superposition In classical
                                                computing, states $0$ and $1$ would be the only states that exist for
                                                the bit. However, in quantum mechanics a quibit can be both in the state
                                                of $|0\rangle$ and $|1\rangle$. This is what gives quantum computers
                                                more processing power, as a single qubit can be in more states and
                                                therefore represent more information than a single classical bit. This
                                                means that the qubit can have an $80$% of being in state $|0\rangle$ and
                                                $20$% of being in state $|1\rangle$, or $75$% of being in state
                                                $|0\rangle$ and $25$% of being in state $|1\rangle$. Unlike a classical
                                                bit where there is either a $100$% of the classical bit being in state
                                                $0$ and $0$% of being a $1$ or a $0$% of the classical bit being a $0$
                                                and $100$% of being a $1$. To allow for the qubit to be in
                                                superposition, we need to levearage Hilbert Space. Again, Hilbert space
                                                is represented using complex vector space. We use complex vector space,
                                                because it is the easiest way for the math to work. Let's represent this
                                                qubit in superposition as a vector using dirac notation. For the qubit
                                                have, for example, a $50$% of being in state $|0\rangle$ and $50$% of
                                                being in state $|1\rangle$, the vector should look like:
                                                $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$
                                                are both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$.
                                                Pauli Matrices Pauli matrices have a core importance in quantum physics,
                                                and when combinex with an identity matrix, pauli matrices form a basis
                                                for all single quantum gates. Pauli matrices are defined as:
                                                $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}$,
                                                $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                                $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is
                                                usally refered to as the NOT gate and can be written as $X$. The NOT
                                                gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$
                                                are phase shifting gates. Multiple Qubits Quantum Registers A system
                                                that contains more than one qubit is represented in a quantum register.
                                                A system with two qubits is represented using a four-dimensional Hilbert
                                                space $H_{4} = H_{2} \otimes H_{2}$. The orthonormal basis is
                                                $\left\{|0\rangle |0\rangle, |0\rangle |1\rangle, |1\rangle |0\rangle,
                                                |1\rangle |1\rangle\ \right\}$. $|0\rangle |0\rangle$ can be more
                                                succinctly written as $|00\rangle$. The same is true for $|0\rangle
                                                |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a part
                                                from four-dimensional Hilbert space is a unit-length vector:
                                                $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$
                                                Again, as it is with a single qubit, it is required that $|c_{0}|^{2} +
                                                |c_{1}|^{2} + |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this
                                                two-qubit system will give $00$, $01$, $10$, and $11$ with the outcomes
                                                $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$, respectively. If we want
                                                to observe one of the qubits, then the standard rules of probabilities
                                                apply. It is important to note that the tensor product of these vectors
                                                does not commute. Meaning, that $|0\rangle|1\rangle \neq
                                                |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                                individually. Quantum Entanglement Maximally Entangled Bell States Bell
                                                states or Einstein, Podolski and Rosen pairs are the maximally entangled
                                                quantum states of a qubit system such that a quantum mechanical system
                                                is composed of two interacting two-level subsystems. The 4 types of
                                                maximially entangled Bell states can be defined as: $|\Phi^{+}\rangle =
                                                \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                                \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                                \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                                \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments
                                                The orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                                $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                                $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$
                                                basis to the $X$ bases, we need to apply a Hadmard gate to the state,
                                                allowing for the state to be halfway:
                                                $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                Y Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases,
                                                we need to apply a Hadmard gate to the state, allowing for the state to
                                                be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                                $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                                \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                                \end{pmatrix}$ Z Bases Measurment No transformations needed, since we
                                                measure in the "normal" bases. Operators Unitary Operator All quantum
                                                gates are unitary, which we will define in the following. Let us use the
                                                basis measurment of $Z$ with the coordinate representation $|0\rangle =
                                                \begin{bmatrix} 1\\ 0 \end{bmatrix}$ and $|1\rangle = \begin{bmatrix}
                                                0\\ 1 \end{bmatrix}$. An operation on a quibit, called an unary quantum
                                                gate, is a unitary mapping $U: H_{2} \rightarrow H_{2}$ with the
                                                following defining linear operation: $|0\rangle \mapsto a|0\rangle +
                                                b|1\rangle$ $|1\rangle \mapsto c|0\rangle + d|1\rangle$ An important
                                                aspect of all quantum gates is that they are unitary. Meaning, that for
                                                some given matrix operation $U$, defined as: $\begin{pmatrix} a & b \\ c
                                                & d \end{pmatrix}$ it is neccesary that this matrix is unitary in order
                                                to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                                unitary and valid, the following equivalency must be true:
                                                $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                                and $I$ is the identity matrix. Another important qualtiy of an unitary
                                                matrix is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get:
                                                $\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^*
                                                \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1
                                                \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                                denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                                d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation
                                                $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                                complex conjugate of a complex number is the number with an equal real
                                                part and an imaginary part equal in magnitude, but opposite in sign of
                                                the complex number. The mapping of for unary quantum operator, when
                                                represented in a quantum circut, can be a quantum gate. Where the output
                                                of the quantum gate must have the same dimensionality as its input. So,
                                                $U: H_{n} \rightarrow H_{n}$, where $n$ is the number of dimensions of
                                                $H$. Hermitian Operator A unitary operator is Hermitian if:
                                                $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                                matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian
                                                matrix is a special case of a unitary matrix, where all Hermitian
                                                operators or unitary operators, but not all unitary operators, are
                                                Hermitian. Natural Operator An Hermitian operator is Natural if:
                                                $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate
                                                transpose of $U$. the operator has spectural decomposition, where $U$
                                                can be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum
                                                Algorithms Quantum Parallelism Suppose we want to evaluate a function
                                                $f(x)$, where the function $f$ expresses some computation or algorithm.
                                                A use case for quantum parallelism is to evaluate $f(x)$ with many
                                                different values for the output of the computation or algorithm on the
                                                input $x$ simultaneously. In essence, we can evaluate many different
                                                values of $x$ on $f$ in parallel by exploiting quantum effects. This
                                                quantum effect exploit feature is fundamental in many quantum
                                                algorithms. To continue, we will look at how quantum parallelism works.
                                                Consider the one-bit domain and range function
                                                $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$ on a
                                                quantum computer, we will use a two-qubit quantum computer with the
                                                starting state $|x,y\rangle$. The transformation on the domain or 'data'
                                                register to the range or 'target' register of this initial two qubit
                                                state is described by the following unitary function:
                                                $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                                represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts
                                                on $y$, and its value is $0$ then the value of the second qubit in the
                                                'target' register is the value $f(x)$, given whatever function $f$
                                                represents. The functions effect on $x$ is arbritrary for now. The final
                                                collapsed state $|\psi\rangle$ is an element of the set of final states
                                                or 'target' register $|x,y\oplus f(x)\rangle$, which again is given by
                                                the unitary transformation $U_{f}$ on the start state $|x,y\rangle$.
                                                Given the input $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$
                                                to $x$, such that now:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting
                                                state is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the
                                                resulting new state is not apart of the starting computational basis
                                                $\{0,1\}$. Next, the unitary function or blackbox computation/algorithm
                                                $U_{f}$ can be applied to the current 'data' register. The resulting
                                                mapping of the unitary function $U_{f}$ is:
                                                $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                                f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                                $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                                Meaning that the final resulting state for a two-qubit quantum computer
                                                has a $50$% chance of being $|0,f(0)\rangle$ and $50$% of being
                                                $.5|1,f(1)\rangle$. Given in the same form as the range for $U_{f}$
                                                given above: $|x,y'\rangle$, where $y'=y\oplus f(x)$ All of this means
                                                that the information given by the mapping for $f(0)$ and $f(1)$ was
                                                simultaneously evaluated by applying superposition and the unitary
                                                function on the starting 'data' register. Thus, $f(x)$ has been computed
                                                for two values of $x$ in parellel. The resulting set of all possible
                                                states computed in parallel is given by the resulting 'target' register
                                                is given by quantum exploitation and aptly named 'quantum parallelism'.
                                                Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$
                                                values of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit
                                                system such that $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply
                                                the NOT gate to $y$, giving the what will be the start state
                                                $|\psi_{0}\rangle=|01\rangle$. Next, we will apply a Hadmard gates to
                                                $x$ and $y$ individually, yielding the state:
                                                $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                where for state $|\psi_{1}\rangle$:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                                $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary
                                                unitary function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                                $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                From this, we can then say that if $f(0)=f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$.
                                                If $f(0)=f(1)$, the resulting state is:
                                                $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$
                                                $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0)
                                                \oplus f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus,
                                                $f(0)$ interfers with $f(1)$ when we simultaneously evalute $f(x)$ with
                                                quantum parallelism. Grovers Search Algorithm The authors describe the
                                                process for Grovers Search Algorithm in the following sequential two
                                                main steps: Hadmard transformation and Grover iteration or Grover
                                                operator $G$. Hadmard Transformation The Hadmard transform puts the
                                                qubits of the quantum computer into equal superposition states, defined
                                                as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                                \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm
                                                implements a repeated quantum subroutine called Grover iteration or
                                                operator, denoted as $G$. This quantum iteration can be broken up in
                                                four steps: Apply oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$
                                                Apply conditional phase shift on quantum register, such that every
                                                quantum basis state except $|r\rangle$ is phased shifted $-1$. Meaning
                                                that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1 \sin
                                                \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                                \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                                $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate
                                                $R_{s}$, such that moving our state towards $|w\rangle$. Lastly, apply
                                                the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2, 3, 4,
                                                or Grovers iteration without the oracle step can be written as:
                                                $H^{\otimes n}(2|0\rangle\langle 0|-I) H^{\otimes
                                                n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly eighted
                                                superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                                Thus, including the oracle step now, Grovers iteration $G$ as a whole
                                                can be written, more generically for a given quantum state $\psi$ as
                                                $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image
                                                Source: $[1]$ To continue. Quantum Computational Theory Quantum Automata
                                                Theory Quantum Turing Machine QTM $\delta$ Function A Quantum Turing
                                                Machine QTM can be expressed similarly to a traditional Turing Machine
                                                TM with all components reformulated canonically except for the
                                                transition function $\delta$. Below, is the formal definition of a QTM.
                                                Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                                q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets
                                                and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                                blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in
                                                \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way
                                                infinite, with squares indexed by the set of integers $\mathbb{Z}$
                                                $\delta$ is a transition function described as $\delta : Q \times \Gamma
                                                \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$
                                                $q_{0} \in Q$ is the initial state $q_{accept} \in Q$ is the accept
                                                state $q_{reject} \in Q$ is the reject state, where $q_{reject} \neq
                                                q_{accept}$ Image Source:
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                                Quantum Networks Superdense Coding and Quantum Teleportation Superdense
                                                Coding Given Alice wants to send classical information to Bob, quantum
                                                entanglement $n$ qubits can store $2n$ qubits total of information. Say
                                                Alice needs to send one qubit of infromation, they can do so but needs
                                                Bob to already share a second qubit. So, given that Alice and Bob
                                                already share a pair of entangled qubits in state $|\Phi^{+}\rangle$,
                                                defined as: (Alice) -- $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ --
                                                (Bob) If Alice wants to send: $00$: Alice does nothing to their qubit,
                                                so the qubit is still in state:
                                                $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice
                                                applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice
                                                applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice
                                                applies $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not
                                                that Bob has both qubits in one of four Bell basis, Bob will know what
                                                the qubit Alice wants to send by measuring the two qubits and seeing
                                                what state they are in. This is called a Bell measurment. If we then
                                                extrapolate this method, then if Alice and Bob want to share $n$ pairs
                                                of entangled qubits they can do so with $2n$ quibits in total. Quantum
                                                Teleportation To continue. Glossary Hilbert Space Hilbert Space is a
                                                nondenumerable infinite complex vector space. Complex space, being a
                                                collection of complex numbers $\mathbb{C}$ with an added structure. The
                                                infinite dimensions of Hilbert Space represents a continious spectra of
                                                alternative physical states. Alternative physical states, for example,
                                                being the position (coordinates) or momentum of a particle.
                                                Probabilistic Systems Pure States Mixed States The nature of a
                                                probabilistic system is that we do not know for certain the state of the
                                                system. However, we do know the probability distribution of the states.
                                                Our probabilistic distribution sums up to 1. The notation can be written
                                                as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands
                                                for the state the system is in with probability $p_{i}$. Where $p_{i}
                                                \ge 0$ and $p_{1} + ... + p_{n} = 1$. Our distribution defined above is
                                                a mixed state. Where $x_{i}$ is a pure state. It is important to note
                                                that our distribution is not an expected value or an average of the
                                                mixed state, but rather represents only the probability distribution for
                                                all states $x_{i}$. Quantum Mechanics Fun From here on out we used will
                                                use a Hilbert space formalism of quantum mechanics where the
                                                representation of quantum mechanical systems are represented as state
                                                vectors. We use this representation because state vectors are
                                                mathematically simpler that the more general ones. The quantum
                                                mechanical description of a physical system resembles the probabilistic
                                                systems we mentioned earlier: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... +
                                                p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can be shown as a
                                                unit-length vector in $n$-dimensional complex vector space. We define
                                                this state space with $H_{n}$. Using ket-notation, which is a part of
                                                Dirac notation, we define our state space $H_{n}$ as an orthonormal
                                                basis $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We
                                                can now write any state of the quantum system as: $\alpha_{1}\left|
                                                x_{1} \right> + \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left|
                                                x_{n} \right>$ Here, $\alpha_{i}$ are probabilistic amplitudes. Finally,
                                                to meet our requirements defining our state space $H_{n}$ as unit-length
                                                we say that $|\alpha_{1}|^{2} + |\alpha_{2}|^{2} + ... +
                                                |\alpha_{n}|^{2} = 1$. This concludes most of the information neccesary
                                                for this page. However, if you are having fun with quantum mechanics,
                                                feel free to read more on my Quantum Mechanics page. Quantum Computing
                                                Theory - Context Switching Quantum Computing Theory - Context
                                                SwitchingQuantum Computing Theory Introduction Quantum Computing Theory
                                                is a field of computer science that uses the principles of quantum
                                                mechanics, mathematics, and computer science. By borrowing concepts from
                                                each field scientists can rigorously define both a broad and narrow
                                                theoretical model of a quantum computer, and later apply it to the real
                                                world. These theoretical models, such as the result of a quantum system
                                                manipulating subatomic particles, the theoretical circuits quantum
                                                computers implement to perform larger operations, and how to optimize
                                                the resource complexity for quantum systems, are just a few of the
                                                fundamental concepts in quantum computing theory. 1-Qubit Qubit A qubit,
                                                short for quantum bit, is a two-level quantum system and is a part of
                                                two-dimensional Hilbert space $H_{2}$, where Hilbert space $H$ is
                                                nondenumerable infinite complex vector space. The two-dimensional
                                                complex vector space $H_{2}$ comes with a fixed orthonormal basis states
                                                $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is in
                                                the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis states,
                                                denoted with Dirac notation. The states of the quantum system or qubit
                                                can be denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This
                                                vector has a unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$.
                                                $|\alpha|^{2}$ and $|\beta|^{2}$ are the probabilities of the system
                                                being in the representative states. Meaning that the probabilities that
                                                when the qubit is measured will give a state $0$ or $1$ in this
                                                two-level quantum system. We will go more in depth into probabilites
                                                here soon. First, however, we will look at formal definition of the
                                                inner dot product of some given some qubit $\theta$. For
                                                $|\theta\rangle$ the unit length is equivalant to its inner product.
                                                Where, for ket $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$,
                                                $a,b$ are both complex numbers and both have two real numbers. The inner
                                                dot product is
                                                $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                                formulated as:
                                                $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                                For some quantum state for the qubit $\psi$ can be defined as:
                                                $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$
                                                $=\langle v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where,
                                                squaring our projections, $\langle v|\psi\rangle$ and $\langle
                                                h|\psi\rangle$, onto axis an axis gives us our respective probabilites
                                                for $|v\rangle$ and $|h\rangle$ respectfully. An example of a qubit is
                                                the spin of an electron. The two levels of this quibit are spin up or
                                                spin down. What differs from a classical system is that quantum
                                                mechanics allows for the qubit to be in a coherent superposition of both
                                                states simultaneously. Measuring a qubit in a basis gives a projective
                                                measurement of a qubit of state $\phi$ in its computational basis can be
                                                expressed as a linear combination of state vectors, such as:
                                                $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured
                                                in a basis, collapses the qubit to either the quantum state $|0\rangle$
                                                or $|1\rangle$ given by the respective norm-square of the probability
                                                amplitudes $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch
                                                Sphere We can use a bloch ball or sphere to help us visualize sping down
                                                $0$ and spin up $1$ of a single qubit. The bloch spehere has a radius of
                                                1, meaning that $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and
                                                $|1\rangle$ corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$
                                                value shows a spin up or sping down. Superposition In classical
                                                computing, states $0$ and $1$ would be the only states that exist for
                                                the bit. However, in quantum mechanics a quibit can be both in the state
                                                of $|0\rangle$ and $|1\rangle$. This is what gives quantum computers
                                                more processing power, as a single qubit can be in more states and
                                                therefore represent more information than a single classical bit. This
                                                means that the qubit can have an $80$% of being in state $|0\rangle$ and
                                                $20$% of being in state $|1\rangle$, or $75$% of being in state
                                                $|0\rangle$ and $25$% of being in state $|1\rangle$. Unlike a classical
                                                bit where there is either a $100$% of the classical bit being in state
                                                $0$ and $0$% of being a $1$ or a $0$% of the classical bit being a $0$
                                                and $100$% of being a $1$. To allow for the qubit to be in
                                                superposition, we need to levearage Hilbert Space. Again, Hilbert space
                                                is represented using complex vector space. We use complex vector space,
                                                because it is the easiest way for the math to work. Let's represent this
                                                qubit in superposition as a vector using dirac notation. For the qubit
                                                have, for example, a $50$% of being in state $|0\rangle$ and $50$% of
                                                being in state $|1\rangle$, the vector should look like:
                                                $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$
                                                are both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$.
                                                Pauli Matrices Pauli matrices have a core importance in quantum physics,
                                                and when combinex with an identity matrix, pauli matrices form a basis
                                                for all single quantum gates. Pauli matrices are defined as:
                                                $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}$,
                                                $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                                $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is
                                                usally refered to as the NOT gate and can be written as $X$. The NOT
                                                gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$
                                                are phase shifting gates. Multiple Qubits Quantum Registers A system
                                                that contains more than one qubit is represented in a quantum register.
                                                A system with two qubits is represented using a four-dimensional Hilbert
                                                space $H_{4} = H_{2} \otimes H_{2}$. The orthonormal basis is
                                                $\left\{|0\rangle |0\rangle, |0\rangle |1\rangle, |1\rangle |0\rangle,
                                                |1\rangle |1\rangle\ \right\}$. $|0\rangle |0\rangle$ can be more
                                                succinctly written as $|00\rangle$. The same is true for $|0\rangle
                                                |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a part
                                                from four-dimensional Hilbert space is a unit-length vector:
                                                $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$
                                                Again, as it is with a single qubit, it is required that $|c_{0}|^{2} +
                                                |c_{1}|^{2} + |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this
                                                two-qubit system will give $00$, $01$, $10$, and $11$ with the outcomes
                                                $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$, respectively. If we want
                                                to observe one of the qubits, then the standard rules of probabilities
                                                apply. It is important to note that the tensor product of these vectors
                                                does not commute. Meaning, that $|0\rangle|1\rangle \neq
                                                |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                                individually. Quantum Entanglement Maximally Entangled Bell States Bell
                                                states or Einstein, Podolski and Rosen pairs are the maximally entangled
                                                quantum states of a qubit system such that a quantum mechanical system
                                                is composed of two interacting two-level subsystems. The 4 types of
                                                maximially entangled Bell states can be defined as: $|\Phi^{+}\rangle =
                                                \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                                \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                                \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                                \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments
                                                The orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                                $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                                $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$
                                                basis to the $X$ bases, we need to apply a Hadmard gate to the state,
                                                allowing for the state to be halfway:
                                                $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                Y Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases,
                                                we need to apply a Hadmard gate to the state, allowing for the state to
                                                be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                                $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                                \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                                \end{pmatrix}$ Z Bases Measurment No transformations needed, since we
                                                measure in the "normal" bases. Operators Unitary Operator All quantum
                                                gates are unitary, which we will define in the following. Let us use the
                                                basis measurment of $Z$ with the coordinate representation $|0\rangle =
                                                \begin{bmatrix} 1\\ 0 \end{bmatrix}$ and $|1\rangle = \begin{bmatrix}
                                                0\\ 1 \end{bmatrix}$. An operation on a quibit, called an unary quantum
                                                gate, is a unitary mapping $U: H_{2} \rightarrow H_{2}$ with the
                                                following defining linear operation: $|0\rangle \mapsto a|0\rangle +
                                                b|1\rangle$ $|1\rangle \mapsto c|0\rangle + d|1\rangle$ An important
                                                aspect of all quantum gates is that they are unitary. Meaning, that for
                                                some given matrix operation $U$, defined as: $\begin{pmatrix} a & b \\ c
                                                & d \end{pmatrix}$ it is neccesary that this matrix is unitary in order
                                                to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                                unitary and valid, the following equivalency must be true:
                                                $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                                and $I$ is the identity matrix. Another important qualtiy of an unitary
                                                matrix is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get:
                                                $\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^*
                                                \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1
                                                \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                                denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                                d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation
                                                $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                                complex conjugate of a complex number is the number with an equal real
                                                part and an imaginary part equal in magnitude, but opposite in sign of
                                                the complex number. The mapping of for unary quantum operator, when
                                                represented in a quantum circut, can be a quantum gate. Where the output
                                                of the quantum gate must have the same dimensionality as its input. So,
                                                $U: H_{n} \rightarrow H_{n}$, where $n$ is the number of dimensions of
                                                $H$. Hermitian Operator A unitary operator is Hermitian if:
                                                $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                                matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian
                                                matrix is a special case of a unitary matrix, where all Hermitian
                                                operators or unitary operators, but not all unitary operators, are
                                                Hermitian. Natural Operator An Hermitian operator is Natural if:
                                                $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate
                                                transpose of $U$. the operator has spectural decomposition, where $U$
                                                can be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum
                                                Algorithms Quantum Parallelism Suppose we want to evaluate a function
                                                $f(x)$, where the function $f$ expresses some computation or algorithm.
                                                A use case for quantum parallelism is to evaluate $f(x)$ with many
                                                different values for the output of the computation or algorithm on the
                                                input $x$ simultaneously. In essence, we can evaluate many different
                                                values of $x$ on $f$ in parallel by exploiting quantum effects. This
                                                quantum effect exploit feature is fundamental in many quantum
                                                algorithms. To continue, we will look at how quantum parallelism works.
                                                Consider the one-bit domain and range function
                                                $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$ on a
                                                quantum computer, we will use a two-qubit quantum computer with the
                                                starting state $|x,y\rangle$. The transformation on the domain or 'data'
                                                register to the range or 'target' register of this initial two qubit
                                                state is described by the following unitary function:
                                                $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                                represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts
                                                on $y$, and its value is $0$ then the value of the second qubit in the
                                                'target' register is the value $f(x)$, given whatever function $f$
                                                represents. The functions effect on $x$ is arbritrary for now. The final
                                                collapsed state $|\psi\rangle$ is an element of the set of final states
                                                or 'target' register $|x,y\oplus f(x)\rangle$, which again is given by
                                                the unitary transformation $U_{f}$ on the start state $|x,y\rangle$.
                                                Given the input $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$
                                                to $x$, such that now:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting
                                                state is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the
                                                resulting new state is not apart of the starting computational basis
                                                $\{0,1\}$. Next, the unitary function or blackbox computation/algorithm
                                                $U_{f}$ can be applied to the current 'data' register. The resulting
                                                mapping of the unitary function $U_{f}$ is:
                                                $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                                f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                                $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                                Meaning that the final resulting state for a two-qubit quantum computer
                                                has a $50$% chance of being $|0,f(0)\rangle$ and $50$% of being
                                                $.5|1,f(1)\rangle$. Given in the same form as the range for $U_{f}$
                                                given above: $|x,y'\rangle$, where $y'=y\oplus f(x)$ All of this means
                                                that the information given by the mapping for $f(0)$ and $f(1)$ was
                                                simultaneously evaluated by applying superposition and the unitary
                                                function on the starting 'data' register. Thus, $f(x)$ has been computed
                                                for two values of $x$ in parellel. The resulting set of all possible
                                                states computed in parallel is given by the resulting 'target' register
                                                is given by quantum exploitation and aptly named 'quantum parallelism'.
                                                Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$
                                                values of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit
                                                system such that $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply
                                                the NOT gate to $y$, giving the what will be the start state
                                                $|\psi_{0}\rangle=|01\rangle$. Next, we will apply a Hadmard gates to
                                                $x$ and $y$ individually, yielding the state:
                                                $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                where for state $|\psi_{1}\rangle$:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                                $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary
                                                unitary function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                                $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                From this, we can then say that if $f(0)=f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$.
                                                If $f(0)=f(1)$, the resulting state is:
                                                $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$
                                                $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0)
                                                \oplus f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus,
                                                $f(0)$ interfers with $f(1)$ when we simultaneously evalute $f(x)$ with
                                                quantum parallelism. Grovers Search Algorithm The authors describe the
                                                process for Grovers Search Algorithm in the following sequential two
                                                main steps: Hadmard transformation and Grover iteration or Grover
                                                operator $G$. Hadmard Transformation The Hadmard transform puts the
                                                qubits of the quantum computer into equal superposition states, defined
                                                as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                                \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm
                                                implements a repeated quantum subroutine called Grover iteration or
                                                operator, denoted as $G$. This quantum iteration can be broken up in
                                                four steps: Apply oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$
                                                Apply conditional phase shift on quantum register, such that every
                                                quantum basis state except $|r\rangle$ is phased shifted $-1$. Meaning
                                                that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1 \sin
                                                \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                                \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                                $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate
                                                $R_{s}$, such that moving our state towards $|w\rangle$. Lastly, apply
                                                the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2, 3, 4,
                                                or Grovers iteration without the oracle step can be written as:
                                                $H^{\otimes n}(2|0\rangle\langle 0|-I) H^{\otimes
                                                n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly eighted
                                                superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                                Thus, including the oracle step now, Grovers iteration $G$ as a whole
                                                can be written, more generically for a given quantum state $\psi$ as
                                                $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image
                                                Source: $[1]$ To continue. Quantum Computational Theory Quantum Automata
                                                Theory Quantum Turing Machine QTM $\delta$ Function A Quantum Turing
                                                Machine QTM can be expressed similarly to a traditional Turing Machine
                                                TM with all components reformulated canonically except for the
                                                transition function $\delta$. Below, is the formal definition of a QTM.
                                                Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                                q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets
                                                and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                                blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in
                                                \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way
                                                infinite, with squares indexed by the set of integers $\mathbb{Z}$
                                                $\delta$ is a transition function described as $\delta : Q \times \Gamma
                                                \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$
                                                $q_{0} \in Q$ is the initial state $q_{accept} \in Q$ is the accept
                                                state $q_{reject} \in Q$ is the reject state, where $q_{reject} \neq
                                                q_{accept}$ Image Source:
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                                Quantum Networks Superdense Coding and Quantum Teleportation Superdense
                                                Coding Given Alice wants to send classical information to Bob, quantum
                                                entanglement $n$ qubits can store $2n$ qubits total of information. Say
                                                Alice needs to send one qubit of infromation, they can do so but needs
                                                Bob to already share a second qubit. So, given that Alice and Bob
                                                already share a pair of entangled qubits in state $|\Phi^{+}\rangle$,
                                                defined as: (Alice) -- $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ --
                                                (Bob) If Alice wants to send: $00$: Alice does nothing to their qubit,
                                                so the qubit is still in state:
                                                $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice
                                                applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice
                                                applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice
                                                applies $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not
                                                that Bob has both qubits in one of four Bell basis, Bob will know what
                                                the qubit Alice wants to send by measuring the two qubits and seeing
                                                what state they are in. This is called a Bell measurment. If we then
                                                extrapolate this method, then if Alice and Bob want to share $n$ pairs
                                                of entangled qubits they can do so with $2n$ quibits in total. Quantum
                                                Teleportation To continue. Glossary Hilbert Space Hilbert Space is a
                                                nondenumerable infinite complex vector space. Complex space, being a
                                                collection of complex numbers $\mathbb{C}$ with an added structure. The
                                                infinite dimensions of Hilbert Space represents a continious spectra of
                                                alternative physical states. Alternative physical states, for example,
                                                being the position (coordinates) or momentum of a particle.
                                                Probabilistic Systems Pure States Mixed States The nature of a
                                                probabilistic system is that we do not know for certain the state of the
                                                system. However, we do know the probability distribution of the states.
                                                Our probabilistic distribution sums up to 1. The notation can be written
                                                as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands
                                                for the state the system is in with probability $p_{i}$. Where $p_{i}
                                                \ge 0$ and $p_{1} + ... + p_{n} = 1$. Our distribution defined above is
                                                a mixed state. Where $x_{i}$ is a pure state. It is important to note
                                                that our distribution is not an expected value or an average of the
                                                mixed state, but rather represents only the probability distribution for
                                                all states $x_{i}$. Quantum Mechanics Fun From here on out we used will
                                                use a Hilbert space formalism of quantum mechanics where the
                                                representation of quantum mechanical systems are represented as state
                                                vectors. We use this representation because state vectors are
                                                mathematically simpler that the more general ones. The quantum
                                                mechanical description of a physical system resembles the probabilistic
                                                systems we mentioned earlier: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... +
                                                p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can be shown as a
                                                unit-length vector in $n$-dimensional complex vector space. We define
                                                this state space with $H_{n}$. Using ket-notation, which is a part of
                                                Dirac notation, we define our state space $H_{n}$ as an orthonormal
                                                basis $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We
                                                can now write any state of the quantum system as: $\alpha_{1}\left|
                                                x_{1} \right> + \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left|
                                                x_{n} \right>$ Here, $\alpha_{i}$ are probabilistic amplitudes. Finally,
                                                to meet our requirements defining our state space $H_{n}$ as unit-length
                                                we say that $|\alpha_{1}|^{2} + |\alpha_{2}|^{2} + ... +
                                                |\alpha_{n}|^{2} = 1$. This concludes most of the information neccesary
                                                for this page. However, if you are having fun with quantum mechanics,
                                                feel free to read more on my Quantum Mechanics page. Quantum Computing
                                                Theory Introduction Quantum Computing Theory is a field of computer
                                                science that uses the principles of quantum mechanics, mathematics, and
                                                computer science. By borrowing concepts from each field scientists can
                                                rigorously define both a broad and narrow theoretical model of a quantum
                                                computer, and later apply it to the real world. These theoretical
                                                models, such as the result of a quantum system manipulating subatomic
                                                particles, the theoretical circuits quantum computers implement to
                                                perform larger operations, and how to optimize the resource complexity
                                                for quantum systems, are just a few of the fundamental concepts in
                                                quantum computing theory. 1-Qubit Qubit A qubit, short for quantum bit,
                                                is a two-level quantum system and is a part of two-dimensional Hilbert
                                                space $H_{2}$, where Hilbert space $H$ is nondenumerable infinite
                                                complex vector space. The two-dimensional complex vector space $H_{2}$
                                                comes with a fixed orthonormal basis states
                                                $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is in
                                                the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis states,
                                                denoted with Dirac notation. The states of the quantum system or qubit
                                                can be denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This
                                                vector has a unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$.
                                                $|\alpha|^{2}$ and $|\beta|^{2}$ are the probabilities of the system
                                                being in the representative states. Meaning that the probabilities that
                                                when the qubit is measured will give a state $0$ or $1$ in this
                                                two-level quantum system. We will go more in depth into probabilites
                                                here soon. First, however, we will look at formal definition of the
                                                inner dot product of some given some qubit $\theta$. For
                                                $|\theta\rangle$ the unit length is equivalant to its inner product.
                                                Where, for ket $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$,
                                                $a,b$ are both complex numbers and both have two real numbers. The inner
                                                dot product is
                                                $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                                formulated as:
                                                $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                                For some quantum state for the qubit $\psi$ can be defined as:
                                                $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$
                                                $=\langle v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where,
                                                squaring our projections, $\langle v|\psi\rangle$ and $\langle
                                                h|\psi\rangle$, onto axis an axis gives us our respective probabilites
                                                for $|v\rangle$ and $|h\rangle$ respectfully. An example of a qubit is
                                                the spin of an electron. The two levels of this quibit are spin up or
                                                spin down. What differs from a classical system is that quantum
                                                mechanics allows for the qubit to be in a coherent superposition of both
                                                states simultaneously. Measuring a qubit in a basis gives a projective
                                                measurement of a qubit of state $\phi$ in its computational basis can be
                                                expressed as a linear combination of state vectors, such as:
                                                $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured
                                                in a basis, collapses the qubit to either the quantum state $|0\rangle$
                                                or $|1\rangle$ given by the respective norm-square of the probability
                                                amplitudes $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch
                                                Sphere We can use a bloch ball or sphere to help us visualize sping down
                                                $0$ and spin up $1$ of a single qubit. The bloch spehere has a radius of
                                                1, meaning that $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and
                                                $|1\rangle$ corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$
                                                value shows a spin up or sping down. Superposition In classical
                                                computing, states $0$ and $1$ would be the only states that exist for
                                                the bit. However, in quantum mechanics a quibit can be both in the state
                                                of $|0\rangle$ and $|1\rangle$. This is what gives quantum computers
                                                more processing power, as a single qubit can be in more states and
                                                therefore represent more information than a single classical bit. This
                                                means that the qubit can have an $80$% of being in state $|0\rangle$ and
                                                $20$% of being in state $|1\rangle$, or $75$% of being in state
                                                $|0\rangle$ and $25$% of being in state $|1\rangle$. Unlike a classical
                                                bit where there is either a $100$% of the classical bit being in state
                                                $0$ and $0$% of being a $1$ or a $0$% of the classical bit being a $0$
                                                and $100$% of being a $1$. To allow for the qubit to be in
                                                superposition, we need to levearage Hilbert Space. Again, Hilbert space
                                                is represented using complex vector space. We use complex vector space,
                                                because it is the easiest way for the math to work. Let's represent this
                                                qubit in superposition as a vector using dirac notation. For the qubit
                                                have, for example, a $50$% of being in state $|0\rangle$ and $50$% of
                                                being in state $|1\rangle$, the vector should look like:
                                                $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$
                                                are both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$.
                                                Pauli Matrices Pauli matrices have a core importance in quantum physics,
                                                and when combinex with an identity matrix, pauli matrices form a basis
                                                for all single quantum gates. Pauli matrices are defined as:
                                                $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}$,
                                                $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                                $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is
                                                usally refered to as the NOT gate and can be written as $X$. The NOT
                                                gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$
                                                are phase shifting gates. Multiple Qubits Quantum Registers A system
                                                that contains more than one qubit is represented in a quantum register.
                                                A system with two qubits is represented using a four-dimensional Hilbert
                                                space $H_{4} = H_{2} \otimes H_{2}$. The orthonormal basis is
                                                $\left\{|0\rangle |0\rangle, |0\rangle |1\rangle, |1\rangle |0\rangle,
                                                |1\rangle |1\rangle\ \right\}$. $|0\rangle |0\rangle$ can be more
                                                succinctly written as $|00\rangle$. The same is true for $|0\rangle
                                                |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a part
                                                from four-dimensional Hilbert space is a unit-length vector:
                                                $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$
                                                Again, as it is with a single qubit, it is required that $|c_{0}|^{2} +
                                                |c_{1}|^{2} + |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this
                                                two-qubit system will give $00$, $01$, $10$, and $11$ with the outcomes
                                                $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$, respectively. If we want
                                                to observe one of the qubits, then the standard rules of probabilities
                                                apply. It is important to note that the tensor product of these vectors
                                                does not commute. Meaning, that $|0\rangle|1\rangle \neq
                                                |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                                individually. Quantum Entanglement Maximally Entangled Bell States Bell
                                                states or Einstein, Podolski and Rosen pairs are the maximally entangled
                                                quantum states of a qubit system such that a quantum mechanical system
                                                is composed of two interacting two-level subsystems. The 4 types of
                                                maximially entangled Bell states can be defined as: $|\Phi^{+}\rangle =
                                                \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                                \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                                \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                                \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments
                                                The orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                                $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                                $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$
                                                basis to the $X$ bases, we need to apply a Hadmard gate to the state,
                                                allowing for the state to be halfway:
                                                $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                Y Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases,
                                                we need to apply a Hadmard gate to the state, allowing for the state to
                                                be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                                $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                                \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                                \end{pmatrix}$ Z Bases Measurment No transformations needed, since we
                                                measure in the "normal" bases. Operators Unitary Operator All quantum
                                                gates are unitary, which we will define in the following. Let us use the
                                                basis measurment of $Z$ with the coordinate representation $|0\rangle =
                                                \begin{bmatrix} 1\\ 0 \end{bmatrix}$ and $|1\rangle = \begin{bmatrix}
                                                0\\ 1 \end{bmatrix}$. An operation on a quibit, called an unary quantum
                                                gate, is a unitary mapping $U: H_{2} \rightarrow H_{2}$ with the
                                                following defining linear operation: $|0\rangle \mapsto a|0\rangle +
                                                b|1\rangle$ $|1\rangle \mapsto c|0\rangle + d|1\rangle$ An important
                                                aspect of all quantum gates is that they are unitary. Meaning, that for
                                                some given matrix operation $U$, defined as: $\begin{pmatrix} a & b \\ c
                                                & d \end{pmatrix}$ it is neccesary that this matrix is unitary in order
                                                to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                                unitary and valid, the following equivalency must be true:
                                                $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                                and $I$ is the identity matrix. Another important qualtiy of an unitary
                                                matrix is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get:
                                                $\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^*
                                                \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1
                                                \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                                denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                                d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation
                                                $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                                complex conjugate of a complex number is the number with an equal real
                                                part and an imaginary part equal in magnitude, but opposite in sign of
                                                the complex number. The mapping of for unary quantum operator, when
                                                represented in a quantum circut, can be a quantum gate. Where the output
                                                of the quantum gate must have the same dimensionality as its input. So,
                                                $U: H_{n} \rightarrow H_{n}$, where $n$ is the number of dimensions of
                                                $H$. Hermitian Operator A unitary operator is Hermitian if:
                                                $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                                matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian
                                                matrix is a special case of a unitary matrix, where all Hermitian
                                                operators or unitary operators, but not all unitary operators, are
                                                Hermitian. Natural Operator An Hermitian operator is Natural if:
                                                $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate
                                                transpose of $U$. the operator has spectural decomposition, where $U$
                                                can be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum
                                                Algorithms Quantum Parallelism Suppose we want to evaluate a function
                                                $f(x)$, where the function $f$ expresses some computation or algorithm.
                                                A use case for quantum parallelism is to evaluate $f(x)$ with many
                                                different values for the output of the computation or algorithm on the
                                                input $x$ simultaneously. In essence, we can evaluate many different
                                                values of $x$ on $f$ in parallel by exploiting quantum effects. This
                                                quantum effect exploit feature is fundamental in many quantum
                                                algorithms. To continue, we will look at how quantum parallelism works.
                                                Consider the one-bit domain and range function
                                                $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$ on a
                                                quantum computer, we will use a two-qubit quantum computer with the
                                                starting state $|x,y\rangle$. The transformation on the domain or 'data'
                                                register to the range or 'target' register of this initial two qubit
                                                state is described by the following unitary function:
                                                $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                                represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts
                                                on $y$, and its value is $0$ then the value of the second qubit in the
                                                'target' register is the value $f(x)$, given whatever function $f$
                                                represents. The functions effect on $x$ is arbritrary for now. The final
                                                collapsed state $|\psi\rangle$ is an element of the set of final states
                                                or 'target' register $|x,y\oplus f(x)\rangle$, which again is given by
                                                the unitary transformation $U_{f}$ on the start state $|x,y\rangle$.
                                                Given the input $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$
                                                to $x$, such that now:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting
                                                state is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the
                                                resulting new state is not apart of the starting computational basis
                                                $\{0,1\}$. Next, the unitary function or blackbox computation/algorithm
                                                $U_{f}$ can be applied to the current 'data' register. The resulting
                                                mapping of the unitary function $U_{f}$ is:
                                                $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                                f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                                $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                                Meaning that the final resulting state for a two-qubit quantum computer
                                                has a $50$% chance of being $|0,f(0)\rangle$ and $50$% of being
                                                $.5|1,f(1)\rangle$. Given in the same form as the range for $U_{f}$
                                                given above: $|x,y'\rangle$, where $y'=y\oplus f(x)$ All of this means
                                                that the information given by the mapping for $f(0)$ and $f(1)$ was
                                                simultaneously evaluated by applying superposition and the unitary
                                                function on the starting 'data' register. Thus, $f(x)$ has been computed
                                                for two values of $x$ in parellel. The resulting set of all possible
                                                states computed in parallel is given by the resulting 'target' register
                                                is given by quantum exploitation and aptly named 'quantum parallelism'.
                                                Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$
                                                values of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit
                                                system such that $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply
                                                the NOT gate to $y$, giving the what will be the start state
                                                $|\psi_{0}\rangle=|01\rangle$. Next, we will apply a Hadmard gates to
                                                $x$ and $y$ individually, yielding the state:
                                                $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                where for state $|\psi_{1}\rangle$:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                                $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary
                                                unitary function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                                $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                From this, we can then say that if $f(0)=f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$.
                                                If $f(0)=f(1)$, the resulting state is:
                                                $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$
                                                $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0)
                                                \oplus f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus,
                                                $f(0)$ interfers with $f(1)$ when we simultaneously evalute $f(x)$ with
                                                quantum parallelism. Grovers Search Algorithm The authors describe the
                                                process for Grovers Search Algorithm in the following sequential two
                                                main steps: Hadmard transformation and Grover iteration or Grover
                                                operator $G$. Hadmard Transformation The Hadmard transform puts the
                                                qubits of the quantum computer into equal superposition states, defined
                                                as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                                \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm
                                                implements a repeated quantum subroutine called Grover iteration or
                                                operator, denoted as $G$. This quantum iteration can be broken up in
                                                four steps: Apply oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$
                                                Apply conditional phase shift on quantum register, such that every
                                                quantum basis state except $|r\rangle$ is phased shifted $-1$. Meaning
                                                that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1 \sin
                                                \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                                \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                                $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate
                                                $R_{s}$, such that moving our state towards $|w\rangle$. Lastly, apply
                                                the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2, 3, 4,
                                                or Grovers iteration without the oracle step can be written as:
                                                $H^{\otimes n}(2|0\rangle\langle 0|-I) H^{\otimes
                                                n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly eighted
                                                superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                                Thus, including the oracle step now, Grovers iteration $G$ as a whole
                                                can be written, more generically for a given quantum state $\psi$ as
                                                $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image
                                                Source: $[1]$ To continue. Quantum Computational Theory Quantum Automata
                                                Theory Quantum Turing Machine QTM $\delta$ Function A Quantum Turing
                                                Machine QTM can be expressed similarly to a traditional Turing Machine
                                                TM with all components reformulated canonically except for the
                                                transition function $\delta$. Below, is the formal definition of a QTM.
                                                Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                                q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets
                                                and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                                blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in
                                                \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way
                                                infinite, with squares indexed by the set of integers $\mathbb{Z}$
                                                $\delta$ is a transition function described as $\delta : Q \times \Gamma
                                                \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$
                                                $q_{0} \in Q$ is the initial state $q_{accept} \in Q$ is the accept
                                                state $q_{reject} \in Q$ is the reject state, where $q_{reject} \neq
                                                q_{accept}$ Image Source:
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                                Quantum Networks Superdense Coding and Quantum Teleportation Superdense
                                                Coding Given Alice wants to send classical information to Bob, quantum
                                                entanglement $n$ qubits can store $2n$ qubits total of information. Say
                                                Alice needs to send one qubit of infromation, they can do so but needs
                                                Bob to already share a second qubit. So, given that Alice and Bob
                                                already share a pair of entangled qubits in state $|\Phi^{+}\rangle$,
                                                defined as: (Alice) -- $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ --
                                                (Bob) If Alice wants to send: $00$: Alice does nothing to their qubit,
                                                so the qubit is still in state:
                                                $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice
                                                applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice
                                                applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice
                                                applies $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not
                                                that Bob has both qubits in one of four Bell basis, Bob will know what
                                                the qubit Alice wants to send by measuring the two qubits and seeing
                                                what state they are in. This is called a Bell measurment. If we then
                                                extrapolate this method, then if Alice and Bob want to share $n$ pairs
                                                of entangled qubits they can do so with $2n$ quibits in total. Quantum
                                                Teleportation To continue. Glossary Hilbert Space Hilbert Space is a
                                                nondenumerable infinite complex vector space. Complex space, being a
                                                collection of complex numbers $\mathbb{C}$ with an added structure. The
                                                infinite dimensions of Hilbert Space represents a continious spectra of
                                                alternative physical states. Alternative physical states, for example,
                                                being the position (coordinates) or momentum of a particle.
                                                Probabilistic Systems Pure States Mixed States The nature of a
                                                probabilistic system is that we do not know for certain the state of the
                                                system. However, we do know the probability distribution of the states.
                                                Our probabilistic distribution sums up to 1. The notation can be written
                                                as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands
                                                for the state the system is in with probability $p_{i}$. Where $p_{i}
                                                \ge 0$ and $p_{1} + ... + p_{n} = 1$. Our distribution defined above is
                                                a mixed state. Where $x_{i}$ is a pure state. It is important to note
                                                that our distribution is not an expected value or an average of the
                                                mixed state, but rather represents only the probability distribution for
                                                all states $x_{i}$. Quantum Mechanics Fun From here on out we used will
                                                use a Hilbert space formalism of quantum mechanics where the
                                                representation of quantum mechanical systems are represented as state
                                                vectors. We use this representation because state vectors are
                                                mathematically simpler that the more general ones. The quantum
                                                mechanical description of a physical system resembles the probabilistic
                                                systems we mentioned earlier: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... +
                                                p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can be shown as a
                                                unit-length vector in $n$-dimensional complex vector space. We define
                                                this state space with $H_{n}$. Using ket-notation, which is a part of
                                                Dirac notation, we define our state space $H_{n}$ as an orthonormal
                                                basis $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We
                                                can now write any state of the quantum system as: $\alpha_{1}\left|
                                                x_{1} \right> + \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left|
                                                x_{n} \right>$ Here, $\alpha_{i}$ are probabilistic amplitudes. Finally,
                                                to meet our requirements defining our state space $H_{n}$ as unit-length
                                                we say that $|\alpha_{1}|^{2} + |\alpha_{2}|^{2} + ... +
                                                |\alpha_{n}|^{2} = 1$. This concludes most of the information neccesary
                                                for this page. However, if you are having fun with quantum mechanics,
                                                feel free to read more on my Quantum Mechanics page. Quantum Computing
                                                Theory Introduction Quantum Computing Theory is a field of computer
                                                science that uses the principles of quantum mechanics, mathematics, and
                                                computer science. By borrowing concepts from each field scientists can
                                                rigorously define both a broad and narrow theoretical model of a quantum
                                                computer, and later apply it to the real world. These theoretical
                                                models, such as the result of a quantum system manipulating subatomic
                                                particles, the theoretical circuits quantum computers implement to
                                                perform larger operations, and how to optimize the resource complexity
                                                for quantum systems, are just a few of the fundamental concepts in
                                                quantum computing theory. 1-Qubit Qubit A qubit, short for quantum bit,
                                                is a two-level quantum system and is a part of two-dimensional Hilbert
                                                space $H_{2}$, where Hilbert space $H$ is nondenumerable infinite
                                                complex vector space. The two-dimensional complex vector space $H_{2}$
                                                comes with a fixed orthonormal basis states
                                                $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is in
                                                the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis states,
                                                denoted with Dirac notation. The states of the quantum system or qubit
                                                can be denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This
                                                vector has a unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$.
                                                $|\alpha|^{2}$ and $|\beta|^{2}$ are the probabilities of the system
                                                being in the representative states. Meaning that the probabilities that
                                                when the qubit is measured will give a state $0$ or $1$ in this
                                                two-level quantum system. We will go more in depth into probabilites
                                                here soon. First, however, we will look at formal definition of the
                                                inner dot product of some given some qubit $\theta$. For
                                                $|\theta\rangle$ the unit length is equivalant to its inner product.
                                                Where, for ket $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$,
                                                $a,b$ are both complex numbers and both have two real numbers. The inner
                                                dot product is
                                                $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                                formulated as:
                                                $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                                For some quantum state for the qubit $\psi$ can be defined as:
                                                $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$
                                                $=\langle v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where,
                                                squaring our projections, $\langle v|\psi\rangle$ and $\langle
                                                h|\psi\rangle$, onto axis an axis gives us our respective probabilites
                                                for $|v\rangle$ and $|h\rangle$ respectfully. An example of a qubit is
                                                the spin of an electron. The two levels of this quibit are spin up or
                                                spin down. What differs from a classical system is that quantum
                                                mechanics allows for the qubit to be in a coherent superposition of both
                                                states simultaneously. Measuring a qubit in a basis gives a projective
                                                measurement of a qubit of state $\phi$ in its computational basis can be
                                                expressed as a linear combination of state vectors, such as:
                                                $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured
                                                in a basis, collapses the qubit to either the quantum state $|0\rangle$
                                                or $|1\rangle$ given by the respective norm-square of the probability
                                                amplitudes $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch
                                                Sphere We can use a bloch ball or sphere to help us visualize sping down
                                                $0$ and spin up $1$ of a single qubit. The bloch spehere has a radius of
                                                1, meaning that $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and
                                                $|1\rangle$ corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$
                                                value shows a spin up or sping down. Superposition In classical
                                                computing, states $0$ and $1$ would be the only states that exist for
                                                the bit. However, in quantum mechanics a quibit can be both in the state
                                                of $|0\rangle$ and $|1\rangle$. This is what gives quantum computers
                                                more processing power, as a single qubit can be in more states and
                                                therefore represent more information than a single classical bit. This
                                                means that the qubit can have an $80$% of being in state $|0\rangle$ and
                                                $20$% of being in state $|1\rangle$, or $75$% of being in state
                                                $|0\rangle$ and $25$% of being in state $|1\rangle$. Unlike a classical
                                                bit where there is either a $100$% of the classical bit being in state
                                                $0$ and $0$% of being a $1$ or a $0$% of the classical bit being a $0$
                                                and $100$% of being a $1$. To allow for the qubit to be in
                                                superposition, we need to levearage Hilbert Space. Again, Hilbert space
                                                is represented using complex vector space. We use complex vector space,
                                                because it is the easiest way for the math to work. Let's represent this
                                                qubit in superposition as a vector using dirac notation. For the qubit
                                                have, for example, a $50$% of being in state $|0\rangle$ and $50$% of
                                                being in state $|1\rangle$, the vector should look like:
                                                $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$
                                                are both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$.
                                                Pauli Matrices Pauli matrices have a core importance in quantum physics,
                                                and when combinex with an identity matrix, pauli matrices form a basis
                                                for all single quantum gates. Pauli matrices are defined as:
                                                $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}$,
                                                $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                                $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is
                                                usally refered to as the NOT gate and can be written as $X$. The NOT
                                                gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$
                                                are phase shifting gates. Multiple Qubits Quantum Registers A system
                                                that contains more than one qubit is represented in a quantum register.
                                                A system with two qubits is represented using a four-dimensional Hilbert
                                                space $H_{4} = H_{2} \otimes H_{2}$. The orthonormal basis is
                                                $\left\{|0\rangle |0\rangle, |0\rangle |1\rangle, |1\rangle |0\rangle,
                                                |1\rangle |1\rangle\ \right\}$. $|0\rangle |0\rangle$ can be more
                                                succinctly written as $|00\rangle$. The same is true for $|0\rangle
                                                |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a part
                                                from four-dimensional Hilbert space is a unit-length vector:
                                                $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$
                                                Again, as it is with a single qubit, it is required that $|c_{0}|^{2} +
                                                |c_{1}|^{2} + |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this
                                                two-qubit system will give $00$, $01$, $10$, and $11$ with the outcomes
                                                $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$, respectively. If we want
                                                to observe one of the qubits, then the standard rules of probabilities
                                                apply. It is important to note that the tensor product of these vectors
                                                does not commute. Meaning, that $|0\rangle|1\rangle \neq
                                                |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                                individually. Quantum Entanglement Maximally Entangled Bell States Bell
                                                states or Einstein, Podolski and Rosen pairs are the maximally entangled
                                                quantum states of a qubit system such that a quantum mechanical system
                                                is composed of two interacting two-level subsystems. The 4 types of
                                                maximially entangled Bell states can be defined as: $|\Phi^{+}\rangle =
                                                \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                                \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                                \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                                \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments
                                                The orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                                $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                                $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$
                                                basis to the $X$ bases, we need to apply a Hadmard gate to the state,
                                                allowing for the state to be halfway:
                                                $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                Y Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases,
                                                we need to apply a Hadmard gate to the state, allowing for the state to
                                                be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                                $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                                \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                                \end{pmatrix}$ Z Bases Measurment No transformations needed, since we
                                                measure in the "normal" bases. Operators Unitary Operator All quantum
                                                gates are unitary, which we will define in the following. Let us use the
                                                basis measurment of $Z$ with the coordinate representation $|0\rangle =
                                                \begin{bmatrix} 1\\ 0 \end{bmatrix}$ and $|1\rangle = \begin{bmatrix}
                                                0\\ 1 \end{bmatrix}$. An operation on a quibit, called an unary quantum
                                                gate, is a unitary mapping $U: H_{2} \rightarrow H_{2}$ with the
                                                following defining linear operation: $|0\rangle \mapsto a|0\rangle +
                                                b|1\rangle$ $|1\rangle \mapsto c|0\rangle + d|1\rangle$ An important
                                                aspect of all quantum gates is that they are unitary. Meaning, that for
                                                some given matrix operation $U$, defined as: $\begin{pmatrix} a & b \\ c
                                                & d \end{pmatrix}$ it is neccesary that this matrix is unitary in order
                                                to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                                unitary and valid, the following equivalency must be true:
                                                $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                                and $I$ is the identity matrix. Another important qualtiy of an unitary
                                                matrix is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get:
                                                $\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^*
                                                \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1
                                                \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                                denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                                d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation
                                                $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                                complex conjugate of a complex number is the number with an equal real
                                                part and an imaginary part equal in magnitude, but opposite in sign of
                                                the complex number. The mapping of for unary quantum operator, when
                                                represented in a quantum circut, can be a quantum gate. Where the output
                                                of the quantum gate must have the same dimensionality as its input. So,
                                                $U: H_{n} \rightarrow H_{n}$, where $n$ is the number of dimensions of
                                                $H$. Hermitian Operator A unitary operator is Hermitian if:
                                                $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                                matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian
                                                matrix is a special case of a unitary matrix, where all Hermitian
                                                operators or unitary operators, but not all unitary operators, are
                                                Hermitian. Natural Operator An Hermitian operator is Natural if:
                                                $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate
                                                transpose of $U$. the operator has spectural decomposition, where $U$
                                                can be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum
                                                Algorithms Quantum Parallelism Suppose we want to evaluate a function
                                                $f(x)$, where the function $f$ expresses some computation or algorithm.
                                                A use case for quantum parallelism is to evaluate $f(x)$ with many
                                                different values for the output of the computation or algorithm on the
                                                input $x$ simultaneously. In essence, we can evaluate many different
                                                values of $x$ on $f$ in parallel by exploiting quantum effects. This
                                                quantum effect exploit feature is fundamental in many quantum
                                                algorithms. To continue, we will look at how quantum parallelism works.
                                                Consider the one-bit domain and range function
                                                $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$ on a
                                                quantum computer, we will use a two-qubit quantum computer with the
                                                starting state $|x,y\rangle$. The transformation on the domain or 'data'
                                                register to the range or 'target' register of this initial two qubit
                                                state is described by the following unitary function:
                                                $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                                represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts
                                                on $y$, and its value is $0$ then the value of the second qubit in the
                                                'target' register is the value $f(x)$, given whatever function $f$
                                                represents. The functions effect on $x$ is arbritrary for now. The final
                                                collapsed state $|\psi\rangle$ is an element of the set of final states
                                                or 'target' register $|x,y\oplus f(x)\rangle$, which again is given by
                                                the unitary transformation $U_{f}$ on the start state $|x,y\rangle$.
                                                Given the input $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$
                                                to $x$, such that now:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting
                                                state is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the
                                                resulting new state is not apart of the starting computational basis
                                                $\{0,1\}$. Next, the unitary function or blackbox computation/algorithm
                                                $U_{f}$ can be applied to the current 'data' register. The resulting
                                                mapping of the unitary function $U_{f}$ is:
                                                $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                                f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                                $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                                Meaning that the final resulting state for a two-qubit quantum computer
                                                has a $50$% chance of being $|0,f(0)\rangle$ and $50$% of being
                                                $.5|1,f(1)\rangle$. Given in the same form as the range for $U_{f}$
                                                given above: $|x,y'\rangle$, where $y'=y\oplus f(x)$ All of this means
                                                that the information given by the mapping for $f(0)$ and $f(1)$ was
                                                simultaneously evaluated by applying superposition and the unitary
                                                function on the starting 'data' register. Thus, $f(x)$ has been computed
                                                for two values of $x$ in parellel. The resulting set of all possible
                                                states computed in parallel is given by the resulting 'target' register
                                                is given by quantum exploitation and aptly named 'quantum parallelism'.
                                                Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$
                                                values of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit
                                                system such that $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply
                                                the NOT gate to $y$, giving the what will be the start state
                                                $|\psi_{0}\rangle=|01\rangle$. Next, we will apply a Hadmard gates to
                                                $x$ and $y$ individually, yielding the state:
                                                $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                where for state $|\psi_{1}\rangle$:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                                $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary
                                                unitary function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                                $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                From this, we can then say that if $f(0)=f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$.
                                                If $f(0)=f(1)$, the resulting state is:
                                                $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$
                                                $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0)
                                                \oplus f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus,
                                                $f(0)$ interfers with $f(1)$ when we simultaneously evalute $f(x)$ with
                                                quantum parallelism. Grovers Search Algorithm The authors describe the
                                                process for Grovers Search Algorithm in the following sequential two
                                                main steps: Hadmard transformation and Grover iteration or Grover
                                                operator $G$. Hadmard Transformation The Hadmard transform puts the
                                                qubits of the quantum computer into equal superposition states, defined
                                                as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                                \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm
                                                implements a repeated quantum subroutine called Grover iteration or
                                                operator, denoted as $G$. This quantum iteration can be broken up in
                                                four steps: Apply oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$
                                                Apply conditional phase shift on quantum register, such that every
                                                quantum basis state except $|r\rangle$ is phased shifted $-1$. Meaning
                                                that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1 \sin
                                                \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                                \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                                $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate
                                                $R_{s}$, such that moving our state towards $|w\rangle$. Lastly, apply
                                                the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2, 3, 4,
                                                or Grovers iteration without the oracle step can be written as:
                                                $H^{\otimes n}(2|0\rangle\langle 0|-I) H^{\otimes
                                                n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly eighted
                                                superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                                Thus, including the oracle step now, Grovers iteration $G$ as a whole
                                                can be written, more generically for a given quantum state $\psi$ as
                                                $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image
                                                Source: $[1]$ To continue. Quantum Computational Theory Quantum Automata
                                                Theory Quantum Turing Machine QTM $\delta$ Function A Quantum Turing
                                                Machine QTM can be expressed similarly to a traditional Turing Machine
                                                TM with all components reformulated canonically except for the
                                                transition function $\delta$. Below, is the formal definition of a QTM.
                                                Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                                q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets
                                                and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                                blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in
                                                \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way
                                                infinite, with squares indexed by the set of integers $\mathbb{Z}$
                                                $\delta$ is a transition function described as $\delta : Q \times \Gamma
                                                \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$
                                                $q_{0} \in Q$ is the initial state $q_{accept} \in Q$ is the accept
                                                state $q_{reject} \in Q$ is the reject state, where $q_{reject} \neq
                                                q_{accept}$ Image Source:
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                                Quantum Networks Superdense Coding and Quantum Teleportation Superdense
                                                Coding Given Alice wants to send classical information to Bob, quantum
                                                entanglement $n$ qubits can store $2n$ qubits total of information. Say
                                                Alice needs to send one qubit of infromation, they can do so but needs
                                                Bob to already share a second qubit. So, given that Alice and Bob
                                                already share a pair of entangled qubits in state $|\Phi^{+}\rangle$,
                                                defined as: (Alice) -- $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ --
                                                (Bob) If Alice wants to send: $00$: Alice does nothing to their qubit,
                                                so the qubit is still in state:
                                                $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice
                                                applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice
                                                applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice
                                                applies $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not
                                                that Bob has both qubits in one of four Bell basis, Bob will know what
                                                the qubit Alice wants to send by measuring the two qubits and seeing
                                                what state they are in. This is called a Bell measurment. If we then
                                                extrapolate this method, then if Alice and Bob want to share $n$ pairs
                                                of entangled qubits they can do so with $2n$ quibits in total. Quantum
                                                Teleportation To continue. Glossary Hilbert Space Hilbert Space is a
                                                nondenumerable infinite complex vector space. Complex space, being a
                                                collection of complex numbers $\mathbb{C}$ with an added structure. The
                                                infinite dimensions of Hilbert Space represents a continious spectra of
                                                alternative physical states. Alternative physical states, for example,
                                                being the position (coordinates) or momentum of a particle.
                                                Probabilistic Systems Pure States Mixed States The nature of a
                                                probabilistic system is that we do not know for certain the state of the
                                                system. However, we do know the probability distribution of the states.
                                                Our probabilistic distribution sums up to 1. The notation can be written
                                                as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands
                                                for the state the system is in with probability $p_{i}$. Where $p_{i}
                                                \ge 0$ and $p_{1} + ... + p_{n} = 1$. Our distribution defined above is
                                                a mixed state. Where $x_{i}$ is a pure state. It is important to note
                                                that our distribution is not an expected value or an average of the
                                                mixed state, but rather represents only the probability distribution for
                                                all states $x_{i}$. Quantum Mechanics Fun From here on out we used will
                                                use a Hilbert space formalism of quantum mechanics where the
                                                representation of quantum mechanical systems are represented as state
                                                vectors. We use this representation because state vectors are
                                                mathematically simpler that the more general ones. The quantum
                                                mechanical description of a physical system resembles the probabilistic
                                                systems we mentioned earlier: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... +
                                                p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can be shown as a
                                                unit-length vector in $n$-dimensional complex vector space. We define
                                                this state space with $H_{n}$. Using ket-notation, which is a part of
                                                Dirac notation, we define our state space $H_{n}$ as an orthonormal
                                                basis $\left\{\left| x_{1} \right>,...,\left| x_{n} \right>\right\}$. We
                                                can now write any state of the quantum system as: $\alpha_{1}\left|
                                                x_{1} \right> + \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left|
                                                x_{n} \right>$ Here, $\alpha_{i}$ are probabilistic amplitudes. Finally,
                                                to meet our requirements defining our state space $H_{n}$ as unit-length
                                                we say that $|\alpha_{1}|^{2} + |\alpha_{2}|^{2} + ... +
                                                |\alpha_{n}|^{2} = 1$. This concludes most of the information neccesary
                                                for this page. However, if you are having fun with quantum mechanics,
                                                feel free to read more on my Quantum Mechanics page. Quantum Computing
                                                TheoryIntroduction Quantum Computing Theory is a field of computer
                                                science that uses the principles of quantum mechanics, mathematics, and
                                                computer science. By borrowing concepts from each field scientists can
                                                rigorously define both a broad and narrow theoretical model of a quantum
                                                computer, and later apply it to the real world. These theoretical
                                                models, such as the result of a quantum system manipulating subatomic
                                                particles, the theoretical circuits quantum computers implement to
                                                perform larger operations, and how to optimize the resource complexity
                                                for quantum systems, are just a few of the fundamental concepts in
                                                quantum computing theory. 1-Qubit Qubit A qubit, short for quantum bit,
                                                is a two-level quantum system and is a part of two-dimensional Hilbert
                                                space $H_{2}$, where Hilbert space $H$ is nondenumerable infinite
                                                complex vector space. The two-dimensional complex vector space $H_{2}$
                                                comes with a fixed orthonormal basis states
                                                $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is in
                                                the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis states,
                                                denoted with Dirac notation. The states of the quantum system or qubit
                                                can be denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This
                                                vector has a unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$.
                                                $|\alpha|^{2}$ and $|\beta|^{2}$ are the probabilities of the system
                                                being in the representative states. Meaning that the probabilities that
                                                when the qubit is measured will give a state $0$ or $1$ in this
                                                two-level quantum system. We will go more in depth into probabilites
                                                here soon. First, however, we will look at formal definition of the
                                                inner dot product of some given some qubit $\theta$. For
                                                $|\theta\rangle$ the unit length is equivalant to its inner product.
                                                Where, for ket $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$,
                                                $a,b$ are both complex numbers and both have two real numbers. The inner
                                                dot product is
                                                $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                                formulated as:
                                                $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                                For some quantum state for the qubit $\psi$ can be defined as:
                                                $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$
                                                $=\langle v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where,
                                                squaring our projections, $\langle v|\psi\rangle$ and $\langle
                                                h|\psi\rangle$, onto axis an axis gives us our respective probabilites
                                                for $|v\rangle$ and $|h\rangle$ respectfully. An example of a qubit is
                                                the spin of an electron. The two levels of this quibit are spin up or
                                                spin down. What differs from a classical system is that quantum
                                                mechanics allows for the qubit to be in a coherent superposition of both
                                                states simultaneously. Measuring a qubit in a basis gives a projective
                                                measurement of a qubit of state $\phi$ in its computational basis can be
                                                expressed as a linear combination of state vectors, such as:
                                                $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured
                                                in a basis, collapses the qubit to either the quantum state $|0\rangle$
                                                or $|1\rangle$ given by the respective norm-square of the probability
                                                amplitudes $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch
                                                Sphere We can use a bloch ball or sphere to help us visualize sping down
                                                $0$ and spin up $1$ of a single qubit. The bloch spehere has a radius of
                                                1, meaning that $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and
                                                $|1\rangle$ corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$
                                                value shows a spin up or sping down. Superposition In classical
                                                computing, states $0$ and $1$ would be the only states that exist for
                                                the bit. However, in quantum mechanics a quibit can be both in the state
                                                of $|0\rangle$ and $|1\rangle$. This is what gives quantum computers
                                                more processing power, as a single qubit can be in more states and
                                                therefore represent more information than a single classical bit. This
                                                means that the qubit can have an $80$% of being in state $|0\rangle$ and
                                                $20$% of being in state $|1\rangle$, or $75$% of being in state
                                                $|0\rangle$ and $25$% of being in state $|1\rangle$. Unlike a classical
                                                bit where there is either a $100$% of the classical bit being in state
                                                $0$ and $0$% of being a $1$ or a $0$% of the classical bit being a $0$
                                                and $100$% of being a $1$. To allow for the qubit to be in
                                                superposition, we need to levearage Hilbert Space. Again, Hilbert space
                                                is represented using complex vector space. We use complex vector space,
                                                because it is the easiest way for the math to work. Let's represent this
                                                qubit in superposition as a vector using dirac notation. For the qubit
                                                have, for example, a $50$% of being in state $|0\rangle$ and $50$% of
                                                being in state $|1\rangle$, the vector should look like:
                                                $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$
                                                are both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$.
                                                Pauli Matrices Pauli matrices have a core importance in quantum physics,
                                                and when combinex with an identity matrix, pauli matrices form a basis
                                                for all single quantum gates. Pauli matrices are defined as:
                                                $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}$,
                                                $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                                $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is
                                                usally refered to as the NOT gate and can be written as $X$. The NOT
                                                gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$
                                                are phase shifting gates. Multiple Qubits Quantum Registers A system
                                                that contains more than one qubit is represented in a quantum register.
                                                A system with two qubits is represented using a four-dimensional Hilbert
                                                space $H_{4} = H_{2} \otimes H_{2}$. The orthonormal basis is
                                                $\left\{|0\rangle |0\rangle, |0\rangle |1\rangle, |1\rangle |0\rangle,
                                                |1\rangle |1\rangle\ \right\}$. $|0\rangle |0\rangle$ can be more
                                                succinctly written as $|00\rangle$. The same is true for $|0\rangle
                                                |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a part
                                                from four-dimensional Hilbert space is a unit-length vector:
                                                $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$
                                                Again, as it is with a single qubit, it is required that $|c_{0}|^{2} +
                                                |c_{1}|^{2} + |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this
                                                two-qubit system will give $00$, $01$, $10$, and $11$ with the outcomes
                                                $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$, respectively. If we want
                                                to observe one of the qubits, then the standard rules of probabilities
                                                apply. It is important to note that the tensor product of these vectors
                                                does not commute. Meaning, that $|0\rangle|1\rangle \neq
                                                |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                                individually. Quantum Entanglement Maximally Entangled Bell States Bell
                                                states or Einstein, Podolski and Rosen pairs are the maximally entangled
                                                quantum states of a qubit system such that a quantum mechanical system
                                                is composed of two interacting two-level subsystems. The 4 types of
                                                maximially entangled Bell states can be defined as: $|\Phi^{+}\rangle =
                                                \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                                \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                                \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                                \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments Bases Measurments
                                                The orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                                $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                                $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$
                                                basis to the $X$ bases, we need to apply a Hadmard gate to the state,
                                                allowing for the state to be halfway:
                                                $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                Y Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases,
                                                we need to apply a Hadmard gate to the state, allowing for the state to
                                                be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                                $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                                \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                                \end{pmatrix}$ Z Bases Measurment No transformations needed, since we
                                                measure in the "normal" bases. Operators Unitary Operator All quantum
                                                gates are unitary, which we will define in the following. Let us use the
                                                basis measurment of $Z$ with the coordinate representation $|0\rangle =
                                                \begin{bmatrix} 1\\ 0 \end{bmatrix}$ and $|1\rangle = \begin{bmatrix}
                                                0\\ 1 \end{bmatrix}$. An operation on a quibit, called an unary quantum
                                                gate, is a unitary mapping $U: H_{2} \rightarrow H_{2}$ with the
                                                following defining linear operation: $|0\rangle \mapsto a|0\rangle +
                                                b|1\rangle$ $|1\rangle \mapsto c|0\rangle + d|1\rangle$ An important
                                                aspect of all quantum gates is that they are unitary. Meaning, that for
                                                some given matrix operation $U$, defined as: $\begin{pmatrix} a & b \\ c
                                                & d \end{pmatrix}$ it is neccesary that this matrix is unitary in order
                                                to be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                                unitary and valid, the following equivalency must be true:
                                                $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                                and $I$ is the identity matrix. Another important qualtiy of an unitary
                                                matrix is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get:
                                                $\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^*
                                                \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1
                                                \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                                denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                                d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation
                                                $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                                complex conjugate of a complex number is the number with an equal real
                                                part and an imaginary part equal in magnitude, but opposite in sign of
                                                the complex number. The mapping of for unary quantum operator, when
                                                represented in a quantum circut, can be a quantum gate. Where the output
                                                of the quantum gate must have the same dimensionality as its input. So,
                                                $U: H_{n} \rightarrow H_{n}$, where $n$ is the number of dimensions of
                                                $H$. Hermitian Operator A unitary operator is Hermitian if:
                                                $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                                matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian
                                                matrix is a special case of a unitary matrix, where all Hermitian
                                                operators or unitary operators, but not all unitary operators, are
                                                Hermitian. Natural Operator An Hermitian operator is Natural if:
                                                $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate
                                                transpose of $U$. the operator has spectural decomposition, where $U$
                                                can be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$
                                                IntroductionQuantum Computing Theory is a field of computer science that
                                                uses the principles of quantum mechanics, mathematics, and computer
                                                science. By borrowing concepts from each field scientists can rigorously
                                                define both a broad and narrow theoretical model of a quantum computer,
                                                and later apply it to the real world. These theoretical models, such as
                                                the result of a quantum system manipulating subatomic particles, the
                                                theoretical circuits quantum computers implement to perform larger
                                                operations, and how to optimize the resource complexity for quantum
                                                systems, are just a few of the fundamental concepts in quantum computing
                                                theory. 1-Qubit Qubit A qubit, short for quantum bit, is a two-level
                                                quantum system and is a part of two-dimensional Hilbert space $H_{2}$,
                                                where Hilbert space $H$ is nondenumerable infinite complex vector space.
                                                The two-dimensional complex vector space $H_{2}$ comes with a fixed
                                                orthonormal basis states $B=\left\{|0\rangle,|1\rangle\right\}$ when the
                                                base measurment is in the $Z$ basis. States $|0\rangle$ and $|1\rangle$
                                                are the basis states, denoted with Dirac notation. The states of the
                                                quantum system or qubit can be denoted as a vector like:
                                                $\alpha|0\rangle + \beta|1\rangle$ This vector has a unit length of $1$,
                                                so $|\alpha|^{2} + |\beta|^{2}=1$. $|\alpha|^{2}$ and $|\beta|^{2}$ are
                                                the probabilities of the system being in the representative states.
                                                Meaning that the probabilities that when the qubit is measured will give
                                                a state $0$ or $1$ in this two-level quantum system. We will go more in
                                                depth into probabilites here soon. First, however, we will look at
                                                formal definition of the inner dot product of some given some qubit
                                                $\theta$. For $|\theta\rangle$ the unit length is equivalant to its
                                                inner product. Where, for ket $|\theta\rangle$ and bra
                                                $\langle\theta|=(a^{*}b^{*})$, $a,b$ are both complex numbers and both
                                                have two real numbers. The inner dot product is
                                                $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                                formulated as:
                                                $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                                For some quantum state for the qubit $\psi$ can be defined as:
                                                $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$
                                                $=\langle v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where,
                                                squaring our projections, $\langle v|\psi\rangle$ and $\langle
                                                h|\psi\rangle$, onto axis an axis gives us our respective probabilites
                                                for $|v\rangle$ and $|h\rangle$ respectfully. An example of a qubit is
                                                the spin of an electron. The two levels of this quibit are spin up or
                                                spin down. What differs from a classical system is that quantum
                                                mechanics allows for the qubit to be in a coherent superposition of both
                                                states simultaneously. Measuring a qubit in a basis gives a projective
                                                measurement of a qubit of state $\phi$ in its computational basis can be
                                                expressed as a linear combination of state vectors, such as:
                                                $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured
                                                in a basis, collapses the qubit to either the quantum state $|0\rangle$
                                                or $|1\rangle$ given by the respective norm-square of the probability
                                                amplitudes $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch
                                                Sphere We can use a bloch ball or sphere to help us visualize sping down
                                                $0$ and spin up $1$ of a single qubit. The bloch spehere has a radius of
                                                1, meaning that $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and
                                                $|1\rangle$ corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$
                                                value shows a spin up or sping down. Superposition In classical
                                                computing, states $0$ and $1$ would be the only states that exist for
                                                the bit. However, in quantum mechanics a quibit can be both in the state
                                                of $|0\rangle$ and $|1\rangle$. This is what gives quantum computers
                                                more processing power, as a single qubit can be in more states and
                                                therefore represent more information than a single classical bit. This
                                                means that the qubit can have an $80$% of being in state $|0\rangle$ and
                                                $20$% of being in state $|1\rangle$, or $75$% of being in state
                                                $|0\rangle$ and $25$% of being in state $|1\rangle$. Unlike a classical
                                                bit where there is either a $100$% of the classical bit being in state
                                                $0$ and $0$% of being a $1$ or a $0$% of the classical bit being a $0$
                                                and $100$% of being a $1$. To allow for the qubit to be in
                                                superposition, we need to levearage Hilbert Space. Again, Hilbert space
                                                is represented using complex vector space. We use complex vector space,
                                                because it is the easiest way for the math to work. Let's represent this
                                                qubit in superposition as a vector using dirac notation. For the qubit
                                                have, for example, a $50$% of being in state $|0\rangle$ and $50$% of
                                                being in state $|1\rangle$, the vector should look like:
                                                $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The coefficent for $0$ and $1$
                                                are both $\frac{1}{\sqrt{2}}$ and therefore equal parts $0$ and $1$.
                                                Pauli Matrices Pauli matrices have a core importance in quantum physics,
                                                and when combinex with an identity matrix, pauli matrices form a basis
                                                for all single quantum gates. Pauli matrices are defined as:
                                                $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0 \end{pmatrix}$,
                                                $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                                $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is
                                                usally refered to as the NOT gate and can be written as $X$. The NOT
                                                gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$
                                                are phase shifting gates. 1-QubitQubit A qubit, short for quantum bit,
                                                is a two-level quantum system and is a part of two-dimensional Hilbert
                                                space $H_{2}$, where Hilbert space $H$ is nondenumerable infinite
                                                complex vector space. The two-dimensional complex vector space $H_{2}$
                                                comes with a fixed orthonormal basis states
                                                $B=\left\{|0\rangle,|1\rangle\right\}$ when the base measurment is in
                                                the $Z$ basis. States $|0\rangle$ and $|1\rangle$ are the basis states,
                                                denoted with Dirac notation. The states of the quantum system or qubit
                                                can be denoted as a vector like: $\alpha|0\rangle + \beta|1\rangle$ This
                                                vector has a unit length of $1$, so $|\alpha|^{2} + |\beta|^{2}=1$.
                                                $|\alpha|^{2}$ and $|\beta|^{2}$ are the probabilities of the system
                                                being in the representative states. Meaning that the probabilities that
                                                when the qubit is measured will give a state $0$ or $1$ in this
                                                two-level quantum system. We will go more in depth into probabilites
                                                here soon. First, however, we will look at formal definition of the
                                                inner dot product of some given some qubit $\theta$. For
                                                $|\theta\rangle$ the unit length is equivalant to its inner product.
                                                Where, for ket $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$,
                                                $a,b$ are both complex numbers and both have two real numbers. The inner
                                                dot product is
                                                $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                                formulated as:
                                                $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                                For some quantum state for the qubit $\psi$ can be defined as:
                                                $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$
                                                $=\langle v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where,
                                                squaring our projections, $\langle v|\psi\rangle$ and $\langle
                                                h|\psi\rangle$, onto axis an axis gives us our respective probabilites
                                                for $|v\rangle$ and $|h\rangle$ respectfully. An example of a qubit is
                                                the spin of an electron. The two levels of this quibit are spin up or
                                                spin down. What differs from a classical system is that quantum
                                                mechanics allows for the qubit to be in a coherent superposition of both
                                                states simultaneously. Measuring a qubit in a basis gives a projective
                                                measurement of a qubit of state $\phi$ in its computational basis can be
                                                expressed as a linear combination of state vectors, such as:
                                                $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured
                                                in a basis, collapses the qubit to either the quantum state $|0\rangle$
                                                or $|1\rangle$ given by the respective norm-square of the probability
                                                amplitudes $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$.
                                                Qubitbase measurmentDirac notationThe states of the quantum system or
                                                qubit can be denoted as a vector like: $\alpha|0\rangle +
                                                \beta|1\rangle$ This vector has a unit length of $1$, so $|\alpha|^{2} +
                                                |\beta|^{2}=1$. $|\alpha|^{2}$ and $|\beta|^{2}$ are the probabilities
                                                of the system being in the representative states. Meaning that the
                                                probabilities that when the qubit is measured will give a state $0$ or
                                                $1$ in this two-level quantum system. We will go more in depth into
                                                probabilites here soon. First, however, we will look at formal
                                                definition of the inner dot product of some given some qubit $\theta$.
                                                For $|\theta\rangle$ the unit length is equivalant to its inner product.
                                                Where, for ket $|\theta\rangle$ and bra $\langle\theta|=(a^{*}b^{*})$,
                                                $a,b$ are both complex numbers and both have two real numbers. The inner
                                                dot product is
                                                $|\theta\rangle|\theta^{*\intercal}\rangle=|\theta\rangle|\theta^{\dagger}\rangle$,
                                                formulated as:
                                                $\langle\theta|\theta\rangle=(a^{*}b^{*})\begin{pmatrix}a\\b\end{pmatrix}=|a|^{2}+|b|^{2}=1$
                                                For some quantum state for the qubit $\psi$ can be defined as:
                                                $|\psi\rangle=\begin{pmatrix}\cos(\theta)\\ \sin(\theta)\end{pmatrix}$
                                                $=\langle v|\psi\rangle|v\rangle+\langle h|\psi\rangle|h\rangle$ Where,
                                                squaring our projections, $\langle v|\psi\rangle$ and $\langle
                                                h|\psi\rangle$, onto axis an axis gives us our respective probabilites
                                                for $|v\rangle$ and $|h\rangle$ respectfully. An example of a qubit is
                                                the spin of an electron. The two levels of this quibit are spin up or
                                                spin down. What differs from a classical system is that quantum
                                                mechanics allows for the qubit to be in a coherent superposition of both
                                                states simultaneously. Measuring a qubit in a basis gives a projective
                                                measurement of a qubit of state $\phi$ in its computational basis can be
                                                expressed as a linear combination of state vectors, such as:
                                                $|\psi\rangle=\alpha|0\rangle+\beta|1\rangle$ When the qubit is measured
                                                in a basis, collapses the qubit to either the quantum state $|0\rangle$
                                                or $|1\rangle$ given by the respective norm-square of the probability
                                                amplitudes $\alpha$ and $\beta$, or $\alpha^{2}$ and $\beta^{2}$. Bloch
                                                Sphere We can use a bloch ball or sphere to help us visualize sping down
                                                $0$ and spin up $1$ of a single qubit. The bloch spehere has a radius of
                                                1, meaning that $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and
                                                $|1\rangle$ corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$
                                                value shows a spin up or sping down. Bloch SphereWe can use a bloch ball
                                                or sphere to help us visualize sping down $0$ and spin up $1$ of a
                                                single qubit. The bloch spehere has a radius of 1, meaning that
                                                $|0\rangle$ corresponds to $(x,y,z)$ point $(x,y,1)$ and $|1\rangle$
                                                corresponds to $(x,y,z)$ point $(x,y,-1)$. Where our $z$ value shows a
                                                spin up or sping down. Superposition In classical computing, states $0$
                                                and $1$ would be the only states that exist for the bit. However, in
                                                quantum mechanics a quibit can be both in the state of $|0\rangle$ and
                                                $|1\rangle$. This is what gives quantum computers more processing power,
                                                as a single qubit can be in more states and therefore represent more
                                                information than a single classical bit. This means that the qubit can
                                                have an $80$% of being in state $|0\rangle$ and $20$% of being in state
                                                $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of being
                                                in state $|1\rangle$. Unlike a classical bit where there is either a
                                                $100$% of the classical bit being in state $0$ and $0$% of being a $1$
                                                or a $0$% of the classical bit being a $0$ and $100$% of being a $1$. To
                                                allow for the qubit to be in superposition, we need to levearage Hilbert
                                                Space. Again, Hilbert space is represented using complex vector space.
                                                We use complex vector space, because it is the easiest way for the math
                                                to work. Let's represent this qubit in superposition as a vector using
                                                dirac notation. For the qubit have, for example, a $50$% of being in
                                                state $|0\rangle$ and $50$% of being in state $|1\rangle$, the vector
                                                should look like: $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The
                                                coefficent for $0$ and $1$ are both $\frac{1}{\sqrt{2}}$ and therefore
                                                equal parts $0$ and $1$. SuperpositionIn classical computing, states $0$
                                                and $1$ would be the only states that exist for the bit. However, in
                                                quantum mechanics a quibit can be both in the state of $|0\rangle$ and
                                                $|1\rangle$. This is what gives quantum computers more processing power,
                                                as a single qubit can be in more states and therefore represent more
                                                information than a single classical bit. This means that the qubit can
                                                have an $80$% of being in state $|0\rangle$ and $20$% of being in state
                                                $|1\rangle$, or $75$% of being in state $|0\rangle$ and $25$% of being
                                                in state $|1\rangle$. Unlike a classical bit where there is either a
                                                $100$% of the classical bit being in state $0$ and $0$% of being a $1$
                                                or a $0$% of the classical bit being a $0$ and $100$% of being a $1$. To
                                                allow for the qubit to be in superposition, we need to levearage Hilbert
                                                Space. Again, Hilbert space is represented using complex vector space.
                                                We use complex vector space, because it is the easiest way for the math
                                                to work. Let's represent this qubit in superposition as a vector using
                                                dirac notation. For the qubit have, for example, a $50$% of being in
                                                state $|0\rangle$ and $50$% of being in state $|1\rangle$, the vector
                                                should look like: $\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$ The
                                                coefficent for $0$ and $1$ are both $\frac{1}{\sqrt{2}}$ and therefore
                                                equal parts $0$ and $1$. Pauli Matrices Pauli matrices have a core
                                                importance in quantum physics, and when combinex with an identity
                                                matrix, pauli matrices form a basis for all single quantum gates. Pauli
                                                matrices are defined as: $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0
                                                \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                                $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is
                                                usally refered to as the NOT gate and can be written as $X$. The NOT
                                                gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$
                                                are phase shifting gates. Pauli MatricesPauli matrices have a core
                                                importance in quantum physics, and when combinex with an identity
                                                matrix, pauli matrices form a basis for all single quantum gates. Pauli
                                                matrices are defined as: $\sigma_{x}=\begin{pmatrix} 0&1 \\ 1&0
                                                \end{pmatrix}$, $\sigma_{y}=\begin{pmatrix} 0&-i \\ i&0 \end{pmatrix}$,
                                                $\sigma_{z}=\begin{pmatrix} 1&0 \\ 0&-1 \end{pmatrix}$ $\sigma_{x}$ is
                                                usally refered to as the NOT gate and can be written as $X$. The NOT
                                                gate inverts $|0\rangle$ and $|1\rangle$ and $\sigma_{y}$, $\sigma_{z}$
                                                are phase shifting gates. Multiple Qubits Quantum Registers A system
                                                that contains more than one qubit is represented in a quantum register.
                                                A system with two qubits is represented using a four-dimensional Hilbert
                                                space $H_{4} = H_{2} \otimes H_{2}$. The orthonormal basis is
                                                $\left\{|0\rangle |0\rangle, |0\rangle |1\rangle, |1\rangle |0\rangle,
                                                |1\rangle |1\rangle\ \right\}$. $|0\rangle |0\rangle$ can be more
                                                succinctly written as $|00\rangle$. The same is true for $|0\rangle
                                                |1\rangle = |01\rangle$, etc. A state of this two-qubit system, a part
                                                from four-dimensional Hilbert space is a unit-length vector:
                                                $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$
                                                Again, as it is with a single qubit, it is required that $|c_{0}|^{2} +
                                                |c_{1}|^{2} + |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this
                                                two-qubit system will give $00$, $01$, $10$, and $11$ with the outcomes
                                                $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$, respectively. If we want
                                                to observe one of the qubits, then the standard rules of probabilities
                                                apply. It is important to note that the tensor product of these vectors
                                                does not commute. Meaning, that $|0\rangle|1\rangle \neq
                                                |1\rangle|0\rangle$. Linear ordering is used to address the qubits
                                                individually. Quantum Entanglement Maximally Entangled Bell States Bell
                                                states or Einstein, Podolski and Rosen pairs are the maximally entangled
                                                quantum states of a qubit system such that a quantum mechanical system
                                                is composed of two interacting two-level subsystems. The 4 types of
                                                maximially entangled Bell states can be defined as: $|\Phi^{+}\rangle =
                                                \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                                \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                                \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                                \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Multiple QubitsQuantum Registers
                                                A system that contains more than one qubit is represented in a quantum
                                                register. A system with two qubits is represented using a
                                                four-dimensional Hilbert space $H_{4} = H_{2} \otimes H_{2}$. The
                                                orthonormal basis is $\left\{|0\rangle |0\rangle, |0\rangle |1\rangle,
                                                |1\rangle |0\rangle, |1\rangle |1\rangle\ \right\}$. $|0\rangle
                                                |0\rangle$ can be more succinctly written as $|00\rangle$. The same is
                                                true for $|0\rangle |1\rangle = |01\rangle$, etc. A state of this
                                                two-qubit system, a part from four-dimensional Hilbert space is a
                                                unit-length vector: $c_{0}|00\rangle + c_{1}|01\rangle + c_{2}|10\rangle
                                                + c_{3}|11\rangle$ Again, as it is with a single qubit, it is required
                                                that $|c_{0}|^{2} + |c_{1}|^{2} + |c_{2}|^{2} + |c_{3}|^{2} = 1$.
                                                Observations of this two-qubit system will give $00$, $01$, $10$, and
                                                $11$ with the outcomes $|c_{0}|^2, |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$,
                                                respectively. If we want to observe one of the qubits, then the standard
                                                rules of probabilities apply. It is important to note that the tensor
                                                product of these vectors does not commute. Meaning, that
                                                $|0\rangle|1\rangle \neq |1\rangle|0\rangle$. Linear ordering is used to
                                                address the qubits individually. Quantum RegistersA system that contains
                                                more than one qubit is represented in a quantum register. A system with
                                                two qubits is represented using a four-dimensional Hilbert space $H_{4}
                                                = H_{2} \otimes H_{2}$. The orthonormal basis is $\left\{|0\rangle
                                                |0\rangle, |0\rangle |1\rangle, |1\rangle |0\rangle, |1\rangle
                                                |1\rangle\ \right\}$. $|0\rangle |0\rangle$ can be more succinctly
                                                written as $|00\rangle$. The same is true for $|0\rangle |1\rangle =
                                                |01\rangle$, etc. A state of this two-qubit system, a part from
                                                four-dimensional Hilbert space is a unit-length vector: $c_{0}|00\rangle
                                                + c_{1}|01\rangle + c_{2}|10\rangle + c_{3}|11\rangle$ Again, as it is
                                                with a single qubit, it is required that $|c_{0}|^{2} + |c_{1}|^{2} +
                                                |c_{2}|^{2} + |c_{3}|^{2} = 1$. Observations of this two-qubit system
                                                will give $00$, $01$, $10$, and $11$ with the outcomes $|c_{0}|^2,
                                                |c_{1}|^2, |c_{2}|^2, |c_{3}|^2$, respectively. If we want to observe
                                                one of the qubits, then the standard rules of probabilities apply. It is
                                                important to note that the tensor product of these vectors does not
                                                commute. Meaning, that $|0\rangle|1\rangle \neq |1\rangle|0\rangle$.
                                                Linear ordering is used to address the qubits individually. Quantum
                                                Entanglement Maximally Entangled Bell States Bell states or Einstein,
                                                Podolski and Rosen pairs are the maximally entangled quantum states of a
                                                qubit system such that a quantum mechanical system is composed of two
                                                interacting two-level subsystems. The 4 types of maximially entangled
                                                Bell states can be defined as: $|\Phi^{+}\rangle =
                                                \frac{|00\rangle+|11\rangle}{\sqrt{2}}$ $|\Phi^{-}\rangle =
                                                \frac{|00\rangle-|11\rangle}{\sqrt{2}}$ $|\Psi^{+}\rangle =
                                                \frac{|01\rangle+|10\rangle}{\sqrt{2}}$ $|\Psi^{-}\rangle =
                                                \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Quantum EntanglementMaximally
                                                Entangled Bell States Bell states or Einstein, Podolski and Rosen pairs
                                                are the maximally entangled quantum states of a qubit system such that a
                                                quantum mechanical system is composed of two interacting two-level
                                                subsystems. The 4 types of maximially entangled Bell states can be
                                                defined as: $|\Phi^{+}\rangle = \frac{|00\rangle+|11\rangle}{\sqrt{2}}$
                                                $|\Phi^{-}\rangle = \frac{|00\rangle-|11\rangle}{\sqrt{2}}$
                                                $|\Psi^{+}\rangle = \frac{|01\rangle+|10\rangle}{\sqrt{2}}$
                                                $|\Psi^{-}\rangle = \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Maximally
                                                Entangled Bell States Bell states or Einstein, Podolski and Rosen pairs
                                                are the maximally entangled quantum states of a qubit system such that a
                                                quantum mechanical system is composed of two interacting two-level
                                                subsystems. The 4 types of maximially entangled Bell states can be
                                                defined as: $|\Phi^{+}\rangle = \frac{|00\rangle+|11\rangle}{\sqrt{2}}$
                                                $|\Phi^{-}\rangle = \frac{|00\rangle-|11\rangle}{\sqrt{2}}$
                                                $|\Psi^{+}\rangle = \frac{|01\rangle+|10\rangle}{\sqrt{2}}$
                                                $|\Psi^{-}\rangle = \frac{|01\rangle-|10\rangle}{\sqrt{2}}$ Measurments
                                                Bases Measurments The orthagonal basis states for $Z$, $X$, and $Y$, are
                                                as follows: $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                                $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$
                                                basis to the $X$ bases, we need to apply a Hadmard gate to the state,
                                                allowing for the state to be halfway:
                                                $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                Y Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases,
                                                we need to apply a Hadmard gate to the state, allowing for the state to
                                                be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                                $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                                \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                                \end{pmatrix}$ Z Bases Measurment No transformations needed, since we
                                                measure in the "normal" bases. MeasurmentsBases Measurments The
                                                orthagonal basis states for $Z$, $X$, and $Y$, are as follows:
                                                $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                                $Y=\{|i\rangle,|-i\rangle\}$ X Bases Measurment To measure from the $Z$
                                                basis to the $X$ bases, we need to apply a Hadmard gate to the state,
                                                allowing for the state to be halfway:
                                                $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                Y Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases,
                                                we need to apply a Hadmard gate to the state, allowing for the state to
                                                be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                                $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                                \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                                \end{pmatrix}$ Z Bases Measurment No transformations needed, since we
                                                measure in the "normal" bases. Bases MeasurmentsThe orthagonal basis
                                                states for $Z$, $X$, and $Y$, are as follows:
                                                $Z=\{|0\rangle,|1\rangle\}$ $X=\{|-\rangle,|+\rangle\}$
                                                $Y=\{|i\rangle,|-i\rangle\}$
                                                $Z=\{|0\rangle,|1\rangle\}$$X=\{|-\rangle,|+\rangle\}$$Y=\{|i\rangle,|-i\rangle\}$X
                                                Bases Measurment To measure from the $Z$ basis to the $X$ bases, we need
                                                to apply a Hadmard gate to the state, allowing for the state to be
                                                halfway: $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                X Bases MeasurmentTo measure from the $Z$ basis to the $X$ bases, we
                                                need to apply a Hadmard gate to the state, allowing for the state to be
                                                halfway: $H|0\rangle=|+\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                \frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle+|1\rangle)$
                                                $H|1\rangle=|-\rangle=\begin{pmatrix}\frac{1}{\sqrt{2}}\\
                                                -\frac{1}{\sqrt{2}}\end{pmatrix}=\frac{1}{\sqrt{2}}(|0\rangle-|1\rangle)$
                                                Y Bases Measurment Then to measure from the $Z$ basis to the $Y$ bases,
                                                we need to apply a Hadmard gate to the state, allowing for the state to
                                                be halfway we need to apply $S^{*\intercal}=S^{\dagger}$:
                                                $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & -i\\1 & i
                                                \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 & 1\\i & -i
                                                \end{pmatrix}$ Y Bases MeasurmentThen to measure from the $Z$ basis to
                                                the $Y$ bases, we need to apply a Hadmard gate to the state, allowing
                                                for the state to be halfway we need to apply
                                                $S^{*\intercal}=S^{\dagger}$: $=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                                -i\\1 & i \end{pmatrix}^{\dagger}=\frac{1}{\sqrt{2}}\begin{pmatrix}1 &
                                                1\\i & -i \end{pmatrix}$ Z Bases Measurment No transformations needed,
                                                since we measure in the "normal" bases. Z Bases MeasurmentNo
                                                transformations needed, since we measure in the "normal" bases.
                                                Operators Unitary Operator All quantum gates are unitary, which we will
                                                define in the following. Let us use the basis measurment of $Z$ with the
                                                coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                                \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                                operation on a quibit, called an unary quantum gate, is a unitary
                                                mapping $U: H_{2} \rightarrow H_{2}$ with the following defining linear
                                                operation: $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle
                                                \mapsto c|0\rangle + d|1\rangle$ An important aspect of all quantum
                                                gates is that they are unitary. Meaning, that for some given matrix
                                                operation $U$, defined as: $\begin{pmatrix} a & b \\ c & d
                                                \end{pmatrix}$ it is neccesary that this matrix is unitary in order to
                                                be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                                unitary and valid, the following equivalency must be true:
                                                $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                                and $I$ is the identity matrix. Another important qualtiy of an unitary
                                                matrix is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get:
                                                $\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^*
                                                \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1
                                                \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                                denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                                d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation
                                                $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                                complex conjugate of a complex number is the number with an equal real
                                                part and an imaginary part equal in magnitude, but opposite in sign of
                                                the complex number. The mapping of for unary quantum operator, when
                                                represented in a quantum circut, can be a quantum gate. Where the output
                                                of the quantum gate must have the same dimensionality as its input. So,
                                                $U: H_{n} \rightarrow H_{n}$, where $n$ is the number of dimensions of
                                                $H$. Hermitian Operator A unitary operator is Hermitian if:
                                                $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                                matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian
                                                matrix is a special case of a unitary matrix, where all Hermitian
                                                operators or unitary operators, but not all unitary operators, are
                                                Hermitian. Natural Operator An Hermitian operator is Natural if:
                                                $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate
                                                transpose of $U$. the operator has spectural decomposition, where $U$
                                                can be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$
                                                OperatorsUnitary Operator All quantum gates are unitary, which we will
                                                define in the following. Let us use the basis measurment of $Z$ with the
                                                coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                                \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                                operation on a quibit, called an unary quantum gate, is a unitary
                                                mapping $U: H_{2} \rightarrow H_{2}$ with the following defining linear
                                                operation: $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle
                                                \mapsto c|0\rangle + d|1\rangle$ An important aspect of all quantum
                                                gates is that they are unitary. Meaning, that for some given matrix
                                                operation $U$, defined as: $\begin{pmatrix} a & b \\ c & d
                                                \end{pmatrix}$ it is neccesary that this matrix is unitary in order to
                                                be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                                unitary and valid, the following equivalency must be true:
                                                $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                                and $I$ is the identity matrix. Another important qualtiy of an unitary
                                                matrix is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get:
                                                $\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^*
                                                \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1
                                                \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                                denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                                d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation
                                                $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                                complex conjugate of a complex number is the number with an equal real
                                                part and an imaginary part equal in magnitude, but opposite in sign of
                                                the complex number. The mapping of for unary quantum operator, when
                                                represented in a quantum circut, can be a quantum gate. Where the output
                                                of the quantum gate must have the same dimensionality as its input. So,
                                                $U: H_{n} \rightarrow H_{n}$, where $n$ is the number of dimensions of
                                                $H$. Unitary OperatorAll quantum gates are unitary, which we will define
                                                in the following. Let us use the basis measurment of $Z$ with the
                                                coordinate representation $|0\rangle = \begin{bmatrix} 1\\ 0
                                                \end{bmatrix}$ and $|1\rangle = \begin{bmatrix} 0\\ 1 \end{bmatrix}$. An
                                                operation on a quibit, called an unary quantum gate, is a unitary
                                                mapping $U: H_{2} \rightarrow H_{2}$ with the following defining linear
                                                operation: $|0\rangle \mapsto a|0\rangle + b|1\rangle$ $|1\rangle
                                                \mapsto c|0\rangle + d|1\rangle$ An important aspect of all quantum
                                                gates is that they are unitary. Meaning, that for some given matrix
                                                operation $U$, defined as: $\begin{pmatrix} a & b \\ c & d
                                                \end{pmatrix}$ it is neccesary that this matrix is unitary in order to
                                                be a valid quatnum gate. So, for some matrix or operator $U$ to be
                                                unitary and valid, the following equivalency must be true:
                                                $UU^{\dagger}=I$ where $U^{\dagger}$ represents the conjugate transpose
                                                and $I$ is the identity matrix. Another important qualtiy of an unitary
                                                matrix is: $U^{\dagger}=U^{-1}$ Represented as matrices, we get:
                                                $\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} a^* & b^*
                                                \\ c^* & d^* \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1
                                                \end{pmatrix}$ Written in vector form, the conjugate transpose of $U$ is
                                                denoted as $U^{\dagger}$ and can be wrriten as: $\begin{pmatrix}a\\
                                                d\end{pmatrix}^{\dagger}=\left(a^{*}d^{*}\right)$ where the notation
                                                $a^{*}$ stands for the complex conjugate of the complex number $a$. The
                                                complex conjugate of a complex number is the number with an equal real
                                                part and an imaginary part equal in magnitude, but opposite in sign of
                                                the complex number. The mapping of for unary quantum operator, when
                                                represented in a quantum circut, can be a quantum gate. Where the output
                                                of the quantum gate must have the same dimensionality as its input. So,
                                                $U: H_{n} \rightarrow H_{n}$, where $n$ is the number of dimensions of
                                                $H$. Hermitian Operator A unitary operator is Hermitian if:
                                                $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                                matrix $U$. $U^{2}=I$, where $I$ is an identity matrix. A Hermatian
                                                matrix is a special case of a unitary matrix, where all Hermitian
                                                operators or unitary operators, but not all unitary operators, are
                                                Hermitian. Hermitian OperatorA unitary operator is Hermitian if: unitary
                                                operator$U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose
                                                of the matrix $U$. $U^{2}=I$, where $I$ is an identity matrix.
                                                $U^{\dagger}=U$, where $U^{\dagger}$ is the conjugate transpose of the
                                                matrix $U$. $U^{2}=I$, where $I$ is an identity matrix.A Hermatian
                                                matrix is a special case of a unitary matrix, where all Hermitian
                                                operators or unitary operators, but not all unitary operators, are
                                                Hermitian. Natural Operator An Hermitian operator is Natural if:
                                                $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate
                                                transpose of $U$. the operator has spectural decomposition, where $U$
                                                can be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Natural
                                                OperatorAn Hermitian operator is Natural if: Hermitian
                                                operator$U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the
                                                conjugate transpose of $U$. the operator has spectural decomposition,
                                                where $U$ can be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$
                                                $U^{\dagger}U=UU^{\dagger}$, where $U^{\dagger}$ is the conjugate
                                                transpose of $U$.the operator has spectural decomposition, where $U$ can
                                                be decomposed as:
                                                $U=\sum_{i}\lambda_{i}|\lambda_{i}\rangle\langle\lambda_{i}|$ Quantum
                                                Algorithms Quantum Parallelism Suppose we want to evaluate a function
                                                $f(x)$, where the function $f$ expresses some computation or algorithm.
                                                A use case for quantum parallelism is to evaluate $f(x)$ with many
                                                different values for the output of the computation or algorithm on the
                                                input $x$ simultaneously. In essence, we can evaluate many different
                                                values of $x$ on $f$ in parallel by exploiting quantum effects. This
                                                quantum effect exploit feature is fundamental in many quantum
                                                algorithms. To continue, we will look at how quantum parallelism works.
                                                Consider the one-bit domain and range function
                                                $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$ on a
                                                quantum computer, we will use a two-qubit quantum computer with the
                                                starting state $|x,y\rangle$. The transformation on the domain or 'data'
                                                register to the range or 'target' register of this initial two qubit
                                                state is described by the following unitary function:
                                                $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                                represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts
                                                on $y$, and its value is $0$ then the value of the second qubit in the
                                                'target' register is the value $f(x)$, given whatever function $f$
                                                represents. The functions effect on $x$ is arbritrary for now. The final
                                                collapsed state $|\psi\rangle$ is an element of the set of final states
                                                or 'target' register $|x,y\oplus f(x)\rangle$, which again is given by
                                                the unitary transformation $U_{f}$ on the start state $|x,y\rangle$.
                                                Given the input $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$
                                                to $x$, such that now:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting
                                                state is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the
                                                resulting new state is not apart of the starting computational basis
                                                $\{0,1\}$. Next, the unitary function or blackbox computation/algorithm
                                                $U_{f}$ can be applied to the current 'data' register. The resulting
                                                mapping of the unitary function $U_{f}$ is:
                                                $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                                f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                                $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                                Meaning that the final resulting state for a two-qubit quantum computer
                                                has a $50$% chance of being $|0,f(0)\rangle$ and $50$% of being
                                                $.5|1,f(1)\rangle$. Given in the same form as the range for $U_{f}$
                                                given above: $|x,y'\rangle$, where $y'=y\oplus f(x)$ All of this means
                                                that the information given by the mapping for $f(0)$ and $f(1)$ was
                                                simultaneously evaluated by applying superposition and the unitary
                                                function on the starting 'data' register. Thus, $f(x)$ has been computed
                                                for two values of $x$ in parellel. The resulting set of all possible
                                                states computed in parallel is given by the resulting 'target' register
                                                is given by quantum exploitation and aptly named 'quantum parallelism'.
                                                Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$
                                                values of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit
                                                system such that $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply
                                                the NOT gate to $y$, giving the what will be the start state
                                                $|\psi_{0}\rangle=|01\rangle$. Next, we will apply a Hadmard gates to
                                                $x$ and $y$ individually, yielding the state:
                                                $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                where for state $|\psi_{1}\rangle$:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                                $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary
                                                unitary function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                                $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                From this, we can then say that if $f(0)=f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$.
                                                If $f(0)=f(1)$, the resulting state is:
                                                $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$
                                                $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0)
                                                \oplus f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus,
                                                $f(0)$ interfers with $f(1)$ when we simultaneously evalute $f(x)$ with
                                                quantum parallelism. Grovers Search Algorithm The authors describe the
                                                process for Grovers Search Algorithm in the following sequential two
                                                main steps: Hadmard transformation and Grover iteration or Grover
                                                operator $G$. Hadmard Transformation The Hadmard transform puts the
                                                qubits of the quantum computer into equal superposition states, defined
                                                as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                                \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm
                                                implements a repeated quantum subroutine called Grover iteration or
                                                operator, denoted as $G$. This quantum iteration can be broken up in
                                                four steps: Apply oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$
                                                Apply conditional phase shift on quantum register, such that every
                                                quantum basis state except $|r\rangle$ is phased shifted $-1$. Meaning
                                                that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1 \sin
                                                \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                                \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                                $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate
                                                $R_{s}$, such that moving our state towards $|w\rangle$. Lastly, apply
                                                the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2, 3, 4,
                                                or Grovers iteration without the oracle step can be written as:
                                                $H^{\otimes n}(2|0\rangle\langle 0|-I) H^{\otimes
                                                n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly eighted
                                                superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                                Thus, including the oracle step now, Grovers iteration $G$ as a whole
                                                can be written, more generically for a given quantum state $\psi$ as
                                                $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image
                                                Source: $[1]$ To continue. Quantum AlgorithmsQuantum Parallelism Suppose
                                                we want to evaluate a function $f(x)$, where the function $f$ expresses
                                                some computation or algorithm. A use case for quantum parallelism is to
                                                evaluate $f(x)$ with many different values for the output of the
                                                computation or algorithm on the input $x$ simultaneously. In essence, we
                                                can evaluate many different values of $x$ on $f$ in parallel by
                                                exploiting quantum effects. This quantum effect exploit feature is
                                                fundamental in many quantum algorithms. To continue, we will look at how
                                                quantum parallelism works. Consider the one-bit domain and range
                                                function $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$
                                                on a quantum computer, we will use a two-qubit quantum computer with the
                                                starting state $|x,y\rangle$. The transformation on the domain or 'data'
                                                register to the range or 'target' register of this initial two qubit
                                                state is described by the following unitary function:
                                                $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                                represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts
                                                on $y$, and its value is $0$ then the value of the second qubit in the
                                                'target' register is the value $f(x)$, given whatever function $f$
                                                represents. The functions effect on $x$ is arbritrary for now. The final
                                                collapsed state $|\psi\rangle$ is an element of the set of final states
                                                or 'target' register $|x,y\oplus f(x)\rangle$, which again is given by
                                                the unitary transformation $U_{f}$ on the start state $|x,y\rangle$.
                                                Given the input $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$
                                                to $x$, such that now:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting
                                                state is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the
                                                resulting new state is not apart of the starting computational basis
                                                $\{0,1\}$. Next, the unitary function or blackbox computation/algorithm
                                                $U_{f}$ can be applied to the current 'data' register. The resulting
                                                mapping of the unitary function $U_{f}$ is:
                                                $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                                f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                                $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                                Meaning that the final resulting state for a two-qubit quantum computer
                                                has a $50$% chance of being $|0,f(0)\rangle$ and $50$% of being
                                                $.5|1,f(1)\rangle$. Given in the same form as the range for $U_{f}$
                                                given above: $|x,y'\rangle$, where $y'=y\oplus f(x)$ All of this means
                                                that the information given by the mapping for $f(0)$ and $f(1)$ was
                                                simultaneously evaluated by applying superposition and the unitary
                                                function on the starting 'data' register. Thus, $f(x)$ has been computed
                                                for two values of $x$ in parellel. The resulting set of all possible
                                                states computed in parallel is given by the resulting 'target' register
                                                is given by quantum exploitation and aptly named 'quantum parallelism'.
                                                Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$
                                                values of $x$ simultaneously. Quantum ParallelismSuppose we want to
                                                evaluate a function $f(x)$, where the function $f$ expresses some
                                                computation or algorithm. A use case for quantum parallelism is to
                                                evaluate $f(x)$ with many different values for the output of the
                                                computation or algorithm on the input $x$ simultaneously. In essence, we
                                                can evaluate many different values of $x$ on $f$ in parallel by
                                                exploiting quantum effects. This quantum effect exploit feature is
                                                fundamental in many quantum algorithms. To continue, we will look at how
                                                quantum parallelism works. Consider the one-bit domain and range
                                                function $f(x):\{0,1\}\rightarrow\{0,1\}$. To compute this function $f$
                                                on a quantum computer, we will use a two-qubit quantum computer with the
                                                starting state $|x,y\rangle$. The transformation on the domain or 'data'
                                                register to the range or 'target' register of this initial two qubit
                                                state is described by the following unitary function:
                                                $U_{f}:|x,y\rangle\rightarrow|x,y\oplus f(x)\rangle$ where the $\oplus$
                                                represents addition modulo 2 and $x=q_{0}, y=q_{1}$. When $\oplus$ acts
                                                on $y$, and its value is $0$ then the value of the second qubit in the
                                                'target' register is the value $f(x)$, given whatever function $f$
                                                represents. The functions effect on $x$ is arbritrary for now. The final
                                                collapsed state $|\psi\rangle$ is an element of the set of final states
                                                or 'target' register $|x,y\oplus f(x)\rangle$, which again is given by
                                                the unitary transformation $U_{f}$ on the start state $|x,y\rangle$.
                                                Given the input $q_{0}=x=|0\rangle$, we will apply the Hadmard gate $H$
                                                to $x$, such that now:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}},y=|0\rangle$ where the resulting
                                                state is: $\frac{|0 f(0)\rangle+|1 f(1)\rangle}{\sqrt{2}}$ which the
                                                resulting new state is not apart of the starting computational basis
                                                $\{0,1\}$. Next, the unitary function or blackbox computation/algorithm
                                                $U_{f}$ can be applied to the current 'data' register. The resulting
                                                mapping of the unitary function $U_{f}$ is:
                                                $U_{f}\left(\frac{|0\rangle+|1\rangle}{\sqrt{2}},|0\rangle\right)=\frac{|0,
                                                f(0)\rangle+|1, f(1)\rangle}{\sqrt{2}}$
                                                $=\frac{1}{\sqrt{2}}(|0,f(0)\rangle+|1,f(1)\rangle)$$=.5|0,f(0)\rangle+.5|1,f(1)\rangle$
                                                Meaning that the final resulting state for a two-qubit quantum computer
                                                has a $50$% chance of being $|0,f(0)\rangle$ and $50$% of being
                                                $.5|1,f(1)\rangle$. Given in the same form as the range for $U_{f}$
                                                given above: $|x,y'\rangle$, where $y'=y\oplus f(x)$ All of this means
                                                that the information given by the mapping for $f(0)$ and $f(1)$ was
                                                simultaneously evaluated by applying superposition and the unitary
                                                function on the starting 'data' register. Thus, $f(x)$ has been computed
                                                for two values of $x$ in parellel. The resulting set of all possible
                                                states computed in parallel is given by the resulting 'target' register
                                                is given by quantum exploitation and aptly named 'quantum parallelism'.
                                                Thus, a single $f(x)$ circuit can be used to evaluate the result for $n$
                                                values of $x$ simultaneously. Deutsch's Algorithm Consider a two-qubit
                                                system such that $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply
                                                the NOT gate to $y$, giving the what will be the start state
                                                $|\psi_{0}\rangle=|01\rangle$. Next, we will apply a Hadmard gates to
                                                $x$ and $y$ individually, yielding the state:
                                                $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                where for state $|\psi_{1}\rangle$:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                                $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary
                                                unitary function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                                $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                From this, we can then say that if $f(0)=f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$.
                                                If $f(0)=f(1)$, the resulting state is:
                                                $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$
                                                $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0)
                                                \oplus f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus,
                                                $f(0)$ interfers with $f(1)$ when we simultaneously evalute $f(x)$ with
                                                quantum parallelism. Deutsch's AlgorithmConsider a two-qubit system such
                                                that $q_{0}=x=|0\rangle,q_{1}=y=|0\rangle$. Now, let's apply the NOT
                                                gate to $y$, giving the what will be the start state
                                                $|\psi_{0}\rangle=|01\rangle$. Next, we will apply a Hadmard gates to
                                                $x$ and $y$ individually, yielding the state:
                                                $|\psi_{1}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                where for state $|\psi_{1}\rangle$:
                                                $x=\frac{|0\rangle+|1\rangle}{\sqrt{2}}$
                                                $y=\frac{|0\rangle-|1\rangle}{\sqrt{2}}$ Applying some arbritrary
                                                unitary function $U_{f}$ on $|xy\rangle$ or $|\psi_{1}\rangle$ gives:
                                                $|\psi_{2}\rangle=\left(-1\right)^{f(x)}\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                From this, we can then say that if $f(0)=f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle+|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                or if $f(0)\neq f(1)$:
                                                $|\psi_{2}\rangle=\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                Next step in Deutsch's algorithm is to apply a Hamdmard gate $H$ to $x$.
                                                If $f(0)=f(1)$, the resulting state is:
                                                $|\psi_{3}\rangle=\pm|0\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$or
                                                if $f(0)\neq f(1)$
                                                $|\psi_{3}\rangle=\pm|1\rangle\left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$
                                                This can then be written more succenctly as: $|\psi_{3}\rangle=\pm f(0)
                                                \oplus f(1) \left[\frac{|0\rangle-|1\rangle}{\sqrt{2}}\right]$ Thus,
                                                $f(0)$ interfers with $f(1)$ when we simultaneously evalute $f(x)$ with
                                                quantum parallelism. Grovers Search Algorithm The authors describe the
                                                process for Grovers Search Algorithm in the following sequential two
                                                main steps: Hadmard transformation and Grover iteration or Grover
                                                operator $G$. Hadmard Transformation The Hadmard transform puts the
                                                qubits of the quantum computer into equal superposition states, defined
                                                as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                                \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm
                                                implements a repeated quantum subroutine called Grover iteration or
                                                operator, denoted as $G$. This quantum iteration can be broken up in
                                                four steps: Apply oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$
                                                Apply conditional phase shift on quantum register, such that every
                                                quantum basis state except $|r\rangle$ is phased shifted $-1$. Meaning
                                                that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1 \sin
                                                \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                                \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                                $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate
                                                $R_{s}$, such that moving our state towards $|w\rangle$. Lastly, apply
                                                the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2, 3, 4,
                                                or Grovers iteration without the oracle step can be written as:
                                                $H^{\otimes n}(2|0\rangle\langle 0|-I) H^{\otimes
                                                n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly eighted
                                                superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                                Thus, including the oracle step now, Grovers iteration $G$ as a whole
                                                can be written, more generically for a given quantum state $\psi$ as
                                                $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image
                                                Source: $[1]$ To continue. Grovers Search AlgorithmThe authors describe
                                                the process for Grovers Search Algorithm in the following sequential two
                                                main steps: Hadmard transformation and Grover iteration or Grover
                                                operator $G$. Hadmard Transformation The Hadmard transform puts the
                                                qubits of the quantum computer into equal superposition states, defined
                                                as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                                \in\{0,1\}^n}|x\rangle$ Grover Operation Grovers search algorithm
                                                implements a repeated quantum subroutine called Grover iteration or
                                                operator, denoted as $G$. This quantum iteration can be broken up in
                                                four steps: Apply oracle $O$ Apply Hadmard transform $H^{\otimes{n}}$
                                                Apply conditional phase shift on quantum register, such that every
                                                quantum basis state except $|r\rangle$ is phased shifted $-1$. Meaning
                                                that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1 \sin
                                                \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                                \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                                $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate
                                                $R_{s}$, such that moving our state towards $|w\rangle$. Lastly, apply
                                                the Hadmard transform $H^{\otimes{n}}$ Where combined steps of 2, 3, 4,
                                                or Grovers iteration without the oracle step can be written as:
                                                $H^{\otimes n}(2|0\rangle\langle 0|-I) H^{\otimes
                                                n}=2|\psi\rangle\langle\psi|-I$ where $|\psi\rangle$ is equaly eighted
                                                superposition of states $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$.
                                                Thus, including the oracle step now, Grovers iteration $G$ as a whole
                                                can be written, more generically for a given quantum state $\psi$ as
                                                $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image
                                                Source: $[1]$ The authors describe the process for Grovers Search
                                                Algorithm in the following sequential two main steps: Hadmard
                                                transformation and Grover iteration or Grover operator $G$. Hadmard
                                                transformationGrover iteration or Grover operator $G$Hadmard
                                                TransformationHadmard TransformationThe Hadmard transform puts the
                                                qubits of the quantum computer into equal superposition states, defined
                                                as: $|s\rangle=|+\rangle^{\otimes n}=\frac{1}{\sqrt{N}} \sum_{x
                                                \in\{0,1\}^n}|x\rangle$ Grover OperationGrover OperationGrovers search
                                                algorithm implements a repeated quantum subroutine called Grover
                                                iteration or operator, denoted as $G$. This quantum iteration can be
                                                broken up in four steps: Apply oracle $O$ Apply Hadmard transform
                                                $H^{\otimes{n}}$ Apply conditional phase shift on quantum register, such
                                                that every quantum basis state except $|r\rangle$ is phased shifted
                                                $-1$. Meaning that for unitary: $\begin{aligned} U_f|s\rangle & =(-1)^1
                                                \sin \theta|w\rangle \\ & +(-1)^0 \cos \theta|r\rangle \\ & =-\sin
                                                \theta|w\rangle+\cos \theta|r\rangle \end{aligned}$ This phase kicks
                                                $|w\rangle$ only. Then, reflect the state around $|s\rangle$ using gate
                                                $R_{s}$, such that moving our state towards $|w\rangle$. Lastly, apply
                                                the Hadmard transform $H^{\otimes{n}}$ Apply oracle $O$Apply Hadmard
                                                transform $H^{\otimes{n}}$Apply conditional phase shift on quantum
                                                register, such that every quantum basis state except $|r\rangle$ is
                                                phased shifted $-1$. Meaning that for unitary:$\begin{aligned}
                                                U_f|s\rangle & =(-1)^1 \sin \theta|w\rangle \\ & +(-1)^0 \cos
                                                \theta|r\rangle \\ & =-\sin \theta|w\rangle+\cos \theta|r\rangle
                                                \end{aligned}$ This phase kicks $|w\rangle$ only. Then, reflect the
                                                state around $|s\rangle$ using gate $R_{s}$, such that moving our state
                                                towards $|w\rangle$. Lastly, apply the Hadmard transform
                                                $H^{\otimes{n}}$ Where combined steps of 2, 3, 4, or Grovers iteration
                                                without the oracle step can be written as: $H^{\otimes
                                                n}(2|0\rangle\langle 0|-I) H^{\otimes n}=2|\psi\rangle\langle\psi|-I$
                                                where $|\psi\rangle$ is equaly eighted superposition of states
                                                $\frac{1}{N^{1 / 2}} \sum_{x=0}^{N-1}|w\rangle$. Thus, including the
                                                oracle step now, Grovers iteration $G$ as a whole can be written, more
                                                generically for a given quantum state $\psi$ as
                                                $G=(2|\psi\rangle\langle\psi|-I) O$. The circut is given as: Image
                                                Source: $[1]$ $[1]$ To continue. Quantum Computational Theory Quantum
                                                Automata Theory Quantum Turing Machine QTM $\delta$ Function A Quantum
                                                Turing Machine QTM can be expressed similarly to a traditional Turing
                                                Machine TM with all components reformulated canonically except for the
                                                transition function $\delta$. Below, is the formal definition of a QTM.
                                                Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0},
                                                q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets
                                                and: $Q$ is a set of states $\Sigma$ is an alphabet not containing the
                                                blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in
                                                \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way
                                                infinite, with squares indexed by the set of integers $\mathbb{Z}$
                                                $\delta$ is a transition function described as $\delta : Q \times \Gamma
                                                \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$
                                                $q_{0} \in Q$ is the initial state $q_{accept} \in Q$ is the accept
                                                state $q_{reject} \in Q$ is the reject state, where $q_{reject} \neq
                                                q_{accept}$ Image Source:
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                                Quantum Computational TheoryQuantum Automata Theory Quantum Turing
                                                Machine QTM $\delta$ Function A Quantum Turing Machine QTM can be
                                                expressed similarly to a traditional Turing Machine TM with all
                                                components reformulated canonically except for the transition function
                                                $\delta$. Below, is the formal definition of a QTM. Quantum Turing
                                                Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta, q_{0}, q_{accept},
                                                q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite sets and: $Q$ is a
                                                set of states $\Sigma$ is an alphabet not containing the blank symbol
                                                $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup \in \Gamma$ and
                                                $\Sigma \subseteq \Gamma$. The tape is assumed to be two-way infinite,
                                                with squares indexed by the set of integers $\mathbb{Z}$ $\delta$ is a
                                                transition function described as $\delta : Q \times \Gamma \rightarrow
                                                \mathbb{C}^{Q \times \Gamma \times \left\{-1, +1\right\}}$ $q_{0} \in Q$
                                                is the initial state $q_{accept} \in Q$ is the accept state $q_{reject}
                                                \in Q$ is the reject state, where $q_{reject} \neq q_{accept}$ Image
                                                Source:
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                                Quantum Automata TheoryQuantum Turing Machine QTM $\delta$ Function A
                                                Quantum Turing Machine QTM can be expressed similarly to a traditional
                                                Turing Machine TM with all components reformulated canonically except
                                                for the transition function $\delta$. Below, is the formal definition of
                                                a QTM. Quantum Turing Machine is a 7-tuple: $(Q, \Sigma, \Gamma, \delta,
                                                q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are all finite
                                                sets and: $Q$ is a set of states $\Sigma$ is an alphabet not containing
                                                the blank symbol $\sqcup$ $\Gamma$ is the tape alphabet, where $\sqcup
                                                \in \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is assumed to be
                                                two-way infinite, with squares indexed by the set of integers
                                                $\mathbb{Z}$ $\delta$ is a transition function described as $\delta : Q
                                                \times \Gamma \rightarrow \mathbb{C}^{Q \times \Gamma \times \left\{-1,
                                                +1\right\}}$ $q_{0} \in Q$ is the initial state $q_{accept} \in Q$ is
                                                the accept state $q_{reject} \in Q$ is the reject state, where
                                                $q_{reject} \neq q_{accept}$ Image Source:
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                                Quantum Turing MachineQTM $\delta$ Function QTM$\delta$ FunctionA
                                                Quantum Turing Machine QTM can be expressed similarly to a traditional
                                                Turing Machine TM with all components reformulated canonically except
                                                for the transition function $\delta$. Below, is the formal definition of
                                                a QTM. exceptQuantum Turing Machine is a 7-tuple:$(Q, \Sigma, \Gamma,
                                                \delta, q_{0}, q_{accept}, q_{reject})$ where $Q, \Sigma, \Gamma$ are
                                                all finite sets and: $Q$ is a set of states $\Sigma$ is an alphabet not
                                                containing the blank symbol $\sqcup$ $\Gamma$ is the tape alphabet,
                                                where $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$. The tape is
                                                assumed to be two-way infinite, with squares indexed by the set of
                                                integers $\mathbb{Z}$ $\delta$ is a transition function described as
                                                $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times \Gamma \times
                                                \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state $q_{accept}
                                                \in Q$ is the accept state $q_{reject} \in Q$ is the reject state, where
                                                $q_{reject} \neq q_{accept}$ $Q$ is a set of states $\Sigma$ is an
                                                alphabet not containing the blank symbol $\sqcup$ $\Gamma$ is the tape
                                                alphabet, where $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$. The
                                                tape is assumed to be two-way infinite, with squares indexed by the set
                                                of integers $\mathbb{Z}$ $\delta$ is a transition function described as
                                                $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times \Gamma \times
                                                \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state $q_{accept}
                                                \in Q$ is the accept state $q_{reject} \in Q$ is the reject state, where
                                                $q_{reject} \neq q_{accept}$ $Q$ is a set of states$\Sigma$ is an
                                                alphabet not containing the blank symbol $\sqcup$$\Gamma$ is the tape
                                                alphabet, where $\sqcup \in \Gamma$ and $\Sigma \subseteq \Gamma$. The
                                                tape is assumed to be two-way infinite, with squares indexed by the set
                                                of integers $\mathbb{Z}$ $\delta$ is a transition function described as
                                                $\delta : Q \times \Gamma \rightarrow \mathbb{C}^{Q \times \Gamma \times
                                                \left\{-1, +1\right\}}$ $q_{0} \in Q$ is the initial state$q_{accept}
                                                \in Q$ is the accept state$q_{reject} \in Q$ is the reject state, where
                                                $q_{reject} \neq q_{accept}$Image Source:
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.png
                                                https://www.mdpi.com/applsci/applsci-10-05551/article_deploy/html/images/applsci-10-05551-g004.pngQuantum
                                                Networks Superdense Coding and Quantum Teleportation Superdense Coding
                                                Given Alice wants to send classical information to Bob, quantum
                                                entanglement $n$ qubits can store $2n$ qubits total of information. Say
                                                Alice needs to send one qubit of infromation, they can do so but needs
                                                Bob to already share a second qubit. So, given that Alice and Bob
                                                already share a pair of entangled qubits in state $|\Phi^{+}\rangle$,
                                                defined as: (Alice) -- $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ --
                                                (Bob) If Alice wants to send: $00$: Alice does nothing to their qubit,
                                                so the qubit is still in state:
                                                $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$: Alice
                                                applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice
                                                applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice
                                                applies $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not
                                                that Bob has both qubits in one of four Bell basis, Bob will know what
                                                the qubit Alice wants to send by measuring the two qubits and seeing
                                                what state they are in. This is called a Bell measurment. If we then
                                                extrapolate this method, then if Alice and Bob want to share $n$ pairs
                                                of entangled qubits they can do so with $2n$ quibits in total. Quantum
                                                Teleportation To continue. Quantum NetworksSuperdense Coding and Quantum
                                                Teleportation Superdense Coding Given Alice wants to send classical
                                                information to Bob, quantum entanglement $n$ qubits can store $2n$
                                                qubits total of information. Say Alice needs to send one qubit of
                                                infromation, they can do so but needs Bob to already share a second
                                                qubit. So, given that Alice and Bob already share a pair of entangled
                                                qubits in state $|\Phi^{+}\rangle$, defined as: (Alice) --
                                                $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to
                                                send: $00$: Alice does nothing to their qubit, so the qubit is still in
                                                state: $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$
                                                $01$: Alice applies $X$ gate to their qubit, transforming
                                                $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice
                                                applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice
                                                applies $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not
                                                that Bob has both qubits in one of four Bell basis, Bob will know what
                                                the qubit Alice wants to send by measuring the two qubits and seeing
                                                what state they are in. This is called a Bell measurment. If we then
                                                extrapolate this method, then if Alice and Bob want to share $n$ pairs
                                                of entangled qubits they can do so with $2n$ quibits in total. Quantum
                                                Teleportation To continue. Superdense Coding and Quantum
                                                TeleportationSuperdense Coding Given Alice wants to send classical
                                                information to Bob, quantum entanglement $n$ qubits can store $2n$
                                                qubits total of information. Say Alice needs to send one qubit of
                                                infromation, they can do so but needs Bob to already share a second
                                                qubit. So, given that Alice and Bob already share a pair of entangled
                                                qubits in state $|\Phi^{+}\rangle$, defined as: (Alice) --
                                                $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to
                                                send: $00$: Alice does nothing to their qubit, so the qubit is still in
                                                state: $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$
                                                $01$: Alice applies $X$ gate to their qubit, transforming
                                                $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice
                                                applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice
                                                applies $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not
                                                that Bob has both qubits in one of four Bell basis, Bob will know what
                                                the qubit Alice wants to send by measuring the two qubits and seeing
                                                what state they are in. This is called a Bell measurment. If we then
                                                extrapolate this method, then if Alice and Bob want to share $n$ pairs
                                                of entangled qubits they can do so with $2n$ quibits in total.
                                                Superdense CodingGiven Alice wants to send classical information to Bob,
                                                quantum entanglement $n$ qubits can store $2n$ qubits total of
                                                information. Say Alice needs to send one qubit of infromation, they can
                                                do so but needs Bob to already share a second qubit. So, given that
                                                Alice and Bob already share a pair of entangled qubits in state
                                                $|\Phi^{+}\rangle$, defined as: already(Alice) --
                                                $\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ -- (Bob) If Alice wants to
                                                send:$00$: Alice does nothing to their qubit, so the qubit is still in
                                                state: $|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$
                                                $01$: Alice applies $X$ gate to their qubit, transforming
                                                $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$: Alice
                                                applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$ to:
                                                $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$: Alice
                                                applies $X$ and $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ $00$:
                                                Alice does nothing to their qubit, so the qubit is still in
                                                state:$|\Phi^{+}\rangle=\frac{1}{\sqrt{2}}(|00\rangle+|11\rangle)$ $01$:
                                                Alice applies $X$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Psi^{+}\rangle=\frac{1}{\sqrt{2}}(|10\rangle+|01\rangle)$ $10$:
                                                Alice applies $Z$ gate to their qubit, transforming $|\Phi^{+}\rangle$
                                                to: $|\Phi^{-}\rangle=\frac{1}{\sqrt{2}}(|00\rangle-|11\rangle)$ $11$:
                                                Alice applies $X$ and $Z$ gate to their qubit, transforming
                                                $|\Phi^{+}\rangle$ to:
                                                $|\Psi^{-}\rangle=\frac{1}{\sqrt{2}}(|01\rangle-|10\rangle)$ Not that
                                                Bob has both qubits in one of four Bell basis, Bob will know what the
                                                qubit Alice wants to send by measuring the two qubits and seeing what
                                                state they are in. This is called a Bell measurment. If we then
                                                extrapolate this method, then if Alice and Bob want to share $n$ pairs
                                                of entangled qubits they can do so with $2n$ quibits in total. Quantum
                                                Teleportation To continue. Quantum TeleportationTo continue. Glossary
                                                Hilbert Space Hilbert Space is a nondenumerable infinite complex vector
                                                space. Complex space, being a collection of complex numbers $\mathbb{C}$
                                                with an added structure. The infinite dimensions of Hilbert Space
                                                represents a continious spectra of alternative physical states.
                                                Alternative physical states, for example, being the position
                                                (coordinates) or momentum of a particle. Probabilistic Systems Pure
                                                States Mixed States The nature of a probabilistic system is that we do
                                                not know for certain the state of the system. However, we do know the
                                                probability distribution of the states. Our probabilistic distribution
                                                sums up to 1. The notation can be written as: $p_{1}[x_{1}] +
                                                p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands for the state the
                                                system is in with probability $p_{i}$. Where $p_{i} \ge 0$ and $p_{1} +
                                                ... + p_{n} = 1$. Our distribution defined above is a mixed state. Where
                                                $x_{i}$ is a pure state. It is important to note that our distribution
                                                is not an expected value or an average of the mixed state, but rather
                                                represents only the probability distribution for all states $x_{i}$.
                                                Quantum Mechanics Fun From here on out we used will use a Hilbert space
                                                formalism of quantum mechanics where the representation of quantum
                                                mechanical systems are represented as state vectors. We use this
                                                representation because state vectors are mathematically simpler that the
                                                more general ones. The quantum mechanical description of a physical
                                                system resembles the probabilistic systems we mentioned earlier:
                                                $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An $n$-level system
                                                in quantum mechanics can be shown as a unit-length vector in
                                                $n$-dimensional complex vector space. We define this state space with
                                                $H_{n}$. Using ket-notation, which is a part of Dirac notation, we
                                                define our state space $H_{n}$ as an orthonormal basis $\left\{\left|
                                                x_{1} \right>,...,\left| x_{n} \right>\right\}$. We can now write any
                                                state of the quantum system as: $\alpha_{1}\left| x_{1} \right> +
                                                \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left| x_{n} \right>$
                                                Here, $\alpha_{i}$ are probabilistic amplitudes. Finally, to meet our
                                                requirements defining our state space $H_{n}$ as unit-length we say that
                                                $|\alpha_{1}|^{2} + |\alpha_{2}|^{2} + ... + |\alpha_{n}|^{2} = 1$. This
                                                concludes most of the information neccesary for this page. However, if
                                                you are having fun with quantum mechanics, feel free to read more on my
                                                Quantum Mechanics page. GlossaryHilbert Space Hilbert Space is a
                                                nondenumerable infinite complex vector space. Complex space, being a
                                                collection of complex numbers $\mathbb{C}$ with an added structure. The
                                                infinite dimensions of Hilbert Space represents a continious spectra of
                                                alternative physical states. Alternative physical states, for example,
                                                being the position (coordinates) or momentum of a particle. Hilbert
                                                SpaceHilbert Space is a nondenumerable infinite complex vector space.
                                                Complex space, being a collection of complex numbers $\mathbb{C}$ with
                                                an added structure. The infinite dimensions of Hilbert Space represents
                                                a continious spectra of alternative physical states. Alternative
                                                physical states, for example, being the position (coordinates) or
                                                momentum of a particle. Probabilistic Systems Pure States Mixed States
                                                The nature of a probabilistic system is that we do not know for certain
                                                the state of the system. However, we do know the probability
                                                distribution of the states. Our probabilistic distribution sums up to 1.
                                                The notation can be written as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... +
                                                p_{n}[x_{n}]$ $x_{i}$ stands for the state the system is in with
                                                probability $p_{i}$. Where $p_{i} \ge 0$ and $p_{1} + ... + p_{n} = 1$.
                                                Our distribution defined above is a mixed state. Where $x_{i}$ is a pure
                                                state. It is important to note that our distribution is not an expected
                                                value or an average of the mixed state, but rather represents only the
                                                probability distribution for all states $x_{i}$. Probabilistic
                                                SystemsPure States Mixed States Pure StatesMixed StatesThe nature of a
                                                probabilistic system is that we do not know for certain the state of the
                                                system. However, we do know the probability distribution of the states.
                                                Our probabilistic distribution sums up to 1. The notation can be written
                                                as: $p_{1}[x_{1}] + p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ $x_{i}$ stands
                                                for the state the system is in with probability $p_{i}$. Where $p_{i}
                                                \ge 0$ and $p_{1} + ... + p_{n} = 1$. Our distribution defined above is
                                                a mixed state. Where $x_{i}$ is a pure state. It is important to note
                                                that our distribution is not an expected value or an average of the
                                                mixed state, but rather represents only the probability distribution for
                                                all states $x_{i}$. onlyprobability distributionQuantum Mechanics Fun
                                                From here on out we used will use a Hilbert space formalism of quantum
                                                mechanics where the representation of quantum mechanical systems are
                                                represented as state vectors. We use this representation because state
                                                vectors are mathematically simpler that the more general ones. The
                                                quantum mechanical description of a physical system resembles the
                                                probabilistic systems we mentioned earlier: $p_{1}[x_{1}] + p_{2}[x_{2}]
                                                + ... + p_{n}[x_{n}]$ An $n$-level system in quantum mechanics can be
                                                shown as a unit-length vector in $n$-dimensional complex vector space.
                                                We define this state space with $H_{n}$. Using ket-notation, which is a
                                                part of Dirac notation, we define our state space $H_{n}$ as an
                                                orthonormal basis $\left\{\left| x_{1} \right>,...,\left| x_{n}
                                                \right>\right\}$. We can now write any state of the quantum system as:
                                                $\alpha_{1}\left| x_{1} \right> + \alpha_{2}\left| x_{2} \right> + ... +
                                                \alpha_{n}\left| x_{n} \right>$ Here, $\alpha_{i}$ are probabilistic
                                                amplitudes. Finally, to meet our requirements defining our state space
                                                $H_{n}$ as unit-length we say that $|\alpha_{1}|^{2} + |\alpha_{2}|^{2}
                                                + ... + |\alpha_{n}|^{2} = 1$. This concludes most of the information
                                                neccesary for this page. However, if you are having fun with quantum
                                                mechanics, feel free to read more on my Quantum Mechanics page. Quantum
                                                Mechanics FunFrom here on out we used will use a Hilbert space formalism
                                                of quantum mechanics where the representation of quantum mechanical
                                                systems are represented as state vectors. We use this representation
                                                because state vectors are mathematically simpler that the more general
                                                ones. The quantum mechanical description of a physical system resembles
                                                the probabilistic systems we mentioned earlier: $p_{1}[x_{1}] +
                                                p_{2}[x_{2}] + ... + p_{n}[x_{n}]$ An $n$-level system in quantum
                                                mechanics can be shown as a unit-length vector in $n$-dimensional
                                                complex vector space. We define this state space with $H_{n}$. Using
                                                ket-notation, which is a part of Dirac notation, we define our state
                                                space $H_{n}$ as an orthonormal basis $\left\{\left| x_{1}
                                                \right>,...,\left| x_{n} \right>\right\}$. We can now write any state of
                                                the quantum system as: $\alpha_{1}\left| x_{1} \right> +
                                                \alpha_{2}\left| x_{2} \right> + ... + \alpha_{n}\left| x_{n} \right>$
                                                Here, $\alpha_{i}$ are probabilistic amplitudes. Finally, to meet our
                                                requirements defining our state space $H_{n}$ as unit-length we say that
                                                $|\alpha_{1}|^{2} + |\alpha_{2}|^{2} + ... + |\alpha_{n}|^{2} = 1$. This
                                                concludes most of the information neccesary for this page. However, if
                                                you are having fun with quantum mechanics, feel free to read more on my
                                                Quantum Mechanics page. Quantum Mechanics
                                                [https://www.contextswitching.org/phys/quantummechanics]
                                                Quantum Mechanics - Context Switching Quantum Mechanics Preliminaries
                                                Kets, Bras, and Operators Ket Space and Operators First we will
                                                introduce the idea of Hilbert Space, which was named after D. Hilbert.
                                                Hilbert Space is a nondenumerable infinite complex vector space. Complex
                                                space, being a collection of complex numbers $\mathbb{C}$ with an added
                                                structure. The infinite dimensions of Hilbert Space represents a
                                                continious spectra of alternative physical states. Alternative physical
                                                states, for example, being the position (coordinates) or momentum of a
                                                particle. In quantum mechanics, we represent a physical state as a state
                                                vector. This state vector, again, is a part of our complex vector space.
                                                Following Dirac, this state vector is called a ket. A ket is represented
                                                by $|\alpha\rangle$. It is postulated that a ket contains complete
                                                information about the physical state. Meaning, in theory, a ket contains
                                                everything we are allowed to know about the state state. We say "allowed
                                                to know", because of the limiting information we can know about a state
                                                due to the uncertaintiy principal. To add two kets, we can do:
                                                $|\alpha\rangle + |\beta\rangle = |\gamma\rangle $ the sum of two kets
                                                is $|\gamma\rangle$, or just another ket. To multiply a ket
                                                $|\alpha\rangle$ by a complex number $c$, we can do: $c|\alpha\rangle =
                                                |\alpha\rangle$ where the product is another ket. Note, the complex
                                                number $c$ can be to the left or right of the ket, it does not matter.
                                                If $c$ is equal to zero, than the resulting ket is called a null ket.
                                                Next, we will talk about operators. Observables, like momentum and spin,
                                                cab be represented by operators. An operator could be denoted as $A$ for
                                                the vector space. Operators act on kets from left to right: $A\cdot
                                                (|\alpha\rangle) = A|\alpha\rangle$ Our result is another ket. Most
                                                often, $A$ is not a constant times $|\alpha\rangle$. Eigenkets of
                                                operator $A$ are specific kets of importance denoted by:
                                                $|\alpha'\rangle,|\alpha'''\rangle,|\alpha'''\rangle,...$ The property
                                                can be written as:
                                                $A|a'\rangle=a'|a'\rangle,\;\;A|a''\rangle=a''|a''\rangle$ Values of
                                                $a',a'',...$ are numbers. So what we are doing is taking our operator
                                                and multiplying it by the eigenket. The set of numbers that make up the
                                                eigenket $\left\{a',a'',a''',...\right\}$ is called the set of
                                                eigenvalues of operator $A$. The set of eigenvalues can be more
                                                compactly denoted by $\left\{a'\right\}$. An eigenstate is a physical
                                                state corresponding to an eigenket. To represent this idea, we will use
                                                a $\frac{1}{2}$ systems. The relationship between eigenvalue to eigenket
                                                can be expressed as:
                                                $S_{z}|S_{z};+\rangle=\frac{\hbar}{2}|S_{z};+\rangle$,
                                                $S_{z}|S_{z};-\rangle=-\frac{\hbar}{2}|S_{z};-\rangle$ We can futher
                                                compact this to:
                                                $S_{z}|S_{z};\pm\rangle=\pm\frac{\hbar}{2}|S_{z};\pm\rangle$ or
                                                $S_{z}|\pm S_{z}\rangle=\pm\frac{\hbar}{2}|\pm\frac{\hbar}{2}\rangle$
                                                Where the number of dimensions of the vector space is determined by the
                                                number of alternatives in the state of a system. Usually, when it comes
                                                to an observable $A$ we are talking about an $n$-dimensional vector
                                                space spanned by $n$ eigenkets. An arbirtary ket $|\alpha\rangle$ can be
                                                defined as: $|\alpha\rangle=\sum_{a'}^{a^{n}}c_{a'}|a'\rangle$ where
                                                $c_{a'}$ is a complex coefficent. Bra Space and Inner Products Bra space
                                                is a vector space that is "dual to" the respective ket space. Meaning,
                                                it is thought that for every ket $|a\rangle$ there exists a bra
                                                $\langle\alpha|$ in the dual space or bra space. To continue. Base Kets
                                                and Matrix Representations Eigenkets as Base Kets Given some ket
                                                $|\alpha\rangle$ in the ket space spanned by eigenkets of $A$, we will
                                                expand it to: $|\alpha\rangle=\sum_{\alpha}c_{a'}|a'\rangle$ To find the
                                                expansion of of the coefficent we multiply $\langle a''|$ on the left
                                                while using the orthonormality principal: $c_{a'}=\langle
                                                a|\alpha\rangle$ Subsituting the exapansion of the coefficent into the
                                                some ket $|\alpha\rangle$ we get:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|\alpha\rangle$ Now,
                                                using the associative axiom of multiplication we can say that for some
                                                ket $|\alpha\rangle$ we must have an identity operator:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|=1$ This equation is
                                                known as the completeness realtion or closure. Using this identity
                                                operator between $\langle\alpha|$ and $|\alpha\rangle$ we get:
                                                $\langle\alpha|\alpha\rangle=\langle\alpha|\cdot
                                                \left(\sum_{a'}|a'\rangle\langle a'|\right) \cdot|\alpha\rangle$
                                                $=\sum_{a'}|\langle a'|\alpha\rangle|^2$ If $\langle\alpha|$ is
                                                normalized then: $\sum_{a'}|\langle
                                                a'|\alpha\rangle|^2=\sum_{a'}|c_{a'}|^2=1$ We can also define another
                                                type of operator; the projection operator denoted by $\Lambda_{a'}$
                                                defined as: $\Lambda_{a'}\equiv|a'\rangle\langle a'|$ The completeness
                                                relation can now be also be defined as: $\sum_{a'}\Lambda_{a'}=1$ Matrix
                                                Representations To continue. Quantum Dynamics To continue. Quantum
                                                Mechanics - Context Switching Quantum Mechanics - Context
                                                SwitchingQuantum Mechanics Preliminaries Kets, Bras, and Operators Ket
                                                Space and Operators First we will introduce the idea of Hilbert Space,
                                                which was named after D. Hilbert. Hilbert Space is a nondenumerable
                                                infinite complex vector space. Complex space, being a collection of
                                                complex numbers $\mathbb{C}$ with an added structure. The infinite
                                                dimensions of Hilbert Space represents a continious spectra of
                                                alternative physical states. Alternative physical states, for example,
                                                being the position (coordinates) or momentum of a particle. In quantum
                                                mechanics, we represent a physical state as a state vector. This state
                                                vector, again, is a part of our complex vector space. Following Dirac,
                                                this state vector is called a ket. A ket is represented by
                                                $|\alpha\rangle$. It is postulated that a ket contains complete
                                                information about the physical state. Meaning, in theory, a ket contains
                                                everything we are allowed to know about the state state. We say "allowed
                                                to know", because of the limiting information we can know about a state
                                                due to the uncertaintiy principal. To add two kets, we can do:
                                                $|\alpha\rangle + |\beta\rangle = |\gamma\rangle $ the sum of two kets
                                                is $|\gamma\rangle$, or just another ket. To multiply a ket
                                                $|\alpha\rangle$ by a complex number $c$, we can do: $c|\alpha\rangle =
                                                |\alpha\rangle$ where the product is another ket. Note, the complex
                                                number $c$ can be to the left or right of the ket, it does not matter.
                                                If $c$ is equal to zero, than the resulting ket is called a null ket.
                                                Next, we will talk about operators. Observables, like momentum and spin,
                                                cab be represented by operators. An operator could be denoted as $A$ for
                                                the vector space. Operators act on kets from left to right: $A\cdot
                                                (|\alpha\rangle) = A|\alpha\rangle$ Our result is another ket. Most
                                                often, $A$ is not a constant times $|\alpha\rangle$. Eigenkets of
                                                operator $A$ are specific kets of importance denoted by:
                                                $|\alpha'\rangle,|\alpha'''\rangle,|\alpha'''\rangle,...$ The property
                                                can be written as:
                                                $A|a'\rangle=a'|a'\rangle,\;\;A|a''\rangle=a''|a''\rangle$ Values of
                                                $a',a'',...$ are numbers. So what we are doing is taking our operator
                                                and multiplying it by the eigenket. The set of numbers that make up the
                                                eigenket $\left\{a',a'',a''',...\right\}$ is called the set of
                                                eigenvalues of operator $A$. The set of eigenvalues can be more
                                                compactly denoted by $\left\{a'\right\}$. An eigenstate is a physical
                                                state corresponding to an eigenket. To represent this idea, we will use
                                                a $\frac{1}{2}$ systems. The relationship between eigenvalue to eigenket
                                                can be expressed as:
                                                $S_{z}|S_{z};+\rangle=\frac{\hbar}{2}|S_{z};+\rangle$,
                                                $S_{z}|S_{z};-\rangle=-\frac{\hbar}{2}|S_{z};-\rangle$ We can futher
                                                compact this to:
                                                $S_{z}|S_{z};\pm\rangle=\pm\frac{\hbar}{2}|S_{z};\pm\rangle$ or
                                                $S_{z}|\pm S_{z}\rangle=\pm\frac{\hbar}{2}|\pm\frac{\hbar}{2}\rangle$
                                                Where the number of dimensions of the vector space is determined by the
                                                number of alternatives in the state of a system. Usually, when it comes
                                                to an observable $A$ we are talking about an $n$-dimensional vector
                                                space spanned by $n$ eigenkets. An arbirtary ket $|\alpha\rangle$ can be
                                                defined as: $|\alpha\rangle=\sum_{a'}^{a^{n}}c_{a'}|a'\rangle$ where
                                                $c_{a'}$ is a complex coefficent. Bra Space and Inner Products Bra space
                                                is a vector space that is "dual to" the respective ket space. Meaning,
                                                it is thought that for every ket $|a\rangle$ there exists a bra
                                                $\langle\alpha|$ in the dual space or bra space. To continue. Base Kets
                                                and Matrix Representations Eigenkets as Base Kets Given some ket
                                                $|\alpha\rangle$ in the ket space spanned by eigenkets of $A$, we will
                                                expand it to: $|\alpha\rangle=\sum_{\alpha}c_{a'}|a'\rangle$ To find the
                                                expansion of of the coefficent we multiply $\langle a''|$ on the left
                                                while using the orthonormality principal: $c_{a'}=\langle
                                                a|\alpha\rangle$ Subsituting the exapansion of the coefficent into the
                                                some ket $|\alpha\rangle$ we get:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|\alpha\rangle$ Now,
                                                using the associative axiom of multiplication we can say that for some
                                                ket $|\alpha\rangle$ we must have an identity operator:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|=1$ This equation is
                                                known as the completeness realtion or closure. Using this identity
                                                operator between $\langle\alpha|$ and $|\alpha\rangle$ we get:
                                                $\langle\alpha|\alpha\rangle=\langle\alpha|\cdot
                                                \left(\sum_{a'}|a'\rangle\langle a'|\right) \cdot|\alpha\rangle$
                                                $=\sum_{a'}|\langle a'|\alpha\rangle|^2$ If $\langle\alpha|$ is
                                                normalized then: $\sum_{a'}|\langle
                                                a'|\alpha\rangle|^2=\sum_{a'}|c_{a'}|^2=1$ We can also define another
                                                type of operator; the projection operator denoted by $\Lambda_{a'}$
                                                defined as: $\Lambda_{a'}\equiv|a'\rangle\langle a'|$ The completeness
                                                relation can now be also be defined as: $\sum_{a'}\Lambda_{a'}=1$ Matrix
                                                Representations To continue. Quantum Dynamics To continue. Quantum
                                                Mechanics Preliminaries Kets, Bras, and Operators Ket Space and
                                                Operators First we will introduce the idea of Hilbert Space, which was
                                                named after D. Hilbert. Hilbert Space is a nondenumerable infinite
                                                complex vector space. Complex space, being a collection of complex
                                                numbers $\mathbb{C}$ with an added structure. The infinite dimensions of
                                                Hilbert Space represents a continious spectra of alternative physical
                                                states. Alternative physical states, for example, being the position
                                                (coordinates) or momentum of a particle. In quantum mechanics, we
                                                represent a physical state as a state vector. This state vector, again,
                                                is a part of our complex vector space. Following Dirac, this state
                                                vector is called a ket. A ket is represented by $|\alpha\rangle$. It is
                                                postulated that a ket contains complete information about the physical
                                                state. Meaning, in theory, a ket contains everything we are allowed to
                                                know about the state state. We say "allowed to know", because of the
                                                limiting information we can know about a state due to the uncertaintiy
                                                principal. To add two kets, we can do: $|\alpha\rangle + |\beta\rangle =
                                                |\gamma\rangle $ the sum of two kets is $|\gamma\rangle$, or just
                                                another ket. To multiply a ket $|\alpha\rangle$ by a complex number $c$,
                                                we can do: $c|\alpha\rangle = |\alpha\rangle$ where the product is
                                                another ket. Note, the complex number $c$ can be to the left or right of
                                                the ket, it does not matter. If $c$ is equal to zero, than the resulting
                                                ket is called a null ket. Next, we will talk about operators.
                                                Observables, like momentum and spin, cab be represented by operators. An
                                                operator could be denoted as $A$ for the vector space. Operators act on
                                                kets from left to right: $A\cdot (|\alpha\rangle) = A|\alpha\rangle$ Our
                                                result is another ket. Most often, $A$ is not a constant times
                                                $|\alpha\rangle$. Eigenkets of operator $A$ are specific kets of
                                                importance denoted by:
                                                $|\alpha'\rangle,|\alpha'''\rangle,|\alpha'''\rangle,...$ The property
                                                can be written as:
                                                $A|a'\rangle=a'|a'\rangle,\;\;A|a''\rangle=a''|a''\rangle$ Values of
                                                $a',a'',...$ are numbers. So what we are doing is taking our operator
                                                and multiplying it by the eigenket. The set of numbers that make up the
                                                eigenket $\left\{a',a'',a''',...\right\}$ is called the set of
                                                eigenvalues of operator $A$. The set of eigenvalues can be more
                                                compactly denoted by $\left\{a'\right\}$. An eigenstate is a physical
                                                state corresponding to an eigenket. To represent this idea, we will use
                                                a $\frac{1}{2}$ systems. The relationship between eigenvalue to eigenket
                                                can be expressed as:
                                                $S_{z}|S_{z};+\rangle=\frac{\hbar}{2}|S_{z};+\rangle$,
                                                $S_{z}|S_{z};-\rangle=-\frac{\hbar}{2}|S_{z};-\rangle$ We can futher
                                                compact this to:
                                                $S_{z}|S_{z};\pm\rangle=\pm\frac{\hbar}{2}|S_{z};\pm\rangle$ or
                                                $S_{z}|\pm S_{z}\rangle=\pm\frac{\hbar}{2}|\pm\frac{\hbar}{2}\rangle$
                                                Where the number of dimensions of the vector space is determined by the
                                                number of alternatives in the state of a system. Usually, when it comes
                                                to an observable $A$ we are talking about an $n$-dimensional vector
                                                space spanned by $n$ eigenkets. An arbirtary ket $|\alpha\rangle$ can be
                                                defined as: $|\alpha\rangle=\sum_{a'}^{a^{n}}c_{a'}|a'\rangle$ where
                                                $c_{a'}$ is a complex coefficent. Bra Space and Inner Products Bra space
                                                is a vector space that is "dual to" the respective ket space. Meaning,
                                                it is thought that for every ket $|a\rangle$ there exists a bra
                                                $\langle\alpha|$ in the dual space or bra space. To continue. Base Kets
                                                and Matrix Representations Eigenkets as Base Kets Given some ket
                                                $|\alpha\rangle$ in the ket space spanned by eigenkets of $A$, we will
                                                expand it to: $|\alpha\rangle=\sum_{\alpha}c_{a'}|a'\rangle$ To find the
                                                expansion of of the coefficent we multiply $\langle a''|$ on the left
                                                while using the orthonormality principal: $c_{a'}=\langle
                                                a|\alpha\rangle$ Subsituting the exapansion of the coefficent into the
                                                some ket $|\alpha\rangle$ we get:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|\alpha\rangle$ Now,
                                                using the associative axiom of multiplication we can say that for some
                                                ket $|\alpha\rangle$ we must have an identity operator:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|=1$ This equation is
                                                known as the completeness realtion or closure. Using this identity
                                                operator between $\langle\alpha|$ and $|\alpha\rangle$ we get:
                                                $\langle\alpha|\alpha\rangle=\langle\alpha|\cdot
                                                \left(\sum_{a'}|a'\rangle\langle a'|\right) \cdot|\alpha\rangle$
                                                $=\sum_{a'}|\langle a'|\alpha\rangle|^2$ If $\langle\alpha|$ is
                                                normalized then: $\sum_{a'}|\langle
                                                a'|\alpha\rangle|^2=\sum_{a'}|c_{a'}|^2=1$ We can also define another
                                                type of operator; the projection operator denoted by $\Lambda_{a'}$
                                                defined as: $\Lambda_{a'}\equiv|a'\rangle\langle a'|$ The completeness
                                                relation can now be also be defined as: $\sum_{a'}\Lambda_{a'}=1$ Matrix
                                                Representations To continue. Quantum Dynamics To continue. Quantum
                                                Mechanics Preliminaries Kets, Bras, and Operators Ket Space and
                                                Operators First we will introduce the idea of Hilbert Space, which was
                                                named after D. Hilbert. Hilbert Space is a nondenumerable infinite
                                                complex vector space. Complex space, being a collection of complex
                                                numbers $\mathbb{C}$ with an added structure. The infinite dimensions of
                                                Hilbert Space represents a continious spectra of alternative physical
                                                states. Alternative physical states, for example, being the position
                                                (coordinates) or momentum of a particle. In quantum mechanics, we
                                                represent a physical state as a state vector. This state vector, again,
                                                is a part of our complex vector space. Following Dirac, this state
                                                vector is called a ket. A ket is represented by $|\alpha\rangle$. It is
                                                postulated that a ket contains complete information about the physical
                                                state. Meaning, in theory, a ket contains everything we are allowed to
                                                know about the state state. We say "allowed to know", because of the
                                                limiting information we can know about a state due to the uncertaintiy
                                                principal. To add two kets, we can do: $|\alpha\rangle + |\beta\rangle =
                                                |\gamma\rangle $ the sum of two kets is $|\gamma\rangle$, or just
                                                another ket. To multiply a ket $|\alpha\rangle$ by a complex number $c$,
                                                we can do: $c|\alpha\rangle = |\alpha\rangle$ where the product is
                                                another ket. Note, the complex number $c$ can be to the left or right of
                                                the ket, it does not matter. If $c$ is equal to zero, than the resulting
                                                ket is called a null ket. Next, we will talk about operators.
                                                Observables, like momentum and spin, cab be represented by operators. An
                                                operator could be denoted as $A$ for the vector space. Operators act on
                                                kets from left to right: $A\cdot (|\alpha\rangle) = A|\alpha\rangle$ Our
                                                result is another ket. Most often, $A$ is not a constant times
                                                $|\alpha\rangle$. Eigenkets of operator $A$ are specific kets of
                                                importance denoted by:
                                                $|\alpha'\rangle,|\alpha'''\rangle,|\alpha'''\rangle,...$ The property
                                                can be written as:
                                                $A|a'\rangle=a'|a'\rangle,\;\;A|a''\rangle=a''|a''\rangle$ Values of
                                                $a',a'',...$ are numbers. So what we are doing is taking our operator
                                                and multiplying it by the eigenket. The set of numbers that make up the
                                                eigenket $\left\{a',a'',a''',...\right\}$ is called the set of
                                                eigenvalues of operator $A$. The set of eigenvalues can be more
                                                compactly denoted by $\left\{a'\right\}$. An eigenstate is a physical
                                                state corresponding to an eigenket. To represent this idea, we will use
                                                a $\frac{1}{2}$ systems. The relationship between eigenvalue to eigenket
                                                can be expressed as:
                                                $S_{z}|S_{z};+\rangle=\frac{\hbar}{2}|S_{z};+\rangle$,
                                                $S_{z}|S_{z};-\rangle=-\frac{\hbar}{2}|S_{z};-\rangle$ We can futher
                                                compact this to:
                                                $S_{z}|S_{z};\pm\rangle=\pm\frac{\hbar}{2}|S_{z};\pm\rangle$ or
                                                $S_{z}|\pm S_{z}\rangle=\pm\frac{\hbar}{2}|\pm\frac{\hbar}{2}\rangle$
                                                Where the number of dimensions of the vector space is determined by the
                                                number of alternatives in the state of a system. Usually, when it comes
                                                to an observable $A$ we are talking about an $n$-dimensional vector
                                                space spanned by $n$ eigenkets. An arbirtary ket $|\alpha\rangle$ can be
                                                defined as: $|\alpha\rangle=\sum_{a'}^{a^{n}}c_{a'}|a'\rangle$ where
                                                $c_{a'}$ is a complex coefficent. Bra Space and Inner Products Bra space
                                                is a vector space that is "dual to" the respective ket space. Meaning,
                                                it is thought that for every ket $|a\rangle$ there exists a bra
                                                $\langle\alpha|$ in the dual space or bra space. To continue. Base Kets
                                                and Matrix Representations Eigenkets as Base Kets Given some ket
                                                $|\alpha\rangle$ in the ket space spanned by eigenkets of $A$, we will
                                                expand it to: $|\alpha\rangle=\sum_{\alpha}c_{a'}|a'\rangle$ To find the
                                                expansion of of the coefficent we multiply $\langle a''|$ on the left
                                                while using the orthonormality principal: $c_{a'}=\langle
                                                a|\alpha\rangle$ Subsituting the exapansion of the coefficent into the
                                                some ket $|\alpha\rangle$ we get:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|\alpha\rangle$ Now,
                                                using the associative axiom of multiplication we can say that for some
                                                ket $|\alpha\rangle$ we must have an identity operator:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|=1$ This equation is
                                                known as the completeness realtion or closure. Using this identity
                                                operator between $\langle\alpha|$ and $|\alpha\rangle$ we get:
                                                $\langle\alpha|\alpha\rangle=\langle\alpha|\cdot
                                                \left(\sum_{a'}|a'\rangle\langle a'|\right) \cdot|\alpha\rangle$
                                                $=\sum_{a'}|\langle a'|\alpha\rangle|^2$ If $\langle\alpha|$ is
                                                normalized then: $\sum_{a'}|\langle
                                                a'|\alpha\rangle|^2=\sum_{a'}|c_{a'}|^2=1$ We can also define another
                                                type of operator; the projection operator denoted by $\Lambda_{a'}$
                                                defined as: $\Lambda_{a'}\equiv|a'\rangle\langle a'|$ The completeness
                                                relation can now be also be defined as: $\sum_{a'}\Lambda_{a'}=1$ Matrix
                                                Representations To continue. Quantum Dynamics To continue. Quantum
                                                MechanicsPreliminaries Kets, Bras, and Operators Ket Space and Operators
                                                First we will introduce the idea of Hilbert Space, which was named after
                                                D. Hilbert. Hilbert Space is a nondenumerable infinite complex vector
                                                space. Complex space, being a collection of complex numbers $\mathbb{C}$
                                                with an added structure. The infinite dimensions of Hilbert Space
                                                represents a continious spectra of alternative physical states.
                                                Alternative physical states, for example, being the position
                                                (coordinates) or momentum of a particle. In quantum mechanics, we
                                                represent a physical state as a state vector. This state vector, again,
                                                is a part of our complex vector space. Following Dirac, this state
                                                vector is called a ket. A ket is represented by $|\alpha\rangle$. It is
                                                postulated that a ket contains complete information about the physical
                                                state. Meaning, in theory, a ket contains everything we are allowed to
                                                know about the state state. We say "allowed to know", because of the
                                                limiting information we can know about a state due to the uncertaintiy
                                                principal. To add two kets, we can do: $|\alpha\rangle + |\beta\rangle =
                                                |\gamma\rangle $ the sum of two kets is $|\gamma\rangle$, or just
                                                another ket. To multiply a ket $|\alpha\rangle$ by a complex number $c$,
                                                we can do: $c|\alpha\rangle = |\alpha\rangle$ where the product is
                                                another ket. Note, the complex number $c$ can be to the left or right of
                                                the ket, it does not matter. If $c$ is equal to zero, than the resulting
                                                ket is called a null ket. Next, we will talk about operators.
                                                Observables, like momentum and spin, cab be represented by operators. An
                                                operator could be denoted as $A$ for the vector space. Operators act on
                                                kets from left to right: $A\cdot (|\alpha\rangle) = A|\alpha\rangle$ Our
                                                result is another ket. Most often, $A$ is not a constant times
                                                $|\alpha\rangle$. Eigenkets of operator $A$ are specific kets of
                                                importance denoted by:
                                                $|\alpha'\rangle,|\alpha'''\rangle,|\alpha'''\rangle,...$ The property
                                                can be written as:
                                                $A|a'\rangle=a'|a'\rangle,\;\;A|a''\rangle=a''|a''\rangle$ Values of
                                                $a',a'',...$ are numbers. So what we are doing is taking our operator
                                                and multiplying it by the eigenket. The set of numbers that make up the
                                                eigenket $\left\{a',a'',a''',...\right\}$ is called the set of
                                                eigenvalues of operator $A$. The set of eigenvalues can be more
                                                compactly denoted by $\left\{a'\right\}$. An eigenstate is a physical
                                                state corresponding to an eigenket. To represent this idea, we will use
                                                a $\frac{1}{2}$ systems. The relationship between eigenvalue to eigenket
                                                can be expressed as:
                                                $S_{z}|S_{z};+\rangle=\frac{\hbar}{2}|S_{z};+\rangle$,
                                                $S_{z}|S_{z};-\rangle=-\frac{\hbar}{2}|S_{z};-\rangle$ We can futher
                                                compact this to:
                                                $S_{z}|S_{z};\pm\rangle=\pm\frac{\hbar}{2}|S_{z};\pm\rangle$ or
                                                $S_{z}|\pm S_{z}\rangle=\pm\frac{\hbar}{2}|\pm\frac{\hbar}{2}\rangle$
                                                Where the number of dimensions of the vector space is determined by the
                                                number of alternatives in the state of a system. Usually, when it comes
                                                to an observable $A$ we are talking about an $n$-dimensional vector
                                                space spanned by $n$ eigenkets. An arbirtary ket $|\alpha\rangle$ can be
                                                defined as: $|\alpha\rangle=\sum_{a'}^{a^{n}}c_{a'}|a'\rangle$ where
                                                $c_{a'}$ is a complex coefficent. Bra Space and Inner Products Bra space
                                                is a vector space that is "dual to" the respective ket space. Meaning,
                                                it is thought that for every ket $|a\rangle$ there exists a bra
                                                $\langle\alpha|$ in the dual space or bra space. To continue. Base Kets
                                                and Matrix Representations Eigenkets as Base Kets Given some ket
                                                $|\alpha\rangle$ in the ket space spanned by eigenkets of $A$, we will
                                                expand it to: $|\alpha\rangle=\sum_{\alpha}c_{a'}|a'\rangle$ To find the
                                                expansion of of the coefficent we multiply $\langle a''|$ on the left
                                                while using the orthonormality principal: $c_{a'}=\langle
                                                a|\alpha\rangle$ Subsituting the exapansion of the coefficent into the
                                                some ket $|\alpha\rangle$ we get:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|\alpha\rangle$ Now,
                                                using the associative axiom of multiplication we can say that for some
                                                ket $|\alpha\rangle$ we must have an identity operator:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|=1$ This equation is
                                                known as the completeness realtion or closure. Using this identity
                                                operator between $\langle\alpha|$ and $|\alpha\rangle$ we get:
                                                $\langle\alpha|\alpha\rangle=\langle\alpha|\cdot
                                                \left(\sum_{a'}|a'\rangle\langle a'|\right) \cdot|\alpha\rangle$
                                                $=\sum_{a'}|\langle a'|\alpha\rangle|^2$ If $\langle\alpha|$ is
                                                normalized then: $\sum_{a'}|\langle
                                                a'|\alpha\rangle|^2=\sum_{a'}|c_{a'}|^2=1$ We can also define another
                                                type of operator; the projection operator denoted by $\Lambda_{a'}$
                                                defined as: $\Lambda_{a'}\equiv|a'\rangle\langle a'|$ The completeness
                                                relation can now be also be defined as: $\sum_{a'}\Lambda_{a'}=1$ Matrix
                                                Representations To continue. PreliminariesKets, Bras, and Operators Ket
                                                Space and Operators First we will introduce the idea of Hilbert Space,
                                                which was named after D. Hilbert. Hilbert Space is a nondenumerable
                                                infinite complex vector space. Complex space, being a collection of
                                                complex numbers $\mathbb{C}$ with an added structure. The infinite
                                                dimensions of Hilbert Space represents a continious spectra of
                                                alternative physical states. Alternative physical states, for example,
                                                being the position (coordinates) or momentum of a particle. In quantum
                                                mechanics, we represent a physical state as a state vector. This state
                                                vector, again, is a part of our complex vector space. Following Dirac,
                                                this state vector is called a ket. A ket is represented by
                                                $|\alpha\rangle$. It is postulated that a ket contains complete
                                                information about the physical state. Meaning, in theory, a ket contains
                                                everything we are allowed to know about the state state. We say "allowed
                                                to know", because of the limiting information we can know about a state
                                                due to the uncertaintiy principal. To add two kets, we can do:
                                                $|\alpha\rangle + |\beta\rangle = |\gamma\rangle $ the sum of two kets
                                                is $|\gamma\rangle$, or just another ket. To multiply a ket
                                                $|\alpha\rangle$ by a complex number $c$, we can do: $c|\alpha\rangle =
                                                |\alpha\rangle$ where the product is another ket. Note, the complex
                                                number $c$ can be to the left or right of the ket, it does not matter.
                                                If $c$ is equal to zero, than the resulting ket is called a null ket.
                                                Next, we will talk about operators. Observables, like momentum and spin,
                                                cab be represented by operators. An operator could be denoted as $A$ for
                                                the vector space. Operators act on kets from left to right: $A\cdot
                                                (|\alpha\rangle) = A|\alpha\rangle$ Our result is another ket. Most
                                                often, $A$ is not a constant times $|\alpha\rangle$. Eigenkets of
                                                operator $A$ are specific kets of importance denoted by:
                                                $|\alpha'\rangle,|\alpha'''\rangle,|\alpha'''\rangle,...$ The property
                                                can be written as:
                                                $A|a'\rangle=a'|a'\rangle,\;\;A|a''\rangle=a''|a''\rangle$ Values of
                                                $a',a'',...$ are numbers. So what we are doing is taking our operator
                                                and multiplying it by the eigenket. The set of numbers that make up the
                                                eigenket $\left\{a',a'',a''',...\right\}$ is called the set of
                                                eigenvalues of operator $A$. The set of eigenvalues can be more
                                                compactly denoted by $\left\{a'\right\}$. An eigenstate is a physical
                                                state corresponding to an eigenket. To represent this idea, we will use
                                                a $\frac{1}{2}$ systems. The relationship between eigenvalue to eigenket
                                                can be expressed as:
                                                $S_{z}|S_{z};+\rangle=\frac{\hbar}{2}|S_{z};+\rangle$,
                                                $S_{z}|S_{z};-\rangle=-\frac{\hbar}{2}|S_{z};-\rangle$ We can futher
                                                compact this to:
                                                $S_{z}|S_{z};\pm\rangle=\pm\frac{\hbar}{2}|S_{z};\pm\rangle$ or
                                                $S_{z}|\pm S_{z}\rangle=\pm\frac{\hbar}{2}|\pm\frac{\hbar}{2}\rangle$
                                                Where the number of dimensions of the vector space is determined by the
                                                number of alternatives in the state of a system. Usually, when it comes
                                                to an observable $A$ we are talking about an $n$-dimensional vector
                                                space spanned by $n$ eigenkets. An arbirtary ket $|\alpha\rangle$ can be
                                                defined as: $|\alpha\rangle=\sum_{a'}^{a^{n}}c_{a'}|a'\rangle$ where
                                                $c_{a'}$ is a complex coefficent. Bra Space and Inner Products Bra space
                                                is a vector space that is "dual to" the respective ket space. Meaning,
                                                it is thought that for every ket $|a\rangle$ there exists a bra
                                                $\langle\alpha|$ in the dual space or bra space. To continue. Kets,
                                                Bras, and OperatorsKet Space and Operators First we will introduce the
                                                idea of Hilbert Space, which was named after D. Hilbert. Hilbert Space
                                                is a nondenumerable infinite complex vector space. Complex space, being
                                                a collection of complex numbers $\mathbb{C}$ with an added structure.
                                                The infinite dimensions of Hilbert Space represents a continious spectra
                                                of alternative physical states. Alternative physical states, for
                                                example, being the position (coordinates) or momentum of a particle. In
                                                quantum mechanics, we represent a physical state as a state vector. This
                                                state vector, again, is a part of our complex vector space. Following
                                                Dirac, this state vector is called a ket. A ket is represented by
                                                $|\alpha\rangle$. It is postulated that a ket contains complete
                                                information about the physical state. Meaning, in theory, a ket contains
                                                everything we are allowed to know about the state state. We say "allowed
                                                to know", because of the limiting information we can know about a state
                                                due to the uncertaintiy principal. To add two kets, we can do:
                                                $|\alpha\rangle + |\beta\rangle = |\gamma\rangle $ the sum of two kets
                                                is $|\gamma\rangle$, or just another ket. To multiply a ket
                                                $|\alpha\rangle$ by a complex number $c$, we can do: $c|\alpha\rangle =
                                                |\alpha\rangle$ where the product is another ket. Note, the complex
                                                number $c$ can be to the left or right of the ket, it does not matter.
                                                If $c$ is equal to zero, than the resulting ket is called a null ket.
                                                Next, we will talk about operators. Observables, like momentum and spin,
                                                cab be represented by operators. An operator could be denoted as $A$ for
                                                the vector space. Operators act on kets from left to right: $A\cdot
                                                (|\alpha\rangle) = A|\alpha\rangle$ Our result is another ket. Most
                                                often, $A$ is not a constant times $|\alpha\rangle$. Eigenkets of
                                                operator $A$ are specific kets of importance denoted by:
                                                $|\alpha'\rangle,|\alpha'''\rangle,|\alpha'''\rangle,...$ The property
                                                can be written as:
                                                $A|a'\rangle=a'|a'\rangle,\;\;A|a''\rangle=a''|a''\rangle$ Values of
                                                $a',a'',...$ are numbers. So what we are doing is taking our operator
                                                and multiplying it by the eigenket. The set of numbers that make up the
                                                eigenket $\left\{a',a'',a''',...\right\}$ is called the set of
                                                eigenvalues of operator $A$. The set of eigenvalues can be more
                                                compactly denoted by $\left\{a'\right\}$. An eigenstate is a physical
                                                state corresponding to an eigenket. To represent this idea, we will use
                                                a $\frac{1}{2}$ systems. The relationship between eigenvalue to eigenket
                                                can be expressed as:
                                                $S_{z}|S_{z};+\rangle=\frac{\hbar}{2}|S_{z};+\rangle$,
                                                $S_{z}|S_{z};-\rangle=-\frac{\hbar}{2}|S_{z};-\rangle$ We can futher
                                                compact this to:
                                                $S_{z}|S_{z};\pm\rangle=\pm\frac{\hbar}{2}|S_{z};\pm\rangle$ or
                                                $S_{z}|\pm S_{z}\rangle=\pm\frac{\hbar}{2}|\pm\frac{\hbar}{2}\rangle$
                                                Where the number of dimensions of the vector space is determined by the
                                                number of alternatives in the state of a system. Usually, when it comes
                                                to an observable $A$ we are talking about an $n$-dimensional vector
                                                space spanned by $n$ eigenkets. An arbirtary ket $|\alpha\rangle$ can be
                                                defined as: $|\alpha\rangle=\sum_{a'}^{a^{n}}c_{a'}|a'\rangle$ where
                                                $c_{a'}$ is a complex coefficent. Ket Space and OperatorsFirst we will
                                                introduce the idea of Hilbert Space, which was named after D. Hilbert.
                                                Hilbert Space is a nondenumerable infinite complex vector space. Complex
                                                space, being a collection of complex numbers $\mathbb{C}$ with an added
                                                structure. The infinite dimensions of Hilbert Space represents a
                                                continious spectra of alternative physical states. Alternative physical
                                                states, for example, being the position (coordinates) or momentum of a
                                                particle. In quantum mechanics, we represent a physical state as a state
                                                vector. This state vector, again, is a part of our complex vector space.
                                                Following Dirac, this state vector is called a ket. A ket is represented
                                                by $|\alpha\rangle$. It is postulated that a ket contains complete
                                                information about the physical state. Meaning, in theory, a ket contains
                                                everything we are allowed to know about the state state. We say "allowed
                                                to know", because of the limiting information we can know about a state
                                                due to the uncertaintiy principal. To add two kets, we can do:
                                                $|\alpha\rangle + |\beta\rangle = |\gamma\rangle $ the sum of two kets
                                                is $|\gamma\rangle$, or just another ket. To multiply a ket
                                                $|\alpha\rangle$ by a complex number $c$, we can do: $c|\alpha\rangle =
                                                |\alpha\rangle$ where the product is another ket. Note, the complex
                                                number $c$ can be to the left or right of the ket, it does not matter.
                                                If $c$ is equal to zero, than the resulting ket is called a null ket.
                                                Next, we will talk about operators. Observables, like momentum and spin,
                                                cab be represented by operators. An operator could be denoted as $A$ for
                                                the vector space. Operators act on kets from left to right: $A\cdot
                                                (|\alpha\rangle) = A|\alpha\rangle$ Our result is another ket. Most
                                                often, $A$ is not a constant times $|\alpha\rangle$. Eigenkets of
                                                operator $A$ are specific kets of importance denoted by:
                                                $|\alpha'\rangle,|\alpha'''\rangle,|\alpha'''\rangle,...$ The property
                                                can be written as:
                                                $A|a'\rangle=a'|a'\rangle,\;\;A|a''\rangle=a''|a''\rangle$ Values of
                                                $a',a'',...$ are numbers. So what we are doing is taking our operator
                                                and multiplying it by the eigenket. The set of numbers that make up the
                                                eigenket $\left\{a',a'',a''',...\right\}$ is called the set of
                                                eigenvalues of operator $A$. The set of eigenvalues can be more
                                                compactly denoted by $\left\{a'\right\}$. An eigenstate is a physical
                                                state corresponding to an eigenket. To represent this idea, we will use
                                                a $\frac{1}{2}$ systems. The relationship between eigenvalue to eigenket
                                                can be expressed as:
                                                $S_{z}|S_{z};+\rangle=\frac{\hbar}{2}|S_{z};+\rangle$,
                                                $S_{z}|S_{z};-\rangle=-\frac{\hbar}{2}|S_{z};-\rangle$ We can futher
                                                compact this to:
                                                $S_{z}|S_{z};\pm\rangle=\pm\frac{\hbar}{2}|S_{z};\pm\rangle$ or
                                                $S_{z}|\pm S_{z}\rangle=\pm\frac{\hbar}{2}|\pm\frac{\hbar}{2}\rangle$
                                                Where the number of dimensions of the vector space is determined by the
                                                number of alternatives in the state of a system. Usually, when it comes
                                                to an observable $A$ we are talking about an $n$-dimensional vector
                                                space spanned by $n$ eigenkets. An arbirtary ket $|\alpha\rangle$ can be
                                                defined as: $|\alpha\rangle=\sum_{a'}^{a^{n}}c_{a'}|a'\rangle$ where
                                                $c_{a'}$ is a complex coefficent. Bra Space and Inner Products Bra space
                                                is a vector space that is "dual to" the respective ket space. Meaning,
                                                it is thought that for every ket $|a\rangle$ there exists a bra
                                                $\langle\alpha|$ in the dual space or bra space. To continue. Bra Space
                                                and Inner Products Bra space is a vector space that is "dual to" the
                                                respective ket space. Meaning, it is thought that for every ket
                                                $|a\rangle$ there exists a bra $\langle\alpha|$ in the dual space or bra
                                                space. To continue. Base Kets and Matrix Representations Eigenkets as
                                                Base Kets Given some ket $|\alpha\rangle$ in the ket space spanned by
                                                eigenkets of $A$, we will expand it to:
                                                $|\alpha\rangle=\sum_{\alpha}c_{a'}|a'\rangle$ To find the expansion of
                                                of the coefficent we multiply $\langle a''|$ on the left while using the
                                                orthonormality principal: $c_{a'}=\langle a|\alpha\rangle$ Subsituting
                                                the exapansion of the coefficent into the some ket $|\alpha\rangle$ we
                                                get: $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|\alpha\rangle$
                                                Now, using the associative axiom of multiplication we can say that for
                                                some ket $|\alpha\rangle$ we must have an identity operator:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|=1$ This equation is
                                                known as the completeness realtion or closure. Using this identity
                                                operator between $\langle\alpha|$ and $|\alpha\rangle$ we get:
                                                $\langle\alpha|\alpha\rangle=\langle\alpha|\cdot
                                                \left(\sum_{a'}|a'\rangle\langle a'|\right) \cdot|\alpha\rangle$
                                                $=\sum_{a'}|\langle a'|\alpha\rangle|^2$ If $\langle\alpha|$ is
                                                normalized then: $\sum_{a'}|\langle
                                                a'|\alpha\rangle|^2=\sum_{a'}|c_{a'}|^2=1$ We can also define another
                                                type of operator; the projection operator denoted by $\Lambda_{a'}$
                                                defined as: $\Lambda_{a'}\equiv|a'\rangle\langle a'|$ The completeness
                                                relation can now be also be defined as: $\sum_{a'}\Lambda_{a'}=1$ Matrix
                                                Representations To continue. Base Kets and Matrix
                                                RepresentationsEigenkets as Base Kets Given some ket $|\alpha\rangle$ in
                                                the ket space spanned by eigenkets of $A$, we will expand it to:
                                                $|\alpha\rangle=\sum_{\alpha}c_{a'}|a'\rangle$ To find the expansion of
                                                of the coefficent we multiply $\langle a''|$ on the left while using the
                                                orthonormality principal: $c_{a'}=\langle a|\alpha\rangle$ Subsituting
                                                the exapansion of the coefficent into the some ket $|\alpha\rangle$ we
                                                get: $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|\alpha\rangle$
                                                Now, using the associative axiom of multiplication we can say that for
                                                some ket $|\alpha\rangle$ we must have an identity operator:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|=1$ This equation is
                                                known as the completeness realtion or closure. Using this identity
                                                operator between $\langle\alpha|$ and $|\alpha\rangle$ we get:
                                                $\langle\alpha|\alpha\rangle=\langle\alpha|\cdot
                                                \left(\sum_{a'}|a'\rangle\langle a'|\right) \cdot|\alpha\rangle$
                                                $=\sum_{a'}|\langle a'|\alpha\rangle|^2$ If $\langle\alpha|$ is
                                                normalized then: $\sum_{a'}|\langle
                                                a'|\alpha\rangle|^2=\sum_{a'}|c_{a'}|^2=1$ We can also define another
                                                type of operator; the projection operator denoted by $\Lambda_{a'}$
                                                defined as: $\Lambda_{a'}\equiv|a'\rangle\langle a'|$ The completeness
                                                relation can now be also be defined as: $\sum_{a'}\Lambda_{a'}=1$
                                                Eigenkets as Base KetsGiven some ket $|\alpha\rangle$ in the ket space
                                                spanned by eigenkets of $A$, we will expand it to:
                                                $|\alpha\rangle=\sum_{\alpha}c_{a'}|a'\rangle$ To find the expansion of
                                                of the coefficent we multiply $\langle a''|$ on the left while using the
                                                orthonormality principal: $c_{a'}=\langle a|\alpha\rangle$ Subsituting
                                                the exapansion of the coefficent into the some ket $|\alpha\rangle$ we
                                                get: $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|\alpha\rangle$
                                                Now, using the associative axiom of multiplication we can say that for
                                                some ket $|\alpha\rangle$ we must have an identity operator:
                                                $|\alpha\rangle=\sum_{\alpha}|a'\rangle\langle a'|=1$ This equation is
                                                known as the completeness realtion or closure. Using this identity
                                                operator between $\langle\alpha|$ and $|\alpha\rangle$ we get:
                                                $\langle\alpha|\alpha\rangle=\langle\alpha|\cdot
                                                \left(\sum_{a'}|a'\rangle\langle a'|\right) \cdot|\alpha\rangle$
                                                $=\sum_{a'}|\langle a'|\alpha\rangle|^2$ If $\langle\alpha|$ is
                                                normalized then:$\sum_{a'}|\langle
                                                a'|\alpha\rangle|^2=\sum_{a'}|c_{a'}|^2=1$ We can also define another
                                                type of operator; the projection operator denoted by $\Lambda_{a'}$
                                                defined as: The completeness relation can now be also be defined as:
                                                $\sum_{a'}\Lambda_{a'}=1$ The completeness relation can now be also be
                                                defined as: $\sum_{a'}\Lambda_{a'}=1$ Matrix Representations To
                                                continue. Matrix RepresentationsTo continue.Quantum Dynamics To
                                                continue. Quantum DynamicsTo continue.
                                                [https://www.contextswitching.org/math/hiddenmarkovprocesses]
                                                Hidden Markov Processes - Context Switching Hidden Markov Processes
                                                Preliminaries Information Theory Convex and Concave Functions
                                                Convex/Concave Combinations Let us first start with a formal definition
                                                of a 2 vector convex combination. Then we will break down the definition
                                                into parts and analyze the definition. Then we will formally define and
                                                analyze a convex combination with a finite number of vectors in the same
                                                manner. A subset $S \subseteq \mathbb{R}^{n}$ is said to be a convex set
                                                if: $\lambda x + (1 - \lambda)y \in S \;\forall \lambda \in [0,1],
                                                \forall x,y \in S$ Our formal definition here is for a convex
                                                combination with 2 vectors. Vectors $x$ and $y$ are elemenets of the
                                                subset $S$. $S$ is a proper subset of the real numbers to $n$
                                                dimensions. Our variable lambda $\lambda$ has the value betweewn 0 and
                                                1. The result of $\lambda x + (1 - \lambda)y$ should be an element in
                                                $S$. Now let us formally define a convex combination with any finite
                                                number of vectors. Let us say that $S \subseteq \mathbb{R}^{n}$ and
                                                $x_{1},...,x_{k}\in S$. Then a vector of the form:
                                                $y=\sum_{i=1}^{k}\lambda_{i}x_{i} \geq 0 \; \forall i, \sum_{i=1}^{k}
                                                \lambda_{i}=1$ is called a convex combination of the vectors
                                                $x_{1},...,x_{k}$. Here the summation of every $\lambda_{i}$ is equal to
                                                1 and every $\lambda_{i}$ is greater is greater than or equal to 0.
                                                Entropy Definition of Entropy Let us first formally define entropy. Say
                                                $\mu \in S_{n}$ is an $n$-dimensional probability distribution. The
                                                entropy of the distribution can be denoted as $H(\mu)$ and is defined
                                                by: $H(\mu) := \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$ $\quad\quad\;\;\;=
                                                -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a probability
                                                distribution that is an element from the set $S_{n}$. The set $S_{n}$ is
                                                a set of $n$ probability distributions. The entropy of the probability
                                                distribution $\mu$ can be seen as a the function $H$ on $\mu$. We take
                                                the summation of every probability in $\mu$ at $i$ multiplied by
                                                $log(1/\mu_{i})$. The final result of this summation over every
                                                probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Properties of the Entropy Function To
                                                continue. Hidden Markov Processes - Context Switching Hidden Markov
                                                Processes - Context SwitchingHidden Markov Processes Preliminaries
                                                Information Theory Convex and Concave Functions Convex/Concave
                                                Combinations Let us first start with a formal definition of a 2 vector
                                                convex combination. Then we will break down the definition into parts
                                                and analyze the definition. Then we will formally define and analyze a
                                                convex combination with a finite number of vectors in the same manner. A
                                                subset $S \subseteq \mathbb{R}^{n}$ is said to be a convex set if:
                                                $\lambda x + (1 - \lambda)y \in S \;\forall \lambda \in [0,1], \forall
                                                x,y \in S$ Our formal definition here is for a convex combination with 2
                                                vectors. Vectors $x$ and $y$ are elemenets of the subset $S$. $S$ is a
                                                proper subset of the real numbers to $n$ dimensions. Our variable lambda
                                                $\lambda$ has the value betweewn 0 and 1. The result of $\lambda x + (1
                                                - \lambda)y$ should be an element in $S$. Now let us formally define a
                                                convex combination with any finite number of vectors. Let us say that $S
                                                \subseteq \mathbb{R}^{n}$ and $x_{1},...,x_{k}\in S$. Then a vector of
                                                the form: $y=\sum_{i=1}^{k}\lambda_{i}x_{i} \geq 0 \; \forall i,
                                                \sum_{i=1}^{k} \lambda_{i}=1$ is called a convex combination of the
                                                vectors $x_{1},...,x_{k}$. Here the summation of every $\lambda_{i}$ is
                                                equal to 1 and every $\lambda_{i}$ is greater is greater than or equal
                                                to 0. Entropy Definition of Entropy Let us first formally define
                                                entropy. Say $\mu \in S_{n}$ is an $n$-dimensional probability
                                                distribution. The entropy of the distribution can be denoted as $H(\mu)$
                                                and is defined by: $H(\mu) := \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$
                                                $\quad\quad\;\;\;= -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a
                                                probability distribution that is an element from the set $S_{n}$. The
                                                set $S_{n}$ is a set of $n$ probability distributions. The entropy of
                                                the probability distribution $\mu$ can be seen as a the function $H$ on
                                                $\mu$. We take the summation of every probability in $\mu$ at $i$
                                                multiplied by $log(1/\mu_{i})$. The final result of this summation over
                                                every probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Properties of the Entropy Function To
                                                continue. Hidden Markov Processes Preliminaries Information Theory
                                                Convex and Concave Functions Convex/Concave Combinations Let us first
                                                start with a formal definition of a 2 vector convex combination. Then we
                                                will break down the definition into parts and analyze the definition.
                                                Then we will formally define and analyze a convex combination with a
                                                finite number of vectors in the same manner. A subset $S \subseteq
                                                \mathbb{R}^{n}$ is said to be a convex set if: $\lambda x + (1 -
                                                \lambda)y \in S \;\forall \lambda \in [0,1], \forall x,y \in S$ Our
                                                formal definition here is for a convex combination with 2 vectors.
                                                Vectors $x$ and $y$ are elemenets of the subset $S$. $S$ is a proper
                                                subset of the real numbers to $n$ dimensions. Our variable lambda
                                                $\lambda$ has the value betweewn 0 and 1. The result of $\lambda x + (1
                                                - \lambda)y$ should be an element in $S$. Now let us formally define a
                                                convex combination with any finite number of vectors. Let us say that $S
                                                \subseteq \mathbb{R}^{n}$ and $x_{1},...,x_{k}\in S$. Then a vector of
                                                the form: $y=\sum_{i=1}^{k}\lambda_{i}x_{i} \geq 0 \; \forall i,
                                                \sum_{i=1}^{k} \lambda_{i}=1$ is called a convex combination of the
                                                vectors $x_{1},...,x_{k}$. Here the summation of every $\lambda_{i}$ is
                                                equal to 1 and every $\lambda_{i}$ is greater is greater than or equal
                                                to 0. Entropy Definition of Entropy Let us first formally define
                                                entropy. Say $\mu \in S_{n}$ is an $n$-dimensional probability
                                                distribution. The entropy of the distribution can be denoted as $H(\mu)$
                                                and is defined by: $H(\mu) := \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$
                                                $\quad\quad\;\;\;= -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a
                                                probability distribution that is an element from the set $S_{n}$. The
                                                set $S_{n}$ is a set of $n$ probability distributions. The entropy of
                                                the probability distribution $\mu$ can be seen as a the function $H$ on
                                                $\mu$. We take the summation of every probability in $\mu$ at $i$
                                                multiplied by $log(1/\mu_{i})$. The final result of this summation over
                                                every probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Properties of the Entropy Function To
                                                continue. Hidden Markov Processes Preliminaries Information Theory
                                                Convex and Concave Functions Convex/Concave Combinations Let us first
                                                start with a formal definition of a 2 vector convex combination. Then we
                                                will break down the definition into parts and analyze the definition.
                                                Then we will formally define and analyze a convex combination with a
                                                finite number of vectors in the same manner. A subset $S \subseteq
                                                \mathbb{R}^{n}$ is said to be a convex set if: $\lambda x + (1 -
                                                \lambda)y \in S \;\forall \lambda \in [0,1], \forall x,y \in S$ Our
                                                formal definition here is for a convex combination with 2 vectors.
                                                Vectors $x$ and $y$ are elemenets of the subset $S$. $S$ is a proper
                                                subset of the real numbers to $n$ dimensions. Our variable lambda
                                                $\lambda$ has the value betweewn 0 and 1. The result of $\lambda x + (1
                                                - \lambda)y$ should be an element in $S$. Now let us formally define a
                                                convex combination with any finite number of vectors. Let us say that $S
                                                \subseteq \mathbb{R}^{n}$ and $x_{1},...,x_{k}\in S$. Then a vector of
                                                the form: $y=\sum_{i=1}^{k}\lambda_{i}x_{i} \geq 0 \; \forall i,
                                                \sum_{i=1}^{k} \lambda_{i}=1$ is called a convex combination of the
                                                vectors $x_{1},...,x_{k}$. Here the summation of every $\lambda_{i}$ is
                                                equal to 1 and every $\lambda_{i}$ is greater is greater than or equal
                                                to 0. Entropy Definition of Entropy Let us first formally define
                                                entropy. Say $\mu \in S_{n}$ is an $n$-dimensional probability
                                                distribution. The entropy of the distribution can be denoted as $H(\mu)$
                                                and is defined by: $H(\mu) := \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$
                                                $\quad\quad\;\;\;= -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a
                                                probability distribution that is an element from the set $S_{n}$. The
                                                set $S_{n}$ is a set of $n$ probability distributions. The entropy of
                                                the probability distribution $\mu$ can be seen as a the function $H$ on
                                                $\mu$. We take the summation of every probability in $\mu$ at $i$
                                                multiplied by $log(1/\mu_{i})$. The final result of this summation over
                                                every probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Properties of the Entropy Function To
                                                continue. Hidden Markov ProcessesPreliminaries Information Theory Convex
                                                and Concave Functions Convex/Concave Combinations Let us first start
                                                with a formal definition of a 2 vector convex combination. Then we will
                                                break down the definition into parts and analyze the definition. Then we
                                                will formally define and analyze a convex combination with a finite
                                                number of vectors in the same manner. A subset $S \subseteq
                                                \mathbb{R}^{n}$ is said to be a convex set if: $\lambda x + (1 -
                                                \lambda)y \in S \;\forall \lambda \in [0,1], \forall x,y \in S$ Our
                                                formal definition here is for a convex combination with 2 vectors.
                                                Vectors $x$ and $y$ are elemenets of the subset $S$. $S$ is a proper
                                                subset of the real numbers to $n$ dimensions. Our variable lambda
                                                $\lambda$ has the value betweewn 0 and 1. The result of $\lambda x + (1
                                                - \lambda)y$ should be an element in $S$. Now let us formally define a
                                                convex combination with any finite number of vectors. Let us say that $S
                                                \subseteq \mathbb{R}^{n}$ and $x_{1},...,x_{k}\in S$. Then a vector of
                                                the form: $y=\sum_{i=1}^{k}\lambda_{i}x_{i} \geq 0 \; \forall i,
                                                \sum_{i=1}^{k} \lambda_{i}=1$ is called a convex combination of the
                                                vectors $x_{1},...,x_{k}$. Here the summation of every $\lambda_{i}$ is
                                                equal to 1 and every $\lambda_{i}$ is greater is greater than or equal
                                                to 0. Entropy Definition of Entropy Let us first formally define
                                                entropy. Say $\mu \in S_{n}$ is an $n$-dimensional probability
                                                distribution. The entropy of the distribution can be denoted as $H(\mu)$
                                                and is defined by: $H(\mu) := \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$
                                                $\quad\quad\;\;\;= -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a
                                                probability distribution that is an element from the set $S_{n}$. The
                                                set $S_{n}$ is a set of $n$ probability distributions. The entropy of
                                                the probability distribution $\mu$ can be seen as a the function $H$ on
                                                $\mu$. We take the summation of every probability in $\mu$ at $i$
                                                multiplied by $log(1/\mu_{i})$. The final result of this summation over
                                                every probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Properties of the Entropy Function To
                                                continue. PreliminariesInformation Theory Convex and Concave Functions
                                                Convex/Concave Combinations Let us first start with a formal definition
                                                of a 2 vector convex combination. Then we will break down the definition
                                                into parts and analyze the definition. Then we will formally define and
                                                analyze a convex combination with a finite number of vectors in the same
                                                manner. A subset $S \subseteq \mathbb{R}^{n}$ is said to be a convex set
                                                if: $\lambda x + (1 - \lambda)y \in S \;\forall \lambda \in [0,1],
                                                \forall x,y \in S$ Our formal definition here is for a convex
                                                combination with 2 vectors. Vectors $x$ and $y$ are elemenets of the
                                                subset $S$. $S$ is a proper subset of the real numbers to $n$
                                                dimensions. Our variable lambda $\lambda$ has the value betweewn 0 and
                                                1. The result of $\lambda x + (1 - \lambda)y$ should be an element in
                                                $S$. Now let us formally define a convex combination with any finite
                                                number of vectors. Let us say that $S \subseteq \mathbb{R}^{n}$ and
                                                $x_{1},...,x_{k}\in S$. Then a vector of the form:
                                                $y=\sum_{i=1}^{k}\lambda_{i}x_{i} \geq 0 \; \forall i, \sum_{i=1}^{k}
                                                \lambda_{i}=1$ is called a convex combination of the vectors
                                                $x_{1},...,x_{k}$. Here the summation of every $\lambda_{i}$ is equal to
                                                1 and every $\lambda_{i}$ is greater is greater than or equal to 0.
                                                Entropy Definition of Entropy Let us first formally define entropy. Say
                                                $\mu \in S_{n}$ is an $n$-dimensional probability distribution. The
                                                entropy of the distribution can be denoted as $H(\mu)$ and is defined
                                                by: $H(\mu) := \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$ $\quad\quad\;\;\;=
                                                -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a probability
                                                distribution that is an element from the set $S_{n}$. The set $S_{n}$ is
                                                a set of $n$ probability distributions. The entropy of the probability
                                                distribution $\mu$ can be seen as a the function $H$ on $\mu$. We take
                                                the summation of every probability in $\mu$ at $i$ multiplied by
                                                $log(1/\mu_{i})$. The final result of this summation over every
                                                probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Properties of the Entropy Function To
                                                continue. Information TheoryConvex and Concave Functions Convex/Concave
                                                Combinations Let us first start with a formal definition of a 2 vector
                                                convex combination. Then we will break down the definition into parts
                                                and analyze the definition. Then we will formally define and analyze a
                                                convex combination with a finite number of vectors in the same manner. A
                                                subset $S \subseteq \mathbb{R}^{n}$ is said to be a convex set if:
                                                $\lambda x + (1 - \lambda)y \in S \;\forall \lambda \in [0,1], \forall
                                                x,y \in S$ Our formal definition here is for a convex combination with 2
                                                vectors. Vectors $x$ and $y$ are elemenets of the subset $S$. $S$ is a
                                                proper subset of the real numbers to $n$ dimensions. Our variable lambda
                                                $\lambda$ has the value betweewn 0 and 1. The result of $\lambda x + (1
                                                - \lambda)y$ should be an element in $S$. Now let us formally define a
                                                convex combination with any finite number of vectors. Let us say that $S
                                                \subseteq \mathbb{R}^{n}$ and $x_{1},...,x_{k}\in S$. Then a vector of
                                                the form: $y=\sum_{i=1}^{k}\lambda_{i}x_{i} \geq 0 \; \forall i,
                                                \sum_{i=1}^{k} \lambda_{i}=1$ is called a convex combination of the
                                                vectors $x_{1},...,x_{k}$. Here the summation of every $\lambda_{i}$ is
                                                equal to 1 and every $\lambda_{i}$ is greater is greater than or equal
                                                to 0. Convex and Concave FunctionsConvex/Concave Combinations
                                                Convex/Concave CombinationsLet us first start with a formal definition
                                                of a 2 vector convex combination. Then we will break down the definition
                                                into parts and analyze the definition. Then we will formally define and
                                                analyze a convex combination with a finite number of vectors in the same
                                                manner. A subset $S \subseteq \mathbb{R}^{n}$ is said to be a convex set
                                                if: $\lambda x + (1 - \lambda)y \in S \;\forall \lambda \in [0,1],
                                                \forall x,y \in S$ Our formal definition here is for a convex
                                                combination with 2 vectors. Vectors $x$ and $y$ are elemenets of the
                                                subset $S$. $S$ is a proper subset of the real numbers to $n$
                                                dimensions. Our variable lambda $\lambda$ has the value betweewn 0 and
                                                1. The result of $\lambda x + (1 - \lambda)y$ should be an element in
                                                $S$. Now let us formally define a convex combination with any finite
                                                number of vectors. Let us say that $S \subseteq \mathbb{R}^{n}$ and
                                                $x_{1},...,x_{k}\in S$. Then a vector of the form:
                                                $y=\sum_{i=1}^{k}\lambda_{i}x_{i} \geq 0 \; \forall i, \sum_{i=1}^{k}
                                                \lambda_{i}=1$ is called a convex combination of the vectors
                                                $x_{1},...,x_{k}$. Here the summation of every $\lambda_{i}$ is equal to
                                                1 and every $\lambda_{i}$ is greater is greater than or equal to 0.
                                                Entropy Definition of Entropy Let us first formally define entropy. Say
                                                $\mu \in S_{n}$ is an $n$-dimensional probability distribution. The
                                                entropy of the distribution can be denoted as $H(\mu)$ and is defined
                                                by: $H(\mu) := \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$ $\quad\quad\;\;\;=
                                                -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a probability
                                                distribution that is an element from the set $S_{n}$. The set $S_{n}$ is
                                                a set of $n$ probability distributions. The entropy of the probability
                                                distribution $\mu$ can be seen as a the function $H$ on $\mu$. We take
                                                the summation of every probability in $\mu$ at $i$ multiplied by
                                                $log(1/\mu_{i})$. The final result of this summation over every
                                                probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Properties of the Entropy Function To
                                                continue. EntropyDefinition of Entropy Let us first formally define
                                                entropy. Say $\mu \in S_{n}$ is an $n$-dimensional probability
                                                distribution. The entropy of the distribution can be denoted as $H(\mu)$
                                                and is defined by: $H(\mu) := \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$
                                                $\quad\quad\;\;\;= -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a
                                                probability distribution that is an element from the set $S_{n}$. The
                                                set $S_{n}$ is a set of $n$ probability distributions. The entropy of
                                                the probability distribution $\mu$ can be seen as a the function $H$ on
                                                $\mu$. We take the summation of every probability in $\mu$ at $i$
                                                multiplied by $log(1/\mu_{i})$. The final result of this summation over
                                                every probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Definition of EntropyLet us first
                                                formally define entropy. Say $\mu \in S_{n}$ is an $n$-dimensional
                                                probability distribution. The entropy of the distribution can be denoted
                                                as $H(\mu)$ and is defined by: $H(\mu) :=
                                                \sum_{i=1}^{n}\mu_{i}log(1/\mu_{i})$ $\quad\quad\;\;\;=
                                                -\sum_{i=1}^{n}\mu_{i}log\mu_{i}$ Here, $\mu$ is a probability
                                                distribution that is an element from the set $S_{n}$. The set $S_{n}$ is
                                                a set of $n$ probability distributions. The entropy of the probability
                                                distribution $\mu$ can be seen as a the function $H$ on $\mu$. We take
                                                the summation of every probability in $\mu$ at $i$ multiplied by
                                                $log(1/\mu_{i})$. The final result of this summation over every
                                                probabiltiy in our probability distribution is the entropy of the
                                                probability distribution $\mu$. Properties of the Entropy Function To
                                                continue. Properties of the Entropy FunctionTo continue.
                                                [https://www.contextswitching.org/my/presentations/qsvm/]
                                                You are being redirected by your loyal guides 🐶🐱 You are being
                                                redirected by your loyal guides 🐶🐱 You are being redirected by your
                                                loyal guides 🐶🐱
                                                [https://www.contextswitching.org/tcs/algorithmicanalysis]
                                                Algorithmic Analysis - Context Switching Algorithmic Anaylsis
                                                Introduction What is it Why we use it Algorithmic analysis is used to
                                                help computer scientists understand the resources required by an
                                                algorithm for time, storage, and other uses. Algorithmic anlysis must
                                                analyze algorithms in a methodical, universal, and fair way. To do this
                                                computer scientist implement mathematical models that describe the
                                                resources used by algorithms. This work, although theoretical, extends
                                                beyound theory and applies mathematical models to real world problems.
                                                Algorithmic anlayis is apart of app development, website development,
                                                simulations, artificial intelligence, and anything that implements
                                                algorithms in its design. This is because computer scientist or
                                                programmers need to implement algorithms into applications that take
                                                minimum time and space, while still being reltively easy to code. In the
                                                rest of this articule we will review the mathematical models that
                                                computer scientists and programmers use to analyze algorithms.
                                                Mathematical Models Best-Case Time Complexity Equation Omega $\Omega$
                                                Lower-bounds The best-case time complexity can be mathematically defined
                                                as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                                \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into
                                                individual parts, then piece them together to arrive at the final
                                                equation. First, lets start with what's left of the equal sign.
                                                $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case time
                                                complexity or the lower-bounds for an algorithm. $n$ is the input for
                                                the function $g$. $g$ is the function that describes the original
                                                algorithm. Next, lets look at $\exists \, c > 0, n_{0} > 0$ The $\exists
                                                $ denotes, "there exists". So the equation states that there exists two
                                                constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$ is
                                                simply a constant that accounts for operations that are not proportional
                                                to the input $n$ for $g$. $n_{0}$ is a theoretical size for the input
                                                $n$ wherein only after input size does $\Omega$ actually maintain the
                                                lower-bound for $g$. Remember, we do not give actual values for the
                                                variables, but leave them ambigious so that we have a lower-bound model
                                                to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq
                                                f(n)$ states that our lower bounds, $g(n)$ times some constant $c$ lies
                                                between $0$ and our original function $f(n)$. This makes sense as the
                                                lower-bound runtime for our algorithm cannot be less than 0 but also
                                                needs to be lower than or equal to the runtime of our original function.
                                                Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                                cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$.
                                                In this instance, $f(n)$ is the original asymptotic function, the one
                                                that is derived from the algorithm itself. $cg(n)$ denotes the
                                                lower-bounds of our original asymptotic function times some constant
                                                $c$. Thus, this is saying that the lower-bounds of the original
                                                asymptotic function for our algorithm lies at or below the original
                                                asymptotic function or at or above $0$. $\exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                                together. This segment of the equation is saying that the lower-bounds
                                                of our original function $g(n)$ times some constant $c$ that is greater
                                                than 0, lies at or below the original function $f(n)$ or at or above
                                                $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \,
                                                c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n
                                                _{0}$ This translates to, "There exists some contant $c$ greater than
                                                $0$ and some constant $n_{0}$ such that $cg(n)$ is greater than or equal
                                                to $0$ and less than or equal to $f(n)$ where all $n$ are greater than
                                                or equal to $n_{0}$". From what we have learned we can say that this
                                                equation here says that there exists a constant $c$ and $n$, $c$ being
                                                runtime constants, that are greater than $0$ such that our lower-bound
                                                exists between $0$ and the original asymptotic function. Best-Case Time
                                                Complexity Graph Worst Case Time Complexity - $O$ Big-O $O$ Upper-bounds
                                                So we know that best case of time-complexity is when an algorithm runs
                                                is the least amount of time it could theoretically operate in. So, it
                                                follows that the worst-case time-complexity is the worst time an
                                                algorith could theoretically operate in. Now, what is an example of
                                                this. Lets take our previous example of insertion sort. Now, recall,
                                                that the type of input for the algorithm can affect its time complexity.
                                                So, I want you to take a moment to come up with an example. Lets
                                                continue. An example for which the time complexity is worse is when all
                                                of the input data is sorted in descending order. Algorithmic Analysis -
                                                Context Switching Algorithmic Analysis - Context SwitchingAlgorithmic
                                                Anaylsis Introduction What is it Why we use it Algorithmic analysis is
                                                used to help computer scientists understand the resources required by an
                                                algorithm for time, storage, and other uses. Algorithmic anlysis must
                                                analyze algorithms in a methodical, universal, and fair way. To do this
                                                computer scientist implement mathematical models that describe the
                                                resources used by algorithms. This work, although theoretical, extends
                                                beyound theory and applies mathematical models to real world problems.
                                                Algorithmic anlayis is apart of app development, website development,
                                                simulations, artificial intelligence, and anything that implements
                                                algorithms in its design. This is because computer scientist or
                                                programmers need to implement algorithms into applications that take
                                                minimum time and space, while still being reltively easy to code. In the
                                                rest of this articule we will review the mathematical models that
                                                computer scientists and programmers use to analyze algorithms.
                                                Mathematical Models Best-Case Time Complexity Equation Omega $\Omega$
                                                Lower-bounds The best-case time complexity can be mathematically defined
                                                as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                                \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into
                                                individual parts, then piece them together to arrive at the final
                                                equation. First, lets start with what's left of the equal sign.
                                                $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case time
                                                complexity or the lower-bounds for an algorithm. $n$ is the input for
                                                the function $g$. $g$ is the function that describes the original
                                                algorithm. Next, lets look at $\exists \, c > 0, n_{0} > 0$ The $\exists
                                                $ denotes, "there exists". So the equation states that there exists two
                                                constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$ is
                                                simply a constant that accounts for operations that are not proportional
                                                to the input $n$ for $g$. $n_{0}$ is a theoretical size for the input
                                                $n$ wherein only after input size does $\Omega$ actually maintain the
                                                lower-bound for $g$. Remember, we do not give actual values for the
                                                variables, but leave them ambigious so that we have a lower-bound model
                                                to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq
                                                f(n)$ states that our lower bounds, $g(n)$ times some constant $c$ lies
                                                between $0$ and our original function $f(n)$. This makes sense as the
                                                lower-bound runtime for our algorithm cannot be less than 0 but also
                                                needs to be lower than or equal to the runtime of our original function.
                                                Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                                cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$.
                                                In this instance, $f(n)$ is the original asymptotic function, the one
                                                that is derived from the algorithm itself. $cg(n)$ denotes the
                                                lower-bounds of our original asymptotic function times some constant
                                                $c$. Thus, this is saying that the lower-bounds of the original
                                                asymptotic function for our algorithm lies at or below the original
                                                asymptotic function or at or above $0$. $\exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                                together. This segment of the equation is saying that the lower-bounds
                                                of our original function $g(n)$ times some constant $c$ that is greater
                                                than 0, lies at or below the original function $f(n)$ or at or above
                                                $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \,
                                                c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n
                                                _{0}$ This translates to, "There exists some contant $c$ greater than
                                                $0$ and some constant $n_{0}$ such that $cg(n)$ is greater than or equal
                                                to $0$ and less than or equal to $f(n)$ where all $n$ are greater than
                                                or equal to $n_{0}$". From what we have learned we can say that this
                                                equation here says that there exists a constant $c$ and $n$, $c$ being
                                                runtime constants, that are greater than $0$ such that our lower-bound
                                                exists between $0$ and the original asymptotic function. Best-Case Time
                                                Complexity Graph Worst Case Time Complexity - $O$ Big-O $O$ Upper-bounds
                                                So we know that best case of time-complexity is when an algorithm runs
                                                is the least amount of time it could theoretically operate in. So, it
                                                follows that the worst-case time-complexity is the worst time an
                                                algorith could theoretically operate in. Now, what is an example of
                                                this. Lets take our previous example of insertion sort. Now, recall,
                                                that the type of input for the algorithm can affect its time complexity.
                                                So, I want you to take a moment to come up with an example. Lets
                                                continue. An example for which the time complexity is worse is when all
                                                of the input data is sorted in descending order. Algorithmic Anaylsis
                                                Introduction What is it Why we use it Algorithmic analysis is used to
                                                help computer scientists understand the resources required by an
                                                algorithm for time, storage, and other uses. Algorithmic anlysis must
                                                analyze algorithms in a methodical, universal, and fair way. To do this
                                                computer scientist implement mathematical models that describe the
                                                resources used by algorithms. This work, although theoretical, extends
                                                beyound theory and applies mathematical models to real world problems.
                                                Algorithmic anlayis is apart of app development, website development,
                                                simulations, artificial intelligence, and anything that implements
                                                algorithms in its design. This is because computer scientist or
                                                programmers need to implement algorithms into applications that take
                                                minimum time and space, while still being reltively easy to code. In the
                                                rest of this articule we will review the mathematical models that
                                                computer scientists and programmers use to analyze algorithms.
                                                Mathematical Models Best-Case Time Complexity Equation Omega $\Omega$
                                                Lower-bounds The best-case time complexity can be mathematically defined
                                                as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                                \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into
                                                individual parts, then piece them together to arrive at the final
                                                equation. First, lets start with what's left of the equal sign.
                                                $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case time
                                                complexity or the lower-bounds for an algorithm. $n$ is the input for
                                                the function $g$. $g$ is the function that describes the original
                                                algorithm. Next, lets look at $\exists \, c > 0, n_{0} > 0$ The $\exists
                                                $ denotes, "there exists". So the equation states that there exists two
                                                constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$ is
                                                simply a constant that accounts for operations that are not proportional
                                                to the input $n$ for $g$. $n_{0}$ is a theoretical size for the input
                                                $n$ wherein only after input size does $\Omega$ actually maintain the
                                                lower-bound for $g$. Remember, we do not give actual values for the
                                                variables, but leave them ambigious so that we have a lower-bound model
                                                to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq
                                                f(n)$ states that our lower bounds, $g(n)$ times some constant $c$ lies
                                                between $0$ and our original function $f(n)$. This makes sense as the
                                                lower-bound runtime for our algorithm cannot be less than 0 but also
                                                needs to be lower than or equal to the runtime of our original function.
                                                Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                                cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$.
                                                In this instance, $f(n)$ is the original asymptotic function, the one
                                                that is derived from the algorithm itself. $cg(n)$ denotes the
                                                lower-bounds of our original asymptotic function times some constant
                                                $c$. Thus, this is saying that the lower-bounds of the original
                                                asymptotic function for our algorithm lies at or below the original
                                                asymptotic function or at or above $0$. $\exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                                together. This segment of the equation is saying that the lower-bounds
                                                of our original function $g(n)$ times some constant $c$ that is greater
                                                than 0, lies at or below the original function $f(n)$ or at or above
                                                $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \,
                                                c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n
                                                _{0}$ This translates to, "There exists some contant $c$ greater than
                                                $0$ and some constant $n_{0}$ such that $cg(n)$ is greater than or equal
                                                to $0$ and less than or equal to $f(n)$ where all $n$ are greater than
                                                or equal to $n_{0}$". From what we have learned we can say that this
                                                equation here says that there exists a constant $c$ and $n$, $c$ being
                                                runtime constants, that are greater than $0$ such that our lower-bound
                                                exists between $0$ and the original asymptotic function. Best-Case Time
                                                Complexity Graph Worst Case Time Complexity - $O$ Big-O $O$ Upper-bounds
                                                So we know that best case of time-complexity is when an algorithm runs
                                                is the least amount of time it could theoretically operate in. So, it
                                                follows that the worst-case time-complexity is the worst time an
                                                algorith could theoretically operate in. Now, what is an example of
                                                this. Lets take our previous example of insertion sort. Now, recall,
                                                that the type of input for the algorithm can affect its time complexity.
                                                So, I want you to take a moment to come up with an example. Lets
                                                continue. An example for which the time complexity is worse is when all
                                                of the input data is sorted in descending order. Algorithmic Anaylsis
                                                Introduction What is it Why we use it Algorithmic analysis is used to
                                                help computer scientists understand the resources required by an
                                                algorithm for time, storage, and other uses. Algorithmic anlysis must
                                                analyze algorithms in a methodical, universal, and fair way. To do this
                                                computer scientist implement mathematical models that describe the
                                                resources used by algorithms. This work, although theoretical, extends
                                                beyound theory and applies mathematical models to real world problems.
                                                Algorithmic anlayis is apart of app development, website development,
                                                simulations, artificial intelligence, and anything that implements
                                                algorithms in its design. This is because computer scientist or
                                                programmers need to implement algorithms into applications that take
                                                minimum time and space, while still being reltively easy to code. In the
                                                rest of this articule we will review the mathematical models that
                                                computer scientists and programmers use to analyze algorithms.
                                                Mathematical Models Best-Case Time Complexity Equation Omega $\Omega$
                                                Lower-bounds The best-case time complexity can be mathematically defined
                                                as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n)
                                                \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into
                                                individual parts, then piece them together to arrive at the final
                                                equation. First, lets start with what's left of the equal sign.
                                                $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case time
                                                complexity or the lower-bounds for an algorithm. $n$ is the input for
                                                the function $g$. $g$ is the function that describes the original
                                                algorithm. Next, lets look at $\exists \, c > 0, n_{0} > 0$ The $\exists
                                                $ denotes, "there exists". So the equation states that there exists two
                                                constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$ is
                                                simply a constant that accounts for operations that are not proportional
                                                to the input $n$ for $g$. $n_{0}$ is a theoretical size for the input
                                                $n$ wherein only after input size does $\Omega$ actually maintain the
                                                lower-bound for $g$. Remember, we do not give actual values for the
                                                variables, but leave them ambigious so that we have a lower-bound model
                                                to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq
                                                f(n)$ states that our lower bounds, $g(n)$ times some constant $c$ lies
                                                between $0$ and our original function $f(n)$. This makes sense as the
                                                lower-bound runtime for our algorithm cannot be less than 0 but also
                                                needs to be lower than or equal to the runtime of our original function.
                                                Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                                cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$.
                                                In this instance, $f(n)$ is the original asymptotic function, the one
                                                that is derived from the algorithm itself. $cg(n)$ denotes the
                                                lower-bounds of our original asymptotic function times some constant
                                                $c$. Thus, this is saying that the lower-bounds of the original
                                                asymptotic function for our algorithm lies at or below the original
                                                asymptotic function or at or above $0$. $\exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                                together. This segment of the equation is saying that the lower-bounds
                                                of our original function $g(n)$ times some constant $c$ that is greater
                                                than 0, lies at or below the original function $f(n)$ or at or above
                                                $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \,
                                                c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n
                                                _{0}$ This translates to, "There exists some contant $c$ greater than
                                                $0$ and some constant $n_{0}$ such that $cg(n)$ is greater than or equal
                                                to $0$ and less than or equal to $f(n)$ where all $n$ are greater than
                                                or equal to $n_{0}$". From what we have learned we can say that this
                                                equation here says that there exists a constant $c$ and $n$, $c$ being
                                                runtime constants, that are greater than $0$ such that our lower-bound
                                                exists between $0$ and the original asymptotic function. Best-Case Time
                                                Complexity Graph Worst Case Time Complexity - $O$ Big-O $O$ Upper-bounds
                                                So we know that best case of time-complexity is when an algorithm runs
                                                is the least amount of time it could theoretically operate in. So, it
                                                follows that the worst-case time-complexity is the worst time an
                                                algorith could theoretically operate in. Now, what is an example of
                                                this. Lets take our previous example of insertion sort. Now, recall,
                                                that the type of input for the algorithm can affect its time complexity.
                                                So, I want you to take a moment to come up with an example. Lets
                                                continue. An example for which the time complexity is worse is when all
                                                of the input data is sorted in descending order. Algorithmic
                                                AnaylsisIntroduction What is it Why we use it Algorithmic analysis is
                                                used to help computer scientists understand the resources required by an
                                                algorithm for time, storage, and other uses. Algorithmic anlysis must
                                                analyze algorithms in a methodical, universal, and fair way. To do this
                                                computer scientist implement mathematical models that describe the
                                                resources used by algorithms. This work, although theoretical, extends
                                                beyound theory and applies mathematical models to real world problems.
                                                Algorithmic anlayis is apart of app development, website development,
                                                simulations, artificial intelligence, and anything that implements
                                                algorithms in its design. This is because computer scientist or
                                                programmers need to implement algorithms into applications that take
                                                minimum time and space, while still being reltively easy to code. In the
                                                rest of this articule we will review the mathematical models that
                                                computer scientists and programmers use to analyze algorithms.
                                                IntroductionWhat is it Why we use it What is itWhy we use itAlgorithmic
                                                analysis is used to help computer scientists understand the resources
                                                required by an algorithm for time, storage, and other uses. Algorithmic
                                                anlysis must analyze algorithms in a methodical, universal, and fair
                                                way. To do this computer scientist implement mathematical models that
                                                describe the resources used by algorithms. This work, although
                                                theoretical, extends beyound theory and applies mathematical models to
                                                real world problems. Algorithmic anlayis is apart of app development,
                                                website development, simulations, artificial intelligence, and anything
                                                that implements algorithms in its design. This is because computer
                                                scientist or programmers need to implement algorithms into applications
                                                that take minimum time and space, while still being reltively easy to
                                                code. In the rest of this articule we will review the mathematical
                                                models that computer scientists and programmers use to analyze
                                                algorithms. Mathematical Models Best-Case Time Complexity Equation Omega
                                                $\Omega$ Lower-bounds The best-case time complexity can be
                                                mathematically defined as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ I will break
                                                this down into individual parts, then piece them together to arrive at
                                                the final equation. First, lets start with what's left of the equal
                                                sign. $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case
                                                time complexity or the lower-bounds for an algorithm. $n$ is the input
                                                for the function $g$. $g$ is the function that describes the original
                                                algorithm. Next, lets look at $\exists \, c > 0, n_{0} > 0$ The $\exists
                                                $ denotes, "there exists". So the equation states that there exists two
                                                constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$ is
                                                simply a constant that accounts for operations that are not proportional
                                                to the input $n$ for $g$. $n_{0}$ is a theoretical size for the input
                                                $n$ wherein only after input size does $\Omega$ actually maintain the
                                                lower-bound for $g$. Remember, we do not give actual values for the
                                                variables, but leave them ambigious so that we have a lower-bound model
                                                to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq
                                                f(n)$ states that our lower bounds, $g(n)$ times some constant $c$ lies
                                                between $0$ and our original function $f(n)$. This makes sense as the
                                                lower-bound runtime for our algorithm cannot be less than 0 but also
                                                needs to be lower than or equal to the runtime of our original function.
                                                Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                                cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$.
                                                In this instance, $f(n)$ is the original asymptotic function, the one
                                                that is derived from the algorithm itself. $cg(n)$ denotes the
                                                lower-bounds of our original asymptotic function times some constant
                                                $c$. Thus, this is saying that the lower-bounds of the original
                                                asymptotic function for our algorithm lies at or below the original
                                                asymptotic function or at or above $0$. $\exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                                together. This segment of the equation is saying that the lower-bounds
                                                of our original function $g(n)$ times some constant $c$ that is greater
                                                than 0, lies at or below the original function $f(n)$ or at or above
                                                $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \,
                                                c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n
                                                _{0}$ This translates to, "There exists some contant $c$ greater than
                                                $0$ and some constant $n_{0}$ such that $cg(n)$ is greater than or equal
                                                to $0$ and less than or equal to $f(n)$ where all $n$ are greater than
                                                or equal to $n_{0}$". From what we have learned we can say that this
                                                equation here says that there exists a constant $c$ and $n$, $c$ being
                                                runtime constants, that are greater than $0$ such that our lower-bound
                                                exists between $0$ and the original asymptotic function. Best-Case Time
                                                Complexity Graph Mathematical ModelsBest-Case Time Complexity Equation
                                                Omega $\Omega$ Lower-bounds The best-case time complexity can be
                                                mathematically defined as: $\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ I will break
                                                this down into individual parts, then piece them together to arrive at
                                                the final equation. First, lets start with what's left of the equal
                                                sign. $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case
                                                time complexity or the lower-bounds for an algorithm. $n$ is the input
                                                for the function $g$. $g$ is the function that describes the original
                                                algorithm. Next, lets look at $\exists \, c > 0, n_{0} > 0$ The $\exists
                                                $ denotes, "there exists". So the equation states that there exists two
                                                constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$ is
                                                simply a constant that accounts for operations that are not proportional
                                                to the input $n$ for $g$. $n_{0}$ is a theoretical size for the input
                                                $n$ wherein only after input size does $\Omega$ actually maintain the
                                                lower-bound for $g$. Remember, we do not give actual values for the
                                                variables, but leave them ambigious so that we have a lower-bound model
                                                to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq
                                                f(n)$ states that our lower bounds, $g(n)$ times some constant $c$ lies
                                                between $0$ and our original function $f(n)$. This makes sense as the
                                                lower-bound runtime for our algorithm cannot be less than 0 but also
                                                needs to be lower than or equal to the runtime of our original function.
                                                Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                                cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$.
                                                In this instance, $f(n)$ is the original asymptotic function, the one
                                                that is derived from the algorithm itself. $cg(n)$ denotes the
                                                lower-bounds of our original asymptotic function times some constant
                                                $c$. Thus, this is saying that the lower-bounds of the original
                                                asymptotic function for our algorithm lies at or below the original
                                                asymptotic function or at or above $0$. $\exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                                together. This segment of the equation is saying that the lower-bounds
                                                of our original function $g(n)$ times some constant $c$ that is greater
                                                than 0, lies at or below the original function $f(n)$ or at or above
                                                $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \,
                                                c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n
                                                _{0}$ This translates to, "There exists some contant $c$ greater than
                                                $0$ and some constant $n_{0}$ such that $cg(n)$ is greater than or equal
                                                to $0$ and less than or equal to $f(n)$ where all $n$ are greater than
                                                or equal to $n_{0}$". From what we have learned we can say that this
                                                equation here says that there exists a constant $c$ and $n$, $c$ being
                                                runtime constants, that are greater than $0$ such that our lower-bound
                                                exists between $0$ and the original asymptotic function. Best-Case Time
                                                Complexity EquationOmega $\Omega$ Lower-bounds Omega
                                                $\Omega$Lower-boundsThe best-case time complexity can be mathematically
                                                defined as:$\Omega(g(n)) = \exists \, c > 0, n_{0} > 0$ $s.t. \; 0 \leq
                                                cg(n) \leq f(n)$ $\forall \, n \geq n _{0}$ I will break this down into
                                                individual parts, then piece them together to arrive at the final
                                                equation. First, lets start with what's left of the equal sign.
                                                $\Omega(g(n)$ $\Omega$ is Big-Omega, which denotes the best-case time
                                                complexity or the lower-bounds for an algorithm. $n$ is the input for
                                                the function $g$. $g$ is the function that describes the original
                                                algorithm. Next, lets look at $\exists \, c > 0, n_{0} > 0$ The $\exists
                                                $ denotes, "there exists". So the equation states that there exists two
                                                constants, $c$ and $n_{0}$ whose values both are greater than 0. $c$ is
                                                simply a constant that accounts for operations that are not proportional
                                                to the input $n$ for $g$. $n_{0}$ is a theoretical size for the input
                                                $n$ wherein only after input size does $\Omega$ actually maintain the
                                                lower-bound for $g$. Remember, we do not give actual values for the
                                                variables, but leave them ambigious so that we have a lower-bound model
                                                to use for all algorithms. $0 \leq cg(n) \leq f(n)$ $0 \leq cg(n) \leq
                                                f(n)$ states that our lower bounds, $g(n)$ times some constant $c$ lies
                                                between $0$ and our original function $f(n)$. This makes sense as the
                                                lower-bound runtime for our algorithm cannot be less than 0 but also
                                                needs to be lower than or equal to the runtime of our original function.
                                                Otherwise it would not be much of a lower bound. Then there is $0 \leq
                                                cg(n) \leq f(n)$. This means that $cg(n)$ lies between $0$ and $f(n)$.
                                                In this instance, $f(n)$ is the original asymptotic function, the one
                                                that is derived from the algorithm itself. $cg(n)$ denotes the
                                                lower-bounds of our original asymptotic function times some constant
                                                $c$. Thus, this is saying that the lower-bounds of the original
                                                asymptotic function for our algorithm lies at or below the original
                                                asymptotic function or at or above $0$. $\exists \, c > 0, n_{0} > 0$
                                                $s.t. \; 0 \leq cg(n) \leq f(n)$ Now, to put the last two parts
                                                together. This segment of the equation is saying that the lower-bounds
                                                of our original function $g(n)$ times some constant $c$ that is greater
                                                than 0, lies at or below the original function $f(n)$ or at or above
                                                $0$. So looking at the final equation again: $\Omega(g(n)) = \exists \,
                                                c > 0, n_{0} > 0$ $s.t. \; 0 \leq cg(n) \leq f(n)$ $\forall \, n \geq n
                                                _{0}$ This translates to, "There exists some contant $c$ greater than
                                                $0$ and some constant $n_{0}$ such that $cg(n)$ is greater than or equal
                                                to $0$ and less than or equal to $f(n)$ where all $n$ are greater than
                                                or equal to $n_{0}$". From what we have learned we can say that this
                                                equation here says that there exists a constant $c$ and $n$, $c$ being
                                                runtime constants, that are greater than $0$ such that our lower-bound
                                                exists between $0$ and the original asymptotic function. Best-Case Time
                                                Complexity Graph Best-Case Time Complexity GraphWorst Case Time
                                                Complexity - $O$ Big-O $O$ Upper-bounds So we know that best case of
                                                time-complexity is when an algorithm runs is the least amount of time it
                                                could theoretically operate in. So, it follows that the worst-case
                                                time-complexity is the worst time an algorith could theoretically
                                                operate in. Now, what is an example of this. Lets take our previous
                                                example of insertion sort. Now, recall, that the type of input for the
                                                algorithm can affect its time complexity. So, I want you to take a
                                                moment to come up with an example. Lets continue. An example for which
                                                the time complexity is worse is when all of the input data is sorted in
                                                descending order. Worst Case Time Complexity - $O$Big-O $O$ Upper-bounds
                                                Big-O $O$Upper-boundsSo we know that best case of time-complexity is
                                                when an algorithm runs is the least amount of time it could
                                                theoretically operate in. So, it follows that the worst-case
                                                time-complexity is the worst time an algorith could theoretically
                                                operate in. Now, what is an example of this. Lets take our previous
                                                example of insertion sort. Now, recall, that the type of input for the
                                                algorithm can affect its time complexity. So, I want you to take a
                                                moment to come up with an example. typeLets continue. An example for
                                                which the time complexity is worse is when all of the input data is
                                                sorted in descending order.
                                                [https://www.contextswitching.org/neuro/thalamicnuclei]
                                                Thalamic Nuclei - Context Switching Thalamic Nuclei Introduction The
                                                thalamic nuclei are paired structures of the thalamus divided into three
                                                main groups: the lateral nuclear, medial nuclear, and anterior nuclear
                                                groups. The internal medullary lamina, a Y-shaped structure that splits
                                                these groups, is present on each side of the thalamus. A midline, thin
                                                thalamic nuclei, adjacent to the third ventricle, envelops the thalamic
                                                reticular nucleus and covers each lateral thalamus. Though the thalamus
                                                can divide into approximately 60 nuclei regions, to continue, we will
                                                focus on the overarching neuroanatomy and functions of the lateral,
                                                medial, and anterior groups. Anterior Thalamic Nuclei To continue.
                                                Lateral Thalamic Nuclei To continue. Medial Thalamic Nuclei To continue.
                                                Thalamic Nuclei - Context Switching Thalamic Nuclei - Context
                                                SwitchingThalamic Nuclei Introduction The thalamic nuclei are paired
                                                structures of the thalamus divided into three main groups: the lateral
                                                nuclear, medial nuclear, and anterior nuclear groups. The internal
                                                medullary lamina, a Y-shaped structure that splits these groups, is
                                                present on each side of the thalamus. A midline, thin thalamic nuclei,
                                                adjacent to the third ventricle, envelops the thalamic reticular nucleus
                                                and covers each lateral thalamus. Though the thalamus can divide into
                                                approximately 60 nuclei regions, to continue, we will focus on the
                                                overarching neuroanatomy and functions of the lateral, medial, and
                                                anterior groups. Anterior Thalamic Nuclei To continue. Lateral Thalamic
                                                Nuclei To continue. Medial Thalamic Nuclei To continue. Thalamic Nuclei
                                                Introduction The thalamic nuclei are paired structures of the thalamus
                                                divided into three main groups: the lateral nuclear, medial nuclear, and
                                                anterior nuclear groups. The internal medullary lamina, a Y-shaped
                                                structure that splits these groups, is present on each side of the
                                                thalamus. A midline, thin thalamic nuclei, adjacent to the third
                                                ventricle, envelops the thalamic reticular nucleus and covers each
                                                lateral thalamus. Though the thalamus can divide into approximately 60
                                                nuclei regions, to continue, we will focus on the overarching
                                                neuroanatomy and functions of the lateral, medial, and anterior groups.
                                                Anterior Thalamic Nuclei To continue. Lateral Thalamic Nuclei To
                                                continue. Medial Thalamic Nuclei To continue. Thalamic Nuclei
                                                Introduction The thalamic nuclei are paired structures of the thalamus
                                                divided into three main groups: the lateral nuclear, medial nuclear, and
                                                anterior nuclear groups. The internal medullary lamina, a Y-shaped
                                                structure that splits these groups, is present on each side of the
                                                thalamus. A midline, thin thalamic nuclei, adjacent to the third
                                                ventricle, envelops the thalamic reticular nucleus and covers each
                                                lateral thalamus. Though the thalamus can divide into approximately 60
                                                nuclei regions, to continue, we will focus on the overarching
                                                neuroanatomy and functions of the lateral, medial, and anterior groups.
                                                Anterior Thalamic Nuclei To continue. Lateral Thalamic Nuclei To
                                                continue. Medial Thalamic Nuclei To continue. Thalamic
                                                NucleiIntroduction The thalamic nuclei are paired structures of the
                                                thalamus divided into three main groups: the lateral nuclear, medial
                                                nuclear, and anterior nuclear groups. The internal medullary lamina, a
                                                Y-shaped structure that splits these groups, is present on each side of
                                                the thalamus. A midline, thin thalamic nuclei, adjacent to the third
                                                ventricle, envelops the thalamic reticular nucleus and covers each
                                                lateral thalamus. Though the thalamus can divide into approximately 60
                                                nuclei regions, to continue, we will focus on the overarching
                                                neuroanatomy and functions of the lateral, medial, and anterior groups.
                                                Introduction The thalamic nuclei are paired structures of the thalamus
                                                divided into three main groups: the lateral nuclear, medial nuclear, and
                                                anterior nuclear groups. The internal medullary lamina, a Y-shaped
                                                structure that splits these groups, is present on each side of the
                                                thalamus. A midline, thin thalamic nuclei, adjacent to the third
                                                ventricle, envelops the thalamic reticular nucleus and covers each
                                                lateral thalamus. Though the thalamus can divide into approximately 60
                                                nuclei regions, to continue, we will focus on the overarching
                                                neuroanatomy and functions of the lateral, medial, and anterior groups.
                                                Anterior Thalamic Nuclei To continue. Anterior Thalamic Nuclei To
                                                continue. Lateral Thalamic Nuclei To continue. Lateral Thalamic Nuclei
                                                To continue. Medial Thalamic Nuclei To continue. Medial Thalamic Nuclei
                                                To continue.
                                                [https://www.contextswitching.org/neuro/memoryformation]
                                                Molecular Bases of Memory Formation - Context Switching Molecular Bases
                                                of Memory Formation Introduction Molecular neuroscience is an area of
                                                chemical neuroscience that studies the molecular basis of intercellular
                                                activity applied to animals' nervous systems. This area of research
                                                covers molecular neuroanatomy, mechanisms of molecular signaling in the
                                                nervous system, and the molecular basis of neuroplasticity and
                                                neurodegenerative disease, which we will focus on here. Stress
                                                Mechanisms and Memory Consolidation Section Reference: Bisaz R,
                                                Travaglia A, Alberini CM. The neurobiological bases of memory formation:
                                                from physiological conditions to psychopathology. Psychopathology.
                                                2014;47(6):347-56. doi: 10.1159/000363702. Epub 2014 Oct 3. PMID:
                                                25301080; PMCID: PMC4246028. $^{[1]}$ Molecular Consolidation via the
                                                GR-BDNF-TrkB Pathway Glucocorticoids BDNF TrkB Schematic representation
                                                of learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway: Image Source:
                                                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246028/ Here, we will look
                                                at learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway. Specifically, the relation between long-term memory
                                                consolidation, stress hormones like glucocorticoids, and glucocorticoid
                                                receptors in pre- and post-synaptic neurons. Where GRs mediate several
                                                intracellular signaling pathways required for memory consolidation.
                                                Activated GRs may influence traffic of TrkB to the membrane surface
                                                and/or BDNF release. The binding of BDNF to its receptor TrkB leads to
                                                its phosphorylation. Phosphorylation results in the activation of
                                                ERK1/2, Akt and PLC$\gamma$. Activated GRs affect this and at the same
                                                time affect transcription-dependent mechanisms. This parrelel activation
                                                rapidly leads to CaMKII$\alpha$ phosphorylation, similarily to the
                                                afforementiond activation of GRs. The activation of ERK1/2, Akt,
                                                PLC$\gamma$, and CaMKII$\alpha$ pathways independently or can converge
                                                in CREB phosphorylation. CREB phosphorylation then leads to the
                                                synthesis of BDNF. The newly synthesized BDNF sustains the activation of
                                                the pathways and results in persistent phosphorylation of CREB,
                                                CaMKII$\alpha$. The end of this side of the domino effect leads to the
                                                downstream pre-synaptic target of CaMKII$\alpha$, phosopho-synapsin-1.
                                                So, GR activation recruits pre- and post-synaptic mechanisms to mediate
                                                memory consolidation. Memory Reconsolidation Memory consolidation was
                                                thought of as a unitary process that transformed a fragile memory trace
                                                into a stable one. However, in research done in the 1960s and in the
                                                last 15 years have shown otherwise. Memories that have become
                                                insensitive to disruption from specific types of interference have been
                                                show that they can become transiently labile when those memories are
                                                recalled. This recalled memory must again become stable, which is termed
                                                memory reconsolidation. We will further examine and look at the answer
                                                provide by the authors with respect to memory reconsolidation. An
                                                important question that naturally arises is what function does memory
                                                reconsolidation serve? There are two main hypothese proposed, the first
                                                being that reconsolidation serves to strengthen memories. The reasoning
                                                is that the phase of temporary fragility of the memory mediates
                                                additional consolidation, thus, producing a stronger and longer lasting
                                                memory. The second hypothesis postulates that the function of the
                                                post-retrieval process allows for the association of new information
                                                into the memories of past events. The purpose of this being the
                                                intergration of new learning with already establised and reactivated
                                                memories. More succenctly, the former hypothesis posits the function of
                                                memory reconsolidation is to strengthen memorires, while the ladder
                                                suggests reconsolidation is for memory updating. The findings of the
                                                researchers in the paper are that reconsolidation strengthens memories
                                                and does not mediate in memory updating. Treatment Approaches via the
                                                GR-BDNF-TrkB pathway The authors mention that it may be possible to
                                                target the mechanisms of consolidation in order to block or reduce the
                                                formation of very intense, persistent memories, like those associated
                                                with PTSD. They mention further that more easily targeted mechanisms
                                                would be the reconsolidation of memory retrieval. Given a combination of
                                                behavioral or psychotherapeutical approaches combined with pharmacology,
                                                it is suggested in the paper that it might be possible to weaken
                                                traumatic memories, or through memory consolidation and reconsolidation,
                                                possinle to re-store the memory with different emotional valence and
                                                intensity. CaMKII$\alpha$, BDNF-CREB pathways and Glucocorticoid
                                                Receptors Section Reference: Glucocorticoid receptors recruit the
                                                CaMKII$\alpha$, BDNF-CREB pathways to mediate memory consolidation by
                                                Dillon Y. Chen, Dhananjay Bambah-Mukku, Gabriella Pollonini, and
                                                Cristina M. Alberin. From the Center for Neural Science, New York
                                                University and Department of Neuroscience and Friedman Brain Institute,
                                                Mount Sinai School of Medicine $^{[2]}$ To continue. Molecular Bases of
                                                Memory Formation - Context Switching Molecular Bases of Memory Formation
                                                - Context SwitchingMolecular Bases of Memory Formation Introduction
                                                Molecular neuroscience is an area of chemical neuroscience that studies
                                                the molecular basis of intercellular activity applied to animals'
                                                nervous systems. This area of research covers molecular neuroanatomy,
                                                mechanisms of molecular signaling in the nervous system, and the
                                                molecular basis of neuroplasticity and neurodegenerative disease, which
                                                we will focus on here. Stress Mechanisms and Memory Consolidation
                                                Section Reference: Bisaz R, Travaglia A, Alberini CM. The
                                                neurobiological bases of memory formation: from physiological conditions
                                                to psychopathology. Psychopathology. 2014;47(6):347-56. doi:
                                                10.1159/000363702. Epub 2014 Oct 3. PMID: 25301080; PMCID: PMC4246028.
                                                $^{[1]}$ Molecular Consolidation via the GR-BDNF-TrkB Pathway
                                                Glucocorticoids BDNF TrkB Schematic representation of learning-induced
                                                changes in the hippocampus via the GR-BDNF-TrkB pathway: Image Source:
                                                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246028/ Here, we will look
                                                at learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway. Specifically, the relation between long-term memory
                                                consolidation, stress hormones like glucocorticoids, and glucocorticoid
                                                receptors in pre- and post-synaptic neurons. Where GRs mediate several
                                                intracellular signaling pathways required for memory consolidation.
                                                Activated GRs may influence traffic of TrkB to the membrane surface
                                                and/or BDNF release. The binding of BDNF to its receptor TrkB leads to
                                                its phosphorylation. Phosphorylation results in the activation of
                                                ERK1/2, Akt and PLC$\gamma$. Activated GRs affect this and at the same
                                                time affect transcription-dependent mechanisms. This parrelel activation
                                                rapidly leads to CaMKII$\alpha$ phosphorylation, similarily to the
                                                afforementiond activation of GRs. The activation of ERK1/2, Akt,
                                                PLC$\gamma$, and CaMKII$\alpha$ pathways independently or can converge
                                                in CREB phosphorylation. CREB phosphorylation then leads to the
                                                synthesis of BDNF. The newly synthesized BDNF sustains the activation of
                                                the pathways and results in persistent phosphorylation of CREB,
                                                CaMKII$\alpha$. The end of this side of the domino effect leads to the
                                                downstream pre-synaptic target of CaMKII$\alpha$, phosopho-synapsin-1.
                                                So, GR activation recruits pre- and post-synaptic mechanisms to mediate
                                                memory consolidation. Memory Reconsolidation Memory consolidation was
                                                thought of as a unitary process that transformed a fragile memory trace
                                                into a stable one. However, in research done in the 1960s and in the
                                                last 15 years have shown otherwise. Memories that have become
                                                insensitive to disruption from specific types of interference have been
                                                show that they can become transiently labile when those memories are
                                                recalled. This recalled memory must again become stable, which is termed
                                                memory reconsolidation. We will further examine and look at the answer
                                                provide by the authors with respect to memory reconsolidation. An
                                                important question that naturally arises is what function does memory
                                                reconsolidation serve? There are two main hypothese proposed, the first
                                                being that reconsolidation serves to strengthen memories. The reasoning
                                                is that the phase of temporary fragility of the memory mediates
                                                additional consolidation, thus, producing a stronger and longer lasting
                                                memory. The second hypothesis postulates that the function of the
                                                post-retrieval process allows for the association of new information
                                                into the memories of past events. The purpose of this being the
                                                intergration of new learning with already establised and reactivated
                                                memories. More succenctly, the former hypothesis posits the function of
                                                memory reconsolidation is to strengthen memorires, while the ladder
                                                suggests reconsolidation is for memory updating. The findings of the
                                                researchers in the paper are that reconsolidation strengthens memories
                                                and does not mediate in memory updating. Treatment Approaches via the
                                                GR-BDNF-TrkB pathway The authors mention that it may be possible to
                                                target the mechanisms of consolidation in order to block or reduce the
                                                formation of very intense, persistent memories, like those associated
                                                with PTSD. They mention further that more easily targeted mechanisms
                                                would be the reconsolidation of memory retrieval. Given a combination of
                                                behavioral or psychotherapeutical approaches combined with pharmacology,
                                                it is suggested in the paper that it might be possible to weaken
                                                traumatic memories, or through memory consolidation and reconsolidation,
                                                possinle to re-store the memory with different emotional valence and
                                                intensity. CaMKII$\alpha$, BDNF-CREB pathways and Glucocorticoid
                                                Receptors Section Reference: Glucocorticoid receptors recruit the
                                                CaMKII$\alpha$, BDNF-CREB pathways to mediate memory consolidation by
                                                Dillon Y. Chen, Dhananjay Bambah-Mukku, Gabriella Pollonini, and
                                                Cristina M. Alberin. From the Center for Neural Science, New York
                                                University and Department of Neuroscience and Friedman Brain Institute,
                                                Mount Sinai School of Medicine $^{[2]}$ To continue. Molecular Bases of
                                                Memory Formation Introduction Molecular neuroscience is an area of
                                                chemical neuroscience that studies the molecular basis of intercellular
                                                activity applied to animals' nervous systems. This area of research
                                                covers molecular neuroanatomy, mechanisms of molecular signaling in the
                                                nervous system, and the molecular basis of neuroplasticity and
                                                neurodegenerative disease, which we will focus on here. Stress
                                                Mechanisms and Memory Consolidation Section Reference: Bisaz R,
                                                Travaglia A, Alberini CM. The neurobiological bases of memory formation:
                                                from physiological conditions to psychopathology. Psychopathology.
                                                2014;47(6):347-56. doi: 10.1159/000363702. Epub 2014 Oct 3. PMID:
                                                25301080; PMCID: PMC4246028. $^{[1]}$ Molecular Consolidation via the
                                                GR-BDNF-TrkB Pathway Glucocorticoids BDNF TrkB Schematic representation
                                                of learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway: Image Source:
                                                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246028/ Here, we will look
                                                at learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway. Specifically, the relation between long-term memory
                                                consolidation, stress hormones like glucocorticoids, and glucocorticoid
                                                receptors in pre- and post-synaptic neurons. Where GRs mediate several
                                                intracellular signaling pathways required for memory consolidation.
                                                Activated GRs may influence traffic of TrkB to the membrane surface
                                                and/or BDNF release. The binding of BDNF to its receptor TrkB leads to
                                                its phosphorylation. Phosphorylation results in the activation of
                                                ERK1/2, Akt and PLC$\gamma$. Activated GRs affect this and at the same
                                                time affect transcription-dependent mechanisms. This parrelel activation
                                                rapidly leads to CaMKII$\alpha$ phosphorylation, similarily to the
                                                afforementiond activation of GRs. The activation of ERK1/2, Akt,
                                                PLC$\gamma$, and CaMKII$\alpha$ pathways independently or can converge
                                                in CREB phosphorylation. CREB phosphorylation then leads to the
                                                synthesis of BDNF. The newly synthesized BDNF sustains the activation of
                                                the pathways and results in persistent phosphorylation of CREB,
                                                CaMKII$\alpha$. The end of this side of the domino effect leads to the
                                                downstream pre-synaptic target of CaMKII$\alpha$, phosopho-synapsin-1.
                                                So, GR activation recruits pre- and post-synaptic mechanisms to mediate
                                                memory consolidation. Memory Reconsolidation Memory consolidation was
                                                thought of as a unitary process that transformed a fragile memory trace
                                                into a stable one. However, in research done in the 1960s and in the
                                                last 15 years have shown otherwise. Memories that have become
                                                insensitive to disruption from specific types of interference have been
                                                show that they can become transiently labile when those memories are
                                                recalled. This recalled memory must again become stable, which is termed
                                                memory reconsolidation. We will further examine and look at the answer
                                                provide by the authors with respect to memory reconsolidation. An
                                                important question that naturally arises is what function does memory
                                                reconsolidation serve? There are two main hypothese proposed, the first
                                                being that reconsolidation serves to strengthen memories. The reasoning
                                                is that the phase of temporary fragility of the memory mediates
                                                additional consolidation, thus, producing a stronger and longer lasting
                                                memory. The second hypothesis postulates that the function of the
                                                post-retrieval process allows for the association of new information
                                                into the memories of past events. The purpose of this being the
                                                intergration of new learning with already establised and reactivated
                                                memories. More succenctly, the former hypothesis posits the function of
                                                memory reconsolidation is to strengthen memorires, while the ladder
                                                suggests reconsolidation is for memory updating. The findings of the
                                                researchers in the paper are that reconsolidation strengthens memories
                                                and does not mediate in memory updating. Treatment Approaches via the
                                                GR-BDNF-TrkB pathway The authors mention that it may be possible to
                                                target the mechanisms of consolidation in order to block or reduce the
                                                formation of very intense, persistent memories, like those associated
                                                with PTSD. They mention further that more easily targeted mechanisms
                                                would be the reconsolidation of memory retrieval. Given a combination of
                                                behavioral or psychotherapeutical approaches combined with pharmacology,
                                                it is suggested in the paper that it might be possible to weaken
                                                traumatic memories, or through memory consolidation and reconsolidation,
                                                possinle to re-store the memory with different emotional valence and
                                                intensity. CaMKII$\alpha$, BDNF-CREB pathways and Glucocorticoid
                                                Receptors Section Reference: Glucocorticoid receptors recruit the
                                                CaMKII$\alpha$, BDNF-CREB pathways to mediate memory consolidation by
                                                Dillon Y. Chen, Dhananjay Bambah-Mukku, Gabriella Pollonini, and
                                                Cristina M. Alberin. From the Center for Neural Science, New York
                                                University and Department of Neuroscience and Friedman Brain Institute,
                                                Mount Sinai School of Medicine $^{[2]}$ To continue. Molecular Bases of
                                                Memory Formation Introduction Molecular neuroscience is an area of
                                                chemical neuroscience that studies the molecular basis of intercellular
                                                activity applied to animals' nervous systems. This area of research
                                                covers molecular neuroanatomy, mechanisms of molecular signaling in the
                                                nervous system, and the molecular basis of neuroplasticity and
                                                neurodegenerative disease, which we will focus on here. Stress
                                                Mechanisms and Memory Consolidation Section Reference: Bisaz R,
                                                Travaglia A, Alberini CM. The neurobiological bases of memory formation:
                                                from physiological conditions to psychopathology. Psychopathology.
                                                2014;47(6):347-56. doi: 10.1159/000363702. Epub 2014 Oct 3. PMID:
                                                25301080; PMCID: PMC4246028. $^{[1]}$ Molecular Consolidation via the
                                                GR-BDNF-TrkB Pathway Glucocorticoids BDNF TrkB Schematic representation
                                                of learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway: Image Source:
                                                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246028/ Here, we will look
                                                at learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway. Specifically, the relation between long-term memory
                                                consolidation, stress hormones like glucocorticoids, and glucocorticoid
                                                receptors in pre- and post-synaptic neurons. Where GRs mediate several
                                                intracellular signaling pathways required for memory consolidation.
                                                Activated GRs may influence traffic of TrkB to the membrane surface
                                                and/or BDNF release. The binding of BDNF to its receptor TrkB leads to
                                                its phosphorylation. Phosphorylation results in the activation of
                                                ERK1/2, Akt and PLC$\gamma$. Activated GRs affect this and at the same
                                                time affect transcription-dependent mechanisms. This parrelel activation
                                                rapidly leads to CaMKII$\alpha$ phosphorylation, similarily to the
                                                afforementiond activation of GRs. The activation of ERK1/2, Akt,
                                                PLC$\gamma$, and CaMKII$\alpha$ pathways independently or can converge
                                                in CREB phosphorylation. CREB phosphorylation then leads to the
                                                synthesis of BDNF. The newly synthesized BDNF sustains the activation of
                                                the pathways and results in persistent phosphorylation of CREB,
                                                CaMKII$\alpha$. The end of this side of the domino effect leads to the
                                                downstream pre-synaptic target of CaMKII$\alpha$, phosopho-synapsin-1.
                                                So, GR activation recruits pre- and post-synaptic mechanisms to mediate
                                                memory consolidation. Memory Reconsolidation Memory consolidation was
                                                thought of as a unitary process that transformed a fragile memory trace
                                                into a stable one. However, in research done in the 1960s and in the
                                                last 15 years have shown otherwise. Memories that have become
                                                insensitive to disruption from specific types of interference have been
                                                show that they can become transiently labile when those memories are
                                                recalled. This recalled memory must again become stable, which is termed
                                                memory reconsolidation. We will further examine and look at the answer
                                                provide by the authors with respect to memory reconsolidation. An
                                                important question that naturally arises is what function does memory
                                                reconsolidation serve? There are two main hypothese proposed, the first
                                                being that reconsolidation serves to strengthen memories. The reasoning
                                                is that the phase of temporary fragility of the memory mediates
                                                additional consolidation, thus, producing a stronger and longer lasting
                                                memory. The second hypothesis postulates that the function of the
                                                post-retrieval process allows for the association of new information
                                                into the memories of past events. The purpose of this being the
                                                intergration of new learning with already establised and reactivated
                                                memories. More succenctly, the former hypothesis posits the function of
                                                memory reconsolidation is to strengthen memorires, while the ladder
                                                suggests reconsolidation is for memory updating. The findings of the
                                                researchers in the paper are that reconsolidation strengthens memories
                                                and does not mediate in memory updating. Treatment Approaches via the
                                                GR-BDNF-TrkB pathway The authors mention that it may be possible to
                                                target the mechanisms of consolidation in order to block or reduce the
                                                formation of very intense, persistent memories, like those associated
                                                with PTSD. They mention further that more easily targeted mechanisms
                                                would be the reconsolidation of memory retrieval. Given a combination of
                                                behavioral or psychotherapeutical approaches combined with pharmacology,
                                                it is suggested in the paper that it might be possible to weaken
                                                traumatic memories, or through memory consolidation and reconsolidation,
                                                possinle to re-store the memory with different emotional valence and
                                                intensity. CaMKII$\alpha$, BDNF-CREB pathways and Glucocorticoid
                                                Receptors Section Reference: Glucocorticoid receptors recruit the
                                                CaMKII$\alpha$, BDNF-CREB pathways to mediate memory consolidation by
                                                Dillon Y. Chen, Dhananjay Bambah-Mukku, Gabriella Pollonini, and
                                                Cristina M. Alberin. From the Center for Neural Science, New York
                                                University and Department of Neuroscience and Friedman Brain Institute,
                                                Mount Sinai School of Medicine $^{[2]}$ To continue. Molecular Bases of
                                                Memory FormationIntroduction Molecular neuroscience is an area of
                                                chemical neuroscience that studies the molecular basis of intercellular
                                                activity applied to animals' nervous systems. This area of research
                                                covers molecular neuroanatomy, mechanisms of molecular signaling in the
                                                nervous system, and the molecular basis of neuroplasticity and
                                                neurodegenerative disease, which we will focus on here.
                                                IntroductionMolecular neuroscience is an area of chemical neuroscience
                                                that studies the molecular basis of intercellular activity applied to
                                                animals' nervous systems. This area of research covers molecular
                                                neuroanatomy, mechanisms of molecular signaling in the nervous system,
                                                and the molecular basis of neuroplasticity and neurodegenerative
                                                disease, which we will focus on here. Stress Mechanisms and Memory
                                                Consolidation Section Reference: Bisaz R, Travaglia A, Alberini CM. The
                                                neurobiological bases of memory formation: from physiological conditions
                                                to psychopathology. Psychopathology. 2014;47(6):347-56. doi:
                                                10.1159/000363702. Epub 2014 Oct 3. PMID: 25301080; PMCID: PMC4246028.
                                                $^{[1]}$ Molecular Consolidation via the GR-BDNF-TrkB Pathway
                                                Glucocorticoids BDNF TrkB Schematic representation of learning-induced
                                                changes in the hippocampus via the GR-BDNF-TrkB pathway: Image Source:
                                                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246028/ Here, we will look
                                                at learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway. Specifically, the relation between long-term memory
                                                consolidation, stress hormones like glucocorticoids, and glucocorticoid
                                                receptors in pre- and post-synaptic neurons. Where GRs mediate several
                                                intracellular signaling pathways required for memory consolidation.
                                                Activated GRs may influence traffic of TrkB to the membrane surface
                                                and/or BDNF release. The binding of BDNF to its receptor TrkB leads to
                                                its phosphorylation. Phosphorylation results in the activation of
                                                ERK1/2, Akt and PLC$\gamma$. Activated GRs affect this and at the same
                                                time affect transcription-dependent mechanisms. This parrelel activation
                                                rapidly leads to CaMKII$\alpha$ phosphorylation, similarily to the
                                                afforementiond activation of GRs. The activation of ERK1/2, Akt,
                                                PLC$\gamma$, and CaMKII$\alpha$ pathways independently or can converge
                                                in CREB phosphorylation. CREB phosphorylation then leads to the
                                                synthesis of BDNF. The newly synthesized BDNF sustains the activation of
                                                the pathways and results in persistent phosphorylation of CREB,
                                                CaMKII$\alpha$. The end of this side of the domino effect leads to the
                                                downstream pre-synaptic target of CaMKII$\alpha$, phosopho-synapsin-1.
                                                So, GR activation recruits pre- and post-synaptic mechanisms to mediate
                                                memory consolidation. Memory Reconsolidation Memory consolidation was
                                                thought of as a unitary process that transformed a fragile memory trace
                                                into a stable one. However, in research done in the 1960s and in the
                                                last 15 years have shown otherwise. Memories that have become
                                                insensitive to disruption from specific types of interference have been
                                                show that they can become transiently labile when those memories are
                                                recalled. This recalled memory must again become stable, which is termed
                                                memory reconsolidation. We will further examine and look at the answer
                                                provide by the authors with respect to memory reconsolidation. An
                                                important question that naturally arises is what function does memory
                                                reconsolidation serve? There are two main hypothese proposed, the first
                                                being that reconsolidation serves to strengthen memories. The reasoning
                                                is that the phase of temporary fragility of the memory mediates
                                                additional consolidation, thus, producing a stronger and longer lasting
                                                memory. The second hypothesis postulates that the function of the
                                                post-retrieval process allows for the association of new information
                                                into the memories of past events. The purpose of this being the
                                                intergration of new learning with already establised and reactivated
                                                memories. More succenctly, the former hypothesis posits the function of
                                                memory reconsolidation is to strengthen memorires, while the ladder
                                                suggests reconsolidation is for memory updating. The findings of the
                                                researchers in the paper are that reconsolidation strengthens memories
                                                and does not mediate in memory updating. Treatment Approaches via the
                                                GR-BDNF-TrkB pathway The authors mention that it may be possible to
                                                target the mechanisms of consolidation in order to block or reduce the
                                                formation of very intense, persistent memories, like those associated
                                                with PTSD. They mention further that more easily targeted mechanisms
                                                would be the reconsolidation of memory retrieval. Given a combination of
                                                behavioral or psychotherapeutical approaches combined with pharmacology,
                                                it is suggested in the paper that it might be possible to weaken
                                                traumatic memories, or through memory consolidation and reconsolidation,
                                                possinle to re-store the memory with different emotional valence and
                                                intensity. Stress Mechanisms and Memory ConsolidationSection Reference:
                                                Bisaz R, Travaglia A, Alberini CM. The neurobiological bases of memory
                                                formation: from physiological conditions to psychopathology.
                                                Psychopathology. 2014;47(6):347-56. doi: 10.1159/000363702. Epub 2014
                                                Oct 3. PMID: 25301080; PMCID: PMC4246028. $^{[1]}$ Bisaz R, Travaglia A,
                                                Alberini CM. The neurobiological bases of memory formation: from
                                                physiological conditions to psychopathology. Psychopathology.
                                                2014;47(6):347-56. doi: 10.1159/000363702. Epub 2014 Oct 3. PMID:
                                                25301080; PMCID: PMC4246028. $^{[1]}$ Bisaz R, Travaglia A, Alberini CM.
                                                The neurobiological bases of memory formation: from physiological
                                                conditions to psychopathology. Psychopathology. 2014;47(6):347-56. doi:
                                                10.1159/000363702. Epub 2014 Oct 3. PMID: 25301080; PMCID: PMC4246028.
                                                Molecular Consolidation via the GR-BDNF-TrkB Pathway Glucocorticoids
                                                BDNF TrkB Schematic representation of learning-induced changes in the
                                                hippocampus via the GR-BDNF-TrkB pathway: Image Source:
                                                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246028/ Here, we will look
                                                at learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway. Specifically, the relation between long-term memory
                                                consolidation, stress hormones like glucocorticoids, and glucocorticoid
                                                receptors in pre- and post-synaptic neurons. Where GRs mediate several
                                                intracellular signaling pathways required for memory consolidation.
                                                Activated GRs may influence traffic of TrkB to the membrane surface
                                                and/or BDNF release. The binding of BDNF to its receptor TrkB leads to
                                                its phosphorylation. Phosphorylation results in the activation of
                                                ERK1/2, Akt and PLC$\gamma$. Activated GRs affect this and at the same
                                                time affect transcription-dependent mechanisms. This parrelel activation
                                                rapidly leads to CaMKII$\alpha$ phosphorylation, similarily to the
                                                afforementiond activation of GRs. The activation of ERK1/2, Akt,
                                                PLC$\gamma$, and CaMKII$\alpha$ pathways independently or can converge
                                                in CREB phosphorylation. CREB phosphorylation then leads to the
                                                synthesis of BDNF. The newly synthesized BDNF sustains the activation of
                                                the pathways and results in persistent phosphorylation of CREB,
                                                CaMKII$\alpha$. The end of this side of the domino effect leads to the
                                                downstream pre-synaptic target of CaMKII$\alpha$, phosopho-synapsin-1.
                                                So, GR activation recruits pre- and post-synaptic mechanisms to mediate
                                                memory consolidation. Molecular Consolidation via the GR-BDNF-TrkB
                                                PathwayGlucocorticoids BDNF TrkB GlucocorticoidsBDNFTrkBSchematic
                                                representation of learning-induced changes in the hippocampus via the
                                                GR-BDNF-TrkB pathway: Image Source:
                                                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246028/
                                                https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4246028/Here, we will look
                                                at learning-induced changes in the hippocampus via the GR-BDNF-TrkB
                                                pathway. Specifically, the relation between long-term memory
                                                consolidation, stress hormones like glucocorticoids, and glucocorticoid
                                                receptors in pre- and post-synaptic neurons. Where GRs mediate several
                                                intracellular signaling pathways required for memory consolidation.
                                                Activated GRs may influence traffic of TrkB to the membrane surface
                                                and/or BDNF release. The binding of BDNF to its receptor TrkB leads to
                                                its phosphorylation. Phosphorylation results in the activation of
                                                ERK1/2, Akt and PLC$\gamma$. Activated GRs affect this and at the same
                                                time affect transcription-dependent mechanisms. This parrelel activation
                                                rapidly leads to CaMKII$\alpha$ phosphorylation, similarily to the
                                                afforementiond activation of GRs. The activation of ERK1/2, Akt,
                                                PLC$\gamma$, and CaMKII$\alpha$ pathways independently or can converge
                                                in CREB phosphorylation. CREB phosphorylation then leads to the
                                                synthesis of BDNF. The newly synthesized BDNF sustains the activation of
                                                the pathways and results in persistent phosphorylation of CREB,
                                                CaMKII$\alpha$. The end of this side of the domino effect leads to the
                                                downstream pre-synaptic target of CaMKII$\alpha$, phosopho-synapsin-1.
                                                So, GR activation recruits pre- and post-synaptic mechanisms to mediate
                                                memory consolidation. Memory Reconsolidation Memory consolidation was
                                                thought of as a unitary process that transformed a fragile memory trace
                                                into a stable one. However, in research done in the 1960s and in the
                                                last 15 years have shown otherwise. Memories that have become
                                                insensitive to disruption from specific types of interference have been
                                                show that they can become transiently labile when those memories are
                                                recalled. This recalled memory must again become stable, which is termed
                                                memory reconsolidation. We will further examine and look at the answer
                                                provide by the authors with respect to memory reconsolidation. An
                                                important question that naturally arises is what function does memory
                                                reconsolidation serve? There are two main hypothese proposed, the first
                                                being that reconsolidation serves to strengthen memories. The reasoning
                                                is that the phase of temporary fragility of the memory mediates
                                                additional consolidation, thus, producing a stronger and longer lasting
                                                memory. The second hypothesis postulates that the function of the
                                                post-retrieval process allows for the association of new information
                                                into the memories of past events. The purpose of this being the
                                                intergration of new learning with already establised and reactivated
                                                memories. More succenctly, the former hypothesis posits the function of
                                                memory reconsolidation is to strengthen memorires, while the ladder
                                                suggests reconsolidation is for memory updating. The findings of the
                                                researchers in the paper are that reconsolidation strengthens memories
                                                and does not mediate in memory updating. Memory ReconsolidationMemory
                                                consolidation was thought of as a unitary process that transformed a
                                                fragile memory trace into a stable one. However, in research done in the
                                                1960s and in the last 15 years have shown otherwise. Memories that have
                                                become insensitive to disruption from specific types of interference
                                                have been show that they can become transiently labile when those
                                                memories are recalled. This recalled memory must again become stable,
                                                which is termed memory reconsolidation. We will further examine and look
                                                at the answer provide by the authors with respect to memory
                                                reconsolidation. An important question that naturally arises is what
                                                function does memory reconsolidation serve? There are two main hypothese
                                                proposed, the first being that reconsolidation serves to strengthen
                                                memories. The reasoning is that the phase of temporary fragility of the
                                                memory mediates additional consolidation, thus, producing a stronger and
                                                longer lasting memory. The second hypothesis postulates that the
                                                function of the post-retrieval process allows for the association of new
                                                information into the memories of past events. The purpose of this being
                                                the intergration of new learning with already establised and reactivated
                                                memories. More succenctly, the former hypothesis posits the function of
                                                memory reconsolidation is to strengthen memorires, while the ladder
                                                suggests reconsolidation is for memory updating. The findings of the
                                                researchers in the paper are that reconsolidation strengthens memories
                                                and does not mediate in memory updating. Treatment Approaches via the
                                                GR-BDNF-TrkB pathway The authors mention that it may be possible to
                                                target the mechanisms of consolidation in order to block or reduce the
                                                formation of very intense, persistent memories, like those associated
                                                with PTSD. They mention further that more easily targeted mechanisms
                                                would be the reconsolidation of memory retrieval. Given a combination of
                                                behavioral or psychotherapeutical approaches combined with pharmacology,
                                                it is suggested in the paper that it might be possible to weaken
                                                traumatic memories, or through memory consolidation and reconsolidation,
                                                possinle to re-store the memory with different emotional valence and
                                                intensity. Treatment Approaches via the GR-BDNF-TrkB pathwayThe authors
                                                mention that it may be possible to target the mechanisms of
                                                consolidation in order to block or reduce the formation of very intense,
                                                persistent memories, like those associated with PTSD. They mention
                                                further that more easily targeted mechanisms would be the
                                                reconsolidation of memory retrieval. Given a combination of behavioral
                                                or psychotherapeutical approaches combined with pharmacology, it is
                                                suggested in the paper that it might be possible to weaken traumatic
                                                memories, or through memory consolidation and reconsolidation, possinle
                                                to re-store the memory with different emotional valence and intensity.
                                                CaMKII$\alpha$, BDNF-CREB pathways and Glucocorticoid Receptors Section
                                                Reference: Glucocorticoid receptors recruit the CaMKII$\alpha$,
                                                BDNF-CREB pathways to mediate memory consolidation by Dillon Y. Chen,
                                                Dhananjay Bambah-Mukku, Gabriella Pollonini, and Cristina M. Alberin.
                                                From the Center for Neural Science, New York University and Department
                                                of Neuroscience and Friedman Brain Institute, Mount Sinai School of
                                                Medicine $^{[2]}$ CaMKII$\alpha$, BDNF-CREB pathways and Glucocorticoid
                                                Receptors Section Reference: Glucocorticoid receptors recruit the
                                                CaMKII$\alpha$, BDNF-CREB pathways to mediate memory consolidation by
                                                Dillon Y. Chen, Dhananjay Bambah-Mukku, Gabriella Pollonini, and
                                                Cristina M. Alberin. From the Center for Neural Science, New York
                                                University and Department of Neuroscience and Friedman Brain Institute,
                                                Mount Sinai School of Medicine $^{[2]}$ Glucocorticoid receptors recruit
                                                the CaMKII$\alpha$, BDNF-CREB pathways to mediate memory consolidation
                                                by Dillon Y. Chen, Dhananjay Bambah-Mukku, Gabriella Pollonini, and
                                                Cristina M. Alberin. From the Center for Neural Science, New York
                                                University and Department of Neuroscience and Friedman Brain Institute,
                                                Mount Sinai School of Medicine $^{[2]}$ Glucocorticoid receptors recruit
                                                the CaMKII$\alpha$, BDNF-CREB pathways to mediate memory consolidation
                                                by Dillon Y. Chen, Dhananjay Bambah-Mukku, Gabriella Pollonini, and
                                                Cristina M. Alberin. From the Center for Neural Science, New York
                                                University and Department of Neuroscience and Friedman Brain Institute,
                                                Mount Sinai School of Medicine To continue.
                                                [https://www.contextswitching.org/misc]
                                                Miscellaneous - Context Switching Miscellaneous Epigenetics and
                                                Inheritance In biology, epigenetics is the study of mitotically and/or
                                                meiotically heritable changes in gene function that cannot be explained
                                                by changes to the DNA sequence. Epigenetics normally involves change
                                                that is not erased by cell division and that also affects the regulation
                                                of gene expression. Epigenetics reflects our understanding... read more
                                                Bioinformatics and Functional Genomics Bioinformatics is a growing
                                                revolution in the field of molecular biology and computers. Here, our
                                                emphasis will be on employing bioinformatics tools and biological
                                                databases to address challenges from current issues in biological,
                                                biotechnological, and biomedical research. Such as looking at
                                                computational algorithms and computer databases to analyze proteins,
                                                genes... read more Deontology, Consequentialism, Virtue Ethics Virtue
                                                Ethics is a branch of one of three major approaches to normative ethics,
                                                where normative ethics, at the risk of oversimplification, is concerned
                                                with criteria for what is right and wrong. The three main philosophical
                                                ideologies concerning normative ethics are the following. Virtue ethics,
                                                which can be identified from... read more ... And More Soon!
                                                Miscellaneous - Context Switching Miscellaneous - Context
                                                SwitchingMiscellaneous Epigenetics and Inheritance In biology,
                                                epigenetics is the study of mitotically and/or meiotically heritable
                                                changes in gene function that cannot be explained by changes to the DNA
                                                sequence. Epigenetics normally involves change that is not erased by
                                                cell division and that also affects the regulation of gene expression.
                                                Epigenetics reflects our understanding... read more Bioinformatics and
                                                Functional Genomics Bioinformatics is a growing revolution in the field
                                                of molecular biology and computers. Here, our emphasis will be on
                                                employing bioinformatics tools and biological databases to address
                                                challenges from current issues in biological, biotechnological, and
                                                biomedical research. Such as looking at computational algorithms and
                                                computer databases to analyze proteins, genes... read more Deontology,
                                                Consequentialism, Virtue Ethics Virtue Ethics is a branch of one of
                                                three major approaches to normative ethics, where normative ethics, at
                                                the risk of oversimplification, is concerned with criteria for what is
                                                right and wrong. The three main philosophical ideologies concerning
                                                normative ethics are the following. Virtue ethics, which can be
                                                identified from... read more ... And More Soon! Miscellaneous
                                                Miscellaneous Miscellaneous Epigenetics and Inheritance Epigenetics and
                                                Inheritance Epigenetics and Inheritance In biology, epigenetics is the
                                                study of mitotically and/or meiotically heritable changes in gene
                                                function that cannot be explained by changes to the DNA sequence.
                                                Epigenetics normally involves change that is not erased by cell division
                                                and that also affects the regulation of gene expression. Epigenetics
                                                reflects our understanding... read more In biology, epigenetics is the
                                                study of mitotically and/or meiotically heritable changes in gene
                                                function that cannot be explained by changes to the DNA sequence.
                                                Epigenetics normally involves change that is not erased by cell division
                                                and that also affects the regulation of gene expression. Epigenetics
                                                reflects our understanding... read more read more Bioinformatics and
                                                Functional Genomics Bioinformatics and Functional Genomics
                                                Bioinformatics and Functional Genomics Bioinformatics is a growing
                                                revolution in the field of molecular biology and computers. Here, our
                                                emphasis will be on employing bioinformatics tools and biological
                                                databases to address challenges from current issues in biological,
                                                biotechnological, and biomedical research. Such as looking at
                                                computational algorithms and computer databases to analyze proteins,
                                                genes... read more Bioinformatics is a growing revolution in the field
                                                of molecular biology and computers. Here, our emphasis will be on
                                                employing bioinformatics tools and biological databases to address
                                                challenges from current issues in biological, biotechnological, and
                                                biomedical research. Such as looking at computational algorithms and
                                                computer databases to analyze proteins, genes... read more
                                                Bioinformatics is a growing revolution in the field of molecular biology
                                                and computers. Here, our emphasis will be on employing bioinformatics
                                                tools and biological databases to address challenges from current issues
                                                in biological, biotechnological, and biomedical research. Such as
                                                looking at computational algorithms and computer databases to analyze
                                                proteins, genes... read more read more Deontology, Consequentialism,
                                                Virtue Ethics Deontology, Consequentialism, Virtue Ethics Deontology,
                                                Consequentialism, Virtue Ethics Virtue Ethics is a branch of one of
                                                three major approaches to normative ethics, where normative ethics, at
                                                the risk of oversimplification, is concerned with criteria for what is
                                                right and wrong. The three main philosophical ideologies concerning
                                                normative ethics are the following. Virtue ethics, which can be
                                                identified from... read more Virtue Ethics is a branch of one of three
                                                major approaches to normative ethics, where normative ethics, at the
                                                risk of oversimplification, is concerned with criteria for what is right
                                                and wrong. The three main philosophical ideologies concerning normative
                                                ethics are the following. Virtue ethics, which can be identified from...
                                                read more Virtue Ethics is a branch of one of three major approaches to
                                                normative ethics, where normative ethics, at the risk of
                                                oversimplification, is concerned with criteria for what is right and
                                                wrong. The three main philosophical ideologies concerning normative
                                                ethics are the following. Virtue ethics, which can be identified from...
                                                read more read more ... And More Soon! ... And More Soon! ... And More
                                                Soon!
                                                [https://www.contextswitching.org/phys]
                                                Physics Context Switching Physics Quantum Fields in Anti-de Sitter Space
                                                and the Maldacena Conjecture In theoretical physics, the Maldacena
                                                Conjecture states supergravity and string theory on the product of
                                                $(n+1)$-dimensional Anti-de Sitter space with a compact manifold capable
                                                of describing large $N$ limits of conformal field theories in
                                                $d$-dimensions. Correlation functions in CFT are dependent on the
                                                supergravity action of asymptotic behavior at infinity... read more
                                                Compatification and Massless Scattering in Anti-de Sitter Space In
                                                theoretical physics, Minkowski Space is a particular type of
                                                $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                                Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                                $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d
                                                x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of
                                                the study of spacetime within special relativity and is... read more
                                                Quantum Mechanics First we will introduce the idea of Hilbert Space,
                                                which was named after D. Hilbert. Hilbert Space is a nondenumerable
                                                infinite complex vector space. Complex space, being a collection of
                                                complex numbers with an added structure. The infinite dimensions of
                                                Hilbert Space represents a continious spectra of alternative physical
                                                states... read more ... And More Soon! Physics Context Switching Physics
                                                Context SwitchingPhysics Quantum Fields in Anti-de Sitter Space and the
                                                Maldacena Conjecture In theoretical physics, the Maldacena Conjecture
                                                states supergravity and string theory on the product of
                                                $(n+1)$-dimensional Anti-de Sitter space with a compact manifold capable
                                                of describing large $N$ limits of conformal field theories in
                                                $d$-dimensions. Correlation functions in CFT are dependent on the
                                                supergravity action of asymptotic behavior at infinity... read more
                                                Compatification and Massless Scattering in Anti-de Sitter Space In
                                                theoretical physics, Minkowski Space is a particular type of
                                                $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                                Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                                $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d
                                                x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of
                                                the study of spacetime within special relativity and is... read more
                                                Quantum Mechanics First we will introduce the idea of Hilbert Space,
                                                which was named after D. Hilbert. Hilbert Space is a nondenumerable
                                                infinite complex vector space. Complex space, being a collection of
                                                complex numbers with an added structure. The infinite dimensions of
                                                Hilbert Space represents a continious spectra of alternative physical
                                                states... read more ... And More Soon! Physics Physics Physics Quantum
                                                Fields in Anti-de Sitter Space and the Maldacena Conjecture Quantum
                                                Fields in Anti-de Sitter Space and the Maldacena Conjecture Quantum
                                                Fields in Anti-de Sitter Space and the Maldacena Conjecture Quantum
                                                Fields in Anti-de Sitter Space and the Maldacena Conjecture In
                                                theoretical physics, the Maldacena Conjecture states supergravity and
                                                string theory on the product of $(n+1)$-dimensional Anti-de Sitter space
                                                with a compact manifold capable of describing large $N$ limits of
                                                conformal field theories in $d$-dimensions. Correlation functions in CFT
                                                are dependent on the supergravity action of asymptotic behavior at
                                                infinity... read more In theoretical physics, the Maldacena Conjecture
                                                states supergravity and string theory on the product of
                                                $(n+1)$-dimensional Anti-de Sitter space with a compact manifold capable
                                                of describing large $N$ limits of conformal field theories in
                                                $d$-dimensions. Correlation functions in CFT are dependent on the
                                                supergravity action of asymptotic behavior at infinity... read more read
                                                more Compatification and Massless Scattering in Anti-de Sitter Space
                                                Compatification and Massless Scattering in Anti-de Sitter Space
                                                Compatification and Massless Scattering in Anti-de Sitter Space In
                                                theoretical physics, Minkowski Space is a particular type of
                                                $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                                Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                                $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d
                                                x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of
                                                the study of spacetime within special relativity and is... read more In
                                                theoretical physics, Minkowski Space is a particular type of
                                                $4$-dimensional Lorentzian space, with a Minkowski metric. Where the
                                                Minkowski metric is a metric tensor denoted as $d\tau^2$ with the form
                                                $-\left(d^0\right)^2+\left(d x^1\right)^2$ $+\;\left(d
                                                x^2\right)^2+\left(d x^3\right)^2$. Minkowski space forms the basis of
                                                the study of spacetime within special relativity and is... read more
                                                read more Quantum Mechanics Quantum Mechanics Quantum Mechanics First we
                                                will introduce the idea of Hilbert Space, which was named after D.
                                                Hilbert. Hilbert Space is a nondenumerable infinite complex vector
                                                space. Complex space, being a collection of complex numbers with an
                                                added structure. The infinite dimensions of Hilbert Space represents a
                                                continious spectra of alternative physical states... read more First we
                                                will introduce the idea of Hilbert Space, which was named after D.
                                                Hilbert. Hilbert Space is a nondenumerable infinite complex vector
                                                space. Complex space, being a collection of complex numbers with an
                                                added structure. The infinite dimensions of Hilbert Space represents a
                                                continious spectra of alternative physical states... read more First we
                                                will introduce the idea of Hilbert Space, which was named after D.
                                                Hilbert. Hilbert Space is a nondenumerable infinite complex vector
                                                space. Complex space, being a collection of complex numbers with an
                                                added structure. The infinite dimensions of Hilbert Space represents a
                                                continious spectra of alternative physical states... read more read more
                                                ... And More Soon! ... And More Soon! ... And More Soon!
                                                [https://www.contextswitching.org/tcs/quantumsvm]
                                                Quantum Support Vector Machine - Context Switching Quantum Support
                                                Vector Machine Page Reference: Supervised Learning with Quantum Enhanced
                                                Feature Spaces by: Vojtech Havlicek,* Antonio D. Corcoles, Kristan
                                                Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M.
                                                Gambetta. IBM T.J. Watson Research Center and Center for Theoretical
                                                Physics, Massachusetts Institute of Technology (Dated: June 7, 2018)
                                                $^{[1]}$ My code for detecting credit card fraud using a Quantum Support
                                                Vector Machine: QuantumSVMFraudDetection Introduction Using quantum
                                                computing, the authors exploit quantum mechanics for the algorithmic
                                                complexity optimization of a Support Vector Machine with
                                                high-dimensional feature space. Where the high-dimensional classical
                                                data is mapped non-linearly to Hilbert Space and a hyperplane in quantum
                                                space is used to separate and label the data. By using the quantum state
                                                space as the feature space, the authors hope to obtain a quantum
                                                advantage. Let's consider we have data from a training set $T$ and a
                                                test set $S$, where $T, S$ are subsets $\Omega\subset\mathbb{R}^{d}$.
                                                For each training set, we assume the data is labeled by a true map $m: T
                                                \cup S \rightarrow\left\{+1, -1\right\}$, which is not given to the
                                                algorithm. The algorithm is given training data, where the goal is for
                                                the algorithm to infer an approximate mappping $\tilde{m}:
                                                T\rightarrow\{+1,-1\}$ for the data. After training, the algorithm is
                                                asked to apply the approximate mapping on a test set $\tilde{m}:
                                                S\rightarrow\{+1,-1\}$. The approximate mapping needs to match the true
                                                map $m(\vec{s})=\tilde{m}(\vec{s})$ with a high probability. To use
                                                quantum state space as feature space a mapping is constructed such that
                                                the data is non-linearly mapped to a quantum state
                                                $\Phi:\vec{x}\in\Omega\rightarrow
                                                |\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. Where,
                                                $|\Phi(\vec{x})\rangle$ is a part of complex vector space or Hilbert
                                                Space $\mathcal{H}$ and $\langle\Phi(\vec{x})|$ is the complex conjugate
                                                that is a dual correspondance. We can see that the non-linear mapping
                                                $\Phi$ takes the classical data $\vec{x}\in\Omega\subset\mathbb{R}^{d}$
                                                or $\vec{x}\in T\cup S$ and reperesents it in quantum state space
                                                $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. To continue, we will look
                                                at the procedure behind quantum mapping, rather than its domain and
                                                codomain, training, and then testing. Quantum Feature Mapping The
                                                authors suggest that in order to have an advantage over classical
                                                approaches, a map based on circuts that are clasically hard to simulate
                                                needs to be constructed, but where the circut is not too deep to test on
                                                during experimentation. The authors provide a circut diagram that
                                                represents a unitary function for feature mapping $n$-qubits. The circut
                                                diagram is given as: Image Source: $^{[1]}$ Note, we will examine more
                                                in depth the feature mapping unitary function later on. Data Generation
                                                First, we will define the map used for the artifical data. The data is
                                                generated of dimension $n=d=2$ for a $2$-qubit system with the mapping
                                                $\phi_{\{i\}}(\vec{x})=x_{i}$ and
                                                $\phi_{\{1,2\}}(\vec{x})=(\pi-x_{1})(\pi-x_{2})$. Next, we look at how
                                                the data is generated. For the data vector labels $\vec{x}\in T\cup S
                                                \subset (0, 2\pi]^{2}$ the authors mention generating it using the
                                                parity function $\mathbf{f}=Z_{1}Z_{2}$ and random unitary $V\in SU(4)$.
                                                Next, to label the data the authors describe the following mapping.
                                                Given $\Delta=0.3$, if:
                                                $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\geq\Delta$
                                                then $m(\vec{x})=+1$. If:
                                                $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\leq-\Delta$
                                                then $m(\vec{x})=-1$. Proposed SVM Classifiers Quantum Variational
                                                Classification In the this section, we will deconstruct and analyze
                                                individual components of the circut, unitary, and steps to better
                                                understand how this processes works. Then, in the following section, we
                                                will look at classifier optimization, testing the trained model, and
                                                analyze the authors experimental findings. Procedure The authors have
                                                provided a figure of the quantum variational classification QVC circut
                                                and it is shown below: Image Source: $^{[1]}$ where $C=\{+1,-1\}$ and
                                                the varitional unitary for QVC as a whole is defined as:
                                                $p_{y}(\vec{x})$$=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                The authors describe the procedure for the QVC in the following four
                                                sequential steps: feature map encoding, variational optimization,
                                                measurement, and then error probability. The training and classification
                                                phase consist of these four main parts and is what we will analyze in
                                                the following. Feature Map Encoding Recall that the circut looks like
                                                the following: Image Source: $^{[1]}$ The data $\vec{x}\in\Omega$ is
                                                mapped to a reference state $|0\rangle^{n}$ using the feature map circut
                                                $\mathcal{U}_{\Phi(\vec{x})}$. This feature map is an injective encoding
                                                of classical information $\vec{x}\in\mathbb{R}^{d}$ to a quantum state
                                                $|\Phi(\vec{x})\rangle\langle(\vec{x})\Phi|$ that is on a $n$-qubit
                                                register such that $d=n$. A quibit is a two-level system of Hilbert
                                                space and can be represented as $\mathcal{H}_{2}=\mathbb{C}^{2}$. To
                                                represent $n$-qubits we denote it as $\mathcal{H}_{2}^{\otimes
                                                n}=\left(\mathbb{C}^2\right)^{\otimes n}$. The offered feature mapping
                                                portrayed above is a family of feature maps, that the authors selected
                                                because they conjecture it is hard to estimate overlap
                                                $|\langle\Phi(\vec{x}) \mid \Phi(\vec{y})\rangle|^2$ on a classical
                                                computer. The family of feature map circuits is defined as follows:
                                                $|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})} H^{\otimes n} U_{\Phi(\vec{x})}
                                                H^{\otimes n}|0\rangle^{\otimes n}$ where $H$ is a conventional Hadmard
                                                gate and $U_{\Phi(\vec{x})}$ is defined as:
                                                $\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in
                                                S}Z_{i}\right)$ and is a diagonal gate in the Pauli Z-basis. For a
                                                single qubit, the unitary function $U_{\Phi(x)}$ acts as a phase-gate
                                                $Z_{x}$ of angle $x\in\Omega$. Feature mapping allows for $2^{n}$
                                                possible coefficents $\phi_{S}\left(\vec{x}\right)$ for non-linear
                                                function of the inputed data $\vec{x}\in\mathbb{R}^{n}$. Variational
                                                Classification A short depth quantum circut $W(\vec{\theta})$ is applied
                                                to the feature state is a variational circut used for the optimization
                                                method. A depiction of this short depth quantum circut is given by the
                                                authors and shown in the figure below: Image Source: $[1]$ For this
                                                short depth quantum circut, the authors use an Ansatz variational
                                                unitary $W(\vec{\theta})$, defined as:
                                                $U_{loc}^{(l)}(\theta_{l})U_{ent}...$$U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})$
                                                Where, $U_{loc}^{(l)}(\theta_{l})$ is full layers single qubit rotations
                                                given as:
                                                $U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})$ and
                                                $U(\theta_{i,t})\in SU(2)$. For the variational unitary, $U_{ent}$ is an
                                                alternating layers of entangling gates and is defined as:
                                                $U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)$ Here, $E$ is an edge in the
                                                circut defined vetice set $\{v_{i}, v_{j}\}\in V$. $\mathbf{CZ}$ is a
                                                controlled phase gate applied along the edges $(i,j)\in E$ which the
                                                authors state is present in the connectivity of the superconducting
                                                chip. This variational circut is parametrized by $\vec{\theta
                                                }\in\mathbb{R}^{2n(l+1)}$ and it is what is optimized during training as
                                                this is what classifies the data. Measurment Given that the problem is a
                                                two label classification $y\in\{+1,-1\}$, the authors apply a binary
                                                measurment $\{M_{y}\}$ to the state
                                                $W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}$. The authors
                                                describe the binary measurment in the following way. The measurment is
                                                in the $Z$-basis, outputs the bit-string $z\in\{0,1\}^{n}$, where the
                                                bit-string is then passed to the boolean function
                                                $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The binary measurment is defined as
                                                $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$, where
                                                $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle z|$. From this
                                                third step, the probability for the outcome $y$ is obtained. This
                                                probability $p_{y}(\vec{x})$ is defined as:
                                                $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                Where the probability for measuring either label $y\in\{+1,-1\}$,
                                                denoted as $p_y$, is defined as:
                                                $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                W(\theta)\right| \Phi(\vec{x})\right\rangle\right)$ Since the expected
                                                value of the measured observable is
                                                $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                W^{\dagger}(\theta, \varphi)\right.$$\left.\mathbf{f} W(\theta,
                                                \varphi)\right]$, it can be expressed in terms of the inner product:
                                                $\frac{1}{2^n} \sum_\alpha w_\alpha(\theta) \Phi_\alpha(\vec{x})$ Next,
                                                we need to incorporate assigning the label $y\in\{+1,-1\}$ over the
                                                label $-y$ with a given fixed basis $b\in[-1,+1]$. In this case, it must
                                                be that $p_{-y}-yb<\ p_{y}$. Subsituting the probability for measuring
                                                    either label $y\in\{+1,-1\}$ in to the inner product of the measured
                                                    observable, we obtain the value for $\tilde{m}(\vec{x})$ as:
                                                    $\operatorname{sign}\left(\frac{1}{2^n} \sum_\alpha w_\alpha(\theta)
                                                    \Phi_\alpha(\vec{x})+b\right)$ For the final decision ruling of $y$,
                                                    $R$ repeated measurment shots are preformed, yielding the empircal
                                                    distribution $\hat{p}(\vec{x})$, where if $\hat{p}_{-y}(\vec{x}-yb)>
                                                    \hat{p}_{y}(\vec{x})$ the label assigned is $\tilde{m}(\vec{x})=y$.
                                                    The authors introduced a bias parameter $b\in[-1,1]$ that can also
                                                    be optimized during training. The authors mention that the feature
                                                    map circut $\mathcal{U}_{\Phi(\vec{x})}$ and the boolean function
                                                    $f$ are fixed choices. The parameters that are being optimized
                                                    during training are $(\vec{\theta},b)$ and in order to be optimized,
                                                    a cost-function is need to be defined. To do so, we need to define
                                                    an error probability first, which is what we will look at in the
                                                    next section. Error Probability In order to find the empirical risk
                                                    $R_{emp}(\vec{\theta})$ of the empirical distribution
                                                    $\hat{p}(\vec{x})$ we define the error probability of assigning the
                                                    incorrect labels averaged over the samples in the training set $T$.
                                                    The authors give the definition: $R_{emp}(\vec{\theta})$ $=$
                                                    $\frac{1}{|T|}\sum_{\vec{x}\in T}Pr(\tilde{m}(\vec{x})\neq
                                                    m(\vec{x}))$ that defines this error probability. For our binary
                                                    classification problem, the error probability of assigning the wrong
                                                    label to some given data can be clacilated using the binomial
                                                    cimulative density function CDF for the empircal distribution
                                                    $\hat{p}(\vec{x})$. For a large number of samples or shots $R\gg 1$
                                                    the CDF is approximated by a sigmoid function
                                                    $\operatorname{sig}(x)=(1+e^{-x})^{-1}$. The probability for label
                                                    $m(\vec{x}=y)$ being assigned incorrectly can be approximated by:
                                                    $Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                    $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                    b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                    \hat{p}_y(\vec{x})}}\right)$ Classifier Optimization and Testing
                                                    Results The first step is to train the classifier and optimize it.
                                                    The authors found that they found Spall's SPSA stochastic gradient
                                                    descent algorithm performs the best in the inherent noisy
                                                    experimental setting of quantum computing. They mentioned after the
                                                    parameters converged onto $\big(\vec{\Theta^{*}},b^{*}\big)$. The
                                                    second step is then to classify data and assign them labels
                                                    according to the decision rule given by $\tilde{m}(\vec{s})$, where
                                                    the binary measurement is obtained from the parity function
                                                    $\mathbf{f}=Z_1 Z_2$, and ensure that for the testing data,
                                                    $m(\vec{s})=\tilde{m}(\vec{s})$ with high probability for the given
                                                    data set $\vec{s}\in S$. The authors note that what is obeserved for
                                                    the empirical risk, or cost value used for the optimizer, is that it
                                                    converges to a lower depth when the number of layers in the short
                                                    depth quantum circuit for optimization is $l=4$ than $l=0$.
                                                    Interesting enough, error mitigation does not appreciably improve
                                                    the empirical risk results when depth is at $0$, however, does
                                                    substantially help for larger depths. An important note, is that
                                                    although $\operatorname{Pr}(\tilde{m}(\vec{x}) \neq m(\vec{x}))$
                                                    includes the number of shots $R$ taken in its calculation, during
                                                    experimentation the authors fixed $R=200$. The authors state the
                                                    reason for doing this, even though $R=2000$ in actual experiment,
                                                    was to avoid gradient problems. To continue, after training, comes
                                                    testing. The yielded trained set of parameters
                                                    $\big(\vec{\theta}^{*},b^*=0\big)$ where used to classify 20
                                                    different test sets randomly drawn each time per data set. After
                                                    analyzing the results, the authors an increasing classification
                                                    success with increasing circut depth $l$. Noting, that the success
                                                    rate very nearl reaches $100$% for circut depths larger than $1$ and
                                                    remains up to depth $4$, despite decoherence associated with $8$
                                                    CNOT gates during training and classification circuts. Quantum
                                                    Variational Classification Analysis An important aspect of SVM's, is
                                                    the classification of the data, which is done here based off of the
                                                    decision rule $p_y(\vec{x})>p_{-y}(\vec{x})-y b$. This decision rule
                                                    $\tilde{m}(\vec{x})$ can can be restated as $\operatorname{sign}
                                                    \big( \langle\Phi(\vec{x}) |W^{\dagger}(\vec{\theta})$$ \mathbf{f}
                                                    W(\vec{\theta}) | \Phi(\vec{x}) \rangle+b \big)$. The step that
                                                    structures the data in such a way for classification, is the
                                                    variational circut $W$, which seperates the data using hyperplane
                                                    $\vec{w}$ in quantum state space. A feature map of a single qubit,
                                                    along with the seperating hyperplane $\vec{w}$ and the invterval of
                                                    binary labels $\Omega=(0,2\pi]$ for the classical data, is given by
                                                    the authors below. Image Source: $[1]$ We start by decomposing the
                                                    quantum variational circut. First, we look to define the properties
                                                    for our group of matrices used. For us, we need a group that has an
                                                    orthogonal, hermatian, matrix basis $\left\{P_\alpha\right\} \subset
                                                    \mathbb{C}^{2^n \times 2^n}$. Such that $\alpha=1,...,4^{n}$ with
                                                    $\operatorname{tr}\left[P_\alpha^{\dagger} P_\beta\right]=2^n
                                                    \delta_{\alpha, \beta}$. A group of matrices adhereing to this
                                                    criteria is the Pauli-group on $n$-qubits. Next, we expand the
                                                    quantum state and the measurment in our choosen matrix basis. The
                                                    expanded quantum state, or expectation value, can be expressed in
                                                    terms of $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$ $\rightarrow$
                                                    $\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
                                                    \Phi(\vec{x})\right\rangle$. The decision rule expressed in our
                                                    choosen matrix basis as $w_\alpha(\vec{\theta})$ can be defined as
                                                    $W^{\dagger}(\vec{\theta}) \mathbf{f} W(\vec{\theta})$ $\rightarrow$
                                                    $\operatorname{tr}\left[W^{\dagger}(\vec{\theta}) \mathbf{f}
                                                    W(\vec{\theta}) P_\alpha\right]$. Lastly, any classification rule or
                                                    mapping $\tilde{m}(x)$ from a variational unitary can be restated in
                                                    the SVM form: $\operatorname{sign}\left(2^{-n} \sum_\alpha
                                                    w_\alpha(\vec{\theta}) \Phi_\alpha(\vec{x})+b\right)$ What we can
                                                    see is that the behavior of the classifier is dependent on the
                                                    larger term $\omega_{\alpha}$, so by improving this term, this
                                                    constraining term is lifted from the variational circut. The authors
                                                    note as well that the optimal value for $\omega_{\alpha}$ can
                                                    alternatively be found through implementing kernel methods and the
                                                    Wolfe dual approach of the SVM. The important idea we can gain from
                                                    decomposing of variational circut, is that we can think of the
                                                    feature space as the quantum state space that has feature vectors
                                                    $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$ and inner products
                                                    $K(\vec{x}, \vec{z})=|\langle\Phi(\vec{x}) \mid
                                                    \Phi(\vec{z})\rangle|^2$. With this more clear image of how feature
                                                    space is being represented as quantum state space, we can see that
                                                    the direct use of Hilbert space for this can lead to conceptual
                                                    errors. Such that, a vector in Hilbert space $|\Phi(\vec{x})\rangle
                                                    \in \mathcal{H}$ is only defined up to a global phase physically.
                                                    What was seen was that a quantum advantage is mainly obtained from
                                                    feature maps that have a classicaly hard to estimate kernel. Quantum
                                                    Kernel Estimation Procedure Unlike, in a variational quantum circut
                                                    to generate a seperating hyperplane for the high-dimesnional feature
                                                    space, Quantum Kernel Estimation a classical SVM classification
                                                    instead. Meaning, this type of classification does not make a direct
                                                    use of Hilbert space, allowing for it to side step the inherent
                                                    conceptual errors of that quantum variational classification feature
                                                    map representation. To continue. Quantum Feature Mapping
                                                    Implementations for Support Vector Machines A feature map can be
                                                    denoted as $\Phi: \Omega \subset \mathbb{R}^d \rightarrow
                                                    \mathcal{S}\left(\mathcal{H}_2^{\otimes n}\right)$ such that $\Phi:
                                                    \vec{x} \mapsto|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. The map
                                                    acting on the data input is a unitary circut family
                                                    $\mathcal{U}_{\Phi(\vec{x})}$ applied to $|0\rangle^n$. The result
                                                    can be denoted as
                                                    $|\Phi(\vec{x})\rangle=\mathcal{U}_{\Phi(\vec{x})}|0\rangle^n$, such
                                                    that the state in the feature space is linearly independent. Let's
                                                    analyze a feature map corresponding to a product state. First,
                                                    assume that a feature map is comprised of single qubit rotations
                                                    $U(\varphi) \in \mathrm{SU}(2)$ aranged in a quantum circuit. These
                                                    angles correspond to a non-linear function $\varphi: \vec{x}
                                                    \rightarrow(0,2 \pi]^2 \times[0, \pi]$ mapped to the space of Euler
                                                    angles. This means that the feature mapping action for an individual
                                                    qubit is: $\vec{x}
                                                    \mapsto\left|\phi_i(\vec{x})\right\rangle=U\left(\varphi_i(\vec{x})\right)|0\rangle$
                                                    and the feature mapping action for the full quibit state is: $\Phi:
                                                    \vec{x}
                                                    \mapsto|\Phi(\vec{x})\rangle\left\langle\Phi(\vec{x})|\right.$$\left.=\bigotimes_{i=1}^n|
                                                    \phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$ Stoudenmire and
                                                    Schwab using tensor networks is an example of unitary implementation
                                                    of feature mapping of classical classifiers. In this implementation,
                                                    each qubit encodes a single component $x_{i}$ of
                                                    $\vec{x}\in[0,1]^{n}$, thus, using $n$ qubits. The prepared state of
                                                    this full qubit feature mapping encoding when expanded to the
                                                    Pauli-matrix basis:
                                                    $\bigotimes_{i=1}^n\left|\phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                    $=$ $\frac{1}{2^n} \bigotimes_{j=1}^n\left(\sum_{\alpha_j}
                                                    \Phi_j^{\alpha_j}\left(\theta_j(\vec{x})\right) P_{\alpha_j}\right)$
                                                    Such that, with respect to the Pauli-matrix basis,
                                                    $\Phi_i^\alpha\left(\theta_i(\vec{x})\right)$ $=$
                                                    $\left\langle\phi_i(x)\left|P_{\alpha_i}\right|
                                                    \phi_i(x)\right\rangle$ for every $i=1\dots,n$ and where
                                                    $P_{\alpha_i} \in\left\{\mathbb{1}, X_i, Z_i, Y_i\right\}$. To
                                                    continue. Quantum Support Vector Machine - Context Switching Quantum
                                                    Support Vector Machine - Context SwitchingQuantum Support Vector
                                                    Machine Page Reference: Supervised Learning with Quantum Enhanced
                                                    Feature Spaces by: Vojtech Havlicek,* Antonio D. Corcoles, Kristan
                                                    Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M.
                                                    Gambetta. IBM T.J. Watson Research Center and Center for Theoretical
                                                    Physics, Massachusetts Institute of Technology (Dated: June 7, 2018)
                                                    $^{[1]}$ My code for detecting credit card fraud using a Quantum
                                                    Support Vector Machine: QuantumSVMFraudDetection Introduction Using
                                                    quantum computing, the authors exploit quantum mechanics for the
                                                    algorithmic complexity optimization of a Support Vector Machine with
                                                    high-dimensional feature space. Where the high-dimensional classical
                                                    data is mapped non-linearly to Hilbert Space and a hyperplane in
                                                    quantum space is used to separate and label the data. By using the
                                                    quantum state space as the feature space, the authors hope to obtain
                                                    a quantum advantage. Let's consider we have data from a training set
                                                    $T$ and a test set $S$, where $T, S$ are subsets
                                                    $\Omega\subset\mathbb{R}^{d}$. For each training set, we assume the
                                                    data is labeled by a true map $m: T \cup S \rightarrow\left\{+1,
                                                    -1\right\}$, which is not given to the algorithm. The algorithm is
                                                    given training data, where the goal is for the algorithm to infer an
                                                    approximate mappping $\tilde{m}: T\rightarrow\{+1,-1\}$ for the
                                                    data. After training, the algorithm is asked to apply the
                                                    approximate mapping on a test set $\tilde{m}:
                                                    S\rightarrow\{+1,-1\}$. The approximate mapping needs to match the
                                                    true map $m(\vec{s})=\tilde{m}(\vec{s})$ with a high probability. To
                                                    use quantum state space as feature space a mapping is constructed
                                                    such that the data is non-linearly mapped to a quantum state
                                                    $\Phi:\vec{x}\in\Omega\rightarrow
                                                    |\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. Where,
                                                    $|\Phi(\vec{x})\rangle$ is a part of complex vector space or Hilbert
                                                    Space $\mathcal{H}$ and $\langle\Phi(\vec{x})|$ is the complex
                                                    conjugate that is a dual correspondance. We can see that the
                                                    non-linear mapping $\Phi$ takes the classical data
                                                    $\vec{x}\in\Omega\subset\mathbb{R}^{d}$ or $\vec{x}\in T\cup S$ and
                                                    reperesents it in quantum state space
                                                    $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. To continue, we will
                                                    look at the procedure behind quantum mapping, rather than its domain
                                                    and codomain, training, and then testing. Quantum Feature Mapping
                                                    The authors suggest that in order to have an advantage over
                                                    classical approaches, a map based on circuts that are clasically
                                                    hard to simulate needs to be constructed, but where the circut is
                                                    not too deep to test on during experimentation. The authors provide
                                                    a circut diagram that represents a unitary function for feature
                                                    mapping $n$-qubits. The circut diagram is given as: Image Source:
                                                    $^{[1]}$ Note, we will examine more in depth the feature mapping
                                                    unitary function later on. Data Generation First, we will define the
                                                    map used for the artifical data. The data is generated of dimension
                                                    $n=d=2$ for a $2$-qubit system with the mapping
                                                    $\phi_{\{i\}}(\vec{x})=x_{i}$ and
                                                    $\phi_{\{1,2\}}(\vec{x})=(\pi-x_{1})(\pi-x_{2})$. Next, we look at
                                                    how the data is generated. For the data vector labels $\vec{x}\in
                                                    T\cup S \subset (0, 2\pi]^{2}$ the authors mention generating it
                                                    using the parity function $\mathbf{f}=Z_{1}Z_{2}$ and random unitary
                                                    $V\in SU(4)$. Next, to label the data the authors describe the
                                                    following mapping. Given $\Delta=0.3$, if:
                                                    $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\geq\Delta$
                                                    then $m(\vec{x})=+1$. If:
                                                    $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\leq-\Delta$
                                                    then $m(\vec{x})=-1$. Proposed SVM Classifiers Quantum Variational
                                                    Classification In the this section, we will deconstruct and analyze
                                                    individual components of the circut, unitary, and steps to better
                                                    understand how this processes works. Then, in the following section,
                                                    we will look at classifier optimization, testing the trained model,
                                                    and analyze the authors experimental findings. Procedure The authors
                                                    have provided a figure of the quantum variational classification QVC
                                                    circut and it is shown below: Image Source: $^{[1]}$ where
                                                    $C=\{+1,-1\}$ and the varitional unitary for QVC as a whole is
                                                    defined as:
                                                    $p_{y}(\vec{x})$$=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                    The authors describe the procedure for the QVC in the following four
                                                    sequential steps: feature map encoding, variational optimization,
                                                    measurement, and then error probability. The training and
                                                    classification phase consist of these four main parts and is what we
                                                    will analyze in the following. Feature Map Encoding Recall that the
                                                    circut looks like the following: Image Source: $^{[1]}$ The data
                                                    $\vec{x}\in\Omega$ is mapped to a reference state $|0\rangle^{n}$
                                                    using the feature map circut $\mathcal{U}_{\Phi(\vec{x})}$. This
                                                    feature map is an injective encoding of classical information
                                                    $\vec{x}\in\mathbb{R}^{d}$ to a quantum state
                                                    $|\Phi(\vec{x})\rangle\langle(\vec{x})\Phi|$ that is on a $n$-qubit
                                                    register such that $d=n$. A quibit is a two-level system of Hilbert
                                                    space and can be represented as $\mathcal{H}_{2}=\mathbb{C}^{2}$. To
                                                    represent $n$-qubits we denote it as $\mathcal{H}_{2}^{\otimes
                                                    n}=\left(\mathbb{C}^2\right)^{\otimes n}$. The offered feature
                                                    mapping portrayed above is a family of feature maps, that the
                                                    authors selected because they conjecture it is hard to estimate
                                                    overlap $|\langle\Phi(\vec{x}) \mid \Phi(\vec{y})\rangle|^2$ on a
                                                    classical computer. The family of feature map circuits is defined as
                                                    follows: $|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})} H^{\otimes n}
                                                    U_{\Phi(\vec{x})} H^{\otimes n}|0\rangle^{\otimes n}$ where $H$ is a
                                                    conventional Hadmard gate and $U_{\Phi(\vec{x})}$ is defined as:
                                                    $\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in
                                                    S}Z_{i}\right)$ and is a diagonal gate in the Pauli Z-basis. For a
                                                    single qubit, the unitary function $U_{\Phi(x)}$ acts as a
                                                    phase-gate $Z_{x}$ of angle $x\in\Omega$. Feature mapping allows for
                                                    $2^{n}$ possible coefficents $\phi_{S}\left(\vec{x}\right)$ for
                                                    non-linear function of the inputed data $\vec{x}\in\mathbb{R}^{n}$.
                                                    Variational Classification A short depth quantum circut
                                                    $W(\vec{\theta})$ is applied to the feature state is a variational
                                                    circut used for the optimization method. A depiction of this short
                                                    depth quantum circut is given by the authors and shown in the figure
                                                    below: Image Source: $[1]$ For this short depth quantum circut, the
                                                    authors use an Ansatz variational unitary $W(\vec{\theta})$, defined
                                                    as:
                                                    $U_{loc}^{(l)}(\theta_{l})U_{ent}...$$U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})$
                                                    Where, $U_{loc}^{(l)}(\theta_{l})$ is full layers single qubit
                                                    rotations given as:
                                                    $U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})$ and
                                                    $U(\theta_{i,t})\in SU(2)$. For the variational unitary, $U_{ent}$
                                                    is an alternating layers of entangling gates and is defined as:
                                                    $U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)$ Here, $E$ is an edge in
                                                    the circut defined vetice set $\{v_{i}, v_{j}\}\in V$. $\mathbf{CZ}$
                                                    is a controlled phase gate applied along the edges $(i,j)\in E$
                                                    which the authors state is present in the connectivity of the
                                                    superconducting chip. This variational circut is parametrized by
                                                    $\vec{\theta }\in\mathbb{R}^{2n(l+1)}$ and it is what is optimized
                                                    during training as this is what classifies the data. Measurment
                                                    Given that the problem is a two label classification
                                                    $y\in\{+1,-1\}$, the authors apply a binary measurment $\{M_{y}\}$
                                                    to the state
                                                    $W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}$. The
                                                    authors describe the binary measurment in the following way. The
                                                    measurment is in the $Z$-basis, outputs the bit-string
                                                    $z\in\{0,1\}^{n}$, where the bit-string is then passed to the
                                                    boolean function $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The binary
                                                    measurment is defined as $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$,
                                                    where $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle z|$. From
                                                    this third step, the probability for the outcome $y$ is obtained.
                                                    This probability $p_{y}(\vec{x})$ is defined as:
                                                    $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                    Where the probability for measuring either label $y\in\{+1,-1\}$,
                                                    denoted as $p_y$, is defined as:
                                                    $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                    W(\theta)\right| \Phi(\vec{x})\right\rangle\right)$ Since the
                                                    expected value of the measured observable is
                                                    $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                    W^{\dagger}(\theta, \varphi)\right.$$\left.\mathbf{f} W(\theta,
                                                    \varphi)\right]$, it can be expressed in terms of the inner product:
                                                    $\frac{1}{2^n} \sum_\alpha w_\alpha(\theta) \Phi_\alpha(\vec{x})$
                                                    Next, we need to incorporate assigning the label $y\in\{+1,-1\}$
                                                    over the label $-y$ with a given fixed basis $b\in[-1,+1]$. In this
                                                    case, it must be that $p_{-y}-yb<\ p_{y}$. Subsituting the
                                                        probability for measuring either label $y\in\{+1,-1\}$ in to the
                                                        inner product of the measured observable, we obtain the value
                                                        for $\tilde{m}(\vec{x})$ as:
                                                        $\operatorname{sign}\left(\frac{1}{2^n} \sum_\alpha
                                                        w_\alpha(\theta) \Phi_\alpha(\vec{x})+b\right)$ For the final
                                                        decision ruling of $y$, $R$ repeated measurment shots are
                                                        preformed, yielding the empircal distribution
                                                        $\hat{p}(\vec{x})$, where if $\hat{p}_{-y}(\vec{x}-yb)>
                                                        \hat{p}_{y}(\vec{x})$ the label assigned is
                                                        $\tilde{m}(\vec{x})=y$. The authors introduced a bias parameter
                                                        $b\in[-1,1]$ that can also be optimized during training. The
                                                        authors mention that the feature map circut
                                                        $\mathcal{U}_{\Phi(\vec{x})}$ and the boolean function $f$ are
                                                        fixed choices. The parameters that are being optimized during
                                                        training are $(\vec{\theta},b)$ and in order to be optimized, a
                                                        cost-function is need to be defined. To do so, we need to define
                                                        an error probability first, which is what we will look at in the
                                                        next section. Error Probability In order to find the empirical
                                                        risk $R_{emp}(\vec{\theta})$ of the empirical distribution
                                                        $\hat{p}(\vec{x})$ we define the error probability of assigning
                                                        the incorrect labels averaged over the samples in the training
                                                        set $T$. The authors give the definition:
                                                        $R_{emp}(\vec{\theta})$ $=$ $\frac{1}{|T|}\sum_{\vec{x}\in
                                                        T}Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$ that defines this error
                                                        probability. For our binary classification problem, the error
                                                        probability of assigning the wrong label to some given data can
                                                        be clacilated using the binomial cimulative density function CDF
                                                        for the empircal distribution $\hat{p}(\vec{x})$. For a large
                                                        number of samples or shots $R\gg 1$ the CDF is approximated by a
                                                        sigmoid function $\operatorname{sig}(x)=(1+e^{-x})^{-1}$. The
                                                        probability for label $m(\vec{x}=y)$ being assigned incorrectly
                                                        can be approximated by: $Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                        $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                        b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                        \hat{p}_y(\vec{x})}}\right)$ Classifier Optimization and Testing
                                                        Results The first step is to train the classifier and optimize
                                                        it. The authors found that they found Spall's SPSA stochastic
                                                        gradient descent algorithm performs the best in the inherent
                                                        noisy experimental setting of quantum computing. They mentioned
                                                        after the parameters converged onto
                                                        $\big(\vec{\Theta^{*}},b^{*}\big)$. The second step is then to
                                                        classify data and assign them labels according to the decision
                                                        rule given by $\tilde{m}(\vec{s})$, where the binary measurement
                                                        is obtained from the parity function $\mathbf{f}=Z_1 Z_2$, and
                                                        ensure that for the testing data,
                                                        $m(\vec{s})=\tilde{m}(\vec{s})$ with high probability for the
                                                        given data set $\vec{s}\in S$. The authors note that what is
                                                        obeserved for the empirical risk, or cost value used for the
                                                        optimizer, is that it converges to a lower depth when the number
                                                        of layers in the short depth quantum circuit for optimization is
                                                        $l=4$ than $l=0$. Interesting enough, error mitigation does not
                                                        appreciably improve the empirical risk results when depth is at
                                                        $0$, however, does substantially help for larger depths. An
                                                        important note, is that although
                                                        $\operatorname{Pr}(\tilde{m}(\vec{x}) \neq m(\vec{x}))$ includes
                                                        the number of shots $R$ taken in its calculation, during
                                                        experimentation the authors fixed $R=200$. The authors state the
                                                        reason for doing this, even though $R=2000$ in actual
                                                        experiment, was to avoid gradient problems. To continue, after
                                                        training, comes testing. The yielded trained set of parameters
                                                        $\big(\vec{\theta}^{*},b^*=0\big)$ where used to classify 20
                                                        different test sets randomly drawn each time per data set. After
                                                        analyzing the results, the authors an increasing classification
                                                        success with increasing circut depth $l$. Noting, that the
                                                        success rate very nearl reaches $100$% for circut depths larger
                                                        than $1$ and remains up to depth $4$, despite decoherence
                                                        associated with $8$ CNOT gates during training and
                                                        classification circuts. Quantum Variational Classification
                                                        Analysis An important aspect of SVM's, is the classification of
                                                        the data, which is done here based off of the decision rule
                                                        $p_y(\vec{x})>p_{-y}(\vec{x})-y b$. This decision rule
                                                        $\tilde{m}(\vec{x})$ can can be restated as $\operatorname{sign}
                                                        \big( \langle\Phi(\vec{x}) |W^{\dagger}(\vec{\theta})$$
                                                        \mathbf{f} W(\vec{\theta}) | \Phi(\vec{x}) \rangle+b \big)$. The
                                                        step that structures the data in such a way for classification,
                                                        is the variational circut $W$, which seperates the data using
                                                        hyperplane $\vec{w}$ in quantum state space. A feature map of a
                                                        single qubit, along with the seperating hyperplane $\vec{w}$ and
                                                        the invterval of binary labels $\Omega=(0,2\pi]$ for the
                                                        classical data, is given by the authors below. Image Source:
                                                        $[1]$ We start by decomposing the quantum variational circut.
                                                        First, we look to define the properties for our group of
                                                        matrices used. For us, we need a group that has an orthogonal,
                                                        hermatian, matrix basis $\left\{P_\alpha\right\} \subset
                                                        \mathbb{C}^{2^n \times 2^n}$. Such that $\alpha=1,...,4^{n}$
                                                        with $\operatorname{tr}\left[P_\alpha^{\dagger}
                                                        P_\beta\right]=2^n \delta_{\alpha, \beta}$. A group of matrices
                                                        adhereing to this criteria is the Pauli-group on $n$-qubits.
                                                        Next, we expand the quantum state and the measurment in our
                                                        choosen matrix basis. The expanded quantum state, or expectation
                                                        value, can be expressed in terms of
                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$ $\rightarrow$
                                                        $\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
                                                        \Phi(\vec{x})\right\rangle$. The decision rule expressed in our
                                                        choosen matrix basis as $w_\alpha(\vec{\theta})$ can be defined
                                                        as $W^{\dagger}(\vec{\theta}) \mathbf{f} W(\vec{\theta})$
                                                        $\rightarrow$ $\operatorname{tr}\left[W^{\dagger}(\vec{\theta})
                                                        \mathbf{f} W(\vec{\theta}) P_\alpha\right]$. Lastly, any
                                                        classification rule or mapping $\tilde{m}(x)$ from a variational
                                                        unitary can be restated in the SVM form:
                                                        $\operatorname{sign}\left(2^{-n} \sum_\alpha
                                                        w_\alpha(\vec{\theta}) \Phi_\alpha(\vec{x})+b\right)$ What we
                                                        can see is that the behavior of the classifier is dependent on
                                                        the larger term $\omega_{\alpha}$, so by improving this term,
                                                        this constraining term is lifted from the variational circut.
                                                        The authors note as well that the optimal value for
                                                        $\omega_{\alpha}$ can alternatively be found through
                                                        implementing kernel methods and the Wolfe dual approach of the
                                                        SVM. The important idea we can gain from decomposing of
                                                        variational circut, is that we can think of the feature space as
                                                        the quantum state space that has feature vectors
                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$ and inner products
                                                        $K(\vec{x}, \vec{z})=|\langle\Phi(\vec{x}) \mid
                                                        \Phi(\vec{z})\rangle|^2$. With this more clear image of how
                                                        feature space is being represented as quantum state space, we
                                                        can see that the direct use of Hilbert space for this can lead
                                                        to conceptual errors. Such that, a vector in Hilbert space
                                                        $|\Phi(\vec{x})\rangle \in \mathcal{H}$ is only defined up to a
                                                        global phase physically. What was seen was that a quantum
                                                        advantage is mainly obtained from feature maps that have a
                                                        classicaly hard to estimate kernel. Quantum Kernel Estimation
                                                        Procedure Unlike, in a variational quantum circut to generate a
                                                        seperating hyperplane for the high-dimesnional feature space,
                                                        Quantum Kernel Estimation a classical SVM classification
                                                        instead. Meaning, this type of classification does not make a
                                                        direct use of Hilbert space, allowing for it to side step the
                                                        inherent conceptual errors of that quantum variational
                                                        classification feature map representation. To continue. Quantum
                                                        Feature Mapping Implementations for Support Vector Machines A
                                                        feature map can be denoted as $\Phi: \Omega \subset \mathbb{R}^d
                                                        \rightarrow \mathcal{S}\left(\mathcal{H}_2^{\otimes n}\right)$
                                                        such that $\Phi: \vec{x}
                                                        \mapsto|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. The map
                                                        acting on the data input is a unitary circut family
                                                        $\mathcal{U}_{\Phi(\vec{x})}$ applied to $|0\rangle^n$. The
                                                        result can be denoted as
                                                        $|\Phi(\vec{x})\rangle=\mathcal{U}_{\Phi(\vec{x})}|0\rangle^n$,
                                                        such that the state in the feature space is linearly
                                                        independent. Let's analyze a feature map corresponding to a
                                                        product state. First, assume that a feature map is comprised of
                                                        single qubit rotations $U(\varphi) \in \mathrm{SU}(2)$ aranged
                                                        in a quantum circuit. These angles correspond to a non-linear
                                                        function $\varphi: \vec{x} \rightarrow(0,2 \pi]^2 \times[0,
                                                        \pi]$ mapped to the space of Euler angles. This means that the
                                                        feature mapping action for an individual qubit is: $\vec{x}
                                                        \mapsto\left|\phi_i(\vec{x})\right\rangle=U\left(\varphi_i(\vec{x})\right)|0\rangle$
                                                        and the feature mapping action for the full quibit state is:
                                                        $\Phi: \vec{x}
                                                        \mapsto|\Phi(\vec{x})\rangle\left\langle\Phi(\vec{x})|\right.$$\left.=\bigotimes_{i=1}^n|
                                                        \phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$ Stoudenmire
                                                        and Schwab using tensor networks is an example of unitary
                                                        implementation of feature mapping of classical classifiers. In
                                                        this implementation, each qubit encodes a single component
                                                        $x_{i}$ of $\vec{x}\in[0,1]^{n}$, thus, using $n$ qubits. The
                                                        prepared state of this full qubit feature mapping encoding when
                                                        expanded to the Pauli-matrix basis:
                                                        $\bigotimes_{i=1}^n\left|\phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                        $=$ $\frac{1}{2^n} \bigotimes_{j=1}^n\left(\sum_{\alpha_j}
                                                        \Phi_j^{\alpha_j}\left(\theta_j(\vec{x})\right)
                                                        P_{\alpha_j}\right)$ Such that, with respect to the Pauli-matrix
                                                        basis, $\Phi_i^\alpha\left(\theta_i(\vec{x})\right)$ $=$
                                                        $\left\langle\phi_i(x)\left|P_{\alpha_i}\right|
                                                        \phi_i(x)\right\rangle$ for every $i=1\dots,n$ and where
                                                        $P_{\alpha_i} \in\left\{\mathbb{1}, X_i, Z_i, Y_i\right\}$. To
                                                        continue. Quantum Support Vector Machine Page Reference:
                                                        Supervised Learning with Quantum Enhanced Feature Spaces by:
                                                        Vojtech Havlicek,* Antonio D. Corcoles, Kristan Temme, Aram W.
                                                        Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. IBM
                                                        T.J. Watson Research Center and Center for Theoretical Physics,
                                                        Massachusetts Institute of Technology (Dated: June 7, 2018)
                                                        $^{[1]}$ My code for detecting credit card fraud using a Quantum
                                                        Support Vector Machine: QuantumSVMFraudDetection Introduction
                                                        Using quantum computing, the authors exploit quantum mechanics
                                                        for the algorithmic complexity optimization of a Support Vector
                                                        Machine with high-dimensional feature space. Where the
                                                        high-dimensional classical data is mapped non-linearly to
                                                        Hilbert Space and a hyperplane in quantum space is used to
                                                        separate and label the data. By using the quantum state space as
                                                        the feature space, the authors hope to obtain a quantum
                                                        advantage. Let's consider we have data from a training set $T$
                                                        and a test set $S$, where $T, S$ are subsets
                                                        $\Omega\subset\mathbb{R}^{d}$. For each training set, we assume
                                                        the data is labeled by a true map $m: T \cup S
                                                        \rightarrow\left\{+1, -1\right\}$, which is not given to the
                                                        algorithm. The algorithm is given training data, where the goal
                                                        is for the algorithm to infer an approximate mappping
                                                        $\tilde{m}: T\rightarrow\{+1,-1\}$ for the data. After training,
                                                        the algorithm is asked to apply the approximate mapping on a
                                                        test set $\tilde{m}: S\rightarrow\{+1,-1\}$. The approximate
                                                        mapping needs to match the true map
                                                        $m(\vec{s})=\tilde{m}(\vec{s})$ with a high probability. To use
                                                        quantum state space as feature space a mapping is constructed
                                                        such that the data is non-linearly mapped to a quantum state
                                                        $\Phi:\vec{x}\in\Omega\rightarrow
                                                        |\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. Where,
                                                        $|\Phi(\vec{x})\rangle$ is a part of complex vector space or
                                                        Hilbert Space $\mathcal{H}$ and $\langle\Phi(\vec{x})|$ is the
                                                        complex conjugate that is a dual correspondance. We can see that
                                                        the non-linear mapping $\Phi$ takes the classical data
                                                        $\vec{x}\in\Omega\subset\mathbb{R}^{d}$ or $\vec{x}\in T\cup S$
                                                        and reperesents it in quantum state space
                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. To continue, we
                                                        will look at the procedure behind quantum mapping, rather than
                                                        its domain and codomain, training, and then testing. Quantum
                                                        Feature Mapping The authors suggest that in order to have an
                                                        advantage over classical approaches, a map based on circuts that
                                                        are clasically hard to simulate needs to be constructed, but
                                                        where the circut is not too deep to test on during
                                                        experimentation. The authors provide a circut diagram that
                                                        represents a unitary function for feature mapping $n$-qubits.
                                                        The circut diagram is given as: Image Source: $^{[1]}$ Note, we
                                                        will examine more in depth the feature mapping unitary function
                                                        later on. Data Generation First, we will define the map used for
                                                        the artifical data. The data is generated of dimension $n=d=2$
                                                        for a $2$-qubit system with the mapping
                                                        $\phi_{\{i\}}(\vec{x})=x_{i}$ and
                                                        $\phi_{\{1,2\}}(\vec{x})=(\pi-x_{1})(\pi-x_{2})$. Next, we look
                                                        at how the data is generated. For the data vector labels
                                                        $\vec{x}\in T\cup S \subset (0, 2\pi]^{2}$ the authors mention
                                                        generating it using the parity function $\mathbf{f}=Z_{1}Z_{2}$
                                                        and random unitary $V\in SU(4)$. Next, to label the data the
                                                        authors describe the following mapping. Given $\Delta=0.3$, if:
                                                        $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\geq\Delta$
                                                        then $m(\vec{x})=+1$. If:
                                                        $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\leq-\Delta$
                                                        then $m(\vec{x})=-1$. Proposed SVM Classifiers Quantum
                                                        Variational Classification In the this section, we will
                                                        deconstruct and analyze individual components of the circut,
                                                        unitary, and steps to better understand how this processes
                                                        works. Then, in the following section, we will look at
                                                        classifier optimization, testing the trained model, and analyze
                                                        the authors experimental findings. Procedure The authors have
                                                        provided a figure of the quantum variational classification QVC
                                                        circut and it is shown below: Image Source: $^{[1]}$ where
                                                        $C=\{+1,-1\}$ and the varitional unitary for QVC as a whole is
                                                        defined as:
                                                        $p_{y}(\vec{x})$$=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                        The authors describe the procedure for the QVC in the following
                                                        four sequential steps: feature map encoding, variational
                                                        optimization, measurement, and then error probability. The
                                                        training and classification phase consist of these four main
                                                        parts and is what we will analyze in the following. Feature Map
                                                        Encoding Recall that the circut looks like the following: Image
                                                        Source: $^{[1]}$ The data $\vec{x}\in\Omega$ is mapped to a
                                                        reference state $|0\rangle^{n}$ using the feature map circut
                                                        $\mathcal{U}_{\Phi(\vec{x})}$. This feature map is an injective
                                                        encoding of classical information $\vec{x}\in\mathbb{R}^{d}$ to
                                                        a quantum state $|\Phi(\vec{x})\rangle\langle(\vec{x})\Phi|$
                                                        that is on a $n$-qubit register such that $d=n$. A quibit is a
                                                        two-level system of Hilbert space and can be represented as
                                                        $\mathcal{H}_{2}=\mathbb{C}^{2}$. To represent $n$-qubits we
                                                        denote it as $\mathcal{H}_{2}^{\otimes
                                                        n}=\left(\mathbb{C}^2\right)^{\otimes n}$. The offered feature
                                                        mapping portrayed above is a family of feature maps, that the
                                                        authors selected because they conjecture it is hard to estimate
                                                        overlap $|\langle\Phi(\vec{x}) \mid \Phi(\vec{y})\rangle|^2$ on
                                                        a classical computer. The family of feature map circuits is
                                                        defined as follows: $|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})}
                                                        H^{\otimes n} U_{\Phi(\vec{x})} H^{\otimes n}|0\rangle^{\otimes
                                                        n}$ where $H$ is a conventional Hadmard gate and
                                                        $U_{\Phi(\vec{x})}$ is defined as:
                                                        $\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in
                                                        S}Z_{i}\right)$ and is a diagonal gate in the Pauli Z-basis. For
                                                        a single qubit, the unitary function $U_{\Phi(x)}$ acts as a
                                                        phase-gate $Z_{x}$ of angle $x\in\Omega$. Feature mapping allows
                                                        for $2^{n}$ possible coefficents $\phi_{S}\left(\vec{x}\right)$
                                                        for non-linear function of the inputed data
                                                        $\vec{x}\in\mathbb{R}^{n}$. Variational Classification A short
                                                        depth quantum circut $W(\vec{\theta})$ is applied to the feature
                                                        state is a variational circut used for the optimization method.
                                                        A depiction of this short depth quantum circut is given by the
                                                        authors and shown in the figure below: Image Source: $[1]$ For
                                                        this short depth quantum circut, the authors use an Ansatz
                                                        variational unitary $W(\vec{\theta})$, defined as:
                                                        $U_{loc}^{(l)}(\theta_{l})U_{ent}...$$U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})$
                                                        Where, $U_{loc}^{(l)}(\theta_{l})$ is full layers single qubit
                                                        rotations given as:
                                                        $U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})$
                                                        and $U(\theta_{i,t})\in SU(2)$. For the variational unitary,
                                                        $U_{ent}$ is an alternating layers of entangling gates and is
                                                        defined as: $U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)$ Here,
                                                        $E$ is an edge in the circut defined vetice set $\{v_{i},
                                                        v_{j}\}\in V$. $\mathbf{CZ}$ is a controlled phase gate applied
                                                        along the edges $(i,j)\in E$ which the authors state is present
                                                        in the connectivity of the superconducting chip. This
                                                        variational circut is parametrized by $\vec{\theta
                                                        }\in\mathbb{R}^{2n(l+1)}$ and it is what is optimized during
                                                        training as this is what classifies the data. Measurment Given
                                                        that the problem is a two label classification $y\in\{+1,-1\}$,
                                                        the authors apply a binary measurment $\{M_{y}\}$ to the state
                                                        $W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}$. The
                                                        authors describe the binary measurment in the following way. The
                                                        measurment is in the $Z$-basis, outputs the bit-string
                                                        $z\in\{0,1\}^{n}$, where the bit-string is then passed to the
                                                        boolean function $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The binary
                                                        measurment is defined as $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$,
                                                        where $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle z|$.
                                                        From this third step, the probability for the outcome $y$ is
                                                        obtained. This probability $p_{y}(\vec{x})$ is defined as:
                                                        $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                        Where the probability for measuring either label
                                                        $y\in\{+1,-1\}$, denoted as $p_y$, is defined as:
                                                        $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                        W(\theta)\right| \Phi(\vec{x})\right\rangle\right)$ Since the
                                                        expected value of the measured observable is
                                                        $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                        W^{\dagger}(\theta, \varphi)\right.$$\left.\mathbf{f} W(\theta,
                                                        \varphi)\right]$, it can be expressed in terms of the inner
                                                        product: $\frac{1}{2^n} \sum_\alpha w_\alpha(\theta)
                                                        \Phi_\alpha(\vec{x})$ Next, we need to incorporate assigning the
                                                        label $y\in\{+1,-1\}$ over the label $-y$ with a given fixed
                                                        basis $b\in[-1,+1]$. In this case, it must be that $p_{-y}-yb<\
                                                            p_{y}$. Subsituting the probability for measuring either
                                                            label $y\in\{+1,-1\}$ in to the inner product of the
                                                            measured observable, we obtain the value for
                                                            $\tilde{m}(\vec{x})$ as:
                                                            $\operatorname{sign}\left(\frac{1}{2^n} \sum_\alpha
                                                            w_\alpha(\theta) \Phi_\alpha(\vec{x})+b\right)$ For the
                                                            final decision ruling of $y$, $R$ repeated measurment shots
                                                            are preformed, yielding the empircal distribution
                                                            $\hat{p}(\vec{x})$, where if $\hat{p}_{-y}(\vec{x}-yb)>
                                                            \hat{p}_{y}(\vec{x})$ the label assigned is
                                                            $\tilde{m}(\vec{x})=y$. The authors introduced a bias
                                                            parameter $b\in[-1,1]$ that can also be optimized during
                                                            training. The authors mention that the feature map circut
                                                            $\mathcal{U}_{\Phi(\vec{x})}$ and the boolean function $f$
                                                            are fixed choices. The parameters that are being optimized
                                                            during training are $(\vec{\theta},b)$ and in order to be
                                                            optimized, a cost-function is need to be defined. To do so,
                                                            we need to define an error probability first, which is what
                                                            we will look at in the next section. Error Probability In
                                                            order to find the empirical risk $R_{emp}(\vec{\theta})$ of
                                                            the empirical distribution $\hat{p}(\vec{x})$ we define the
                                                            error probability of assigning the incorrect labels averaged
                                                            over the samples in the training set $T$. The authors give
                                                            the definition: $R_{emp}(\vec{\theta})$ $=$
                                                            $\frac{1}{|T|}\sum_{\vec{x}\in T}Pr(\tilde{m}(\vec{x})\neq
                                                            m(\vec{x}))$ that defines this error probability. For our
                                                            binary classification problem, the error probability of
                                                            assigning the wrong label to some given data can be
                                                            clacilated using the binomial cimulative density function
                                                            CDF for the empircal distribution $\hat{p}(\vec{x})$. For a
                                                            large number of samples or shots $R\gg 1$ the CDF is
                                                            approximated by a sigmoid function
                                                            $\operatorname{sig}(x)=(1+e^{-x})^{-1}$. The probability for
                                                            label $m(\vec{x}=y)$ being assigned incorrectly can be
                                                            approximated by: $Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                            $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                            b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                            \hat{p}_y(\vec{x})}}\right)$ Classifier Optimization and
                                                            Testing Results The first step is to train the classifier
                                                            and optimize it. The authors found that they found Spall's
                                                            SPSA stochastic gradient descent algorithm performs the best
                                                            in the inherent noisy experimental setting of quantum
                                                            computing. They mentioned after the parameters converged
                                                            onto $\big(\vec{\Theta^{*}},b^{*}\big)$. The second step is
                                                            then to classify data and assign them labels according to
                                                            the decision rule given by $\tilde{m}(\vec{s})$, where the
                                                            binary measurement is obtained from the parity function
                                                            $\mathbf{f}=Z_1 Z_2$, and ensure that for the testing data,
                                                            $m(\vec{s})=\tilde{m}(\vec{s})$ with high probability for
                                                            the given data set $\vec{s}\in S$. The authors note that
                                                            what is obeserved for the empirical risk, or cost value used
                                                            for the optimizer, is that it converges to a lower depth
                                                            when the number of layers in the short depth quantum circuit
                                                            for optimization is $l=4$ than $l=0$. Interesting enough,
                                                            error mitigation does not appreciably improve the empirical
                                                            risk results when depth is at $0$, however, does
                                                            substantially help for larger depths. An important note, is
                                                            that although $\operatorname{Pr}(\tilde{m}(\vec{x}) \neq
                                                            m(\vec{x}))$ includes the number of shots $R$ taken in its
                                                            calculation, during experimentation the authors fixed
                                                            $R=200$. The authors state the reason for doing this, even
                                                            though $R=2000$ in actual experiment, was to avoid gradient
                                                            problems. To continue, after training, comes testing. The
                                                            yielded trained set of parameters
                                                            $\big(\vec{\theta}^{*},b^*=0\big)$ where used to classify 20
                                                            different test sets randomly drawn each time per data set.
                                                            After analyzing the results, the authors an increasing
                                                            classification success with increasing circut depth $l$.
                                                            Noting, that the success rate very nearl reaches $100$% for
                                                            circut depths larger than $1$ and remains up to depth $4$,
                                                            despite decoherence associated with $8$ CNOT gates during
                                                            training and classification circuts. Quantum Variational
                                                            Classification Analysis An important aspect of SVM's, is the
                                                            classification of the data, which is done here based off of
                                                            the decision rule $p_y(\vec{x})>p_{-y}(\vec{x})-y b$. This
                                                            decision rule $\tilde{m}(\vec{x})$ can can be restated as
                                                            $\operatorname{sign} \big( \langle\Phi(\vec{x})
                                                            |W^{\dagger}(\vec{\theta})$$ \mathbf{f} W(\vec{\theta}) |
                                                            \Phi(\vec{x}) \rangle+b \big)$. The step that structures the
                                                            data in such a way for classification, is the variational
                                                            circut $W$, which seperates the data using hyperplane
                                                            $\vec{w}$ in quantum state space. A feature map of a single
                                                            qubit, along with the seperating hyperplane $\vec{w}$ and
                                                            the invterval of binary labels $\Omega=(0,2\pi]$ for the
                                                            classical data, is given by the authors below. Image Source:
                                                            $[1]$ We start by decomposing the quantum variational
                                                            circut. First, we look to define the properties for our
                                                            group of matrices used. For us, we need a group that has an
                                                            orthogonal, hermatian, matrix basis $\left\{P_\alpha\right\}
                                                            \subset \mathbb{C}^{2^n \times 2^n}$. Such that
                                                            $\alpha=1,...,4^{n}$ with
                                                            $\operatorname{tr}\left[P_\alpha^{\dagger}
                                                            P_\beta\right]=2^n \delta_{\alpha, \beta}$. A group of
                                                            matrices adhereing to this criteria is the Pauli-group on
                                                            $n$-qubits. Next, we expand the quantum state and the
                                                            measurment in our choosen matrix basis. The expanded quantum
                                                            state, or expectation value, can be expressed in terms of
                                                            $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$ $\rightarrow$
                                                            $\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
                                                            \Phi(\vec{x})\right\rangle$. The decision rule expressed in
                                                            our choosen matrix basis as $w_\alpha(\vec{\theta})$ can be
                                                            defined as $W^{\dagger}(\vec{\theta}) \mathbf{f}
                                                            W(\vec{\theta})$ $\rightarrow$
                                                            $\operatorname{tr}\left[W^{\dagger}(\vec{\theta}) \mathbf{f}
                                                            W(\vec{\theta}) P_\alpha\right]$. Lastly, any classification
                                                            rule or mapping $\tilde{m}(x)$ from a variational unitary
                                                            can be restated in the SVM form:
                                                            $\operatorname{sign}\left(2^{-n} \sum_\alpha
                                                            w_\alpha(\vec{\theta}) \Phi_\alpha(\vec{x})+b\right)$ What
                                                            we can see is that the behavior of the classifier is
                                                            dependent on the larger term $\omega_{\alpha}$, so by
                                                            improving this term, this constraining term is lifted from
                                                            the variational circut. The authors note as well that the
                                                            optimal value for $\omega_{\alpha}$ can alternatively be
                                                            found through implementing kernel methods and the Wolfe dual
                                                            approach of the SVM. The important idea we can gain from
                                                            decomposing of variational circut, is that we can think of
                                                            the feature space as the quantum state space that has
                                                            feature vectors $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$
                                                            and inner products $K(\vec{x},
                                                            \vec{z})=|\langle\Phi(\vec{x}) \mid
                                                            \Phi(\vec{z})\rangle|^2$. With this more clear image of how
                                                            feature space is being represented as quantum state space,
                                                            we can see that the direct use of Hilbert space for this can
                                                            lead to conceptual errors. Such that, a vector in Hilbert
                                                            space $|\Phi(\vec{x})\rangle \in \mathcal{H}$ is only
                                                            defined up to a global phase physically. What was seen was
                                                            that a quantum advantage is mainly obtained from feature
                                                            maps that have a classicaly hard to estimate kernel. Quantum
                                                            Kernel Estimation Procedure Unlike, in a variational quantum
                                                            circut to generate a seperating hyperplane for the
                                                            high-dimesnional feature space, Quantum Kernel Estimation a
                                                            classical SVM classification instead. Meaning, this type of
                                                            classification does not make a direct use of Hilbert space,
                                                            allowing for it to side step the inherent conceptual errors
                                                            of that quantum variational classification feature map
                                                            representation. To continue. Quantum Feature Mapping
                                                            Implementations for Support Vector Machines A feature map
                                                            can be denoted as $\Phi: \Omega \subset \mathbb{R}^d
                                                            \rightarrow \mathcal{S}\left(\mathcal{H}_2^{\otimes
                                                            n}\right)$ such that $\Phi: \vec{x}
                                                            \mapsto|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. The map
                                                            acting on the data input is a unitary circut family
                                                            $\mathcal{U}_{\Phi(\vec{x})}$ applied to $|0\rangle^n$. The
                                                            result can be denoted as
                                                            $|\Phi(\vec{x})\rangle=\mathcal{U}_{\Phi(\vec{x})}|0\rangle^n$,
                                                            such that the state in the feature space is linearly
                                                            independent. Let's analyze a feature map corresponding to a
                                                            product state. First, assume that a feature map is comprised
                                                            of single qubit rotations $U(\varphi) \in \mathrm{SU}(2)$
                                                            aranged in a quantum circuit. These angles correspond to a
                                                            non-linear function $\varphi: \vec{x} \rightarrow(0,2 \pi]^2
                                                            \times[0, \pi]$ mapped to the space of Euler angles. This
                                                            means that the feature mapping action for an individual
                                                            qubit is: $\vec{x}
                                                            \mapsto\left|\phi_i(\vec{x})\right\rangle=U\left(\varphi_i(\vec{x})\right)|0\rangle$
                                                            and the feature mapping action for the full quibit state is:
                                                            $\Phi: \vec{x}
                                                            \mapsto|\Phi(\vec{x})\rangle\left\langle\Phi(\vec{x})|\right.$$\left.=\bigotimes_{i=1}^n|
                                                            \phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                            Stoudenmire and Schwab using tensor networks is an example
                                                            of unitary implementation of feature mapping of classical
                                                            classifiers. In this implementation, each qubit encodes a
                                                            single component $x_{i}$ of $\vec{x}\in[0,1]^{n}$, thus,
                                                            using $n$ qubits. The prepared state of this full qubit
                                                            feature mapping encoding when expanded to the Pauli-matrix
                                                            basis:
                                                            $\bigotimes_{i=1}^n\left|\phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                            $=$ $\frac{1}{2^n} \bigotimes_{j=1}^n\left(\sum_{\alpha_j}
                                                            \Phi_j^{\alpha_j}\left(\theta_j(\vec{x})\right)
                                                            P_{\alpha_j}\right)$ Such that, with respect to the
                                                            Pauli-matrix basis,
                                                            $\Phi_i^\alpha\left(\theta_i(\vec{x})\right)$ $=$
                                                            $\left\langle\phi_i(x)\left|P_{\alpha_i}\right|
                                                            \phi_i(x)\right\rangle$ for every $i=1\dots,n$ and where
                                                            $P_{\alpha_i} \in\left\{\mathbb{1}, X_i, Z_i, Y_i\right\}$.
                                                            To continue. Quantum Support Vector Machine Page Reference:
                                                            Supervised Learning with Quantum Enhanced Feature Spaces by:
                                                            Vojtech Havlicek,* Antonio D. Corcoles, Kristan Temme, Aram
                                                            W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M.
                                                            Gambetta. IBM T.J. Watson Research Center and Center for
                                                            Theoretical Physics, Massachusetts Institute of Technology
                                                            (Dated: June 7, 2018) $^{[1]}$ My code for detecting credit
                                                            card fraud using a Quantum Support Vector Machine:
                                                            QuantumSVMFraudDetection Introduction Using quantum
                                                            computing, the authors exploit quantum mechanics for the
                                                            algorithmic complexity optimization of a Support Vector
                                                            Machine with high-dimensional feature space. Where the
                                                            high-dimensional classical data is mapped non-linearly to
                                                            Hilbert Space and a hyperplane in quantum space is used to
                                                            separate and label the data. By using the quantum state
                                                            space as the feature space, the authors hope to obtain a
                                                            quantum advantage. Let's consider we have data from a
                                                            training set $T$ and a test set $S$, where $T, S$ are
                                                            subsets $\Omega\subset\mathbb{R}^{d}$. For each training
                                                            set, we assume the data is labeled by a true map $m: T \cup
                                                            S \rightarrow\left\{+1, -1\right\}$, which is not given to
                                                            the algorithm. The algorithm is given training data, where
                                                            the goal is for the algorithm to infer an approximate
                                                            mappping $\tilde{m}: T\rightarrow\{+1,-1\}$ for the data.
                                                            After training, the algorithm is asked to apply the
                                                            approximate mapping on a test set $\tilde{m}:
                                                            S\rightarrow\{+1,-1\}$. The approximate mapping needs to
                                                            match the true map $m(\vec{s})=\tilde{m}(\vec{s})$ with a
                                                            high probability. To use quantum state space as feature
                                                            space a mapping is constructed such that the data is
                                                            non-linearly mapped to a quantum state
                                                            $\Phi:\vec{x}\in\Omega\rightarrow
                                                            |\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. Where,
                                                            $|\Phi(\vec{x})\rangle$ is a part of complex vector space or
                                                            Hilbert Space $\mathcal{H}$ and $\langle\Phi(\vec{x})|$ is
                                                            the complex conjugate that is a dual correspondance. We can
                                                            see that the non-linear mapping $\Phi$ takes the classical
                                                            data $\vec{x}\in\Omega\subset\mathbb{R}^{d}$ or $\vec{x}\in
                                                            T\cup S$ and reperesents it in quantum state space
                                                            $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. To continue,
                                                            we will look at the procedure behind quantum mapping, rather
                                                            than its domain and codomain, training, and then testing.
                                                            Quantum Feature Mapping The authors suggest that in order to
                                                            have an advantage over classical approaches, a map based on
                                                            circuts that are clasically hard to simulate needs to be
                                                            constructed, but where the circut is not too deep to test on
                                                            during experimentation. The authors provide a circut diagram
                                                            that represents a unitary function for feature mapping
                                                            $n$-qubits. The circut diagram is given as: Image Source:
                                                            $^{[1]}$ Note, we will examine more in depth the feature
                                                            mapping unitary function later on. Data Generation First, we
                                                            will define the map used for the artifical data. The data is
                                                            generated of dimension $n=d=2$ for a $2$-qubit system with
                                                            the mapping $\phi_{\{i\}}(\vec{x})=x_{i}$ and
                                                            $\phi_{\{1,2\}}(\vec{x})=(\pi-x_{1})(\pi-x_{2})$. Next, we
                                                            look at how the data is generated. For the data vector
                                                            labels $\vec{x}\in T\cup S \subset (0, 2\pi]^{2}$ the
                                                            authors mention generating it using the parity function
                                                            $\mathbf{f}=Z_{1}Z_{2}$ and random unitary $V\in SU(4)$.
                                                            Next, to label the data the authors describe the following
                                                            mapping. Given $\Delta=0.3$, if:
                                                            $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\geq\Delta$
                                                            then $m(\vec{x})=+1$. If:
                                                            $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\leq-\Delta$
                                                            then $m(\vec{x})=-1$. Proposed SVM Classifiers Quantum
                                                            Variational Classification In the this section, we will
                                                            deconstruct and analyze individual components of the circut,
                                                            unitary, and steps to better understand how this processes
                                                            works. Then, in the following section, we will look at
                                                            classifier optimization, testing the trained model, and
                                                            analyze the authors experimental findings. Procedure The
                                                            authors have provided a figure of the quantum variational
                                                            classification QVC circut and it is shown below: Image
                                                            Source: $^{[1]}$ where $C=\{+1,-1\}$ and the varitional
                                                            unitary for QVC as a whole is defined as:
                                                            $p_{y}(\vec{x})$$=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                            The authors describe the procedure for the QVC in the
                                                            following four sequential steps: feature map encoding,
                                                            variational optimization, measurement, and then error
                                                            probability. The training and classification phase consist
                                                            of these four main parts and is what we will analyze in the
                                                            following. Feature Map Encoding Recall that the circut looks
                                                            like the following: Image Source: $^{[1]}$ The data
                                                            $\vec{x}\in\Omega$ is mapped to a reference state
                                                            $|0\rangle^{n}$ using the feature map circut
                                                            $\mathcal{U}_{\Phi(\vec{x})}$. This feature map is an
                                                            injective encoding of classical information
                                                            $\vec{x}\in\mathbb{R}^{d}$ to a quantum state
                                                            $|\Phi(\vec{x})\rangle\langle(\vec{x})\Phi|$ that is on a
                                                            $n$-qubit register such that $d=n$. A quibit is a two-level
                                                            system of Hilbert space and can be represented as
                                                            $\mathcal{H}_{2}=\mathbb{C}^{2}$. To represent $n$-qubits we
                                                            denote it as $\mathcal{H}_{2}^{\otimes
                                                            n}=\left(\mathbb{C}^2\right)^{\otimes n}$. The offered
                                                            feature mapping portrayed above is a family of feature maps,
                                                            that the authors selected because they conjecture it is hard
                                                            to estimate overlap $|\langle\Phi(\vec{x}) \mid
                                                            \Phi(\vec{y})\rangle|^2$ on a classical computer. The family
                                                            of feature map circuits is defined as follows:
                                                            $|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})} H^{\otimes n}
                                                            U_{\Phi(\vec{x})} H^{\otimes n}|0\rangle^{\otimes n}$ where
                                                            $H$ is a conventional Hadmard gate and $U_{\Phi(\vec{x})}$
                                                            is defined as:
                                                            $\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in
                                                            S}Z_{i}\right)$ and is a diagonal gate in the Pauli Z-basis.
                                                            For a single qubit, the unitary function $U_{\Phi(x)}$ acts
                                                            as a phase-gate $Z_{x}$ of angle $x\in\Omega$. Feature
                                                            mapping allows for $2^{n}$ possible coefficents
                                                            $\phi_{S}\left(\vec{x}\right)$ for non-linear function of
                                                            the inputed data $\vec{x}\in\mathbb{R}^{n}$. Variational
                                                            Classification A short depth quantum circut
                                                            $W(\vec{\theta})$ is applied to the feature state is a
                                                            variational circut used for the optimization method. A
                                                            depiction of this short depth quantum circut is given by the
                                                            authors and shown in the figure below: Image Source: $[1]$
                                                            For this short depth quantum circut, the authors use an
                                                            Ansatz variational unitary $W(\vec{\theta})$, defined as:
                                                            $U_{loc}^{(l)}(\theta_{l})U_{ent}...$$U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})$
                                                            Where, $U_{loc}^{(l)}(\theta_{l})$ is full layers single
                                                            qubit rotations given as:
                                                            $U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})$
                                                            and $U(\theta_{i,t})\in SU(2)$. For the variational unitary,
                                                            $U_{ent}$ is an alternating layers of entangling gates and
                                                            is defined as: $U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)$
                                                            Here, $E$ is an edge in the circut defined vetice set
                                                            $\{v_{i}, v_{j}\}\in V$. $\mathbf{CZ}$ is a controlled phase
                                                            gate applied along the edges $(i,j)\in E$ which the authors
                                                            state is present in the connectivity of the superconducting
                                                            chip. This variational circut is parametrized by
                                                            $\vec{\theta }\in\mathbb{R}^{2n(l+1)}$ and it is what is
                                                            optimized during training as this is what classifies the
                                                            data. Measurment Given that the problem is a two label
                                                            classification $y\in\{+1,-1\}$, the authors apply a binary
                                                            measurment $\{M_{y}\}$ to the state
                                                            $W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}$.
                                                            The authors describe the binary measurment in the following
                                                            way. The measurment is in the $Z$-basis, outputs the
                                                            bit-string $z\in\{0,1\}^{n}$, where the bit-string is then
                                                            passed to the boolean function
                                                            $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The binary measurment
                                                            is defined as $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$, where
                                                            $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle z|$.
                                                            From this third step, the probability for the outcome $y$ is
                                                            obtained. This probability $p_{y}(\vec{x})$ is defined as:
                                                            $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                            Where the probability for measuring either label
                                                            $y\in\{+1,-1\}$, denoted as $p_y$, is defined as:
                                                            $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                            W(\theta)\right| \Phi(\vec{x})\right\rangle\right)$ Since
                                                            the expected value of the measured observable is
                                                            $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                            W^{\dagger}(\theta, \varphi)\right.$$\left.\mathbf{f}
                                                            W(\theta, \varphi)\right]$, it can be expressed in terms of
                                                            the inner product: $\frac{1}{2^n} \sum_\alpha
                                                            w_\alpha(\theta) \Phi_\alpha(\vec{x})$ Next, we need to
                                                            incorporate assigning the label $y\in\{+1,-1\}$ over the
                                                            label $-y$ with a given fixed basis $b\in[-1,+1]$. In this
                                                            case, it must be that $p_{-y}-yb<\ p_{y}$. Subsituting the
                                                                probability for measuring either label $y\in\{+1,-1\}$
                                                                in to the inner product of the measured observable, we
                                                                obtain the value for $\tilde{m}(\vec{x})$ as:
                                                                $\operatorname{sign}\left(\frac{1}{2^n} \sum_\alpha
                                                                w_\alpha(\theta) \Phi_\alpha(\vec{x})+b\right)$ For the
                                                                final decision ruling of $y$, $R$ repeated measurment
                                                                shots are preformed, yielding the empircal distribution
                                                                $\hat{p}(\vec{x})$, where if $\hat{p}_{-y}(\vec{x}-yb)>
                                                                \hat{p}_{y}(\vec{x})$ the label assigned is
                                                                $\tilde{m}(\vec{x})=y$. The authors introduced a bias
                                                                parameter $b\in[-1,1]$ that can also be optimized during
                                                                training. The authors mention that the feature map
                                                                circut $\mathcal{U}_{\Phi(\vec{x})}$ and the boolean
                                                                function $f$ are fixed choices. The parameters that are
                                                                being optimized during training are $(\vec{\theta},b)$
                                                                and in order to be optimized, a cost-function is need to
                                                                be defined. To do so, we need to define an error
                                                                probability first, which is what we will look at in the
                                                                next section. Error Probability In order to find the
                                                                empirical risk $R_{emp}(\vec{\theta})$ of the empirical
                                                                distribution $\hat{p}(\vec{x})$ we define the error
                                                                probability of assigning the incorrect labels averaged
                                                                over the samples in the training set $T$. The authors
                                                                give the definition: $R_{emp}(\vec{\theta})$ $=$
                                                                $\frac{1}{|T|}\sum_{\vec{x}\in
                                                                T}Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$ that defines
                                                                this error probability. For our binary classification
                                                                problem, the error probability of assigning the wrong
                                                                label to some given data can be clacilated using the
                                                                binomial cimulative density function CDF for the
                                                                empircal distribution $\hat{p}(\vec{x})$. For a large
                                                                number of samples or shots $R\gg 1$ the CDF is
                                                                approximated by a sigmoid function
                                                                $\operatorname{sig}(x)=(1+e^{-x})^{-1}$. The probability
                                                                for label $m(\vec{x}=y)$ being assigned incorrectly can
                                                                be approximated by: $Pr(\tilde{m}(\vec{x})\neq
                                                                m(\vec{x}))$
                                                                $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                                b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                                \hat{p}_y(\vec{x})}}\right)$ Classifier Optimization and
                                                                Testing Results The first step is to train the
                                                                classifier and optimize it. The authors found that they
                                                                found Spall's SPSA stochastic gradient descent algorithm
                                                                performs the best in the inherent noisy experimental
                                                                setting of quantum computing. They mentioned after the
                                                                parameters converged onto
                                                                $\big(\vec{\Theta^{*}},b^{*}\big)$. The second step is
                                                                then to classify data and assign them labels according
                                                                to the decision rule given by $\tilde{m}(\vec{s})$,
                                                                where the binary measurement is obtained from the parity
                                                                function $\mathbf{f}=Z_1 Z_2$, and ensure that for the
                                                                testing data, $m(\vec{s})=\tilde{m}(\vec{s})$ with high
                                                                probability for the given data set $\vec{s}\in S$. The
                                                                authors note that what is obeserved for the empirical
                                                                risk, or cost value used for the optimizer, is that it
                                                                converges to a lower depth when the number of layers in
                                                                the short depth quantum circuit for optimization is
                                                                $l=4$ than $l=0$. Interesting enough, error mitigation
                                                                does not appreciably improve the empirical risk results
                                                                when depth is at $0$, however, does substantially help
                                                                for larger depths. An important note, is that although
                                                                $\operatorname{Pr}(\tilde{m}(\vec{x}) \neq m(\vec{x}))$
                                                                includes the number of shots $R$ taken in its
                                                                calculation, during experimentation the authors fixed
                                                                $R=200$. The authors state the reason for doing this,
                                                                even though $R=2000$ in actual experiment, was to avoid
                                                                gradient problems. To continue, after training, comes
                                                                testing. The yielded trained set of parameters
                                                                $\big(\vec{\theta}^{*},b^*=0\big)$ where used to
                                                                classify 20 different test sets randomly drawn each time
                                                                per data set. After analyzing the results, the authors
                                                                an increasing classification success with increasing
                                                                circut depth $l$. Noting, that the success rate very
                                                                nearl reaches $100$% for circut depths larger than $1$
                                                                and remains up to depth $4$, despite decoherence
                                                                associated with $8$ CNOT gates during training and
                                                                classification circuts. Quantum Variational
                                                                Classification Analysis An important aspect of SVM's, is
                                                                the classification of the data, which is done here based
                                                                off of the decision rule $p_y(\vec{x})>p_{-y}(\vec{x})-y
                                                                b$. This decision rule $\tilde{m}(\vec{x})$ can can be
                                                                restated as $\operatorname{sign} \big(
                                                                \langle\Phi(\vec{x}) |W^{\dagger}(\vec{\theta})$$
                                                                \mathbf{f} W(\vec{\theta}) | \Phi(\vec{x}) \rangle+b
                                                                \big)$. The step that structures the data in such a way
                                                                for classification, is the variational circut $W$, which
                                                                seperates the data using hyperplane $\vec{w}$ in quantum
                                                                state space. A feature map of a single qubit, along with
                                                                the seperating hyperplane $\vec{w}$ and the invterval of
                                                                binary labels $\Omega=(0,2\pi]$ for the classical data,
                                                                is given by the authors below. Image Source: $[1]$ We
                                                                start by decomposing the quantum variational circut.
                                                                First, we look to define the properties for our group of
                                                                matrices used. For us, we need a group that has an
                                                                orthogonal, hermatian, matrix basis
                                                                $\left\{P_\alpha\right\} \subset \mathbb{C}^{2^n \times
                                                                2^n}$. Such that $\alpha=1,...,4^{n}$ with
                                                                $\operatorname{tr}\left[P_\alpha^{\dagger}
                                                                P_\beta\right]=2^n \delta_{\alpha, \beta}$. A group of
                                                                matrices adhereing to this criteria is the Pauli-group
                                                                on $n$-qubits. Next, we expand the quantum state and the
                                                                measurment in our choosen matrix basis. The expanded
                                                                quantum state, or expectation value, can be expressed in
                                                                terms of $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$
                                                                $\rightarrow$
                                                                $\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
                                                                \Phi(\vec{x})\right\rangle$. The decision rule expressed
                                                                in our choosen matrix basis as $w_\alpha(\vec{\theta})$
                                                                can be defined as $W^{\dagger}(\vec{\theta}) \mathbf{f}
                                                                W(\vec{\theta})$ $\rightarrow$
                                                                $\operatorname{tr}\left[W^{\dagger}(\vec{\theta})
                                                                \mathbf{f} W(\vec{\theta}) P_\alpha\right]$. Lastly, any
                                                                classification rule or mapping $\tilde{m}(x)$ from a
                                                                variational unitary can be restated in the SVM form:
                                                                $\operatorname{sign}\left(2^{-n} \sum_\alpha
                                                                w_\alpha(\vec{\theta}) \Phi_\alpha(\vec{x})+b\right)$
                                                                What we can see is that the behavior of the classifier
                                                                is dependent on the larger term $\omega_{\alpha}$, so by
                                                                improving this term, this constraining term is lifted
                                                                from the variational circut. The authors note as well
                                                                that the optimal value for $\omega_{\alpha}$ can
                                                                alternatively be found through implementing kernel
                                                                methods and the Wolfe dual approach of the SVM. The
                                                                important idea we can gain from decomposing of
                                                                variational circut, is that we can think of the feature
                                                                space as the quantum state space that has feature
                                                                vectors $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$ and
                                                                inner products $K(\vec{x},
                                                                \vec{z})=|\langle\Phi(\vec{x}) \mid
                                                                \Phi(\vec{z})\rangle|^2$. With this more clear image of
                                                                how feature space is being represented as quantum state
                                                                space, we can see that the direct use of Hilbert space
                                                                for this can lead to conceptual errors. Such that, a
                                                                vector in Hilbert space $|\Phi(\vec{x})\rangle \in
                                                                \mathcal{H}$ is only defined up to a global phase
                                                                physically. What was seen was that a quantum advantage
                                                                is mainly obtained from feature maps that have a
                                                                classicaly hard to estimate kernel. Quantum Kernel
                                                                Estimation Procedure Unlike, in a variational quantum
                                                                circut to generate a seperating hyperplane for the
                                                                high-dimesnional feature space, Quantum Kernel
                                                                Estimation a classical SVM classification instead.
                                                                Meaning, this type of classification does not make a
                                                                direct use of Hilbert space, allowing for it to side
                                                                step the inherent conceptual errors of that quantum
                                                                variational classification feature map representation.
                                                                To continue. Quantum Feature Mapping Implementations for
                                                                Support Vector Machines A feature map can be denoted as
                                                                $\Phi: \Omega \subset \mathbb{R}^d \rightarrow
                                                                \mathcal{S}\left(\mathcal{H}_2^{\otimes n}\right)$ such
                                                                that $\Phi: \vec{x}
                                                                \mapsto|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. The
                                                                map acting on the data input is a unitary circut family
                                                                $\mathcal{U}_{\Phi(\vec{x})}$ applied to $|0\rangle^n$.
                                                                The result can be denoted as
                                                                $|\Phi(\vec{x})\rangle=\mathcal{U}_{\Phi(\vec{x})}|0\rangle^n$,
                                                                such that the state in the feature space is linearly
                                                                independent. Let's analyze a feature map corresponding
                                                                to a product state. First, assume that a feature map is
                                                                comprised of single qubit rotations $U(\varphi) \in
                                                                \mathrm{SU}(2)$ aranged in a quantum circuit. These
                                                                angles correspond to a non-linear function $\varphi:
                                                                \vec{x} \rightarrow(0,2 \pi]^2 \times[0, \pi]$ mapped to
                                                                the space of Euler angles. This means that the feature
                                                                mapping action for an individual qubit is: $\vec{x}
                                                                \mapsto\left|\phi_i(\vec{x})\right\rangle=U\left(\varphi_i(\vec{x})\right)|0\rangle$
                                                                and the feature mapping action for the full quibit state
                                                                is: $\Phi: \vec{x}
                                                                \mapsto|\Phi(\vec{x})\rangle\left\langle\Phi(\vec{x})|\right.$$\left.=\bigotimes_{i=1}^n|
                                                                \phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                                Stoudenmire and Schwab using tensor networks is an
                                                                example of unitary implementation of feature mapping of
                                                                classical classifiers. In this implementation, each
                                                                qubit encodes a single component $x_{i}$ of
                                                                $\vec{x}\in[0,1]^{n}$, thus, using $n$ qubits. The
                                                                prepared state of this full qubit feature mapping
                                                                encoding when expanded to the Pauli-matrix basis:
                                                                $\bigotimes_{i=1}^n\left|\phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                                $=$ $\frac{1}{2^n}
                                                                \bigotimes_{j=1}^n\left(\sum_{\alpha_j}
                                                                \Phi_j^{\alpha_j}\left(\theta_j(\vec{x})\right)
                                                                P_{\alpha_j}\right)$ Such that, with respect to the
                                                                Pauli-matrix basis,
                                                                $\Phi_i^\alpha\left(\theta_i(\vec{x})\right)$ $=$
                                                                $\left\langle\phi_i(x)\left|P_{\alpha_i}\right|
                                                                \phi_i(x)\right\rangle$ for every $i=1\dots,n$ and where
                                                                $P_{\alpha_i} \in\left\{\mathbb{1}, X_i, Z_i,
                                                                Y_i\right\}$. To continue. Quantum Support Vector
                                                                MachinePage Reference: Supervised Learning with Quantum
                                                                Enhanced Feature Spaces by: Vojtech Havlicek,* Antonio
                                                                D. Corcoles, Kristan Temme, Aram W. Harrow, Abhinav
                                                                Kandala, Jerry M. Chow, and Jay M. Gambetta. IBM T.J.
                                                                Watson Research Center and Center for Theoretical
                                                                Physics, Massachusetts Institute of Technology (Dated:
                                                                June 7, 2018) $^{[1]}$ Supervised Learning with Quantum
                                                                Enhanced Feature Spaces by: Vojtech Havlicek,* Antonio
                                                                D. Corcoles, Kristan Temme, Aram W. Harrow, Abhinav
                                                                Kandala, Jerry M. Chow, and Jay M. Gambetta. IBM T.J.
                                                                Watson Research Center and Center for Theoretical
                                                                Physics, Massachusetts Institute of Technology (Dated:
                                                                June 7, 2018) $^{[1]}$ Supervised Learning with Quantum
                                                                Enhanced Feature Spaces by: Vojtech Havlicek,* Antonio
                                                                D. Corcoles, Kristan Temme, Aram W. Harrow, Abhinav
                                                                Kandala, Jerry M. Chow, and Jay M. Gambetta. IBM T.J.
                                                                Watson Research Center and Center for Theoretical
                                                                Physics, Massachusetts Institute of Technology (Dated:
                                                                June 7, 2018) My code for detecting credit card fraud
                                                                using a Quantum Support Vector Machine:
                                                                QuantumSVMFraudDetection QuantumSVMFraudDetection
                                                                QuantumSVMFraudDetection Introduction Using quantum
                                                                computing, the authors exploit quantum mechanics for the
                                                                algorithmic complexity optimization of a Support Vector
                                                                Machine with high-dimensional feature space. Where the
                                                                high-dimensional classical data is mapped non-linearly
                                                                to Hilbert Space and a hyperplane in quantum space is
                                                                used to separate and label the data. By using the
                                                                quantum state space as the feature space, the authors
                                                                hope to obtain a quantum advantage. Let's consider we
                                                                have data from a training set $T$ and a test set $S$,
                                                                where $T, S$ are subsets $\Omega\subset\mathbb{R}^{d}$.
                                                                For each training set, we assume the data is labeled by
                                                                a true map $m: T \cup S \rightarrow\left\{+1,
                                                                -1\right\}$, which is not given to the algorithm. The
                                                                algorithm is given training data, where the goal is for
                                                                the algorithm to infer an approximate mappping
                                                                $\tilde{m}: T\rightarrow\{+1,-1\}$ for the data. After
                                                                training, the algorithm is asked to apply the
                                                                approximate mapping on a test set $\tilde{m}:
                                                                S\rightarrow\{+1,-1\}$. The approximate mapping needs to
                                                                match the true map $m(\vec{s})=\tilde{m}(\vec{s})$ with
                                                                a high probability. To use quantum state space as
                                                                feature space a mapping is constructed such that the
                                                                data is non-linearly mapped to a quantum state
                                                                $\Phi:\vec{x}\in\Omega\rightarrow
                                                                |\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. Where,
                                                                $|\Phi(\vec{x})\rangle$ is a part of complex vector
                                                                space or Hilbert Space $\mathcal{H}$ and
                                                                $\langle\Phi(\vec{x})|$ is the complex conjugate that is
                                                                a dual correspondance. We can see that the non-linear
                                                                mapping $\Phi$ takes the classical data
                                                                $\vec{x}\in\Omega\subset\mathbb{R}^{d}$ or $\vec{x}\in
                                                                T\cup S$ and reperesents it in quantum state space
                                                                $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. To
                                                                continue, we will look at the procedure behind quantum
                                                                mapping, rather than its domain and codomain, training,
                                                                and then testing. Quantum Feature Mapping The authors
                                                                suggest that in order to have an advantage over
                                                                classical approaches, a map based on circuts that are
                                                                clasically hard to simulate needs to be constructed, but
                                                                where the circut is not too deep to test on during
                                                                experimentation. The authors provide a circut diagram
                                                                that represents a unitary function for feature mapping
                                                                $n$-qubits. The circut diagram is given as: Image
                                                                Source: $^{[1]}$ Note, we will examine more in depth the
                                                                feature mapping unitary function later on.
                                                                IntroductionUsing quantum computing, the authors exploit
                                                                quantum mechanics for the algorithmic complexity
                                                                optimization of a Support Vector Machine with
                                                                high-dimensional feature space. Where the
                                                                high-dimensional classical data is mapped non-linearly
                                                                to Hilbert Space and a hyperplane in quantum space is
                                                                used to separate and label the data. By using the
                                                                quantum state space as the feature space, the authors
                                                                hope to obtain a quantum advantage. Let's consider we
                                                                have data from a training set $T$ and a test set $S$,
                                                                where $T, S$ are subsets $\Omega\subset\mathbb{R}^{d}$.
                                                                For each training set, we assume the data is labeled by
                                                                a true map $m: T \cup S \rightarrow\left\{+1,
                                                                -1\right\}$, which is not given to the algorithm. The
                                                                algorithm is given training data, where the goal is for
                                                                the algorithm to infer an approximate mappping
                                                                $\tilde{m}: T\rightarrow\{+1,-1\}$ for the data. After
                                                                training, the algorithm is asked to apply the
                                                                approximate mapping on a test set $\tilde{m}:
                                                                S\rightarrow\{+1,-1\}$. The approximate mapping needs to
                                                                match the true map $m(\vec{s})=\tilde{m}(\vec{s})$ with
                                                                a high probability. To use quantum state space as
                                                                feature space a mapping is constructed such that the
                                                                data is non-linearly mapped to a quantum state
                                                                $\Phi:\vec{x}\in\Omega\rightarrow
                                                                |\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. Where,
                                                                $|\Phi(\vec{x})\rangle$ is a part of complex vector
                                                                space or Hilbert Space $\mathcal{H}$ and
                                                                $\langle\Phi(\vec{x})|$ is the complex conjugate that is
                                                                a dual correspondance. We can see that the non-linear
                                                                mapping $\Phi$ takes the classical data
                                                                $\vec{x}\in\Omega\subset\mathbb{R}^{d}$ or $\vec{x}\in
                                                                T\cup S$ and reperesents it in quantum state space
                                                                $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$. To
                                                                continue, we will look at the procedure behind quantum
                                                                mapping, rather than its domain and codomain, training,
                                                                and then testing. Quantum Feature Mapping The authors
                                                                suggest that in order to have an advantage over
                                                                classical approaches, a map based on circuts that are
                                                                clasically hard to simulate needs to be constructed, but
                                                                where the circut is not too deep to test on during
                                                                experimentation. The authors provide a circut diagram
                                                                that represents a unitary function for feature mapping
                                                                $n$-qubits. The circut diagram is given as: Image
                                                                Source: $^{[1]}$ Note, we will examine more in depth the
                                                                feature mapping unitary function later on. Quantum
                                                                Feature MappingThe authors suggest that in order to have
                                                                an advantage over classical approaches, a map based on
                                                                circuts that are clasically hard to simulate needs to be
                                                                constructed, but where the circut is not too deep to
                                                                test on during experimentation. The authors provide a
                                                                circut diagram that represents a unitary function for
                                                                feature mapping $n$-qubits. The circut diagram is given
                                                                as: Image Source: $^{[1]}$ $^{[1]}$ Note, we will
                                                                examine more in depth the feature mapping unitary
                                                                function later on. Data Generation First, we will define
                                                                the map used for the artifical data. The data is
                                                                generated of dimension $n=d=2$ for a $2$-qubit system
                                                                with the mapping $\phi_{\{i\}}(\vec{x})=x_{i}$ and
                                                                $\phi_{\{1,2\}}(\vec{x})=(\pi-x_{1})(\pi-x_{2})$. Next,
                                                                we look at how the data is generated. For the data
                                                                vector labels $\vec{x}\in T\cup S \subset (0, 2\pi]^{2}$
                                                                the authors mention generating it using the parity
                                                                function $\mathbf{f}=Z_{1}Z_{2}$ and random unitary
                                                                $V\in SU(4)$. Next, to label the data the authors
                                                                describe the following mapping. Given $\Delta=0.3$, if:
                                                                $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\geq\Delta$
                                                                then $m(\vec{x})=+1$. If:
                                                                $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\leq-\Delta$
                                                                then $m(\vec{x})=-1$. Data Generation First, we will
                                                                define the map used for the artifical data. The data is
                                                                generated of dimension $n=d=2$ for a $2$-qubit system
                                                                with the mapping $\phi_{\{i\}}(\vec{x})=x_{i}$ and
                                                                $\phi_{\{1,2\}}(\vec{x})=(\pi-x_{1})(\pi-x_{2})$. Next,
                                                                we look at how the data is generated. For the data
                                                                vector labels $\vec{x}\in T\cup S \subset (0, 2\pi]^{2}$
                                                                the authors mention generating it using the parity
                                                                function $\mathbf{f}=Z_{1}Z_{2}$ and random unitary
                                                                $V\in SU(4)$. Next, to label the data the authors
                                                                describe the following mapping. Given $\Delta=0.3$, if:
                                                                $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\geq\Delta$
                                                                then $m(\vec{x})=+1$. If:
                                                                $\langle\Phi(\vec{x})|V^{\dagger}\mathbf{f}V|\Phi(\vec{x})\rangle\leq-\Delta$
                                                                then $m(\vec{x})=-1$. Proposed SVM Classifiers Quantum
                                                                Variational Classification In the this section, we will
                                                                deconstruct and analyze individual components of the
                                                                circut, unitary, and steps to better understand how this
                                                                processes works. Then, in the following section, we will
                                                                look at classifier optimization, testing the trained
                                                                model, and analyze the authors experimental findings.
                                                                Procedure The authors have provided a figure of the
                                                                quantum variational classification QVC circut and it is
                                                                shown below: Image Source: $^{[1]}$ where $C=\{+1,-1\}$
                                                                and the varitional unitary for QVC as a whole is defined
                                                                as:
                                                                $p_{y}(\vec{x})$$=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                The authors describe the procedure for the QVC in the
                                                                following four sequential steps: feature map encoding,
                                                                variational optimization, measurement, and then error
                                                                probability. The training and classification phase
                                                                consist of these four main parts and is what we will
                                                                analyze in the following. Feature Map Encoding Recall
                                                                that the circut looks like the following: Image Source:
                                                                $^{[1]}$ The data $\vec{x}\in\Omega$ is mapped to a
                                                                reference state $|0\rangle^{n}$ using the feature map
                                                                circut $\mathcal{U}_{\Phi(\vec{x})}$. This feature map
                                                                is an injective encoding of classical information
                                                                $\vec{x}\in\mathbb{R}^{d}$ to a quantum state
                                                                $|\Phi(\vec{x})\rangle\langle(\vec{x})\Phi|$ that is on
                                                                a $n$-qubit register such that $d=n$. A quibit is a
                                                                two-level system of Hilbert space and can be represented
                                                                as $\mathcal{H}_{2}=\mathbb{C}^{2}$. To represent
                                                                $n$-qubits we denote it as $\mathcal{H}_{2}^{\otimes
                                                                n}=\left(\mathbb{C}^2\right)^{\otimes n}$. The offered
                                                                feature mapping portrayed above is a family of feature
                                                                maps, that the authors selected because they conjecture
                                                                it is hard to estimate overlap $|\langle\Phi(\vec{x})
                                                                \mid \Phi(\vec{y})\rangle|^2$ on a classical computer.
                                                                The family of feature map circuits is defined as
                                                                follows: $|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})}
                                                                H^{\otimes n} U_{\Phi(\vec{x})} H^{\otimes
                                                                n}|0\rangle^{\otimes n}$ where $H$ is a conventional
                                                                Hadmard gate and $U_{\Phi(\vec{x})}$ is defined as:
                                                                $\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in
                                                                S}Z_{i}\right)$ and is a diagonal gate in the Pauli
                                                                Z-basis. For a single qubit, the unitary function
                                                                $U_{\Phi(x)}$ acts as a phase-gate $Z_{x}$ of angle
                                                                $x\in\Omega$. Feature mapping allows for $2^{n}$
                                                                possible coefficents $\phi_{S}\left(\vec{x}\right)$ for
                                                                non-linear function of the inputed data
                                                                $\vec{x}\in\mathbb{R}^{n}$. Variational Classification A
                                                                short depth quantum circut $W(\vec{\theta})$ is applied
                                                                to the feature state is a variational circut used for
                                                                the optimization method. A depiction of this short depth
                                                                quantum circut is given by the authors and shown in the
                                                                figure below: Image Source: $[1]$ For this short depth
                                                                quantum circut, the authors use an Ansatz variational
                                                                unitary $W(\vec{\theta})$, defined as:
                                                                $U_{loc}^{(l)}(\theta_{l})U_{ent}...$$U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})$
                                                                Where, $U_{loc}^{(l)}(\theta_{l})$ is full layers single
                                                                qubit rotations given as:
                                                                $U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})$
                                                                and $U(\theta_{i,t})\in SU(2)$. For the variational
                                                                unitary, $U_{ent}$ is an alternating layers of
                                                                entangling gates and is defined as:
                                                                $U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)$ Here, $E$
                                                                is an edge in the circut defined vetice set $\{v_{i},
                                                                v_{j}\}\in V$. $\mathbf{CZ}$ is a controlled phase gate
                                                                applied along the edges $(i,j)\in E$ which the authors
                                                                state is present in the connectivity of the
                                                                superconducting chip. This variational circut is
                                                                parametrized by $\vec{\theta }\in\mathbb{R}^{2n(l+1)}$
                                                                and it is what is optimized during training as this is
                                                                what classifies the data. Measurment Given that the
                                                                problem is a two label classification $y\in\{+1,-1\}$,
                                                                the authors apply a binary measurment $\{M_{y}\}$ to the
                                                                state
                                                                $W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}$.
                                                                The authors describe the binary measurment in the
                                                                following way. The measurment is in the $Z$-basis,
                                                                outputs the bit-string $z\in\{0,1\}^{n}$, where the
                                                                bit-string is then passed to the boolean function
                                                                $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The binary
                                                                measurment is defined as
                                                                $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$, where
                                                                $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle
                                                                z|$. From this third step, the probability for the
                                                                outcome $y$ is obtained. This probability
                                                                $p_{y}(\vec{x})$ is defined as:
                                                                $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                Where the probability for measuring either label
                                                                $y\in\{+1,-1\}$, denoted as $p_y$, is defined as:
                                                                $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                                W(\theta)\right| \Phi(\vec{x})\right\rangle\right)$
                                                                Since the expected value of the measured observable is
                                                                $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                                W^{\dagger}(\theta, \varphi)\right.$$\left.\mathbf{f}
                                                                W(\theta, \varphi)\right]$, it can be expressed in terms
                                                                of the inner product: $\frac{1}{2^n} \sum_\alpha
                                                                w_\alpha(\theta) \Phi_\alpha(\vec{x})$ Next, we need to
                                                                incorporate assigning the label $y\in\{+1,-1\}$ over the
                                                                label $-y$ with a given fixed basis $b\in[-1,+1]$. In
                                                                this case, it must be that $p_{-y}-yb<\ p_{y}$.
                                                                    Subsituting the probability for measuring either
                                                                    label $y\in\{+1,-1\}$ in to the inner product of the
                                                                    measured observable, we obtain the value for
                                                                    $\tilde{m}(\vec{x})$ as:
                                                                    $\operatorname{sign}\left(\frac{1}{2^n} \sum_\alpha
                                                                    w_\alpha(\theta) \Phi_\alpha(\vec{x})+b\right)$ For
                                                                    the final decision ruling of $y$, $R$ repeated
                                                                    measurment shots are preformed, yielding the
                                                                    empircal distribution $\hat{p}(\vec{x})$, where if
                                                                    $\hat{p}_{-y}(\vec{x}-yb)>\hat{p}_{y}(\vec{x})$ the
                                                                    label assigned is $\tilde{m}(\vec{x})=y$. The
                                                                    authors introduced a bias parameter $b\in[-1,1]$
                                                                    that can also be optimized during training. The
                                                                    authors mention that the feature map circut
                                                                    $\mathcal{U}_{\Phi(\vec{x})}$ and the boolean
                                                                    function $f$ are fixed choices. The parameters that
                                                                    are being optimized during training are
                                                                    $(\vec{\theta},b)$ and in order to be optimized, a
                                                                    cost-function is need to be defined. To do so, we
                                                                    need to define an error probability first, which is
                                                                    what we will look at in the next section. Error
                                                                    Probability In order to find the empirical risk
                                                                    $R_{emp}(\vec{\theta})$ of the empirical
                                                                    distribution $\hat{p}(\vec{x})$ we define the error
                                                                    probability of assigning the incorrect labels
                                                                    averaged over the samples in the training set $T$.
                                                                    The authors give the definition:
                                                                    $R_{emp}(\vec{\theta})$ $=$
                                                                    $\frac{1}{|T|}\sum_{\vec{x}\in
                                                                    T}Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$ that
                                                                    defines this error probability. For our binary
                                                                    classification problem, the error probability of
                                                                    assigning the wrong label to some given data can be
                                                                    clacilated using the binomial cimulative density
                                                                    function CDF for the empircal distribution
                                                                    $\hat{p}(\vec{x})$. For a large number of samples or
                                                                    shots $R\gg 1$ the CDF is approximated by a sigmoid
                                                                    function $\operatorname{sig}(x)=(1+e^{-x})^{-1}$.
                                                                    The probability for label $m(\vec{x}=y)$ being
                                                                    assigned incorrectly can be approximated by:
                                                                    $Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                                    $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                                    b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                                    \hat{p}_y(\vec{x})}}\right)$ Classifier Optimization
                                                                    and Testing Results The first step is to train the
                                                                    classifier and optimize it. The authors found that
                                                                    they found Spall's SPSA stochastic gradient descent
                                                                    algorithm performs the best in the inherent noisy
                                                                    experimental setting of quantum computing. They
                                                                    mentioned after the parameters converged onto
                                                                    $\big(\vec{\Theta^{*}},b^{*}\big)$. The second step
                                                                    is then to classify data and assign them labels
                                                                    according to the decision rule given by
                                                                    $\tilde{m}(\vec{s})$, where the binary measurement
                                                                    is obtained from the parity function $\mathbf{f}=Z_1
                                                                    Z_2$, and ensure that for the testing data,
                                                                    $m(\vec{s})=\tilde{m}(\vec{s})$ with high
                                                                    probability for the given data set $\vec{s}\in S$.
                                                                    The authors note that what is obeserved for the
                                                                    empirical risk, or cost value used for the
                                                                    optimizer, is that it converges to a lower depth
                                                                    when the number of layers in the short depth quantum
                                                                    circuit for optimization is $l=4$ than $l=0$.
                                                                    Interesting enough, error mitigation does not
                                                                    appreciably improve the empirical risk results when
                                                                    depth is at $0$, however, does substantially help
                                                                    for larger depths. An important note, is that
                                                                    although $\operatorname{Pr}(\tilde{m}(\vec{x}) \neq
                                                                    m(\vec{x}))$ includes the number of shots $R$ taken
                                                                    in its calculation, during experimentation the
                                                                    authors fixed $R=200$. The authors state the reason
                                                                    for doing this, even though $R=2000$ in actual
                                                                    experiment, was to avoid gradient problems. To
                                                                    continue, after training, comes testing. The yielded
                                                                    trained set of parameters
                                                                    $\big(\vec{\theta}^{*},b^*=0\big)$ where used to
                                                                    classify 20 different test sets randomly drawn each
                                                                    time per data set. After analyzing the results, the
                                                                    authors an increasing classification success with
                                                                    increasing circut depth $l$. Noting, that the
                                                                    success rate very nearl reaches $100$% for circut
                                                                    depths larger than $1$ and remains up to depth $4$,
                                                                    despite decoherence associated with $8$ CNOT gates
                                                                    during training and classification circuts. Quantum
                                                                    Variational Classification Analysis An important
                                                                    aspect of SVM's, is the classification of the data,
                                                                    which is done here based off of the decision rule
                                                                    $p_y(\vec{x})>p_{-y}(\vec{x})-y b$. This decision
                                                                    rule $\tilde{m}(\vec{x})$ can can be restated as
                                                                    $\operatorname{sign} \big( \langle\Phi(\vec{x})
                                                                    |W^{\dagger}(\vec{\theta})$$ \mathbf{f}
                                                                    W(\vec{\theta}) | \Phi(\vec{x}) \rangle+b \big)$.
                                                                    The step that structures the data in such a way for
                                                                    classification, is the variational circut $W$, which
                                                                    seperates the data using hyperplane $\vec{w}$ in
                                                                    quantum state space. A feature map of a single
                                                                    qubit, along with the seperating hyperplane
                                                                    $\vec{w}$ and the invterval of binary labels
                                                                    $\Omega=(0,2\pi]$ for the classical data, is given
                                                                    by the authors below. Image Source: $[1]$ We start
                                                                    by decomposing the quantum variational circut.
                                                                    First, we look to define the properties for our
                                                                    group of matrices used. For us, we need a group that
                                                                    has an orthogonal, hermatian, matrix basis
                                                                    $\left\{P_\alpha\right\} \subset \mathbb{C}^{2^n
                                                                    \times 2^n}$. Such that $\alpha=1,...,4^{n}$ with
                                                                    $\operatorname{tr}\left[P_\alpha^{\dagger}
                                                                    P_\beta\right]=2^n \delta_{\alpha, \beta}$. A group
                                                                    of matrices adhereing to this criteria is the
                                                                    Pauli-group on $n$-qubits. Next, we expand the
                                                                    quantum state and the measurment in our choosen
                                                                    matrix basis. The expanded quantum state, or
                                                                    expectation value, can be expressed in terms of
                                                                    $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$
                                                                    $\rightarrow$
                                                                    $\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
                                                                    \Phi(\vec{x})\right\rangle$. The decision rule
                                                                    expressed in our choosen matrix basis as
                                                                    $w_\alpha(\vec{\theta})$ can be defined as
                                                                    $W^{\dagger}(\vec{\theta}) \mathbf{f}
                                                                    W(\vec{\theta})$ $\rightarrow$
                                                                    $\operatorname{tr}\left[W^{\dagger}(\vec{\theta})
                                                                    \mathbf{f} W(\vec{\theta}) P_\alpha\right]$. Lastly,
                                                                    any classification rule or mapping $\tilde{m}(x)$
                                                                    from a variational unitary can be restated in the
                                                                    SVM form: $\operatorname{sign}\left(2^{-n}
                                                                    \sum_\alpha w_\alpha(\vec{\theta})
                                                                    \Phi_\alpha(\vec{x})+b\right)$ What we can see is
                                                                    that the behavior of the classifier is dependent on
                                                                    the larger term $\omega_{\alpha}$, so by improving
                                                                    this term, this constraining term is lifted from the
                                                                    variational circut. The authors note as well that
                                                                    the optimal value for $\omega_{\alpha}$ can
                                                                    alternatively be found through implementing kernel
                                                                    methods and the Wolfe dual approach of the SVM. The
                                                                    important idea we can gain from decomposing of
                                                                    variational circut, is that we can think of the
                                                                    feature space as the quantum state space that has
                                                                    feature vectors
                                                                    $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$ and
                                                                    inner products $K(\vec{x},
                                                                    \vec{z})=|\langle\Phi(\vec{x}) \mid
                                                                    \Phi(\vec{z})\rangle|^2$. With this more clear image
                                                                    of how feature space is being represented as quantum
                                                                    state space, we can see that the direct use of
                                                                    Hilbert space for this can lead to conceptual
                                                                    errors. Such that, a vector in Hilbert space
                                                                    $|\Phi(\vec{x})\rangle \in \mathcal{H}$ is only
                                                                    defined up to a global phase physically. What was
                                                                    seen was that a quantum advantage is mainly obtained
                                                                    from feature maps that have a classicaly hard to
                                                                    estimate kernel. Quantum Kernel Estimation Procedure
                                                                    Unlike, in a variational quantum circut to generate
                                                                    a seperating hyperplane for the high-dimesnional
                                                                    feature space, Quantum Kernel Estimation a classical
                                                                    SVM classification instead. Meaning, this type of
                                                                    classification does not make a direct use of Hilbert
                                                                    space, allowing for it to side step the inherent
                                                                    conceptual errors of that quantum variational
                                                                    classification feature map representation. To
                                                                    continue. Quantum Feature Mapping Implementations
                                                                    for Support Vector Machines A feature map can be
                                                                    denoted as $\Phi: \Omega \subset \mathbb{R}^d
                                                                    \rightarrow \mathcal{S}\left(\mathcal{H}_2^{\otimes
                                                                    n}\right)$ such that $\Phi: \vec{x}
                                                                    \mapsto|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$.
                                                                    The map acting on the data input is a unitary circut
                                                                    family $\mathcal{U}_{\Phi(\vec{x})}$ applied to
                                                                    $|0\rangle^n$. The result can be denoted as
                                                                    $|\Phi(\vec{x})\rangle=\mathcal{U}_{\Phi(\vec{x})}|0\rangle^n$,
                                                                    such that the state in the feature space is linearly
                                                                    independent. Let's analyze a feature map
                                                                    corresponding to a product state. First, assume that
                                                                    a feature map is comprised of single qubit rotations
                                                                    $U(\varphi) \in \mathrm{SU}(2)$ aranged in a quantum
                                                                    circuit. These angles correspond to a non-linear
                                                                    function $\varphi: \vec{x} \rightarrow(0,2 \pi]^2
                                                                    \times[0, \pi]$ mapped to the space of Euler angles.
                                                                    This means that the feature mapping action for an
                                                                    individual qubit is: $\vec{x}
                                                                    \mapsto\left|\phi_i(\vec{x})\right\rangle=U\left(\varphi_i(\vec{x})\right)|0\rangle$
                                                                    and the feature mapping action for the full quibit
                                                                    state is: $\Phi: \vec{x}
                                                                    \mapsto|\Phi(\vec{x})\rangle\left\langle\Phi(\vec{x})|\right.$$\left.=\bigotimes_{i=1}^n|
                                                                    \phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                                    Stoudenmire and Schwab using tensor networks is an
                                                                    example of unitary implementation of feature mapping
                                                                    of classical classifiers. In this implementation,
                                                                    each qubit encodes a single component $x_{i}$ of
                                                                    $\vec{x}\in[0,1]^{n}$, thus, using $n$ qubits. The
                                                                    prepared state of this full qubit feature mapping
                                                                    encoding when expanded to the Pauli-matrix basis:
                                                                    $\bigotimes_{i=1}^n\left|\phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                                    $=$ $\frac{1}{2^n}
                                                                    \bigotimes_{j=1}^n\left(\sum_{\alpha_j}
                                                                    \Phi_j^{\alpha_j}\left(\theta_j(\vec{x})\right)
                                                                    P_{\alpha_j}\right)$ Such that, with respect to the
                                                                    Pauli-matrix basis,
                                                                    $\Phi_i^\alpha\left(\theta_i(\vec{x})\right)$ $=$
                                                                    $\left\langle\phi_i(x)\left|P_{\alpha_i}\right|
                                                                    \phi_i(x)\right\rangle$ for every $i=1\dots,n$ and
                                                                    where $P_{\alpha_i} \in\left\{\mathbb{1}, X_i, Z_i,
                                                                    Y_i\right\}$. To continue. Proposed SVM
                                                                    ClassifiersQuantum Variational Classification In the
                                                                    this section, we will deconstruct and analyze
                                                                    individual components of the circut, unitary, and
                                                                    steps to better understand how this processes works.
                                                                    Then, in the following section, we will look at
                                                                    classifier optimization, testing the trained model,
                                                                    and analyze the authors experimental findings.
                                                                    Procedure The authors have provided a figure of the
                                                                    quantum variational classification QVC circut and it
                                                                    is shown below: Image Source: $^{[1]}$ where
                                                                    $C=\{+1,-1\}$ and the varitional unitary for QVC as
                                                                    a whole is defined as:
                                                                    $p_{y}(\vec{x})$$=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                    The authors describe the procedure for the QVC in
                                                                    the following four sequential steps: feature map
                                                                    encoding, variational optimization, measurement, and
                                                                    then error probability. The training and
                                                                    classification phase consist of these four main
                                                                    parts and is what we will analyze in the following.
                                                                    Feature Map Encoding Recall that the circut looks
                                                                    like the following: Image Source: $^{[1]}$ The data
                                                                    $\vec{x}\in\Omega$ is mapped to a reference state
                                                                    $|0\rangle^{n}$ using the feature map circut
                                                                    $\mathcal{U}_{\Phi(\vec{x})}$. This feature map is
                                                                    an injective encoding of classical information
                                                                    $\vec{x}\in\mathbb{R}^{d}$ to a quantum state
                                                                    $|\Phi(\vec{x})\rangle\langle(\vec{x})\Phi|$ that is
                                                                    on a $n$-qubit register such that $d=n$. A quibit is
                                                                    a two-level system of Hilbert space and can be
                                                                    represented as $\mathcal{H}_{2}=\mathbb{C}^{2}$. To
                                                                    represent $n$-qubits we denote it as
                                                                    $\mathcal{H}_{2}^{\otimes
                                                                    n}=\left(\mathbb{C}^2\right)^{\otimes n}$. The
                                                                    offered feature mapping portrayed above is a family
                                                                    of feature maps, that the authors selected because
                                                                    they conjecture it is hard to estimate overlap
                                                                    $|\langle\Phi(\vec{x}) \mid \Phi(\vec{y})\rangle|^2$
                                                                    on a classical computer. The family of feature map
                                                                    circuits is defined as follows:
                                                                    $|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})} H^{\otimes
                                                                    n} U_{\Phi(\vec{x})} H^{\otimes n}|0\rangle^{\otimes
                                                                    n}$ where $H$ is a conventional Hadmard gate and
                                                                    $U_{\Phi(\vec{x})}$ is defined as:
                                                                    $\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in
                                                                    S}Z_{i}\right)$ and is a diagonal gate in the Pauli
                                                                    Z-basis. For a single qubit, the unitary function
                                                                    $U_{\Phi(x)}$ acts as a phase-gate $Z_{x}$ of angle
                                                                    $x\in\Omega$. Feature mapping allows for $2^{n}$
                                                                    possible coefficents $\phi_{S}\left(\vec{x}\right)$
                                                                    for non-linear function of the inputed data
                                                                    $\vec{x}\in\mathbb{R}^{n}$. Variational
                                                                    Classification A short depth quantum circut
                                                                    $W(\vec{\theta})$ is applied to the feature state is
                                                                    a variational circut used for the optimization
                                                                    method. A depiction of this short depth quantum
                                                                    circut is given by the authors and shown in the
                                                                    figure below: Image Source: $[1]$ For this short
                                                                    depth quantum circut, the authors use an Ansatz
                                                                    variational unitary $W(\vec{\theta})$, defined as:
                                                                    $U_{loc}^{(l)}(\theta_{l})U_{ent}...$$U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})$
                                                                    Where, $U_{loc}^{(l)}(\theta_{l})$ is full layers
                                                                    single qubit rotations given as:
                                                                    $U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})$
                                                                    and $U(\theta_{i,t})\in SU(2)$. For the variational
                                                                    unitary, $U_{ent}$ is an alternating layers of
                                                                    entangling gates and is defined as:
                                                                    $U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)$ Here,
                                                                    $E$ is an edge in the circut defined vetice set
                                                                    $\{v_{i}, v_{j}\}\in V$. $\mathbf{CZ}$ is a
                                                                    controlled phase gate applied along the edges
                                                                    $(i,j)\in E$ which the authors state is present in
                                                                    the connectivity of the superconducting chip. This
                                                                    variational circut is parametrized by $\vec{\theta
                                                                    }\in\mathbb{R}^{2n(l+1)}$ and it is what is
                                                                    optimized during training as this is what classifies
                                                                    the data. Measurment Given that the problem is a two
                                                                    label classification $y\in\{+1,-1\}$, the authors
                                                                    apply a binary measurment $\{M_{y}\}$ to the state
                                                                    $W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}$.
                                                                    The authors describe the binary measurment in the
                                                                    following way. The measurment is in the $Z$-basis,
                                                                    outputs the bit-string $z\in\{0,1\}^{n}$, where the
                                                                    bit-string is then passed to the boolean function
                                                                    $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The binary
                                                                    measurment is defined as
                                                                    $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$, where
                                                                    $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle
                                                                    z|$. From this third step, the probability for the
                                                                    outcome $y$ is obtained. This probability
                                                                    $p_{y}(\vec{x})$ is defined as:
                                                                    $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                    Where the probability for measuring either label
                                                                    $y\in\{+1,-1\}$, denoted as $p_y$, is defined as:
                                                                    $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                                    W(\theta)\right| \Phi(\vec{x})\right\rangle\right)$
                                                                    Since the expected value of the measured observable
                                                                    is
                                                                    $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                                    W^{\dagger}(\theta,
                                                                    \varphi)\right.$$\left.\mathbf{f} W(\theta,
                                                                    \varphi)\right]$, it can be expressed in terms of
                                                                    the inner product: $\frac{1}{2^n} \sum_\alpha
                                                                    w_\alpha(\theta) \Phi_\alpha(\vec{x})$ Next, we need
                                                                    to incorporate assigning the label $y\in\{+1,-1\}$
                                                                    over the label $-y$ with a given fixed basis
                                                                    $b\in[-1,+1]$. In this case, it must be that
                                                                    $p_{-y}-yb<\ p_{y}$. Subsituting the probability for
                                                                        measuring either label $y\in\{+1,-1\}$ in to the
                                                                        inner product of the measured observable, we
                                                                        obtain the value for $\tilde{m}(\vec{x})$ as:
                                                                        $\operatorname{sign}\left(\frac{1}{2^n}
                                                                        \sum_\alpha w_\alpha(\theta)
                                                                        \Phi_\alpha(\vec{x})+b\right)$ For the final
                                                                        decision ruling of $y$, $R$ repeated measurment
                                                                        shots are preformed, yielding the empircal
                                                                        distribution $\hat{p}(\vec{x})$, where if
                                                                        $\hat{p}_{-y}(\vec{x}-yb)>\hat{p}_{y}(\vec{x})$
                                                                        the label assigned is $\tilde{m}(\vec{x})=y$.
                                                                        The authors introduced a bias parameter
                                                                        $b\in[-1,1]$ that can also be optimized during
                                                                        training. The authors mention that the feature
                                                                        map circut $\mathcal{U}_{\Phi(\vec{x})}$ and the
                                                                        boolean function $f$ are fixed choices. The
                                                                        parameters that are being optimized during
                                                                        training are $(\vec{\theta},b)$ and in order to
                                                                        be optimized, a cost-function is need to be
                                                                        defined. To do so, we need to define an error
                                                                        probability first, which is what we will look at
                                                                        in the next section. Error Probability In order
                                                                        to find the empirical risk
                                                                        $R_{emp}(\vec{\theta})$ of the empirical
                                                                        distribution $\hat{p}(\vec{x})$ we define the
                                                                        error probability of assigning the incorrect
                                                                        labels averaged over the samples in the training
                                                                        set $T$. The authors give the definition:
                                                                        $R_{emp}(\vec{\theta})$ $=$
                                                                        $\frac{1}{|T|}\sum_{\vec{x}\in
                                                                        T}Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$ that
                                                                        defines this error probability. For our binary
                                                                        classification problem, the error probability of
                                                                        assigning the wrong label to some given data can
                                                                        be clacilated using the binomial cimulative
                                                                        density function CDF for the empircal
                                                                        distribution $\hat{p}(\vec{x})$. For a large
                                                                        number of samples or shots $R\gg 1$ the CDF is
                                                                        approximated by a sigmoid function
                                                                        $\operatorname{sig}(x)=(1+e^{-x})^{-1}$. The
                                                                        probability for label $m(\vec{x}=y)$ being
                                                                        assigned incorrectly can be approximated by:
                                                                        $Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                                        $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                                        b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                                        \hat{p}_y(\vec{x})}}\right)$ Classifier
                                                                        Optimization and Testing Results The first step
                                                                        is to train the classifier and optimize it. The
                                                                        authors found that they found Spall's SPSA
                                                                        stochastic gradient descent algorithm performs
                                                                        the best in the inherent noisy experimental
                                                                        setting of quantum computing. They mentioned
                                                                        after the parameters converged onto
                                                                        $\big(\vec{\Theta^{*}},b^{*}\big)$. The second
                                                                        step is then to classify data and assign them
                                                                        labels according to the decision rule given by
                                                                        $\tilde{m}(\vec{s})$, where the binary
                                                                        measurement is obtained from the parity function
                                                                        $\mathbf{f}=Z_1 Z_2$, and ensure that for the
                                                                        testing data, $m(\vec{s})=\tilde{m}(\vec{s})$
                                                                        with high probability for the given data set
                                                                        $\vec{s}\in S$. The authors note that what is
                                                                        obeserved for the empirical risk, or cost value
                                                                        used for the optimizer, is that it converges to
                                                                        a lower depth when the number of layers in the
                                                                        short depth quantum circuit for optimization is
                                                                        $l=4$ than $l=0$. Interesting enough, error
                                                                        mitigation does not appreciably improve the
                                                                        empirical risk results when depth is at $0$,
                                                                        however, does substantially help for larger
                                                                        depths. An important note, is that although
                                                                        $\operatorname{Pr}(\tilde{m}(\vec{x}) \neq
                                                                        m(\vec{x}))$ includes the number of shots $R$
                                                                        taken in its calculation, during experimentation
                                                                        the authors fixed $R=200$. The authors state the
                                                                        reason for doing this, even though $R=2000$ in
                                                                        actual experiment, was to avoid gradient
                                                                        problems. To continue, after training, comes
                                                                        testing. The yielded trained set of parameters
                                                                        $\big(\vec{\theta}^{*},b^*=0\big)$ where used to
                                                                        classify 20 different test sets randomly drawn
                                                                        each time per data set. After analyzing the
                                                                        results, the authors an increasing
                                                                        classification success with increasing circut
                                                                        depth $l$. Noting, that the success rate very
                                                                        nearl reaches $100$% for circut depths larger
                                                                        than $1$ and remains up to depth $4$, despite
                                                                        decoherence associated with $8$ CNOT gates
                                                                        during training and classification circuts.
                                                                        Quantum Variational Classification Analysis An
                                                                        important aspect of SVM's, is the classification
                                                                        of the data, which is done here based off of the
                                                                        decision rule $p_y(\vec{x})>p_{-y}(\vec{x})-y
                                                                        b$. This decision rule $\tilde{m}(\vec{x})$ can
                                                                        can be restated as $\operatorname{sign} \big(
                                                                        \langle\Phi(\vec{x})
                                                                        |W^{\dagger}(\vec{\theta})$$ \mathbf{f}
                                                                        W(\vec{\theta}) | \Phi(\vec{x}) \rangle+b
                                                                        \big)$. The step that structures the data in
                                                                        such a way for classification, is the
                                                                        variational circut $W$, which seperates the data
                                                                        using hyperplane $\vec{w}$ in quantum state
                                                                        space. A feature map of a single qubit, along
                                                                        with the seperating hyperplane $\vec{w}$ and the
                                                                        invterval of binary labels $\Omega=(0,2\pi]$ for
                                                                        the classical data, is given by the authors
                                                                        below. Image Source: $[1]$ We start by
                                                                        decomposing the quantum variational circut.
                                                                        First, we look to define the properties for our
                                                                        group of matrices used. For us, we need a group
                                                                        that has an orthogonal, hermatian, matrix basis
                                                                        $\left\{P_\alpha\right\} \subset \mathbb{C}^{2^n
                                                                        \times 2^n}$. Such that $\alpha=1,...,4^{n}$
                                                                        with $\operatorname{tr}\left[P_\alpha^{\dagger}
                                                                        P_\beta\right]=2^n \delta_{\alpha, \beta}$. A
                                                                        group of matrices adhereing to this criteria is
                                                                        the Pauli-group on $n$-qubits. Next, we expand
                                                                        the quantum state and the measurment in our
                                                                        choosen matrix basis. The expanded quantum
                                                                        state, or expectation value, can be expressed in
                                                                        terms of
                                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$
                                                                        $\rightarrow$
                                                                        $\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
                                                                        \Phi(\vec{x})\right\rangle$. The decision rule
                                                                        expressed in our choosen matrix basis as
                                                                        $w_\alpha(\vec{\theta})$ can be defined as
                                                                        $W^{\dagger}(\vec{\theta}) \mathbf{f}
                                                                        W(\vec{\theta})$ $\rightarrow$
                                                                        $\operatorname{tr}\left[W^{\dagger}(\vec{\theta})
                                                                        \mathbf{f} W(\vec{\theta}) P_\alpha\right]$.
                                                                        Lastly, any classification rule or mapping
                                                                        $\tilde{m}(x)$ from a variational unitary can be
                                                                        restated in the SVM form:
                                                                        $\operatorname{sign}\left(2^{-n} \sum_\alpha
                                                                        w_\alpha(\vec{\theta})
                                                                        \Phi_\alpha(\vec{x})+b\right)$ What we can see
                                                                        is that the behavior of the classifier is
                                                                        dependent on the larger term $\omega_{\alpha}$,
                                                                        so by improving this term, this constraining
                                                                        term is lifted from the variational circut. The
                                                                        authors note as well that the optimal value for
                                                                        $\omega_{\alpha}$ can alternatively be found
                                                                        through implementing kernel methods and the
                                                                        Wolfe dual approach of the SVM. The important
                                                                        idea we can gain from decomposing of variational
                                                                        circut, is that we can think of the feature
                                                                        space as the quantum state space that has
                                                                        feature vectors
                                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$ and
                                                                        inner products $K(\vec{x},
                                                                        \vec{z})=|\langle\Phi(\vec{x}) \mid
                                                                        \Phi(\vec{z})\rangle|^2$. With this more clear
                                                                        image of how feature space is being represented
                                                                        as quantum state space, we can see that the
                                                                        direct use of Hilbert space for this can lead to
                                                                        conceptual errors. Such that, a vector in
                                                                        Hilbert space $|\Phi(\vec{x})\rangle \in
                                                                        \mathcal{H}$ is only defined up to a global
                                                                        phase physically. What was seen was that a
                                                                        quantum advantage is mainly obtained from
                                                                        feature maps that have a classicaly hard to
                                                                        estimate kernel. Quantum Variational
                                                                        ClassificationIn the this section, we will
                                                                        deconstruct and analyze individual components of
                                                                        the circut, unitary, and steps to better
                                                                        understand how this processes works. Then, in
                                                                        the following section, we will look at
                                                                        classifier optimization, testing the trained
                                                                        model, and analyze the authors experimental
                                                                        findings. Procedure The authors have provided a
                                                                        figure of the quantum variational classification
                                                                        QVC circut and it is shown below: Image Source:
                                                                        $^{[1]}$ where $C=\{+1,-1\}$ and the varitional
                                                                        unitary for QVC as a whole is defined as:
                                                                        $p_{y}(\vec{x})$$=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                        The authors describe the procedure for the QVC
                                                                        in the following four sequential steps: feature
                                                                        map encoding, variational optimization,
                                                                        measurement, and then error probability. The
                                                                        training and classification phase consist of
                                                                        these four main parts and is what we will
                                                                        analyze in the following. Feature Map Encoding
                                                                        Recall that the circut looks like the following:
                                                                        Image Source: $^{[1]}$ The data
                                                                        $\vec{x}\in\Omega$ is mapped to a reference
                                                                        state $|0\rangle^{n}$ using the feature map
                                                                        circut $\mathcal{U}_{\Phi(\vec{x})}$. This
                                                                        feature map is an injective encoding of
                                                                        classical information $\vec{x}\in\mathbb{R}^{d}$
                                                                        to a quantum state
                                                                        $|\Phi(\vec{x})\rangle\langle(\vec{x})\Phi|$
                                                                        that is on a $n$-qubit register such that $d=n$.
                                                                        A quibit is a two-level system of Hilbert space
                                                                        and can be represented as
                                                                        $\mathcal{H}_{2}=\mathbb{C}^{2}$. To represent
                                                                        $n$-qubits we denote it as
                                                                        $\mathcal{H}_{2}^{\otimes
                                                                        n}=\left(\mathbb{C}^2\right)^{\otimes n}$. The
                                                                        offered feature mapping portrayed above is a
                                                                        family of feature maps, that the authors
                                                                        selected because they conjecture it is hard to
                                                                        estimate overlap $|\langle\Phi(\vec{x}) \mid
                                                                        \Phi(\vec{y})\rangle|^2$ on a classical
                                                                        computer. The family of feature map circuits is
                                                                        defined as follows:
                                                                        $|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})}
                                                                        H^{\otimes n} U_{\Phi(\vec{x})} H^{\otimes
                                                                        n}|0\rangle^{\otimes n}$ where $H$ is a
                                                                        conventional Hadmard gate and
                                                                        $U_{\Phi(\vec{x})}$ is defined as:
                                                                        $\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in
                                                                        S}Z_{i}\right)$ and is a diagonal gate in the
                                                                        Pauli Z-basis. For a single qubit, the unitary
                                                                        function $U_{\Phi(x)}$ acts as a phase-gate
                                                                        $Z_{x}$ of angle $x\in\Omega$. Feature mapping
                                                                        allows for $2^{n}$ possible coefficents
                                                                        $\phi_{S}\left(\vec{x}\right)$ for non-linear
                                                                        function of the inputed data
                                                                        $\vec{x}\in\mathbb{R}^{n}$. Variational
                                                                        Classification A short depth quantum circut
                                                                        $W(\vec{\theta})$ is applied to the feature
                                                                        state is a variational circut used for the
                                                                        optimization method. A depiction of this short
                                                                        depth quantum circut is given by the authors and
                                                                        shown in the figure below: Image Source: $[1]$
                                                                        For this short depth quantum circut, the authors
                                                                        use an Ansatz variational unitary
                                                                        $W(\vec{\theta})$, defined as:
                                                                        $U_{loc}^{(l)}(\theta_{l})U_{ent}...$$U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})$
                                                                        Where, $U_{loc}^{(l)}(\theta_{l})$ is full
                                                                        layers single qubit rotations given as:
                                                                        $U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})$
                                                                        and $U(\theta_{i,t})\in SU(2)$. For the
                                                                        variational unitary, $U_{ent}$ is an alternating
                                                                        layers of entangling gates and is defined as:
                                                                        $U_{ent}=\prod_{(i,j)\in E}\mathbf{CZ}(i,j)$
                                                                        Here, $E$ is an edge in the circut defined
                                                                        vetice set $\{v_{i}, v_{j}\}\in V$.
                                                                        $\mathbf{CZ}$ is a controlled phase gate applied
                                                                        along the edges $(i,j)\in E$ which the authors
                                                                        state is present in the connectivity of the
                                                                        superconducting chip. This variational circut is
                                                                        parametrized by $\vec{\theta
                                                                        }\in\mathbb{R}^{2n(l+1)}$ and it is what is
                                                                        optimized during training as this is what
                                                                        classifies the data. Measurment Given that the
                                                                        problem is a two label classification
                                                                        $y\in\{+1,-1\}$, the authors apply a binary
                                                                        measurment $\{M_{y}\}$ to the state
                                                                        $W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}$.
                                                                        The authors describe the binary measurment in
                                                                        the following way. The measurment is in the
                                                                        $Z$-basis, outputs the bit-string
                                                                        $z\in\{0,1\}^{n}$, where the bit-string is then
                                                                        passed to the boolean function
                                                                        $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The binary
                                                                        measurment is defined as
                                                                        $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$, where
                                                                        $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle
                                                                        z|$. From this third step, the probability for
                                                                        the outcome $y$ is obtained. This probability
                                                                        $p_{y}(\vec{x})$ is defined as:
                                                                        $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                        Where the probability for measuring either label
                                                                        $y\in\{+1,-1\}$, denoted as $p_y$, is defined
                                                                        as:
                                                                        $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                                        W(\theta)\right|
                                                                        \Phi(\vec{x})\right\rangle\right)$ Since the
                                                                        expected value of the measured observable is
                                                                        $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                                        W^{\dagger}(\theta,
                                                                        \varphi)\right.$$\left.\mathbf{f} W(\theta,
                                                                        \varphi)\right]$, it can be expressed in terms
                                                                        of the inner product: $\frac{1}{2^n} \sum_\alpha
                                                                        w_\alpha(\theta) \Phi_\alpha(\vec{x})$ Next, we
                                                                        need to incorporate assigning the label
                                                                        $y\in\{+1,-1\}$ over the label $-y$ with a given
                                                                        fixed basis $b\in[-1,+1]$. In this case, it must
                                                                        be that $p_{-y}-yb<\ p_{y}$. Subsituting the
                                                                            probability for measuring either label
                                                                            $y\in\{+1,-1\}$ in to the inner product of
                                                                            the measured observable, we obtain the value
                                                                            for $\tilde{m}(\vec{x})$ as:
                                                                            $\operatorname{sign}\left(\frac{1}{2^n}
                                                                            \sum_\alpha w_\alpha(\theta)
                                                                            \Phi_\alpha(\vec{x})+b\right)$ For the final
                                                                            decision ruling of $y$, $R$ repeated
                                                                            measurment shots are preformed, yielding the
                                                                            empircal distribution $\hat{p}(\vec{x})$,
                                                                            where if $\hat{p}_{-y}(\vec{x}-yb)>
                                                                            \hat{p}_{y}(\vec{x})$ the label assigned is
                                                                            $\tilde{m}(\vec{x})=y$. The authors
                                                                            introduced a bias parameter $b\in[-1,1]$
                                                                            that can also be optimized during training.
                                                                            The authors mention that the feature map
                                                                            circut $\mathcal{U}_{\Phi(\vec{x})}$ and the
                                                                            boolean function $f$ are fixed choices. The
                                                                            parameters that are being optimized during
                                                                            training are $(\vec{\theta},b)$ and in order
                                                                            to be optimized, a cost-function is need to
                                                                            be defined. To do so, we need to define an
                                                                            error probability first, which is what we
                                                                            will look at in the next section. Error
                                                                            Probability In order to find the empirical
                                                                            risk $R_{emp}(\vec{\theta})$ of the
                                                                            empirical distribution $\hat{p}(\vec{x})$ we
                                                                            define the error probability of assigning
                                                                            the incorrect labels averaged over the
                                                                            samples in the training set $T$. The authors
                                                                            give the definition: $R_{emp}(\vec{\theta})$
                                                                            $=$ $\frac{1}{|T|}\sum_{\vec{x}\in
                                                                            T}Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                                            that defines this error probability. For our
                                                                            binary classification problem, the error
                                                                            probability of assigning the wrong label to
                                                                            some given data can be clacilated using the
                                                                            binomial cimulative density function CDF for
                                                                            the empircal distribution
                                                                            $\hat{p}(\vec{x})$. For a large number of
                                                                            samples or shots $R\gg 1$ the CDF is
                                                                            approximated by a sigmoid function
                                                                            $\operatorname{sig}(x)=(1+e^{-x})^{-1}$. The
                                                                            probability for label $m(\vec{x}=y)$ being
                                                                            assigned incorrectly can be approximated by:
                                                                            $Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                                            $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                                            b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                                            \hat{p}_y(\vec{x})}}\right)$ ProcedureThe
                                                                            authors have provided a figure of the
                                                                            quantum variational classification QVC
                                                                            circut and it is shown below: Image Source:
                                                                            $^{[1]}$ $^{[1]}$ where $C=\{+1,-1\}$ and
                                                                            the varitional unitary for QVC as a whole is
                                                                            defined as:
                                                                            $p_{y}(\vec{x})$$=\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                            The authors describe the procedure for the
                                                                            QVC in the following four sequential steps:
                                                                            feature map encoding, variational
                                                                            optimization, measurement, and then error
                                                                            probability. The training and classification
                                                                            phase consist of these four main parts and
                                                                            is what we will analyze in the following.
                                                                            feature map encodingvariational
                                                                            optimizationmeasurementerror
                                                                            probabilityFeature Map EncodingFeature Map
                                                                            EncodingRecall that the circut looks like
                                                                            the following: Image Source: $^{[1]}$
                                                                            $^{[1]}$ The data $\vec{x}\in\Omega$ is
                                                                            mapped to a reference state $|0\rangle^{n}$
                                                                            using the feature map circut
                                                                            $\mathcal{U}_{\Phi(\vec{x})}$. This feature
                                                                            map is an injective encoding of classical
                                                                            information $\vec{x}\in\mathbb{R}^{d}$ to a
                                                                            quantum state
                                                                            $|\Phi(\vec{x})\rangle\langle(\vec{x})\Phi|$
                                                                            that is on a $n$-qubit register such that
                                                                            $d=n$. A quibit is a two-level system of
                                                                            Hilbert space and can be represented as
                                                                            $\mathcal{H}_{2}=\mathbb{C}^{2}$. To
                                                                            represent $n$-qubits we denote it as
                                                                            $\mathcal{H}_{2}^{\otimes
                                                                            n}=\left(\mathbb{C}^2\right)^{\otimes n}$.
                                                                            The offered feature mapping portrayed above
                                                                            is a family of feature maps, that the
                                                                            authors selected because they conjecture it
                                                                            is hard to estimate overlap
                                                                            $|\langle\Phi(\vec{x}) \mid
                                                                            \Phi(\vec{y})\rangle|^2$ on a classical
                                                                            computer. The family of feature map circuits
                                                                            is defined as follows:
                                                                            $|\Phi(\vec{x})\rangle=U_{\Phi(\vec{x})}
                                                                            H^{\otimes n} U_{\Phi(\vec{x})} H^{\otimes
                                                                            n}|0\rangle^{\otimes n}$ where $H$ is a
                                                                            conventional Hadmard gate and
                                                                            $U_{\Phi(\vec{x})}$ is defined as:
                                                                            $\exp\left(i\sum_{S\subseteq[n]}\phi_{S}(\vec{x})\prod_{i\in
                                                                            S}Z_{i}\right)$ and is a diagonal gate in
                                                                            the Pauli Z-basis. For a single qubit, the
                                                                            unitary function $U_{\Phi(x)}$ acts as a
                                                                            phase-gate $Z_{x}$ of angle $x\in\Omega$.
                                                                            Feature mapping allows for $2^{n}$ possible
                                                                            coefficents $\phi_{S}\left(\vec{x}\right)$
                                                                            for non-linear function of the inputed data
                                                                            $\vec{x}\in\mathbb{R}^{n}$. Variational
                                                                            ClassificationVariational ClassificationA
                                                                            short depth quantum circut $W(\vec{\theta})$
                                                                            is applied to the feature state is a
                                                                            variational circut used for the optimization
                                                                            method. A depiction of this short depth
                                                                            quantum circut is given by the authors and
                                                                            shown in the figure below: Image Source:
                                                                            $[1]$ $[1]$ For this short depth quantum
                                                                            circut, the authors use an Ansatz
                                                                            variational unitary $W(\vec{\theta})$,
                                                                            defined as:
                                                                            $U_{loc}^{(l)}(\theta_{l})U_{ent}...$$U_{loc}^{(2)}(\theta_{2})U_{ent}U_{loc}^{(1)}(\theta_{1})$
                                                                            Where, $U_{loc}^{(l)}(\theta_{l})$ is full
                                                                            layers single qubit rotations given as:
                                                                            $U_{loc}^{(t)}(\theta_{t})=\bigotimes_{i=1}^{n}U(\theta_{i,t})$
                                                                            and $U(\theta_{i,t})\in SU(2)$. For the
                                                                            variational unitary, $U_{ent}$ is an
                                                                            alternating layers of entangling gates and
                                                                            is defined as: $U_{ent}=\prod_{(i,j)\in
                                                                            E}\mathbf{CZ}(i,j)$ Here, $E$ is an edge in
                                                                            the circut defined vetice set $\{v_{i},
                                                                            v_{j}\}\in V$. $\mathbf{CZ}$ is a controlled
                                                                            phase gate applied along the edges $(i,j)\in
                                                                            E$ which the authors state is present in the
                                                                            connectivity of the superconducting chip.
                                                                            This variational circut is parametrized by
                                                                            $\vec{\theta }\in\mathbb{R}^{2n(l+1)}$ and
                                                                            it is what is optimized during training as
                                                                            this is what classifies the data.
                                                                            MeasurmentMeasurmentGiven that the problem
                                                                            is a two label classification
                                                                            $y\in\{+1,-1\}$, the authors apply a binary
                                                                            measurment $\{M_{y}\}$ to the state
                                                                            $W(\vec{\theta})\mathcal{U}_{\Phi(\vec{x})}|0\rangle^{n}$.
                                                                            The authors describe the binary measurment
                                                                            in the following way. The measurment is in
                                                                            the $Z$-basis, outputs the bit-string
                                                                            $z\in\{0,1\}^{n}$, where the bit-string is
                                                                            then passed to the boolean function
                                                                            $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The
                                                                            binary measurment is defined as
                                                                            $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$,
                                                                            where
                                                                            $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle
                                                                            z|$. From this third step, the probability
                                                                            for the outcome $y$ is obtained. This
                                                                            probability $p_{y}(\vec{x})$ is defined as:
                                                                            $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                            Where the probability for measuring either
                                                                            label $y\in\{+1,-1\}$, denoted as $p_y$, is
                                                                            defined as:
                                                                            $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                                            W(\theta)\right|
                                                                            \Phi(\vec{x})\right\rangle\right)$ Since the
                                                                            expected value of the measured observable is
                                                                            $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                                            W^{\dagger}(\theta,
                                                                            \varphi)\right.$$\left.\mathbf{f} W(\theta,
                                                                            \varphi)\right]$, it can be expressed in
                                                                            terms of the inner product: $\frac{1}{2^n}
                                                                            \sum_\alpha w_\alpha(\theta)
                                                                            \Phi_\alpha(\vec{x})$ Next, we need to
                                                                            incorporate assigning the label
                                                                            $y\in\{+1,-1\}$ over the label $-y$ with a
                                                                            given fixed basis $b\in[-1,+1]$. In this
                                                                            case, it must be that $p_{-y}-yb<\ p_{y}$.
                                                                                Subsituting the probability for
                                                                                measuring either label $y\in\{+1,-1\}$
                                                                                in to the inner product of the measured
                                                                                observable, we obtain the value for
                                                                                $\tilde{m}(\vec{x})$ as:
                                                                                $\operatorname{sign}\left(\frac{1}{2^n}
                                                                                \sum_\alpha w_\alpha(\theta)
                                                                                \Phi_\alpha(\vec{x})+b\right)$ For the
                                                                                final decision ruling of $y$, $R$
                                                                                repeated measurment shots are preformed,
                                                                                yielding the empircal distribution
                                                                                $\hat{p}(\vec{x})$, where if
                                                                                $\hat{p}_{-y}(\vec{x}-yb)>
                                                                                \hat{p}_{y}(\vec{x})$ the label assigned
                                                                                is $\tilde{m}(\vec{x})=y$. The authors
                                                                                introduced a bias parameter $b\in[-1,1]$
                                                                                that can also be optimized during
                                                                                training. The authors mention that the
                                                                                feature map circut
                                                                                $\mathcal{U}_{\Phi(\vec{x})}$ and the
                                                                                boolean function $f$ are fixed choices.
                                                                                The parameters that are being optimized
                                                                                during training are $(\vec{\theta},b)$
                                                                                and in order to be optimized, a
                                                                                cost-function is need to be defined. To
                                                                                do so, we need to define an error
                                                                                probability first, which is what we will
                                                                                look at in the next section. Error
                                                                                Probability In order to find the
                                                                                empirical risk $R_{emp}(\vec{\theta})$
                                                                                of the empirical distribution
                                                                                $\hat{p}(\vec{x})$ we define the error
                                                                                probability of assigning the incorrect
                                                                                labels averaged over the samples in the
                                                                                training set $T$. The authors give the
                                                                                definition: $R_{emp}(\vec{\theta})$ $=$
                                                                                $\frac{1}{|T|}\sum_{\vec{x}\in
                                                                                T}Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                                                that defines this error probability. For
                                                                                our binary classification problem, the
                                                                                error probability of assigning the wrong
                                                                                label to some given data can be
                                                                                clacilated using the binomial cimulative
                                                                                density function CDF for the empircal
                                                                                distribution $\hat{p}(\vec{x})$. For a
                                                                                large number of samples or shots $R\gg
                                                                                1$ the CDF is approximated by a sigmoid
                                                                                function
                                                                                $\operatorname{sig}(x)=(1+e^{-x})^{-1}$.
                                                                                The probability for label $m(\vec{x}=y)$
                                                                                being assigned incorrectly can be
                                                                                approximated by:
                                                                                $Pr(\tilde{m}(\vec{x})\neq m(\vec{x}))$
                                                                                $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                                                b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                                                \hat{p}_y(\vec{x})}}\right)$ The authors
                                                                                describe the binary measurment in the
                                                                                following way. The measurment is in the
                                                                                $Z$-basis, outputs the bit-string
                                                                                $z\in\{0,1\}^{n}$, where the bit-string
                                                                                is then passed to the boolean function
                                                                                $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$. The
                                                                                binary measurment is defined as
                                                                                $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$,
                                                                                where
                                                                                $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle
                                                                                z|$. From this third step, the
                                                                                probability for the outcome $y$ is
                                                                                obtained. This probability
                                                                                $p_{y}(\vec{x})$ is defined as:
                                                                                $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                                Where the probability for measuring
                                                                                either label $y\in\{+1,-1\}$, denoted as
                                                                                $p_y$, is defined as:
                                                                                $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                                                W(\theta)\right|
                                                                                \Phi(\vec{x})\right\rangle\right)$ Since
                                                                                the expected value of the measured
                                                                                observable is
                                                                                $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                                                W^{\dagger}(\theta,
                                                                                \varphi)\right.$$\left.\mathbf{f}
                                                                                W(\theta, \varphi)\right]$, it can be
                                                                                expressed in terms of the inner product:
                                                                                $\frac{1}{2^n} \sum_\alpha
                                                                                w_\alpha(\theta) \Phi_\alpha(\vec{x})$
                                                                                Next, we need to incorporate assigning
                                                                                the label $y\in\{+1,-1\}$ over the label
                                                                                $-y$ with a given fixed basis
                                                                                $b\in[-1,+1]$. In this case, it must be
                                                                                that $p_{-y}-yb<\ p_{y}$. Subsituting
                                                                                    the probability for measuring either
                                                                                    label $y\in\{+1,-1\}$ in to the
                                                                                    inner product of the measured
                                                                                    observable, we obtain the value for
                                                                                    $\tilde{m}(\vec{x})$ as:
                                                                                    $\operatorname{sign}\left(\frac{1}{2^n}
                                                                                    \sum_\alpha w_\alpha(\theta)
                                                                                    \Phi_\alpha(\vec{x})+b\right)$ For
                                                                                    the final decision ruling of $y$,
                                                                                    $R$ repeated measurment shots are
                                                                                    preformed, yielding the empircal
                                                                                    distribution $\hat{p}(\vec{x})$,
                                                                                    where if $\hat{p}_{-y}(\vec{x}-yb)>
                                                                                    \hat{p}_{y}(\vec{x})$ the label
                                                                                    assigned is $\tilde{m}(\vec{x})=y$.
                                                                                    The authors introduced a bias
                                                                                    parameter $b\in[-1,1]$ that can also
                                                                                    be optimized during training. The
                                                                                    authors mention that the feature map
                                                                                    circut $\mathcal{U}_{\Phi(\vec{x})}$
                                                                                    and the boolean function $f$ are
                                                                                    fixed choices. The parameters that
                                                                                    are being optimized during training
                                                                                    are $(\vec{\theta},b)$ and in order
                                                                                    to be optimized, a cost-function is
                                                                                    need to be defined. To do so, we
                                                                                    need to define an error probability
                                                                                    first, which is what we will look at
                                                                                    in the next section. Error
                                                                                    Probability In order to find the
                                                                                    empirical risk
                                                                                    $R_{emp}(\vec{\theta})$ of the
                                                                                    empirical distribution
                                                                                    $\hat{p}(\vec{x})$ we define the
                                                                                    error probability of assigning the
                                                                                    incorrect labels averaged over the
                                                                                    samples in the training set $T$. The
                                                                                    authors give the definition:
                                                                                    $R_{emp}(\vec{\theta})$ $=$
                                                                                    $\frac{1}{|T|}\sum_{\vec{x}\in
                                                                                    T}Pr(\tilde{m}(\vec{x})\neq
                                                                                    m(\vec{x}))$ that defines this error
                                                                                    probability. For our binary
                                                                                    classification problem, the error
                                                                                    probability of assigning the wrong
                                                                                    label to some given data can be
                                                                                    clacilated using the binomial
                                                                                    cimulative density function CDF for
                                                                                    the empircal distribution
                                                                                    $\hat{p}(\vec{x})$. For a large
                                                                                    number of samples or shots $R\gg 1$
                                                                                    the CDF is approximated by a sigmoid
                                                                                    function
                                                                                    $\operatorname{sig}(x)=(1+e^{-x})^{-1}$.
                                                                                    The probability for label
                                                                                    $m(\vec{x}=y)$ being assigned
                                                                                    incorrectly can be approximated by:
                                                                                    $Pr(\tilde{m}(\vec{x})\neq
                                                                                    m(\vec{x}))$
                                                                                    $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                                                    b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                                                    \hat{p}_y(\vec{x})}}\right)$ The
                                                                                    authors describe the binary
                                                                                    measurment in the following way. The
                                                                                    measurment is in the $Z$-basis,
                                                                                    outputs the bit-string
                                                                                    $z\in\{0,1\}^{n}$, where the
                                                                                    bit-string is then passed to the
                                                                                    boolean function
                                                                                    $f:\{0,1\}^{n}\rightarrow\{+1,-1\}$.
                                                                                    The binary measurment is defined as
                                                                                    $M_{y}=2^{-1}(\mathbb{1}+y\mathbf{f})$,
                                                                                    where
                                                                                    $\mathbf{f}=\sum_{z\in\{0,1\}^n}f(z)|z\rangle\langle
                                                                                    z|$. From this third step, the
                                                                                    probability for the outcome $y$ is
                                                                                    obtained. This probability
                                                                                    $p_{y}(\vec{x})$ is defined as:
                                                                                    $p_{y}(\vec{x})$=$\langle\Phi(\vec{x})|W^{\dagger}(\vec{\theta})M_{y}W(\vec{\theta})|\Phi(\vec{x})\rangle$
                                                                                    Where the probability for measuring
                                                                                    either label $y\in\{+1,-1\}$,
                                                                                    denoted as $p_y$, is defined as:
                                                                                    $\frac{1}{2}\left(1+y\left\langle\Phi(\vec{x})\left|W^{\dagger}(\theta)\right.\right.\right.$$\left.\left.\left.\mathbf{f}
                                                                                    W(\theta)\right|
                                                                                    \Phi(\vec{x})\right\rangle\right)$
                                                                                    Since the expected value of the
                                                                                    measured observable is
                                                                                    $\operatorname{tr}\left[|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|
                                                                                    W^{\dagger}(\theta,
                                                                                    \varphi)\right.$$\left.\mathbf{f}
                                                                                    W(\theta, \varphi)\right]$, it can
                                                                                    be expressed in terms of the inner
                                                                                    product: $\frac{1}{2^n} \sum_\alpha
                                                                                    w_\alpha(\theta)
                                                                                    \Phi_\alpha(\vec{x})$ Next, we need
                                                                                    to incorporate assigning the label
                                                                                    $y\in\{+1,-1\}$ over the label $-y$
                                                                                    with a given fixed basis
                                                                                    $b\in[-1,+1]$. In this case, it must
                                                                                    be that $p_{-y}-yb<\ p_{y}$.
                                                                                        Subsituting the probability for
                                                                                        measuring either label
                                                                                        $y\in\{+1,-1\}$ in to the inner
                                                                                        product of the measured
                                                                                        observable, we obtain the value
                                                                                        for $\tilde{m}(\vec{x})$
                                                                                        as:$\operatorname{sign}\left(\frac{1}{2^n}
                                                                                        \sum_\alpha w_\alpha(\theta)
                                                                                        \Phi_\alpha(\vec{x})+b\right)$
                                                                                        For the final decision ruling of
                                                                                        $y$, $R$ repeated measurment
                                                                                        shots are preformed, yielding
                                                                                        the empircal distribution
                                                                                        $\hat{p}(\vec{x})$, where if
                                                                                        $\hat{p}_{-y}(\vec{x}-yb)>
                                                                                        \hat{p}_{y}(\vec{x})$ the label
                                                                                        assigned is
                                                                                        $\tilde{m}(\vec{x})=y$. The
                                                                                        authors introduced a bias
                                                                                        parameter $b\in[-1,1]$ that can
                                                                                        also be optimized during
                                                                                        training. The authors mention
                                                                                        that the feature map circut
                                                                                        $\mathcal{U}_{\Phi(\vec{x})}$
                                                                                        and the boolean function $f$ are
                                                                                        fixed choices. The parameters
                                                                                        that are being optimized during
                                                                                        training are $(\vec{\theta},b)$
                                                                                        and in order to be optimized, a
                                                                                        cost-function is need to be
                                                                                        defined. To do so, we need to
                                                                                        define an error probability
                                                                                        first, which is what we will
                                                                                        look at in the next section.
                                                                                        Error ProbabilityError
                                                                                        ProbabilityIn order to find the
                                                                                        empirical risk
                                                                                        $R_{emp}(\vec{\theta})$ of the
                                                                                        empirical distribution
                                                                                        $\hat{p}(\vec{x})$ we define the
                                                                                        error probability of assigning
                                                                                        the incorrect labels averaged
                                                                                        over the samples in the training
                                                                                        set $T$. The authors give the
                                                                                        definition:
                                                                                        $R_{emp}(\vec{\theta})$ $=$
                                                                                        $\frac{1}{|T|}\sum_{\vec{x}\in
                                                                                        T}Pr(\tilde{m}(\vec{x})\neq
                                                                                        m(\vec{x}))$ that defines this
                                                                                        error probability. For our
                                                                                        binary classification problem,
                                                                                        the error probability of
                                                                                        assigning the wrong label to
                                                                                        some given data can be
                                                                                        clacilated using the binomial
                                                                                        cimulative density function CDF
                                                                                        for the empircal distribution
                                                                                        $\hat{p}(\vec{x})$. For a large
                                                                                        number of samples or shots $R\gg
                                                                                        1$ the CDF is approximated by a
                                                                                        sigmoid function
                                                                                        $\operatorname{sig}(x)=(1+e^{-x})^{-1}$.
                                                                                        The probability for label
                                                                                        $m(\vec{x}=y)$ being assigned
                                                                                        incorrectly can be approximated
                                                                                        by: $Pr(\tilde{m}(\vec{x})\neq
                                                                                        m(\vec{x}))$
                                                                                        $\approx\operatorname{sig}\left(\frac{\sqrt{R}\left(\frac{1}{2}-\left(\hat{p}_y(\vec{x})-\frac{y
                                                                                        b}{2}\right)\right)}{\sqrt{2\left(1-\hat{p}_y(\vec{x})\right)
                                                                                        \hat{p}_y(\vec{x})}}\right)$
                                                                                        Classifier Optimization and
                                                                                        Testing Results The first step
                                                                                        is to train the classifier and
                                                                                        optimize it. The authors found
                                                                                        that they found Spall's SPSA
                                                                                        stochastic gradient descent
                                                                                        algorithm performs the best in
                                                                                        the inherent noisy experimental
                                                                                        setting of quantum computing.
                                                                                        They mentioned after the
                                                                                        parameters converged onto
                                                                                        $\big(\vec{\Theta^{*}},b^{*}\big)$.
                                                                                        The second step is then to
                                                                                        classify data and assign them
                                                                                        labels according to the decision
                                                                                        rule given by
                                                                                        $\tilde{m}(\vec{s})$, where the
                                                                                        binary measurement is obtained
                                                                                        from the parity function
                                                                                        $\mathbf{f}=Z_1 Z_2$, and ensure
                                                                                        that for the testing data,
                                                                                        $m(\vec{s})=\tilde{m}(\vec{s})$
                                                                                        with high probability for the
                                                                                        given data set $\vec{s}\in S$.
                                                                                        The authors note that what is
                                                                                        obeserved for the empirical
                                                                                        risk, or cost value used for the
                                                                                        optimizer, is that it converges
                                                                                        to a lower depth when the number
                                                                                        of layers in the short depth
                                                                                        quantum circuit for optimization
                                                                                        is $l=4$ than $l=0$. Interesting
                                                                                        enough, error mitigation does
                                                                                        not appreciably improve the
                                                                                        empirical risk results when
                                                                                        depth is at $0$, however, does
                                                                                        substantially help for larger
                                                                                        depths. An important note, is
                                                                                        that although
                                                                                        $\operatorname{Pr}(\tilde{m}(\vec{x})
                                                                                        \neq m(\vec{x}))$ includes the
                                                                                        number of shots $R$ taken in its
                                                                                        calculation, during
                                                                                        experimentation the authors
                                                                                        fixed $R=200$. The authors state
                                                                                        the reason for doing this, even
                                                                                        though $R=2000$ in actual
                                                                                        experiment, was to avoid
                                                                                        gradient problems. To continue,
                                                                                        after training, comes testing.
                                                                                        The yielded trained set of
                                                                                        parameters
                                                                                        $\big(\vec{\theta}^{*},b^*=0\big)$
                                                                                        where used to classify 20
                                                                                        different test sets randomly
                                                                                        drawn each time per data set.
                                                                                        After analyzing the results, the
                                                                                        authors an increasing
                                                                                        classification success with
                                                                                        increasing circut depth $l$.
                                                                                        Noting, that the success rate
                                                                                        very nearl reaches $100$% for
                                                                                        circut depths larger than $1$
                                                                                        and remains up to depth $4$,
                                                                                        despite decoherence associated
                                                                                        with $8$ CNOT gates during
                                                                                        training and classification
                                                                                        circuts. Classifier Optimization
                                                                                        and Testing Results The first
                                                                                        step is to train the classifier
                                                                                        and optimize it. The authors
                                                                                        found that they found Spall's
                                                                                        SPSA stochastic gradient descent
                                                                                        algorithm performs the best in
                                                                                        the inherent noisy experimental
                                                                                        setting of quantum computing.
                                                                                        They mentioned after the
                                                                                        parameters converged onto
                                                                                        $\big(\vec{\Theta^{*}},b^{*}\big)$.
                                                                                        The second step is then to
                                                                                        classify data and assign them
                                                                                        labels according to the decision
                                                                                        rule given by
                                                                                        $\tilde{m}(\vec{s})$, where the
                                                                                        binary measurement is obtained
                                                                                        from the parity function
                                                                                        $\mathbf{f}=Z_1 Z_2$, and ensure
                                                                                        that for the testing data,
                                                                                        $m(\vec{s})=\tilde{m}(\vec{s})$
                                                                                        with high probability for the
                                                                                        given data set $\vec{s}\in S$.
                                                                                        The authors note that what is
                                                                                        obeserved for the empirical
                                                                                        risk, or cost value used for the
                                                                                        optimizer, is that it converges
                                                                                        to a lower depth when the number
                                                                                        of layers in the short depth
                                                                                        quantum circuit for optimization
                                                                                        is $l=4$ than $l=0$. Interesting
                                                                                        enough, error mitigation does
                                                                                        not appreciably improve the
                                                                                        empirical risk results when
                                                                                        depth is at $0$, however, does
                                                                                        substantially help for larger
                                                                                        depths. An important note, is
                                                                                        that although
                                                                                        $\operatorname{Pr}(\tilde{m}(\vec{x})
                                                                                        \neq m(\vec{x}))$ includes the
                                                                                        number of shots $R$ taken in its
                                                                                        calculation, during
                                                                                        experimentation the authors
                                                                                        fixed $R=200$. The authors state
                                                                                        the reason for doing this, even
                                                                                        though $R=2000$ in actual
                                                                                        experiment, was to avoid
                                                                                        gradient problems. To continue,
                                                                                        after training, comes testing.
                                                                                        The yielded trained set of
                                                                                        parameters
                                                                                        $\big(\vec{\theta}^{*},b^*=0\big)$
                                                                                        where used to classify 20
                                                                                        different test sets randomly
                                                                                        drawn each time per data set.
                                                                                        After analyzing the results, the
                                                                                        authors an increasing
                                                                                        classification success with
                                                                                        increasing circut depth $l$.
                                                                                        Noting, that the success rate
                                                                                        very nearl reaches $100$% for
                                                                                        circut depths larger than $1$
                                                                                        and remains up to depth $4$,
                                                                                        despite decoherence associated
                                                                                        with $8$ CNOT gates during
                                                                                        training and classification
                                                                                        circuts. Quantum Variational
                                                                                        Classification Analysis An
                                                                                        important aspect of SVM's, is
                                                                                        the classification of the data,
                                                                                        which is done here based off of
                                                                                        the decision rule
                                                                                        $p_y(\vec{x})>p_{-y}(\vec{x})-y
                                                                                        b$. This decision rule
                                                                                        $\tilde{m}(\vec{x})$ can can be
                                                                                        restated as $\operatorname{sign}
                                                                                        \big( \langle\Phi(\vec{x})
                                                                                        |W^{\dagger}(\vec{\theta})$$
                                                                                        \mathbf{f} W(\vec{\theta}) |
                                                                                        \Phi(\vec{x}) \rangle+b \big)$.
                                                                                        The step that structures the
                                                                                        data in such a way for
                                                                                        classification, is the
                                                                                        variational circut $W$, which
                                                                                        seperates the data using
                                                                                        hyperplane $\vec{w}$ in quantum
                                                                                        state space. A feature map of a
                                                                                        single qubit, along with the
                                                                                        seperating hyperplane $\vec{w}$
                                                                                        and the invterval of binary
                                                                                        labels $\Omega=(0,2\pi]$ for the
                                                                                        classical data, is given by the
                                                                                        authors below. Image Source:
                                                                                        $[1]$ We start by decomposing
                                                                                        the quantum variational circut.
                                                                                        First, we look to define the
                                                                                        properties for our group of
                                                                                        matrices used. For us, we need a
                                                                                        group that has an orthogonal,
                                                                                        hermatian, matrix basis
                                                                                        $\left\{P_\alpha\right\} \subset
                                                                                        \mathbb{C}^{2^n \times 2^n}$.
                                                                                        Such that $\alpha=1,...,4^{n}$
                                                                                        with
                                                                                        $\operatorname{tr}\left[P_\alpha^{\dagger}
                                                                                        P_\beta\right]=2^n
                                                                                        \delta_{\alpha, \beta}$. A group
                                                                                        of matrices adhereing to this
                                                                                        criteria is the Pauli-group on
                                                                                        $n$-qubits. Next, we expand the
                                                                                        quantum state and the measurment
                                                                                        in our choosen matrix basis. The
                                                                                        expanded quantum state, or
                                                                                        expectation value, can be
                                                                                        expressed in terms of
                                                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$
                                                                                        $\rightarrow$
                                                                                        $\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
                                                                                        \Phi(\vec{x})\right\rangle$. The
                                                                                        decision rule expressed in our
                                                                                        choosen matrix basis as
                                                                                        $w_\alpha(\vec{\theta})$ can be
                                                                                        defined as
                                                                                        $W^{\dagger}(\vec{\theta})
                                                                                        \mathbf{f} W(\vec{\theta})$
                                                                                        $\rightarrow$
                                                                                        $\operatorname{tr}\left[W^{\dagger}(\vec{\theta})
                                                                                        \mathbf{f} W(\vec{\theta})
                                                                                        P_\alpha\right]$. Lastly, any
                                                                                        classification rule or mapping
                                                                                        $\tilde{m}(x)$ from a
                                                                                        variational unitary can be
                                                                                        restated in the SVM form:
                                                                                        $\operatorname{sign}\left(2^{-n}
                                                                                        \sum_\alpha
                                                                                        w_\alpha(\vec{\theta})
                                                                                        \Phi_\alpha(\vec{x})+b\right)$
                                                                                        What we can see is that the
                                                                                        behavior of the classifier is
                                                                                        dependent on the larger term
                                                                                        $\omega_{\alpha}$, so by
                                                                                        improving this term, this
                                                                                        constraining term is lifted from
                                                                                        the variational circut. The
                                                                                        authors note as well that the
                                                                                        optimal value for
                                                                                        $\omega_{\alpha}$ can
                                                                                        alternatively be found through
                                                                                        implementing kernel methods and
                                                                                        the Wolfe dual approach of the
                                                                                        SVM. The important idea we can
                                                                                        gain from decomposing of
                                                                                        variational circut, is that we
                                                                                        can think of the feature space
                                                                                        as the quantum state space that
                                                                                        has feature vectors
                                                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$
                                                                                        and inner products $K(\vec{x},
                                                                                        \vec{z})=|\langle\Phi(\vec{x})
                                                                                        \mid \Phi(\vec{z})\rangle|^2$.
                                                                                        With this more clear image of
                                                                                        how feature space is being
                                                                                        represented as quantum state
                                                                                        space, we can see that the
                                                                                        direct use of Hilbert space for
                                                                                        this can lead to conceptual
                                                                                        errors. Such that, a vector in
                                                                                        Hilbert space
                                                                                        $|\Phi(\vec{x})\rangle \in
                                                                                        \mathcal{H}$ is only defined up
                                                                                        to a global phase physically.
                                                                                        What was seen was that a quantum
                                                                                        advantage is mainly obtained
                                                                                        from feature maps that have a
                                                                                        classicaly hard to estimate
                                                                                        kernel. Quantum Variational
                                                                                        Classification Analysis An
                                                                                        important aspect of SVM's, is
                                                                                        the classification of the data,
                                                                                        which is done here based off of
                                                                                        the decision rule
                                                                                        $p_y(\vec{x})>p_{-y}(\vec{x})-y
                                                                                        b$. This decision rule
                                                                                        $\tilde{m}(\vec{x})$ can can be
                                                                                        restated as $\operatorname{sign}
                                                                                        \big( \langle\Phi(\vec{x})
                                                                                        |W^{\dagger}(\vec{\theta})$$
                                                                                        \mathbf{f} W(\vec{\theta}) |
                                                                                        \Phi(\vec{x}) \rangle+b \big)$.
                                                                                        The step that structures the
                                                                                        data in such a way for
                                                                                        classification, is the
                                                                                        variational circut $W$, which
                                                                                        seperates the data using
                                                                                        hyperplane $\vec{w}$ in quantum
                                                                                        state space. A feature map of a
                                                                                        single qubit, along with the
                                                                                        seperating hyperplane $\vec{w}$
                                                                                        and the invterval of binary
                                                                                        labels $\Omega=(0,2\pi]$ for the
                                                                                        classical data, is given by the
                                                                                        authors below. Image Source:
                                                                                        $[1]$ $[1]$ We start by
                                                                                        decomposing the quantum
                                                                                        variational circut. First, we
                                                                                        look to define the properties
                                                                                        for our group of matrices used.
                                                                                        For us, we need a group that has
                                                                                        an orthogonal, hermatian, matrix
                                                                                        basis $\left\{P_\alpha\right\}
                                                                                        \subset \mathbb{C}^{2^n \times
                                                                                        2^n}$. Such that
                                                                                        $\alpha=1,...,4^{n}$ with
                                                                                        $\operatorname{tr}\left[P_\alpha^{\dagger}
                                                                                        P_\beta\right]=2^n
                                                                                        \delta_{\alpha, \beta}$. A group
                                                                                        of matrices adhereing to this
                                                                                        criteria is the Pauli-group on
                                                                                        $n$-qubits. Next, we expand the
                                                                                        quantum state and the measurment
                                                                                        in our choosen matrix basis. The
                                                                                        expanded quantum state, or
                                                                                        expectation value, can be
                                                                                        expressed in terms of
                                                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$
                                                                                        $\rightarrow$
                                                                                        $\Phi_\alpha(\vec{x})=\left\langle\Phi(\vec{x})\left|P_\alpha\right|
                                                                                        \Phi(\vec{x})\right\rangle$. The
                                                                                        decision rule expressed in our
                                                                                        choosen matrix basis as
                                                                                        $w_\alpha(\vec{\theta})$ can be
                                                                                        defined as
                                                                                        $W^{\dagger}(\vec{\theta})
                                                                                        \mathbf{f} W(\vec{\theta})$
                                                                                        $\rightarrow$
                                                                                        $\operatorname{tr}\left[W^{\dagger}(\vec{\theta})
                                                                                        \mathbf{f} W(\vec{\theta})
                                                                                        P_\alpha\right]$. Lastly, any
                                                                                        classification rule or mapping
                                                                                        $\tilde{m}(x)$ from a
                                                                                        variational unitary can be
                                                                                        restated in the SVM form:
                                                                                        $\operatorname{sign}\left(2^{-n}
                                                                                        \sum_\alpha
                                                                                        w_\alpha(\vec{\theta})
                                                                                        \Phi_\alpha(\vec{x})+b\right)$
                                                                                        What we can see is that the
                                                                                        behavior of the classifier is
                                                                                        dependent on the larger term
                                                                                        $\omega_{\alpha}$, so by
                                                                                        improving this term, this
                                                                                        constraining term is lifted from
                                                                                        the variational circut. The
                                                                                        authors note as well that the
                                                                                        optimal value for
                                                                                        $\omega_{\alpha}$ can
                                                                                        alternatively be found through
                                                                                        implementing kernel methods and
                                                                                        the Wolfe dual approach of the
                                                                                        SVM. The important idea we can
                                                                                        gain from decomposing of
                                                                                        variational circut, is that we
                                                                                        can think of the feature space
                                                                                        as the quantum state space that
                                                                                        has feature vectors
                                                                                        $|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$
                                                                                        and inner products $K(\vec{x},
                                                                                        \vec{z})=|\langle\Phi(\vec{x})
                                                                                        \mid \Phi(\vec{z})\rangle|^2$.
                                                                                        With this more clear image of
                                                                                        how feature space is being
                                                                                        represented as quantum state
                                                                                        space, we can see that the
                                                                                        direct use of Hilbert space for
                                                                                        this can lead to conceptual
                                                                                        errors. Such that, a vector in
                                                                                        Hilbert space
                                                                                        $|\Phi(\vec{x})\rangle \in
                                                                                        \mathcal{H}$ is only defined up
                                                                                        to a global phase physically.
                                                                                        What was seen was that a quantum
                                                                                        advantage is mainly obtained
                                                                                        from feature maps that have a
                                                                                        classicaly hard to estimate
                                                                                        kernel. Quantum Kernel
                                                                                        Estimation Procedure Unlike, in
                                                                                        a variational quantum circut to
                                                                                        generate a seperating hyperplane
                                                                                        for the high-dimesnional feature
                                                                                        space, Quantum Kernel Estimation
                                                                                        a classical SVM classification
                                                                                        instead. Meaning, this type of
                                                                                        classification does not make a
                                                                                        direct use of Hilbert space,
                                                                                        allowing for it to side step the
                                                                                        inherent conceptual errors of
                                                                                        that quantum variational
                                                                                        classification feature map
                                                                                        representation. To continue.
                                                                                        Quantum Kernel
                                                                                        EstimationProcedure Unlike, in a
                                                                                        variational quantum circut to
                                                                                        generate a seperating hyperplane
                                                                                        for the high-dimesnional feature
                                                                                        space, Quantum Kernel Estimation
                                                                                        a classical SVM classification
                                                                                        instead. Meaning, this type of
                                                                                        classification does not make a
                                                                                        direct use of Hilbert space,
                                                                                        allowing for it to side step the
                                                                                        inherent conceptual errors of
                                                                                        that quantum variational
                                                                                        classification feature map
                                                                                        representation. To continue.
                                                                                        ProcedureUnlike, in a
                                                                                        variational quantum circut to
                                                                                        generate a seperating hyperplane
                                                                                        for the high-dimesnional feature
                                                                                        space, Quantum Kernel Estimation
                                                                                        a classical SVM classification
                                                                                        instead. Meaning, this type of
                                                                                        classification does not make a
                                                                                        direct use of Hilbert space,
                                                                                        allowing for it to side step the
                                                                                        inherent conceptual errors of
                                                                                        that quantum variational
                                                                                        classification feature map
                                                                                        representation. To continue.
                                                                                        Quantum Feature Mapping
                                                                                        Implementations for Support
                                                                                        Vector Machines A feature map
                                                                                        can be denoted as $\Phi: \Omega
                                                                                        \subset \mathbb{R}^d \rightarrow
                                                                                        \mathcal{S}\left(\mathcal{H}_2^{\otimes
                                                                                        n}\right)$ such that $\Phi:
                                                                                        \vec{x}
                                                                                        \mapsto|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$.
                                                                                        The map acting on the data input
                                                                                        is a unitary circut family
                                                                                        $\mathcal{U}_{\Phi(\vec{x})}$
                                                                                        applied to $|0\rangle^n$. The
                                                                                        result can be denoted as
                                                                                        $|\Phi(\vec{x})\rangle=\mathcal{U}_{\Phi(\vec{x})}|0\rangle^n$,
                                                                                        such that the state in the
                                                                                        feature space is linearly
                                                                                        independent. Let's analyze a
                                                                                        feature map corresponding to a
                                                                                        product state. First, assume
                                                                                        that a feature map is comprised
                                                                                        of single qubit rotations
                                                                                        $U(\varphi) \in \mathrm{SU}(2)$
                                                                                        aranged in a quantum circuit.
                                                                                        These angles correspond to a
                                                                                        non-linear function $\varphi:
                                                                                        \vec{x} \rightarrow(0,2 \pi]^2
                                                                                        \times[0, \pi]$ mapped to the
                                                                                        space of Euler angles. This
                                                                                        means that the feature mapping
                                                                                        action for an individual qubit
                                                                                        is: $\vec{x}
                                                                                        \mapsto\left|\phi_i(\vec{x})\right\rangle=U\left(\varphi_i(\vec{x})\right)|0\rangle$
                                                                                        and the feature mapping action
                                                                                        for the full quibit state is:
                                                                                        $\Phi: \vec{x}
                                                                                        \mapsto|\Phi(\vec{x})\rangle\left\langle\Phi(\vec{x})|\right.$$\left.=\bigotimes_{i=1}^n|
                                                                                        \phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                                                        Stoudenmire and Schwab using
                                                                                        tensor networks is an example of
                                                                                        unitary implementation of
                                                                                        feature mapping of classical
                                                                                        classifiers. In this
                                                                                        implementation, each qubit
                                                                                        encodes a single component
                                                                                        $x_{i}$ of
                                                                                        $\vec{x}\in[0,1]^{n}$, thus,
                                                                                        using $n$ qubits. The prepared
                                                                                        state of this full qubit feature
                                                                                        mapping encoding when expanded
                                                                                        to the Pauli-matrix basis:
                                                                                        $\bigotimes_{i=1}^n\left|\phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                                                        $=$ $\frac{1}{2^n}
                                                                                        \bigotimes_{j=1}^n\left(\sum_{\alpha_j}
                                                                                        \Phi_j^{\alpha_j}\left(\theta_j(\vec{x})\right)
                                                                                        P_{\alpha_j}\right)$ Such that,
                                                                                        with respect to the Pauli-matrix
                                                                                        basis,
                                                                                        $\Phi_i^\alpha\left(\theta_i(\vec{x})\right)$
                                                                                        $=$
                                                                                        $\left\langle\phi_i(x)\left|P_{\alpha_i}\right|
                                                                                        \phi_i(x)\right\rangle$ for
                                                                                        every $i=1\dots,n$ and where
                                                                                        $P_{\alpha_i}
                                                                                        \in\left\{\mathbb{1}, X_i, Z_i,
                                                                                        Y_i\right\}$. To continue.
                                                                                        Quantum Feature Mapping
                                                                                        Implementations for Support
                                                                                        Vector MachinesA feature map can
                                                                                        be denoted as $\Phi: \Omega
                                                                                        \subset \mathbb{R}^d \rightarrow
                                                                                        \mathcal{S}\left(\mathcal{H}_2^{\otimes
                                                                                        n}\right)$ such that $\Phi:
                                                                                        \vec{x}
                                                                                        \mapsto|\Phi(\vec{x})\rangle\langle\Phi(\vec{x})|$.
                                                                                        The map acting on the data input
                                                                                        is a unitary circut family
                                                                                        $\mathcal{U}_{\Phi(\vec{x})}$
                                                                                        applied to $|0\rangle^n$. The
                                                                                        result can be denoted as
                                                                                        $|\Phi(\vec{x})\rangle=\mathcal{U}_{\Phi(\vec{x})}|0\rangle^n$,
                                                                                        such that the state in the
                                                                                        feature space is linearly
                                                                                        independent. Let's analyze a
                                                                                        feature map corresponding to a
                                                                                        product state. First, assume
                                                                                        that a feature map is comprised
                                                                                        of single qubit rotations
                                                                                        $U(\varphi) \in \mathrm{SU}(2)$
                                                                                        aranged in a quantum circuit.
                                                                                        These angles correspond to a
                                                                                        non-linear function $\varphi:
                                                                                        \vec{x} \rightarrow(0,2 \pi]^2
                                                                                        \times[0, \pi]$ mapped to the
                                                                                        space of Euler angles. This
                                                                                        means that the feature mapping
                                                                                        action for an individual qubit
                                                                                        is: $\vec{x}
                                                                                        \mapsto\left|\phi_i(\vec{x})\right\rangle=U\left(\varphi_i(\vec{x})\right)|0\rangle$
                                                                                        and the feature mapping action
                                                                                        for the full quibit state is:
                                                                                        $\Phi: \vec{x}
                                                                                        \mapsto|\Phi(\vec{x})\rangle\left\langle\Phi(\vec{x})|\right.$$\left.=\bigotimes_{i=1}^n|
                                                                                        \phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                                                        Stoudenmire and Schwab using
                                                                                        tensor networks is an example of
                                                                                        unitary implementation of
                                                                                        feature mapping of classical
                                                                                        classifiers. In this
                                                                                        implementation, each qubit
                                                                                        encodes a single component
                                                                                        $x_{i}$ of
                                                                                        $\vec{x}\in[0,1]^{n}$, thus,
                                                                                        using $n$ qubits. The prepared
                                                                                        state of this full qubit feature
                                                                                        mapping encoding when expanded
                                                                                        to the Pauli-matrix basis:
                                                                                        $\bigotimes_{i=1}^n\left|\phi_i(x)\right\rangle\left\langle\phi_i(x)\right|$
                                                                                        $=$ $\frac{1}{2^n}
                                                                                        \bigotimes_{j=1}^n\left(\sum_{\alpha_j}
                                                                                        \Phi_j^{\alpha_j}\left(\theta_j(\vec{x})\right)
                                                                                        P_{\alpha_j}\right)$ Such that,
                                                                                        with respect to the Pauli-matrix
                                                                                        basis,
                                                                                        $\Phi_i^\alpha\left(\theta_i(\vec{x})\right)$
                                                                                        $=$
                                                                                        $\left\langle\phi_i(x)\left|P_{\alpha_i}\right|
                                                                                        \phi_i(x)\right\rangle$ for
                                                                                        every $i=1\dots,n$ and where
                                                                                        $P_{\alpha_i}
                                                                                        \in\left\{\mathbb{1}, X_i, Z_i,
                                                                                        Y_i\right\}$. To continue.
                                                                                        [https://www.contextswitching.org/all]
                                                                                        All Articles - Context Switching
                                                                                        All Articles Quantum
                                                                                        Convolutional Neural Network The
                                                                                        input for the quantum
                                                                                        convolution layer is a $3$D
                                                                                        tensor input given as $X^{\ell}
                                                                                        \in \mathbb{R}^{H^{\ell} \times
                                                                                        W^{\ell} \times D^{\ell}}$. The
                                                                                        weights layer, filter layer, or
                                                                                        $4$D tensor kernel layer is
                                                                                        denoted as $K^{\ell} \in
                                                                                        \mathbb{R}^{H \times W \times
                                                                                        D^{\ell} \times D^{\ell+1}}$.
                                                                                        The input and kernel layer are
                                                                                        both stored in QRAM. There are
                                                                                        precision parameters $\epsilon$
                                                                                        and $\Delta>0$... read more
                                                                                        Thalamic Nuclei The thalamic
                                                                                        nuclei are paired structures of
                                                                                        the thalamus divided into three
                                                                                        main groups: the lateral
                                                                                        nuclear, medial nuclear, and
                                                                                        anterior nuclear groups. The
                                                                                        internal medullary lamina, a
                                                                                        Y-shaped structure that splits
                                                                                        these groups, is present on each
                                                                                        side of the thalamus. A midline,
                                                                                        thin thalamic nuclei, adjacent
                                                                                        to the... read more Epigenetics
                                                                                        and Inheritance In biology,
                                                                                        epigenetics is the study of
                                                                                        mitotically and/or meiotically
                                                                                        heritable changes in gene
                                                                                        function that cannot be
                                                                                        explained by changes to the DNA
                                                                                        sequence. Epigenetics normally
                                                                                        involves change that is not
                                                                                        erased by cell division and that
                                                                                        also affects the regulation of
                                                                                        gene expression. Epigenetics
                                                                                        reflects our understanding...
                                                                                        read more High-Dimensional
                                                                                        Quantum Feature Mapping Quantum
                                                                                        Principal Component Analysis
                                                                                        identifies large eigenvalues of
                                                                                        unknown density matrices
                                                                                        utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for... read more
                                                                                        Quantum Fields in Anti-de Sitter
                                                                                        Space and the Maldacena
                                                                                        Conjecture In theoretical
                                                                                        physics, the Maldacena
                                                                                        Conjecture states supergravity
                                                                                        and string theory on the product
                                                                                        of $(n+1)$-dimensional Anti-de
                                                                                        Sitter space with a compact
                                                                                        manifold capable of describing
                                                                                        large $N$ limits of conformal
                                                                                        field theories in
                                                                                        $d$-dimensions. Correlation
                                                                                        functions in CFT are dependent
                                                                                        on the supergravity action of
                                                                                        asymptotic behavior at
                                                                                        infinity... read more
                                                                                        Bioinformatics and Functional
                                                                                        Genomics Bioinformatics is a
                                                                                        growing revolution in the field
                                                                                        of molecular biology and
                                                                                        computers. Here, our emphasis
                                                                                        will be on employing
                                                                                        bioinformatics tools and
                                                                                        biological databases to address
                                                                                        challenges from current issues
                                                                                        in biological, biotechnological,
                                                                                        and biomedical research. Such as
                                                                                        looking at computational
                                                                                        algorithms and computer
                                                                                        databases to analyze proteins,
                                                                                        genes... read more Multi-Store
                                                                                        Memory Model In neuroscience,
                                                                                        the central executive is
                                                                                        responsible for controlled
                                                                                        processing and allocation of
                                                                                        data to subsystems in working
                                                                                        memory. Such subsystems include
                                                                                        the visuospatial sketchpad and
                                                                                        phonological loop, where the
                                                                                        phonological loop can further be
                                                                                        subdivided into the phonological
                                                                                        store and the articulatory
                                                                                        process. The primary functions
                                                                                        of the central... read more
                                                                                        Compatification and Massless
                                                                                        Scattering in Anti-de Sitter
                                                                                        Space In theoretical physics,
                                                                                        Minkowski Space is a particular
                                                                                        type of $4$-dimensional
                                                                                        Lorentzian space, with a
                                                                                        Minkowski metric. Where the
                                                                                        Minkowski metric is a metric
                                                                                        tensor denoted as $d\tau^2$ with
                                                                                        the form
                                                                                        $-\left(d^0\right)^2+\left(d
                                                                                        x^1\right)^2$ $+\;\left(d
                                                                                        x^2\right)^2+\left(d
                                                                                        x^3\right)^2$. Minkowski space
                                                                                        forms the basis of the study of
                                                                                        spacetime within special
                                                                                        relativity and is... read more
                                                                                        Quantum Support Vector Machine
                                                                                        Using quantum computing, the
                                                                                        authors exploit quantum
                                                                                        mechanics for the algorithmic
                                                                                        complexity optimization of a
                                                                                        Support Vector Machine with
                                                                                        high-dimensional feature space.
                                                                                        Where the high-dimensional
                                                                                        classical data is mapped
                                                                                        non-linearly to Hilbert Space
                                                                                        and a hyperplane in quantum
                                                                                        space is used to separate and
                                                                                        label the data. By using the...
                                                                                        read more Multilevel Development
                                                                                        of Cognitive Abilities for
                                                                                        Artificial Intelligence In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more Deontology,
                                                                                        Consequentialism, Virtue Ethics
                                                                                        Virtue Ethics is a branch of one
                                                                                        of three major approaches to
                                                                                        normative ethics, where
                                                                                        normative ethics, at the risk of
                                                                                        oversimplification, is concerned
                                                                                        with criteria for what is right
                                                                                        and wrong. The three main
                                                                                        philosophical ideologies
                                                                                        concerning normative ethics are
                                                                                        the following. Virtue ethics,
                                                                                        which can be identified from...
                                                                                        read more Anatomy and Histology
                                                                                        of the Hippocampus The
                                                                                        hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that... read more Connections in
                                                                                        the Human Structural Connectome
                                                                                        From the human structural
                                                                                        connectome, the authors attempt
                                                                                        to extract architectural
                                                                                        feautres using diffusion
                                                                                        spectrum imaging DSI and encode
                                                                                        the data required into triplcate
                                                                                        as undirected, weighted network.
                                                                                        They do so to capute as much
                                                                                        information about possible paths
                                                                                        which transmit as human process
                                                                                        and preform complex behaviors...
                                                                                        read more Chern Classes Chern
                                                                                        classes are a part of algebraic
                                                                                        topology, as well as other math
                                                                                        groups, and are characteristic
                                                                                        classes related to complex
                                                                                        vector bundles. Let $X$ be a
                                                                                        topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more Topological Neuroscience
                                                                                        One main theoretical framework
                                                                                        that is used to model, estimate,
                                                                                        and simulate brain networks from
                                                                                        complex network science is graph
                                                                                        theory. A graph being a
                                                                                        composition of a set of
                                                                                        intereconnected elements know as
                                                                                        vertices and edges. The vertices
                                                                                        in a network can represent brain
                                                                                        areas, while edges can
                                                                                        represent... read more
                                                                                        Information Theory Three
                                                                                        properties were required by
                                                                                        Shannon: $I(p) \geq 0$, i.e.
                                                                                        information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more Abstract
                                                                                        Algebra Abstract Algebra or
                                                                                        modern algebra can be defined as
                                                                                        the theory of algebraic
                                                                                        structures. For the most part,
                                                                                        abstract algebra deals with four
                                                                                        algebraic structures: groups,
                                                                                        rings, fields, and vector
                                                                                        spaces. We will look at and
                                                                                        examine these four algebraic
                                                                                        strucutres in this page. The
                                                                                        three most commonly studied
                                                                                        algebraic... read more Hidden
                                                                                        Markov Processes Let us first
                                                                                        start with a formal definition
                                                                                        of a 2 vector convex
                                                                                        combination. Then we will break
                                                                                        down the definition into parts
                                                                                        and analyze the definition. Then
                                                                                        we will formally define and
                                                                                        analyze a convex combination
                                                                                        with a finite number of vectors
                                                                                        in the same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more Quantum Computing
                                                                                        Theory Quantum Computing Theory
                                                                                        is a field of computer science
                                                                                        that uses the principles of
                                                                                        quantum mechanics, mathematics,
                                                                                        and computer science. By
                                                                                        borrowing concepts from each
                                                                                        field scientists can rigorously
                                                                                        define both a broad and narrow
                                                                                        theoretical model of a quantum
                                                                                        computer and later apply it to
                                                                                        the real world. These... read
                                                                                        more Quantum Mechanics First we
                                                                                        will introduce the idea of
                                                                                        Hilbert Space, which was named
                                                                                        after D. Hilbert. Hilbert Space
                                                                                        is a nondenumerable infinite
                                                                                        complex vector space. Complex
                                                                                        space, being a collection of
                                                                                        complex numbers with an added
                                                                                        structure. The infinite
                                                                                        dimensions of Hilbert Space
                                                                                        represents a continious spectra
                                                                                        of alternative physical
                                                                                        states... read more Differential
                                                                                        Manifolds An $n$-dimensional
                                                                                        manifold is a topological space
                                                                                        where each point has a
                                                                                        neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more Molecular Bases
                                                                                        of Memory Formation Molecular
                                                                                        neuroscience is an area of
                                                                                        chemical neuroscience that
                                                                                        studies the molecular basis of
                                                                                        intercellular activity applied
                                                                                        to animals' nervous systems.
                                                                                        This area of research covers
                                                                                        molecular neuroanatomy,
                                                                                        mechanisms of molecular
                                                                                        signaling in the nervous system,
                                                                                        and the molecular basis of
                                                                                        neuroplasticity and
                                                                                        neurodegenerative disease, which
                                                                                        we will focus on... read more
                                                                                        Graph Theory $G = (V, E)$ $V$ is
                                                                                        a set of vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        Theory of Computation A
                                                                                        deterministic finite automaton
                                                                                        DFA is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more Algorithmic
                                                                                        Anaylsis Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more ... And More Soon! All
                                                                                        Articles - Context Switching All
                                                                                        Articles - Context SwitchingAll
                                                                                        Articles Quantum Convolutional
                                                                                        Neural Network The input for the
                                                                                        quantum convolution layer is a
                                                                                        $3$D tensor input given as
                                                                                        $X^{\ell} \in
                                                                                        \mathbb{R}^{H^{\ell} \times
                                                                                        W^{\ell} \times D^{\ell}}$. The
                                                                                        weights layer, filter layer, or
                                                                                        $4$D tensor kernel layer is
                                                                                        denoted as $K^{\ell} \in
                                                                                        \mathbb{R}^{H \times W \times
                                                                                        D^{\ell} \times D^{\ell+1}}$.
                                                                                        The input and kernel layer are
                                                                                        both stored in QRAM. There are
                                                                                        precision parameters $\epsilon$
                                                                                        and $\Delta>0$... read more
                                                                                        Thalamic Nuclei The thalamic
                                                                                        nuclei are paired structures of
                                                                                        the thalamus divided into three
                                                                                        main groups: the lateral
                                                                                        nuclear, medial nuclear, and
                                                                                        anterior nuclear groups. The
                                                                                        internal medullary lamina, a
                                                                                        Y-shaped structure that splits
                                                                                        these groups, is present on each
                                                                                        side of the thalamus. A midline,
                                                                                        thin thalamic nuclei, adjacent
                                                                                        to the... read more Epigenetics
                                                                                        and Inheritance In biology,
                                                                                        epigenetics is the study of
                                                                                        mitotically and/or meiotically
                                                                                        heritable changes in gene
                                                                                        function that cannot be
                                                                                        explained by changes to the DNA
                                                                                        sequence. Epigenetics normally
                                                                                        involves change that is not
                                                                                        erased by cell division and that
                                                                                        also affects the regulation of
                                                                                        gene expression. Epigenetics
                                                                                        reflects our understanding...
                                                                                        read more High-Dimensional
                                                                                        Quantum Feature Mapping Quantum
                                                                                        Principal Component Analysis
                                                                                        identifies large eigenvalues of
                                                                                        unknown density matrices
                                                                                        utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for... read more
                                                                                        Quantum Fields in Anti-de Sitter
                                                                                        Space and the Maldacena
                                                                                        Conjecture In theoretical
                                                                                        physics, the Maldacena
                                                                                        Conjecture states supergravity
                                                                                        and string theory on the product
                                                                                        of $(n+1)$-dimensional Anti-de
                                                                                        Sitter space with a compact
                                                                                        manifold capable of describing
                                                                                        large $N$ limits of conformal
                                                                                        field theories in
                                                                                        $d$-dimensions. Correlation
                                                                                        functions in CFT are dependent
                                                                                        on the supergravity action of
                                                                                        asymptotic behavior at
                                                                                        infinity... read more
                                                                                        Bioinformatics and Functional
                                                                                        Genomics Bioinformatics is a
                                                                                        growing revolution in the field
                                                                                        of molecular biology and
                                                                                        computers. Here, our emphasis
                                                                                        will be on employing
                                                                                        bioinformatics tools and
                                                                                        biological databases to address
                                                                                        challenges from current issues
                                                                                        in biological, biotechnological,
                                                                                        and biomedical research. Such as
                                                                                        looking at computational
                                                                                        algorithms and computer
                                                                                        databases to analyze proteins,
                                                                                        genes... read more Multi-Store
                                                                                        Memory Model In neuroscience,
                                                                                        the central executive is
                                                                                        responsible for controlled
                                                                                        processing and allocation of
                                                                                        data to subsystems in working
                                                                                        memory. Such subsystems include
                                                                                        the visuospatial sketchpad and
                                                                                        phonological loop, where the
                                                                                        phonological loop can further be
                                                                                        subdivided into the phonological
                                                                                        store and the articulatory
                                                                                        process. The primary functions
                                                                                        of the central... read more
                                                                                        Compatification and Massless
                                                                                        Scattering in Anti-de Sitter
                                                                                        Space In theoretical physics,
                                                                                        Minkowski Space is a particular
                                                                                        type of $4$-dimensional
                                                                                        Lorentzian space, with a
                                                                                        Minkowski metric. Where the
                                                                                        Minkowski metric is a metric
                                                                                        tensor denoted as $d\tau^2$ with
                                                                                        the form
                                                                                        $-\left(d^0\right)^2+\left(d
                                                                                        x^1\right)^2$ $+\;\left(d
                                                                                        x^2\right)^2+\left(d
                                                                                        x^3\right)^2$. Minkowski space
                                                                                        forms the basis of the study of
                                                                                        spacetime within special
                                                                                        relativity and is... read more
                                                                                        Quantum Support Vector Machine
                                                                                        Using quantum computing, the
                                                                                        authors exploit quantum
                                                                                        mechanics for the algorithmic
                                                                                        complexity optimization of a
                                                                                        Support Vector Machine with
                                                                                        high-dimensional feature space.
                                                                                        Where the high-dimensional
                                                                                        classical data is mapped
                                                                                        non-linearly to Hilbert Space
                                                                                        and a hyperplane in quantum
                                                                                        space is used to separate and
                                                                                        label the data. By using the...
                                                                                        read more Multilevel Development
                                                                                        of Cognitive Abilities for
                                                                                        Artificial Intelligence In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more Deontology,
                                                                                        Consequentialism, Virtue Ethics
                                                                                        Virtue Ethics is a branch of one
                                                                                        of three major approaches to
                                                                                        normative ethics, where
                                                                                        normative ethics, at the risk of
                                                                                        oversimplification, is concerned
                                                                                        with criteria for what is right
                                                                                        and wrong. The three main
                                                                                        philosophical ideologies
                                                                                        concerning normative ethics are
                                                                                        the following. Virtue ethics,
                                                                                        which can be identified from...
                                                                                        read more Anatomy and Histology
                                                                                        of the Hippocampus The
                                                                                        hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that... read more Connections in
                                                                                        the Human Structural Connectome
                                                                                        From the human structural
                                                                                        connectome, the authors attempt
                                                                                        to extract architectural
                                                                                        feautres using diffusion
                                                                                        spectrum imaging DSI and encode
                                                                                        the data required into triplcate
                                                                                        as undirected, weighted network.
                                                                                        They do so to capute as much
                                                                                        information about possible paths
                                                                                        which transmit as human process
                                                                                        and preform complex behaviors...
                                                                                        read more Chern Classes Chern
                                                                                        classes are a part of algebraic
                                                                                        topology, as well as other math
                                                                                        groups, and are characteristic
                                                                                        classes related to complex
                                                                                        vector bundles. Let $X$ be a
                                                                                        topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more Topological Neuroscience
                                                                                        One main theoretical framework
                                                                                        that is used to model, estimate,
                                                                                        and simulate brain networks from
                                                                                        complex network science is graph
                                                                                        theory. A graph being a
                                                                                        composition of a set of
                                                                                        intereconnected elements know as
                                                                                        vertices and edges. The vertices
                                                                                        in a network can represent brain
                                                                                        areas, while edges can
                                                                                        represent... read more
                                                                                        Information Theory Three
                                                                                        properties were required by
                                                                                        Shannon: $I(p) \geq 0$, i.e.
                                                                                        information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more Abstract
                                                                                        Algebra Abstract Algebra or
                                                                                        modern algebra can be defined as
                                                                                        the theory of algebraic
                                                                                        structures. For the most part,
                                                                                        abstract algebra deals with four
                                                                                        algebraic structures: groups,
                                                                                        rings, fields, and vector
                                                                                        spaces. We will look at and
                                                                                        examine these four algebraic
                                                                                        strucutres in this page. The
                                                                                        three most commonly studied
                                                                                        algebraic... read more Hidden
                                                                                        Markov Processes Let us first
                                                                                        start with a formal definition
                                                                                        of a 2 vector convex
                                                                                        combination. Then we will break
                                                                                        down the definition into parts
                                                                                        and analyze the definition. Then
                                                                                        we will formally define and
                                                                                        analyze a convex combination
                                                                                        with a finite number of vectors
                                                                                        in the same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more Quantum Computing
                                                                                        Theory Quantum Computing Theory
                                                                                        is a field of computer science
                                                                                        that uses the principles of
                                                                                        quantum mechanics, mathematics,
                                                                                        and computer science. By
                                                                                        borrowing concepts from each
                                                                                        field scientists can rigorously
                                                                                        define both a broad and narrow
                                                                                        theoretical model of a quantum
                                                                                        computer and later apply it to
                                                                                        the real world. These... read
                                                                                        more Quantum Mechanics First we
                                                                                        will introduce the idea of
                                                                                        Hilbert Space, which was named
                                                                                        after D. Hilbert. Hilbert Space
                                                                                        is a nondenumerable infinite
                                                                                        complex vector space. Complex
                                                                                        space, being a collection of
                                                                                        complex numbers with an added
                                                                                        structure. The infinite
                                                                                        dimensions of Hilbert Space
                                                                                        represents a continious spectra
                                                                                        of alternative physical
                                                                                        states... read more Differential
                                                                                        Manifolds An $n$-dimensional
                                                                                        manifold is a topological space
                                                                                        where each point has a
                                                                                        neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more Molecular Bases
                                                                                        of Memory Formation Molecular
                                                                                        neuroscience is an area of
                                                                                        chemical neuroscience that
                                                                                        studies the molecular basis of
                                                                                        intercellular activity applied
                                                                                        to animals' nervous systems.
                                                                                        This area of research covers
                                                                                        molecular neuroanatomy,
                                                                                        mechanisms of molecular
                                                                                        signaling in the nervous system,
                                                                                        and the molecular basis of
                                                                                        neuroplasticity and
                                                                                        neurodegenerative disease, which
                                                                                        we will focus on... read more
                                                                                        Graph Theory $G = (V, E)$ $V$ is
                                                                                        a set of vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        Theory of Computation A
                                                                                        deterministic finite automaton
                                                                                        DFA is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more Algorithmic
                                                                                        Anaylsis Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more ... And More Soon! All
                                                                                        Articles All Articles All
                                                                                        Articles Quantum Convolutional
                                                                                        Neural Network Quantum
                                                                                        Convolutional Neural Network
                                                                                        Quantum Convolutional Neural
                                                                                        Network The input for the
                                                                                        quantum convolution layer is a
                                                                                        $3$D tensor input given as
                                                                                        $X^{\ell} \in
                                                                                        \mathbb{R}^{H^{\ell} \times
                                                                                        W^{\ell} \times D^{\ell}}$. The
                                                                                        weights layer, filter layer, or
                                                                                        $4$D tensor kernel layer is
                                                                                        denoted as $K^{\ell} \in
                                                                                        \mathbb{R}^{H \times W \times
                                                                                        D^{\ell} \times D^{\ell+1}}$.
                                                                                        The input and kernel layer are
                                                                                        both stored in QRAM. There are
                                                                                        precision parameters $\epsilon$
                                                                                        and $\Delta>0$... read more The
                                                                                        input for the quantum
                                                                                        convolution layer is a $3$D
                                                                                        tensor input given as $X^{\ell}
                                                                                        \in \mathbb{R}^{H^{\ell} \times
                                                                                        W^{\ell} \times D^{\ell}}$. The
                                                                                        weights layer, filter layer, or
                                                                                        $4$D tensor kernel layer is
                                                                                        denoted as $K^{\ell} \in
                                                                                        \mathbb{R}^{H \times W \times
                                                                                        D^{\ell} \times D^{\ell+1}}$.
                                                                                        The input and kernel layer are
                                                                                        both stored in QRAM. There are
                                                                                        precision parameters $\epsilon$
                                                                                        and $\Delta>0$... read more read
                                                                                        more Thalamic Nuclei Thalamic
                                                                                        Nuclei Thalamic Nuclei The
                                                                                        thalamic nuclei are paired
                                                                                        structures of the thalamus
                                                                                        divided into three main groups:
                                                                                        the lateral nuclear, medial
                                                                                        nuclear, and anterior nuclear
                                                                                        groups. The internal medullary
                                                                                        lamina, a Y-shaped structure
                                                                                        that splits these groups, is
                                                                                        present on each side of the
                                                                                        thalamus. A midline, thin
                                                                                        thalamic nuclei, adjacent to
                                                                                        the... read more The thalamic
                                                                                        nuclei are paired structures of
                                                                                        the thalamus divided into three
                                                                                        main groups: the lateral
                                                                                        nuclear, medial nuclear, and
                                                                                        anterior nuclear groups. The
                                                                                        internal medullary lamina, a
                                                                                        Y-shaped structure that splits
                                                                                        these groups, is present on each
                                                                                        side of the thalamus. A midline,
                                                                                        thin thalamic nuclei, adjacent
                                                                                        to the... read more read more
                                                                                        Epigenetics and Inheritance
                                                                                        Epigenetics and Inheritance
                                                                                        Epigenetics and Inheritance In
                                                                                        biology, epigenetics is the
                                                                                        study of mitotically and/or
                                                                                        meiotically heritable changes in
                                                                                        gene function that cannot be
                                                                                        explained by changes to the DNA
                                                                                        sequence. Epigenetics normally
                                                                                        involves change that is not
                                                                                        erased by cell division and that
                                                                                        also affects the regulation of
                                                                                        gene expression. Epigenetics
                                                                                        reflects our understanding...
                                                                                        read more In biology,
                                                                                        epigenetics is the study of
                                                                                        mitotically and/or meiotically
                                                                                        heritable changes in gene
                                                                                        function that cannot be
                                                                                        explained by changes to the DNA
                                                                                        sequence. Epigenetics normally
                                                                                        involves change that is not
                                                                                        erased by cell division and that
                                                                                        also affects the regulation of
                                                                                        gene expression. Epigenetics
                                                                                        reflects our understanding...
                                                                                        read more read more
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping High-Dimensional Quantum
                                                                                        Feature Mapping High-Dimensional
                                                                                        Quantum Feature Mapping Quantum
                                                                                        Principal Component Analysis
                                                                                        identifies large eigenvalues of
                                                                                        unknown density matrices
                                                                                        utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for... read more
                                                                                        Quantum Principal Component
                                                                                        Analysis identifies large
                                                                                        eigenvalues of unknown density
                                                                                        matrices utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for... read more
                                                                                        read more Quantum Fields in
                                                                                        Anti-de Sitter Space and the
                                                                                        Maldacena Conjecture Quantum
                                                                                        Fields in Anti-de Sitter Space
                                                                                        and the Maldacena Conjecture
                                                                                        Quantum Fields in Anti-de Sitter
                                                                                        Space and the Maldacena
                                                                                        Conjecture Quantum Fields in
                                                                                        Anti-de Sitter Space and the
                                                                                        Maldacena Conjecture In
                                                                                        theoretical physics, the
                                                                                        Maldacena Conjecture states
                                                                                        supergravity and string theory
                                                                                        on the product of
                                                                                        $(n+1)$-dimensional Anti-de
                                                                                        Sitter space with a compact
                                                                                        manifold capable of describing
                                                                                        large $N$ limits of conformal
                                                                                        field theories in
                                                                                        $d$-dimensions. Correlation
                                                                                        functions in CFT are dependent
                                                                                        on the supergravity action of
                                                                                        asymptotic behavior at
                                                                                        infinity... read more In
                                                                                        theoretical physics, the
                                                                                        Maldacena Conjecture states
                                                                                        supergravity and string theory
                                                                                        on the product of
                                                                                        $(n+1)$-dimensional Anti-de
                                                                                        Sitter space with a compact
                                                                                        manifold capable of describing
                                                                                        large $N$ limits of conformal
                                                                                        field theories in
                                                                                        $d$-dimensions. Correlation
                                                                                        functions in CFT are dependent
                                                                                        on the supergravity action of
                                                                                        asymptotic behavior at
                                                                                        infinity... read more read more
                                                                                        Bioinformatics and Functional
                                                                                        Genomics Bioinformatics and
                                                                                        Functional Genomics
                                                                                        Bioinformatics and Functional
                                                                                        Genomics Bioinformatics is a
                                                                                        growing revolution in the field
                                                                                        of molecular biology and
                                                                                        computers. Here, our emphasis
                                                                                        will be on employing
                                                                                        bioinformatics tools and
                                                                                        biological databases to address
                                                                                        challenges from current issues
                                                                                        in biological, biotechnological,
                                                                                        and biomedical research. Such as
                                                                                        looking at computational
                                                                                        algorithms and computer
                                                                                        databases to analyze proteins,
                                                                                        genes... read more
                                                                                        Bioinformatics is a growing
                                                                                        revolution in the field of
                                                                                        molecular biology and computers.
                                                                                        Here, our emphasis will be on
                                                                                        employing bioinformatics tools
                                                                                        and biological databases to
                                                                                        address challenges from current
                                                                                        issues in biological,
                                                                                        biotechnological, and biomedical
                                                                                        research. Such as looking at
                                                                                        computational algorithms and
                                                                                        computer databases to analyze
                                                                                        proteins, genes... read more
                                                                                        Bioinformatics is a growing
                                                                                        revolution in the field of
                                                                                        molecular biology and computers.
                                                                                        Here, our emphasis will be on
                                                                                        employing bioinformatics tools
                                                                                        and biological databases to
                                                                                        address challenges from current
                                                                                        issues in biological,
                                                                                        biotechnological, and biomedical
                                                                                        research. Such as looking at
                                                                                        computational algorithms and
                                                                                        computer databases to analyze
                                                                                        proteins, genes... read more
                                                                                        read more Multi-Store Memory
                                                                                        Model Multi-Store Memory Model
                                                                                        Multi-Store Memory Model In
                                                                                        neuroscience, the central
                                                                                        executive is responsible for
                                                                                        controlled processing and
                                                                                        allocation of data to subsystems
                                                                                        in working memory. Such
                                                                                        subsystems include the
                                                                                        visuospatial sketchpad and
                                                                                        phonological loop, where the
                                                                                        phonological loop can further be
                                                                                        subdivided into the phonological
                                                                                        store and the articulatory
                                                                                        process. The primary functions
                                                                                        of the central... read more In
                                                                                        neuroscience, the central
                                                                                        executive is responsible for
                                                                                        controlled processing and
                                                                                        allocation of data to subsystems
                                                                                        in working memory. Such
                                                                                        subsystems include the
                                                                                        visuospatial sketchpad and
                                                                                        phonological loop, where the
                                                                                        phonological loop can further be
                                                                                        subdivided into the phonological
                                                                                        store and the articulatory
                                                                                        process. The primary functions
                                                                                        of the central... read more In
                                                                                        neuroscience, the central
                                                                                        executive is responsible for
                                                                                        controlled processing and
                                                                                        allocation of data to subsystems
                                                                                        in working memory. Such
                                                                                        subsystems include the
                                                                                        visuospatial sketchpad and
                                                                                        phonological loop, where the
                                                                                        phonological loop can further be
                                                                                        subdivided into the phonological
                                                                                        store and the articulatory
                                                                                        process. The primary functions
                                                                                        of the central... read more read
                                                                                        more Compatification and
                                                                                        Massless Scattering in Anti-de
                                                                                        Sitter Space Compatification and
                                                                                        Massless Scattering in Anti-de
                                                                                        Sitter Space Compatification and
                                                                                        Massless Scattering in Anti-de
                                                                                        Sitter Space In theoretical
                                                                                        physics, Minkowski Space is a
                                                                                        particular type of
                                                                                        $4$-dimensional Lorentzian
                                                                                        space, with a Minkowski metric.
                                                                                        Where the Minkowski metric is a
                                                                                        metric tensor denoted as
                                                                                        $d\tau^2$ with the form
                                                                                        $-\left(d^0\right)^2+\left(d
                                                                                        x^1\right)^2$ $+\;\left(d
                                                                                        x^2\right)^2+\left(d
                                                                                        x^3\right)^2$. Minkowski space
                                                                                        forms the basis of the study of
                                                                                        spacetime within special
                                                                                        relativity and is... read more
                                                                                        In theoretical physics,
                                                                                        Minkowski Space is a particular
                                                                                        type of $4$-dimensional
                                                                                        Lorentzian space, with a
                                                                                        Minkowski metric. Where the
                                                                                        Minkowski metric is a metric
                                                                                        tensor denoted as $d\tau^2$ with
                                                                                        the form
                                                                                        $-\left(d^0\right)^2+\left(d
                                                                                        x^1\right)^2$ $+\;\left(d
                                                                                        x^2\right)^2+\left(d
                                                                                        x^3\right)^2$. Minkowski space
                                                                                        forms the basis of the study of
                                                                                        spacetime within special
                                                                                        relativity and is... read more
                                                                                        read more Quantum Support Vector
                                                                                        Machine Quantum Support Vector
                                                                                        Machine Quantum Support Vector
                                                                                        Machine Using quantum computing,
                                                                                        the authors exploit quantum
                                                                                        mechanics for the algorithmic
                                                                                        complexity optimization of a
                                                                                        Support Vector Machine with
                                                                                        high-dimensional feature space.
                                                                                        Where the high-dimensional
                                                                                        classical data is mapped
                                                                                        non-linearly to Hilbert Space
                                                                                        and a hyperplane in quantum
                                                                                        space is used to separate and
                                                                                        label the data. By using the...
                                                                                        read more Using quantum
                                                                                        computing, the authors exploit
                                                                                        quantum mechanics for the
                                                                                        algorithmic complexity
                                                                                        optimization of a Support Vector
                                                                                        Machine with high-dimensional
                                                                                        feature space. Where the
                                                                                        high-dimensional classical data
                                                                                        is mapped non-linearly to
                                                                                        Hilbert Space and a hyperplane
                                                                                        in quantum space is used to
                                                                                        separate and label the data. By
                                                                                        using the... read more read more
                                                                                        Multilevel Development of
                                                                                        Cognitive Abilities for
                                                                                        Artificial Intelligence
                                                                                        Multilevel Development of
                                                                                        Cognitive Abilities for
                                                                                        Artificial Intelligence
                                                                                        Multilevel Development of
                                                                                        Cognitive Abilities for
                                                                                        Artificial Intelligence In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more read more
                                                                                        Deontology, Consequentialism,
                                                                                        Virtue Ethics Deontology,
                                                                                        Consequentialism, Virtue Ethics
                                                                                        Deontology, Consequentialism,
                                                                                        Virtue Ethics Virtue Ethics is a
                                                                                        branch of one of three major
                                                                                        approaches to normative ethics,
                                                                                        where normative ethics, at the
                                                                                        risk of oversimplification, is
                                                                                        concerned with criteria for what
                                                                                        is right and wrong. The three
                                                                                        main philosophical ideologies
                                                                                        concerning normative ethics are
                                                                                        the following. Virtue ethics,
                                                                                        which can be identified from...
                                                                                        read more Virtue Ethics is a
                                                                                        branch of one of three major
                                                                                        approaches to normative ethics,
                                                                                        where normative ethics, at the
                                                                                        risk of oversimplification, is
                                                                                        concerned with criteria for what
                                                                                        is right and wrong. The three
                                                                                        main philosophical ideologies
                                                                                        concerning normative ethics are
                                                                                        the following. Virtue ethics,
                                                                                        which can be identified from...
                                                                                        read more Virtue Ethics is a
                                                                                        branch of one of three major
                                                                                        approaches to normative ethics,
                                                                                        where normative ethics, at the
                                                                                        risk of oversimplification, is
                                                                                        concerned with criteria for what
                                                                                        is right and wrong. The three
                                                                                        main philosophical ideologies
                                                                                        concerning normative ethics are
                                                                                        the following. Virtue ethics,
                                                                                        which can be identified from...
                                                                                        read more read more Anatomy and
                                                                                        Histology of the Hippocampus
                                                                                        Anatomy and Histology of the
                                                                                        Hippocampus Anatomy and
                                                                                        Histology of the Hippocampus The
                                                                                        hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that... read more The
                                                                                        hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that... read more The
                                                                                        hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that... read more read more
                                                                                        Connections in the Human
                                                                                        Structural Connectome
                                                                                        Connections in the Human
                                                                                        Structural Connectome
                                                                                        Connections in the Human
                                                                                        Structural Connectome From the
                                                                                        human structural connectome, the
                                                                                        authors attempt to extract
                                                                                        architectural feautres using
                                                                                        diffusion spectrum imaging DSI
                                                                                        and encode the data required
                                                                                        into triplcate as undirected,
                                                                                        weighted network. They do so to
                                                                                        capute as much information about
                                                                                        possible paths which transmit as
                                                                                        human process and preform
                                                                                        complex behaviors... read more
                                                                                        From the human structural
                                                                                        connectome, the authors attempt
                                                                                        to extract architectural
                                                                                        feautres using diffusion
                                                                                        spectrum imaging DSI and encode
                                                                                        the data required into triplcate
                                                                                        as undirected, weighted network.
                                                                                        They do so to capute as much
                                                                                        information about possible paths
                                                                                        which transmit as human process
                                                                                        and preform complex behaviors...
                                                                                        read more From the human
                                                                                        structural connectome, the
                                                                                        authors attempt to extract
                                                                                        architectural feautres using
                                                                                        diffusion spectrum imaging DSI
                                                                                        and encode the data required
                                                                                        into triplcate as undirected,
                                                                                        weighted network. They do so to
                                                                                        capute as much information about
                                                                                        possible paths which transmit as
                                                                                        human process and preform
                                                                                        complex behaviors... read more
                                                                                        read more Chern Classes Chern
                                                                                        Classes Chern Classes Chern
                                                                                        classes are a part of algebraic
                                                                                        topology, as well as other math
                                                                                        groups, and are characteristic
                                                                                        classes related to complex
                                                                                        vector bundles. Let $X$ be a
                                                                                        topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more Chern classes are a part of
                                                                                        algebraic topology, as well as
                                                                                        other math groups, and are
                                                                                        characteristic classes related
                                                                                        to complex vector bundles. Let
                                                                                        $X$ be a topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more Chern classes are a part of
                                                                                        algebraic topology, as well as
                                                                                        other math groups, and are
                                                                                        characteristic classes related
                                                                                        to complex vector bundles. Let
                                                                                        $X$ be a topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more read more Topological
                                                                                        Neuroscience Topological
                                                                                        Neuroscience Topological
                                                                                        Neuroscience One main
                                                                                        theoretical framework that is
                                                                                        used to model, estimate, and
                                                                                        simulate brain networks from
                                                                                        complex network science is graph
                                                                                        theory. A graph being a
                                                                                        composition of a set of
                                                                                        intereconnected elements know as
                                                                                        vertices and edges. The vertices
                                                                                        in a network can represent brain
                                                                                        areas, while edges can
                                                                                        represent... read more One main
                                                                                        theoretical framework that is
                                                                                        used to model, estimate, and
                                                                                        simulate brain networks from
                                                                                        complex network science is graph
                                                                                        theory. A graph being a
                                                                                        composition of a set of
                                                                                        intereconnected elements know as
                                                                                        vertices and edges. The vertices
                                                                                        in a network can represent brain
                                                                                        areas, while edges can
                                                                                        represent... read more One main
                                                                                        theoretical framework that is
                                                                                        used to model, estimate, and
                                                                                        simulate brain networks from
                                                                                        complex network science is graph
                                                                                        theory. A graph being a
                                                                                        composition of a set of
                                                                                        intereconnected elements know as
                                                                                        vertices and edges. The vertices
                                                                                        in a network can represent brain
                                                                                        areas, while edges can
                                                                                        represent... read more read more
                                                                                        Information Theory Information
                                                                                        Theory Information Theory Three
                                                                                        properties were required by
                                                                                        Shannon: $I(p) \geq 0$, i.e.
                                                                                        information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more Three
                                                                                        properties were required by
                                                                                        Shannon: $I(p) \geq 0$, i.e.
                                                                                        information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more Three
                                                                                        properties were required by
                                                                                        Shannon: $I(p) \geq 0$, i.e.
                                                                                        information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more read more
                                                                                        Abstract Algebra Abstract
                                                                                        Algebra Abstract Algebra
                                                                                        Abstract Algebra or modern
                                                                                        algebra can be defined as the
                                                                                        theory of algebraic structures.
                                                                                        For the most part, abstract
                                                                                        algebra deals with four
                                                                                        algebraic structures: groups,
                                                                                        rings, fields, and vector
                                                                                        spaces. We will look at and
                                                                                        examine these four algebraic
                                                                                        strucutres in this page. The
                                                                                        three most commonly studied
                                                                                        algebraic... read more Abstract
                                                                                        Algebra or modern algebra can be
                                                                                        defined as the theory of
                                                                                        algebraic structures. For the
                                                                                        most part, abstract algebra
                                                                                        deals with four algebraic
                                                                                        structures: groups, rings,
                                                                                        fields, and vector spaces. We
                                                                                        will look at and examine these
                                                                                        four algebraic strucutres in
                                                                                        this page. The three most
                                                                                        commonly studied algebraic...
                                                                                        read more Abstract Algebra or
                                                                                        modern algebra can be defined as
                                                                                        the theory of algebraic
                                                                                        structures. For the most part,
                                                                                        abstract algebra deals with four
                                                                                        algebraic structures: groups,
                                                                                        rings, fields, and vector
                                                                                        spaces. We will look at and
                                                                                        examine these four algebraic
                                                                                        strucutres in this page. The
                                                                                        three most commonly studied
                                                                                        algebraic... read more read more
                                                                                        Hidden Markov Processes Hidden
                                                                                        Markov Processes Hidden Markov
                                                                                        Processes Let us first start
                                                                                        with a formal definition of a 2
                                                                                        vector convex combination. Then
                                                                                        we will break down the
                                                                                        definition into parts and
                                                                                        analyze the definition. Then we
                                                                                        will formally define and analyze
                                                                                        a convex combination with a
                                                                                        finite number of vectors in the
                                                                                        same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more Let us first start
                                                                                        with a formal definition of a 2
                                                                                        vector convex combination. Then
                                                                                        we will break down the
                                                                                        definition into parts and
                                                                                        analyze the definition. Then we
                                                                                        will formally define and analyze
                                                                                        a convex combination with a
                                                                                        finite number of vectors in the
                                                                                        same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more Let us first start
                                                                                        with a formal definition of a 2
                                                                                        vector convex combination. Then
                                                                                        we will break down the
                                                                                        definition into parts and
                                                                                        analyze the definition. Then we
                                                                                        will formally define and analyze
                                                                                        a convex combination with a
                                                                                        finite number of vectors in the
                                                                                        same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more read more Quantum
                                                                                        Computing Theory Quantum
                                                                                        Computing Theory Quantum
                                                                                        Computing Theory Quantum
                                                                                        Computing Theory is a field of
                                                                                        computer science that uses the
                                                                                        principles of quantum mechanics,
                                                                                        mathematics, and computer
                                                                                        science. By borrowing concepts
                                                                                        from each field scientists can
                                                                                        rigorously define both a broad
                                                                                        and narrow theoretical model of
                                                                                        a quantum computer and later
                                                                                        apply it to the real world.
                                                                                        These... read more Quantum
                                                                                        Mechanics First we will
                                                                                        introduce the idea of Hilbert
                                                                                        Space, which was named after D.
                                                                                        Hilbert. Hilbert Space is a
                                                                                        nondenumerable infinite complex
                                                                                        vector space. Complex space,
                                                                                        being a collection of complex
                                                                                        numbers with an added structure.
                                                                                        The infinite dimensions of
                                                                                        Hilbert Space represents a
                                                                                        continious spectra of
                                                                                        alternative physical states...
                                                                                        read more Differential Manifolds
                                                                                        An $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more Molecular Bases
                                                                                        of Memory Formation Molecular
                                                                                        neuroscience is an area of
                                                                                        chemical neuroscience that
                                                                                        studies the molecular basis of
                                                                                        intercellular activity applied
                                                                                        to animals' nervous systems.
                                                                                        This area of research covers
                                                                                        molecular neuroanatomy,
                                                                                        mechanisms of molecular
                                                                                        signaling in the nervous system,
                                                                                        and the molecular basis of
                                                                                        neuroplasticity and
                                                                                        neurodegenerative disease, which
                                                                                        we will focus on... read more
                                                                                        Graph Theory $G = (V, E)$ $V$ is
                                                                                        a set of vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        Theory of Computation A
                                                                                        deterministic finite automaton
                                                                                        DFA is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more Algorithmic
                                                                                        Anaylsis Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more ... And More Soon! Quantum
                                                                                        Computing Theory is a field of
                                                                                        computer science that uses the
                                                                                        principles of quantum mechanics,
                                                                                        mathematics, and computer
                                                                                        science. By borrowing concepts
                                                                                        from each field scientists can
                                                                                        rigorously define both a broad
                                                                                        and narrow theoretical model of
                                                                                        a quantum computer and later
                                                                                        apply it to the real world.
                                                                                        These... read more Quantum
                                                                                        Computing Theory is a field of
                                                                                        computer science that uses the
                                                                                        principles of quantum mechanics,
                                                                                        mathematics, and computer
                                                                                        science. By borrowing concepts
                                                                                        from each field scientists can
                                                                                        rigorously define both a broad
                                                                                        and narrow theoretical model of
                                                                                        a quantum computer and later
                                                                                        apply it to the real world.
                                                                                        These... read more read more
                                                                                        Quantum Mechanics Quantum
                                                                                        Mechanics Quantum Mechanics
                                                                                        First we will introduce the idea
                                                                                        of Hilbert Space, which was
                                                                                        named after D. Hilbert. Hilbert
                                                                                        Space is a nondenumerable
                                                                                        infinite complex vector space.
                                                                                        Complex space, being a
                                                                                        collection of complex numbers
                                                                                        with an added structure. The
                                                                                        infinite dimensions of Hilbert
                                                                                        Space represents a continious
                                                                                        spectra of alternative physical
                                                                                        states... read more First we
                                                                                        will introduce the idea of
                                                                                        Hilbert Space, which was named
                                                                                        after D. Hilbert. Hilbert Space
                                                                                        is a nondenumerable infinite
                                                                                        complex vector space. Complex
                                                                                        space, being a collection of
                                                                                        complex numbers with an added
                                                                                        structure. The infinite
                                                                                        dimensions of Hilbert Space
                                                                                        represents a continious spectra
                                                                                        of alternative physical
                                                                                        states... read more First we
                                                                                        will introduce the idea of
                                                                                        Hilbert Space, which was named
                                                                                        after D. Hilbert. Hilbert Space
                                                                                        is a nondenumerable infinite
                                                                                        complex vector space. Complex
                                                                                        space, being a collection of
                                                                                        complex numbers with an added
                                                                                        structure. The infinite
                                                                                        dimensions of Hilbert Space
                                                                                        represents a continious spectra
                                                                                        of alternative physical
                                                                                        states... read more read more
                                                                                        Differential Manifolds
                                                                                        Differential Manifolds
                                                                                        Differential Manifolds An
                                                                                        $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more An
                                                                                        $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more An
                                                                                        $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more read more
                                                                                        Molecular Bases of Memory
                                                                                        Formation Molecular Bases of
                                                                                        Memory Formation Molecular Bases
                                                                                        of Memory Formation Molecular
                                                                                        neuroscience is an area of
                                                                                        chemical neuroscience that
                                                                                        studies the molecular basis of
                                                                                        intercellular activity applied
                                                                                        to animals' nervous systems.
                                                                                        This area of research covers
                                                                                        molecular neuroanatomy,
                                                                                        mechanisms of molecular
                                                                                        signaling in the nervous system,
                                                                                        and the molecular basis of
                                                                                        neuroplasticity and
                                                                                        neurodegenerative disease, which
                                                                                        we will focus on... read more
                                                                                        Molecular neuroscience is an
                                                                                        area of chemical neuroscience
                                                                                        that studies the molecular basis
                                                                                        of intercellular activity
                                                                                        applied to animals' nervous
                                                                                        systems. This area of research
                                                                                        covers molecular neuroanatomy,
                                                                                        mechanisms of molecular
                                                                                        signaling in the nervous system,
                                                                                        and the molecular basis of
                                                                                        neuroplasticity and
                                                                                        neurodegenerative disease, which
                                                                                        we will focus on... read more
                                                                                        Molecular neuroscience is an
                                                                                        area of chemical neuroscience
                                                                                        that studies the molecular basis
                                                                                        of intercellular activity
                                                                                        applied to animals' nervous
                                                                                        systems. This area of research
                                                                                        covers molecular neuroanatomy,
                                                                                        mechanisms of molecular
                                                                                        signaling in the nervous system,
                                                                                        and the molecular basis of
                                                                                        neuroplasticity and
                                                                                        neurodegenerative disease, which
                                                                                        we will focus on... read more
                                                                                        read more Graph Theory Graph
                                                                                        Theory Graph Theory $G = (V, E)$
                                                                                        $V$ is a set of vertices $E
                                                                                        \subseteq \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more $G
                                                                                        = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more $G
                                                                                        = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        read more Theory of Computation
                                                                                        Theory of Computation Theory of
                                                                                        Computation A deterministic
                                                                                        finite automaton DFA is a
                                                                                        5-tuple: $(Q, \Sigma, \delta,
                                                                                        q_{0}, F)$ where: $Q$ is a
                                                                                        finite set of states $\Sigma$ is
                                                                                        an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more A
                                                                                        deterministic finite automaton
                                                                                        DFA is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more A
                                                                                        deterministic finite automaton
                                                                                        DFA is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more read more
                                                                                        Algorithmic Anaylsis Algorithmic
                                                                                        Anaylsis Algorithmic Anaylsis
                                                                                        Algorithmic analysis is used to
                                                                                        help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more read more ... And More
                                                                                        Soon! ... And More Soon! ... And
                                                                                        More Soon!
                                                                                        [https://www.contextswitching.org/tcs/highdimquantumfm]
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping - Context Switching
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping Quantum Principal
                                                                                        Component Analysis Quantum
                                                                                        Principal Component Analysis
                                                                                        identifies large eigenvalues of
                                                                                        unknown density matrices
                                                                                        utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for state
                                                                                        discrimination and cluster
                                                                                        assignment for variational and
                                                                                        kernel classification machine
                                                                                        learning algorithms. Classical
                                                                                        Principal Component Analysis is
                                                                                        greatly used in signal
                                                                                        processing and machine learning
                                                                                        for dimension reduction with a
                                                                                        time complexity of $O(N^{3})$,
                                                                                        where $N$ is the dimension of
                                                                                        the data. The current issue with
                                                                                        this form of dimension reduction
                                                                                        is that when the data is large,
                                                                                        the classical PCA ends up
                                                                                        becoming non-tractable. A
                                                                                        possible solution for
                                                                                        dimensional reduction of large
                                                                                        data is utilizing Quantum
                                                                                        computing parallelism, where the
                                                                                        qPCA algorithm can then run with
                                                                                        a time complexity of $O(N
                                                                                        \operatorname{ploy}(\log N))$.
                                                                                        The outputs of qPCA are the
                                                                                        quantum states containing all
                                                                                        eigenvalues and eigenvectors of
                                                                                        the principal components of the
                                                                                        data for sampling. Consider a
                                                                                        dataset
                                                                                        $\left\{\mathbf{x}_i\right\}_{i=1}^N$
                                                                                        with $N$ data points where each
                                                                                        data point is a $D$-dimensional
                                                                                        column vector $\mathbf{x}_i=$
                                                                                        $\left(x_{i 1}, x_{i 2}, \cdots,
                                                                                        x_{i D}\right)^T \in
                                                                                        \mathbb{R}^D$. Through principal
                                                                                        component analysis we can lower
                                                                                        the dimensional space to a
                                                                                        $N\times D$ matrix
                                                                                        $X=\left(\mathbf{x}_1, \cdots,
                                                                                        \mathbf{x}_N\right)^T$ while
                                                                                        maximally preserving data
                                                                                        variance. Where an
                                                                                        eigendecomposition for a single
                                                                                        matrix $X$ is: $X=\sum_{j=1}^D
                                                                                        \sigma_j\left|\mathbf{u}_j\right\rangle\left\langle\mathbf{v}_j\right|$
                                                                                        such that $\left\{\sigma_j \in
                                                                                        \mathbb{R}_{\geq
                                                                                        0}\right\}_{j=1}^D$ are the
                                                                                        singular eigenvalues of the
                                                                                        matrix in descending order and
                                                                                        $\left\{\left|\mathbf{u}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^N\right\}_{j=1}^D$
                                                                                        and
                                                                                        $\left\{\left|\mathbf{v}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^D\right\}_{j=1}^D$
                                                                                        are, respectively, the left and
                                                                                        right singular eigenvectors or
                                                                                        principal components. For the
                                                                                        rest of this section, we will
                                                                                        look at why we need qPCA, how we
                                                                                        can achieve it, and what is the
                                                                                        procedure for the underlying
                                                                                        algorithm. High-Dimensional
                                                                                        Quantum Data Reduction Algorithm
                                                                                        Here, we will analyze matrix
                                                                                        dimensionality reduction using
                                                                                        qPCA. Let $\xi$ be the precision
                                                                                        parameter and $p$ denote
                                                                                        variance retained after
                                                                                        dimensionality reduction. Given
                                                                                        efficient quantum access to the
                                                                                        matrix $A=U \Sigma V^T$ $=$
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$, let
                                                                                        the top $k$ right singular
                                                                                        vectors $\bar{V}^{(k)} \in
                                                                                        \mathbb{R}^{m \times k}$, where
                                                                                        $\left\|V^{(k)}-\bar{V}^{(k)}\right\|$
                                                                                        $\leq$ $\frac{\xi
                                                                                        \sqrt{p}}{\sqrt{2}}$. There
                                                                                        exists a quantum algorithm that
                                                                                        creates $|\bar{Y}\rangle$ $=$
                                                                                        $\frac{1}{\|Y\|_F}
                                                                                        \sum_i^n\left\|y_{i,
                                                                                        \cdot}\right\||i\rangle\left|y_{i,
                                                                                        \cdot}\right\rangle$
                                                                                        proportional to the projection
                                                                                        of $A$ in the PCA subspace.
                                                                                        Where the algorithmic
                                                                                        lower-bound probability $1-1 /
                                                                                        \operatorname{poly}(m)$ and with
                                                                                        error
                                                                                        $\||Y\rangle-|\bar{Y}\rangle \|$
                                                                                        $\leq$ $\xi$ with the time
                                                                                        complexity $\widetilde{O}(1 /
                                                                                        \sqrt{p})$. If, instead, we
                                                                                        estimate the value of
                                                                                        $\|\bar{Y}\|_F$ to a realtive
                                                                                        error of $\eta$ then the time
                                                                                        complexity becomes
                                                                                        $\widetilde{O}\left(\frac{1}{\sqrt{p}
                                                                                        \eta}\right)$ It is important to
                                                                                        note what data is
                                                                                        PCA-representable and
                                                                                        qPCA-representable. First, we
                                                                                        begin by defining
                                                                                        PCA-representable data. Let a
                                                                                        set of $n$ data points be
                                                                                        defined $m$ coordinates and
                                                                                        represented by the matrix $A$,
                                                                                        which was given earlier as
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$. Matrix
                                                                                        or data $A$ is PCA-representable
                                                                                        if there exists $p
                                                                                        \in\left[\frac{1}{2}, 1\right]$,
                                                                                        $\varepsilon \in[0,1 / 2]$,
                                                                                        $\beta \in[p-\varepsilon,
                                                                                        p+\varepsilon]$, and $\alpha
                                                                                        \in[0,1]$ such that: $\exists k
                                                                                        \in O(1)$ where the variance
                                                                                        retained after dimensionality
                                                                                        reduction $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$. For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.
                                                                                        Note, this allows for
                                                                                        algorithmic lower-bound
                                                                                        probability of $1-1 /
                                                                                        \operatorname{poly}(m)$ we saw
                                                                                        before. Let's consider
                                                                                        qPCA-representable case. Given
                                                                                        $a_{i}$ is a row of
                                                                                        $A\in\mathbb{R}^{n\times d}$,
                                                                                        the time complexity is
                                                                                        $\frac{\left\|a_i\right\|}{\left\|\bar{y}_i\right\|}$
                                                                                        $=$ $\frac{1}{\beta}$ $=$ $O(1)$
                                                                                        with the a probability greater
                                                                                        than $\alpha$. Now that we have
                                                                                        defined the parameters, data
                                                                                        assumptions, and goal for our
                                                                                        qPCA algorithm, let's walk
                                                                                        through the algorithm itself.
                                                                                        The abstract concept is that the
                                                                                        algorithm transforms the quantum
                                                                                        state $|\psi_{s}\rangle$ that
                                                                                        holds the original data in
                                                                                        quantum parallel to another
                                                                                        quantum state
                                                                                        $|\psi_{e}\rangle$, that will
                                                                                        store the new low-dimensional
                                                                                        data points: ${\displaystyle
                                                                                        |\psi_s\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes
                                                                                        \mathbf{x}_{i}}{||X||_{F}} }$
                                                                                        $\mapsto$ ${\displaystyle
                                                                                        |\psi_{e}\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes \mathbf{y}_{i}}{\|Y\|_F}
                                                                                        }$ such that, $\|X\|_F$ and
                                                                                        $\|Y\|_F$ are the Frobenius
                                                                                        norms of $X$ and $Y$. Let:
                                                                                        $\mathbf{x}_i=\sum_{j=1}^D x_{i
                                                                                        j}|j\rangle=||\mathbf{x}_i||\;|\mathbf{x}_{i}\rangle$
                                                                                        $\mathbf{y}_i=\sum_{j=1}^d y_{i
                                                                                        j}|j\rangle=||\mathbf{y}_i||\;|\mathbf{y}_i\rangle$
                                                                                        For the implementation of the
                                                                                        quantum algorithm, first
                                                                                        consider the data set
                                                                                        $\{\mathbf{x}_i\}_{i=1}^N$, or
                                                                                        matrix $X$, is stored in a
                                                                                        quantum random access memory
                                                                                        qRAM, such that qRAM takes
                                                                                        $|i\rangle|0\rangle|0\rangle
                                                                                        \rightarrow|i\rangle\left|a_i\right\rangle||
                                                                                        \vec{a}_i|\rangle$. Next, using
                                                                                        quantum access to the vectors
                                                                                        and norms, we constuct the state
                                                                                        $\sum_i\left|\vec{a}_i\right|\left|e_i\right\rangle\left|a_i\right\rangle$,
                                                                                        where the density matrix for the
                                                                                        first register is exactly $X$.
                                                                                        Now, we must create
                                                                                        $n=O\left(t^2
                                                                                        \epsilon^{-1}\right)$ copies of
                                                                                        $X$, in order to have an
                                                                                        accuracy $\epsilon$ in time
                                                                                        $O(n\log d)$ by completing
                                                                                        $e^{-i X t}$ implementations of
                                                                                        this process. An important
                                                                                        aspect to keep in mind about
                                                                                        this algorithm is that the
                                                                                        density matrix exponentiation is
                                                                                        most effective when some of the
                                                                                        eigenvalues are large. This is
                                                                                        because we are utilizing quantum
                                                                                        parallelism to amplify
                                                                                        deviations in the probability
                                                                                        amplitudes in order to reveal
                                                                                        eigenvectors corresponding to
                                                                                        the large eigenvalues of the
                                                                                        unknown state. If all
                                                                                        eigenvalues are of size
                                                                                        $O(1/d)$, then the time
                                                                                        complexity increases to $t=(d)$
                                                                                        to generate a transformation
                                                                                        such that it rotates the input
                                                                                        state $\sigma$ to an orthogonal
                                                                                        state. Extracting Quantum
                                                                                        Principal Components Let's
                                                                                        obtain the first $d$ principal
                                                                                        components in the quantum state
                                                                                        form $|v_{1}\rangle
                                                                                        ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue.
                                                                                        Efficient Discrete Feature
                                                                                        Encoding To continue.
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping - Context Switching
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping - Context
                                                                                        SwitchingHigh-Dimensional
                                                                                        Quantum Feature Mapping Quantum
                                                                                        Principal Component Analysis
                                                                                        Quantum Principal Component
                                                                                        Analysis identifies large
                                                                                        eigenvalues of unknown density
                                                                                        matrices utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for state
                                                                                        discrimination and cluster
                                                                                        assignment for variational and
                                                                                        kernel classification machine
                                                                                        learning algorithms. Classical
                                                                                        Principal Component Analysis is
                                                                                        greatly used in signal
                                                                                        processing and machine learning
                                                                                        for dimension reduction with a
                                                                                        time complexity of $O(N^{3})$,
                                                                                        where $N$ is the dimension of
                                                                                        the data. The current issue with
                                                                                        this form of dimension reduction
                                                                                        is that when the data is large,
                                                                                        the classical PCA ends up
                                                                                        becoming non-tractable. A
                                                                                        possible solution for
                                                                                        dimensional reduction of large
                                                                                        data is utilizing Quantum
                                                                                        computing parallelism, where the
                                                                                        qPCA algorithm can then run with
                                                                                        a time complexity of $O(N
                                                                                        \operatorname{ploy}(\log N))$.
                                                                                        The outputs of qPCA are the
                                                                                        quantum states containing all
                                                                                        eigenvalues and eigenvectors of
                                                                                        the principal components of the
                                                                                        data for sampling. Consider a
                                                                                        dataset
                                                                                        $\left\{\mathbf{x}_i\right\}_{i=1}^N$
                                                                                        with $N$ data points where each
                                                                                        data point is a $D$-dimensional
                                                                                        column vector $\mathbf{x}_i=$
                                                                                        $\left(x_{i 1}, x_{i 2}, \cdots,
                                                                                        x_{i D}\right)^T \in
                                                                                        \mathbb{R}^D$. Through principal
                                                                                        component analysis we can lower
                                                                                        the dimensional space to a
                                                                                        $N\times D$ matrix
                                                                                        $X=\left(\mathbf{x}_1, \cdots,
                                                                                        \mathbf{x}_N\right)^T$ while
                                                                                        maximally preserving data
                                                                                        variance. Where an
                                                                                        eigendecomposition for a single
                                                                                        matrix $X$ is: $X=\sum_{j=1}^D
                                                                                        \sigma_j\left|\mathbf{u}_j\right\rangle\left\langle\mathbf{v}_j\right|$
                                                                                        such that $\left\{\sigma_j \in
                                                                                        \mathbb{R}_{\geq
                                                                                        0}\right\}_{j=1}^D$ are the
                                                                                        singular eigenvalues of the
                                                                                        matrix in descending order and
                                                                                        $\left\{\left|\mathbf{u}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^N\right\}_{j=1}^D$
                                                                                        and
                                                                                        $\left\{\left|\mathbf{v}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^D\right\}_{j=1}^D$
                                                                                        are, respectively, the left and
                                                                                        right singular eigenvectors or
                                                                                        principal components. For the
                                                                                        rest of this section, we will
                                                                                        look at why we need qPCA, how we
                                                                                        can achieve it, and what is the
                                                                                        procedure for the underlying
                                                                                        algorithm. High-Dimensional
                                                                                        Quantum Data Reduction Algorithm
                                                                                        Here, we will analyze matrix
                                                                                        dimensionality reduction using
                                                                                        qPCA. Let $\xi$ be the precision
                                                                                        parameter and $p$ denote
                                                                                        variance retained after
                                                                                        dimensionality reduction. Given
                                                                                        efficient quantum access to the
                                                                                        matrix $A=U \Sigma V^T$ $=$
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$, let
                                                                                        the top $k$ right singular
                                                                                        vectors $\bar{V}^{(k)} \in
                                                                                        \mathbb{R}^{m \times k}$, where
                                                                                        $\left\|V^{(k)}-\bar{V}^{(k)}\right\|$
                                                                                        $\leq$ $\frac{\xi
                                                                                        \sqrt{p}}{\sqrt{2}}$. There
                                                                                        exists a quantum algorithm that
                                                                                        creates $|\bar{Y}\rangle$ $=$
                                                                                        $\frac{1}{\|Y\|_F}
                                                                                        \sum_i^n\left\|y_{i,
                                                                                        \cdot}\right\||i\rangle\left|y_{i,
                                                                                        \cdot}\right\rangle$
                                                                                        proportional to the projection
                                                                                        of $A$ in the PCA subspace.
                                                                                        Where the algorithmic
                                                                                        lower-bound probability $1-1 /
                                                                                        \operatorname{poly}(m)$ and with
                                                                                        error
                                                                                        $\||Y\rangle-|\bar{Y}\rangle \|$
                                                                                        $\leq$ $\xi$ with the time
                                                                                        complexity $\widetilde{O}(1 /
                                                                                        \sqrt{p})$. If, instead, we
                                                                                        estimate the value of
                                                                                        $\|\bar{Y}\|_F$ to a realtive
                                                                                        error of $\eta$ then the time
                                                                                        complexity becomes
                                                                                        $\widetilde{O}\left(\frac{1}{\sqrt{p}
                                                                                        \eta}\right)$ It is important to
                                                                                        note what data is
                                                                                        PCA-representable and
                                                                                        qPCA-representable. First, we
                                                                                        begin by defining
                                                                                        PCA-representable data. Let a
                                                                                        set of $n$ data points be
                                                                                        defined $m$ coordinates and
                                                                                        represented by the matrix $A$,
                                                                                        which was given earlier as
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$. Matrix
                                                                                        or data $A$ is PCA-representable
                                                                                        if there exists $p
                                                                                        \in\left[\frac{1}{2}, 1\right]$,
                                                                                        $\varepsilon \in[0,1 / 2]$,
                                                                                        $\beta \in[p-\varepsilon,
                                                                                        p+\varepsilon]$, and $\alpha
                                                                                        \in[0,1]$ such that: $\exists k
                                                                                        \in O(1)$ where the variance
                                                                                        retained after dimensionality
                                                                                        reduction $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$. For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.
                                                                                        Note, this allows for
                                                                                        algorithmic lower-bound
                                                                                        probability of $1-1 /
                                                                                        \operatorname{poly}(m)$ we saw
                                                                                        before. Let's consider
                                                                                        qPCA-representable case. Given
                                                                                        $a_{i}$ is a row of
                                                                                        $A\in\mathbb{R}^{n\times d}$,
                                                                                        the time complexity is
                                                                                        $\frac{\left\|a_i\right\|}{\left\|\bar{y}_i\right\|}$
                                                                                        $=$ $\frac{1}{\beta}$ $=$ $O(1)$
                                                                                        with the a probability greater
                                                                                        than $\alpha$. Now that we have
                                                                                        defined the parameters, data
                                                                                        assumptions, and goal for our
                                                                                        qPCA algorithm, let's walk
                                                                                        through the algorithm itself.
                                                                                        The abstract concept is that the
                                                                                        algorithm transforms the quantum
                                                                                        state $|\psi_{s}\rangle$ that
                                                                                        holds the original data in
                                                                                        quantum parallel to another
                                                                                        quantum state
                                                                                        $|\psi_{e}\rangle$, that will
                                                                                        store the new low-dimensional
                                                                                        data points: ${\displaystyle
                                                                                        |\psi_s\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes
                                                                                        \mathbf{x}_{i}}{||X||_{F}} }$
                                                                                        $\mapsto$ ${\displaystyle
                                                                                        |\psi_{e}\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes \mathbf{y}_{i}}{\|Y\|_F}
                                                                                        }$ such that, $\|X\|_F$ and
                                                                                        $\|Y\|_F$ are the Frobenius
                                                                                        norms of $X$ and $Y$. Let:
                                                                                        $\mathbf{x}_i=\sum_{j=1}^D x_{i
                                                                                        j}|j\rangle=||\mathbf{x}_i||\;|\mathbf{x}_{i}\rangle$
                                                                                        $\mathbf{y}_i=\sum_{j=1}^d y_{i
                                                                                        j}|j\rangle=||\mathbf{y}_i||\;|\mathbf{y}_i\rangle$
                                                                                        For the implementation of the
                                                                                        quantum algorithm, first
                                                                                        consider the data set
                                                                                        $\{\mathbf{x}_i\}_{i=1}^N$, or
                                                                                        matrix $X$, is stored in a
                                                                                        quantum random access memory
                                                                                        qRAM, such that qRAM takes
                                                                                        $|i\rangle|0\rangle|0\rangle
                                                                                        \rightarrow|i\rangle\left|a_i\right\rangle||
                                                                                        \vec{a}_i|\rangle$. Next, using
                                                                                        quantum access to the vectors
                                                                                        and norms, we constuct the state
                                                                                        $\sum_i\left|\vec{a}_i\right|\left|e_i\right\rangle\left|a_i\right\rangle$,
                                                                                        where the density matrix for the
                                                                                        first register is exactly $X$.
                                                                                        Now, we must create
                                                                                        $n=O\left(t^2
                                                                                        \epsilon^{-1}\right)$ copies of
                                                                                        $X$, in order to have an
                                                                                        accuracy $\epsilon$ in time
                                                                                        $O(n\log d)$ by completing
                                                                                        $e^{-i X t}$ implementations of
                                                                                        this process. An important
                                                                                        aspect to keep in mind about
                                                                                        this algorithm is that the
                                                                                        density matrix exponentiation is
                                                                                        most effective when some of the
                                                                                        eigenvalues are large. This is
                                                                                        because we are utilizing quantum
                                                                                        parallelism to amplify
                                                                                        deviations in the probability
                                                                                        amplitudes in order to reveal
                                                                                        eigenvectors corresponding to
                                                                                        the large eigenvalues of the
                                                                                        unknown state. If all
                                                                                        eigenvalues are of size
                                                                                        $O(1/d)$, then the time
                                                                                        complexity increases to $t=(d)$
                                                                                        to generate a transformation
                                                                                        such that it rotates the input
                                                                                        state $\sigma$ to an orthogonal
                                                                                        state. Extracting Quantum
                                                                                        Principal Components Let's
                                                                                        obtain the first $d$ principal
                                                                                        components in the quantum state
                                                                                        form $|v_{1}\rangle
                                                                                        ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue.
                                                                                        Efficient Discrete Feature
                                                                                        Encoding To continue.
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping Quantum Principal
                                                                                        Component Analysis Quantum
                                                                                        Principal Component Analysis
                                                                                        identifies large eigenvalues of
                                                                                        unknown density matrices
                                                                                        utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for state
                                                                                        discrimination and cluster
                                                                                        assignment for variational and
                                                                                        kernel classification machine
                                                                                        learning algorithms. Classical
                                                                                        Principal Component Analysis is
                                                                                        greatly used in signal
                                                                                        processing and machine learning
                                                                                        for dimension reduction with a
                                                                                        time complexity of $O(N^{3})$,
                                                                                        where $N$ is the dimension of
                                                                                        the data. The current issue with
                                                                                        this form of dimension reduction
                                                                                        is that when the data is large,
                                                                                        the classical PCA ends up
                                                                                        becoming non-tractable. A
                                                                                        possible solution for
                                                                                        dimensional reduction of large
                                                                                        data is utilizing Quantum
                                                                                        computing parallelism, where the
                                                                                        qPCA algorithm can then run with
                                                                                        a time complexity of $O(N
                                                                                        \operatorname{ploy}(\log N))$.
                                                                                        The outputs of qPCA are the
                                                                                        quantum states containing all
                                                                                        eigenvalues and eigenvectors of
                                                                                        the principal components of the
                                                                                        data for sampling. Consider a
                                                                                        dataset
                                                                                        $\left\{\mathbf{x}_i\right\}_{i=1}^N$
                                                                                        with $N$ data points where each
                                                                                        data point is a $D$-dimensional
                                                                                        column vector $\mathbf{x}_i=$
                                                                                        $\left(x_{i 1}, x_{i 2}, \cdots,
                                                                                        x_{i D}\right)^T \in
                                                                                        \mathbb{R}^D$. Through principal
                                                                                        component analysis we can lower
                                                                                        the dimensional space to a
                                                                                        $N\times D$ matrix
                                                                                        $X=\left(\mathbf{x}_1, \cdots,
                                                                                        \mathbf{x}_N\right)^T$ while
                                                                                        maximally preserving data
                                                                                        variance. Where an
                                                                                        eigendecomposition for a single
                                                                                        matrix $X$ is: $X=\sum_{j=1}^D
                                                                                        \sigma_j\left|\mathbf{u}_j\right\rangle\left\langle\mathbf{v}_j\right|$
                                                                                        such that $\left\{\sigma_j \in
                                                                                        \mathbb{R}_{\geq
                                                                                        0}\right\}_{j=1}^D$ are the
                                                                                        singular eigenvalues of the
                                                                                        matrix in descending order and
                                                                                        $\left\{\left|\mathbf{u}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^N\right\}_{j=1}^D$
                                                                                        and
                                                                                        $\left\{\left|\mathbf{v}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^D\right\}_{j=1}^D$
                                                                                        are, respectively, the left and
                                                                                        right singular eigenvectors or
                                                                                        principal components. For the
                                                                                        rest of this section, we will
                                                                                        look at why we need qPCA, how we
                                                                                        can achieve it, and what is the
                                                                                        procedure for the underlying
                                                                                        algorithm. High-Dimensional
                                                                                        Quantum Data Reduction Algorithm
                                                                                        Here, we will analyze matrix
                                                                                        dimensionality reduction using
                                                                                        qPCA. Let $\xi$ be the precision
                                                                                        parameter and $p$ denote
                                                                                        variance retained after
                                                                                        dimensionality reduction. Given
                                                                                        efficient quantum access to the
                                                                                        matrix $A=U \Sigma V^T$ $=$
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$, let
                                                                                        the top $k$ right singular
                                                                                        vectors $\bar{V}^{(k)} \in
                                                                                        \mathbb{R}^{m \times k}$, where
                                                                                        $\left\|V^{(k)}-\bar{V}^{(k)}\right\|$
                                                                                        $\leq$ $\frac{\xi
                                                                                        \sqrt{p}}{\sqrt{2}}$. There
                                                                                        exists a quantum algorithm that
                                                                                        creates $|\bar{Y}\rangle$ $=$
                                                                                        $\frac{1}{\|Y\|_F}
                                                                                        \sum_i^n\left\|y_{i,
                                                                                        \cdot}\right\||i\rangle\left|y_{i,
                                                                                        \cdot}\right\rangle$
                                                                                        proportional to the projection
                                                                                        of $A$ in the PCA subspace.
                                                                                        Where the algorithmic
                                                                                        lower-bound probability $1-1 /
                                                                                        \operatorname{poly}(m)$ and with
                                                                                        error
                                                                                        $\||Y\rangle-|\bar{Y}\rangle \|$
                                                                                        $\leq$ $\xi$ with the time
                                                                                        complexity $\widetilde{O}(1 /
                                                                                        \sqrt{p})$. If, instead, we
                                                                                        estimate the value of
                                                                                        $\|\bar{Y}\|_F$ to a realtive
                                                                                        error of $\eta$ then the time
                                                                                        complexity becomes
                                                                                        $\widetilde{O}\left(\frac{1}{\sqrt{p}
                                                                                        \eta}\right)$ It is important to
                                                                                        note what data is
                                                                                        PCA-representable and
                                                                                        qPCA-representable. First, we
                                                                                        begin by defining
                                                                                        PCA-representable data. Let a
                                                                                        set of $n$ data points be
                                                                                        defined $m$ coordinates and
                                                                                        represented by the matrix $A$,
                                                                                        which was given earlier as
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$. Matrix
                                                                                        or data $A$ is PCA-representable
                                                                                        if there exists $p
                                                                                        \in\left[\frac{1}{2}, 1\right]$,
                                                                                        $\varepsilon \in[0,1 / 2]$,
                                                                                        $\beta \in[p-\varepsilon,
                                                                                        p+\varepsilon]$, and $\alpha
                                                                                        \in[0,1]$ such that: $\exists k
                                                                                        \in O(1)$ where the variance
                                                                                        retained after dimensionality
                                                                                        reduction $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$. For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.
                                                                                        Note, this allows for
                                                                                        algorithmic lower-bound
                                                                                        probability of $1-1 /
                                                                                        \operatorname{poly}(m)$ we saw
                                                                                        before. Let's consider
                                                                                        qPCA-representable case. Given
                                                                                        $a_{i}$ is a row of
                                                                                        $A\in\mathbb{R}^{n\times d}$,
                                                                                        the time complexity is
                                                                                        $\frac{\left\|a_i\right\|}{\left\|\bar{y}_i\right\|}$
                                                                                        $=$ $\frac{1}{\beta}$ $=$ $O(1)$
                                                                                        with the a probability greater
                                                                                        than $\alpha$. Now that we have
                                                                                        defined the parameters, data
                                                                                        assumptions, and goal for our
                                                                                        qPCA algorithm, let's walk
                                                                                        through the algorithm itself.
                                                                                        The abstract concept is that the
                                                                                        algorithm transforms the quantum
                                                                                        state $|\psi_{s}\rangle$ that
                                                                                        holds the original data in
                                                                                        quantum parallel to another
                                                                                        quantum state
                                                                                        $|\psi_{e}\rangle$, that will
                                                                                        store the new low-dimensional
                                                                                        data points: ${\displaystyle
                                                                                        |\psi_s\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes
                                                                                        \mathbf{x}_{i}}{||X||_{F}} }$
                                                                                        $\mapsto$ ${\displaystyle
                                                                                        |\psi_{e}\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes \mathbf{y}_{i}}{\|Y\|_F}
                                                                                        }$ such that, $\|X\|_F$ and
                                                                                        $\|Y\|_F$ are the Frobenius
                                                                                        norms of $X$ and $Y$. Let:
                                                                                        $\mathbf{x}_i=\sum_{j=1}^D x_{i
                                                                                        j}|j\rangle=||\mathbf{x}_i||\;|\mathbf{x}_{i}\rangle$
                                                                                        $\mathbf{y}_i=\sum_{j=1}^d y_{i
                                                                                        j}|j\rangle=||\mathbf{y}_i||\;|\mathbf{y}_i\rangle$
                                                                                        For the implementation of the
                                                                                        quantum algorithm, first
                                                                                        consider the data set
                                                                                        $\{\mathbf{x}_i\}_{i=1}^N$, or
                                                                                        matrix $X$, is stored in a
                                                                                        quantum random access memory
                                                                                        qRAM, such that qRAM takes
                                                                                        $|i\rangle|0\rangle|0\rangle
                                                                                        \rightarrow|i\rangle\left|a_i\right\rangle||
                                                                                        \vec{a}_i|\rangle$. Next, using
                                                                                        quantum access to the vectors
                                                                                        and norms, we constuct the state
                                                                                        $\sum_i\left|\vec{a}_i\right|\left|e_i\right\rangle\left|a_i\right\rangle$,
                                                                                        where the density matrix for the
                                                                                        first register is exactly $X$.
                                                                                        Now, we must create
                                                                                        $n=O\left(t^2
                                                                                        \epsilon^{-1}\right)$ copies of
                                                                                        $X$, in order to have an
                                                                                        accuracy $\epsilon$ in time
                                                                                        $O(n\log d)$ by completing
                                                                                        $e^{-i X t}$ implementations of
                                                                                        this process. An important
                                                                                        aspect to keep in mind about
                                                                                        this algorithm is that the
                                                                                        density matrix exponentiation is
                                                                                        most effective when some of the
                                                                                        eigenvalues are large. This is
                                                                                        because we are utilizing quantum
                                                                                        parallelism to amplify
                                                                                        deviations in the probability
                                                                                        amplitudes in order to reveal
                                                                                        eigenvectors corresponding to
                                                                                        the large eigenvalues of the
                                                                                        unknown state. If all
                                                                                        eigenvalues are of size
                                                                                        $O(1/d)$, then the time
                                                                                        complexity increases to $t=(d)$
                                                                                        to generate a transformation
                                                                                        such that it rotates the input
                                                                                        state $\sigma$ to an orthogonal
                                                                                        state. Extracting Quantum
                                                                                        Principal Components Let's
                                                                                        obtain the first $d$ principal
                                                                                        components in the quantum state
                                                                                        form $|v_{1}\rangle
                                                                                        ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue.
                                                                                        Efficient Discrete Feature
                                                                                        Encoding To continue.
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping Quantum Principal
                                                                                        Component Analysis Quantum
                                                                                        Principal Component Analysis
                                                                                        identifies large eigenvalues of
                                                                                        unknown density matrices
                                                                                        utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for state
                                                                                        discrimination and cluster
                                                                                        assignment for variational and
                                                                                        kernel classification machine
                                                                                        learning algorithms. Classical
                                                                                        Principal Component Analysis is
                                                                                        greatly used in signal
                                                                                        processing and machine learning
                                                                                        for dimension reduction with a
                                                                                        time complexity of $O(N^{3})$,
                                                                                        where $N$ is the dimension of
                                                                                        the data. The current issue with
                                                                                        this form of dimension reduction
                                                                                        is that when the data is large,
                                                                                        the classical PCA ends up
                                                                                        becoming non-tractable. A
                                                                                        possible solution for
                                                                                        dimensional reduction of large
                                                                                        data is utilizing Quantum
                                                                                        computing parallelism, where the
                                                                                        qPCA algorithm can then run with
                                                                                        a time complexity of $O(N
                                                                                        \operatorname{ploy}(\log N))$.
                                                                                        The outputs of qPCA are the
                                                                                        quantum states containing all
                                                                                        eigenvalues and eigenvectors of
                                                                                        the principal components of the
                                                                                        data for sampling. Consider a
                                                                                        dataset
                                                                                        $\left\{\mathbf{x}_i\right\}_{i=1}^N$
                                                                                        with $N$ data points where each
                                                                                        data point is a $D$-dimensional
                                                                                        column vector $\mathbf{x}_i=$
                                                                                        $\left(x_{i 1}, x_{i 2}, \cdots,
                                                                                        x_{i D}\right)^T \in
                                                                                        \mathbb{R}^D$. Through principal
                                                                                        component analysis we can lower
                                                                                        the dimensional space to a
                                                                                        $N\times D$ matrix
                                                                                        $X=\left(\mathbf{x}_1, \cdots,
                                                                                        \mathbf{x}_N\right)^T$ while
                                                                                        maximally preserving data
                                                                                        variance. Where an
                                                                                        eigendecomposition for a single
                                                                                        matrix $X$ is: $X=\sum_{j=1}^D
                                                                                        \sigma_j\left|\mathbf{u}_j\right\rangle\left\langle\mathbf{v}_j\right|$
                                                                                        such that $\left\{\sigma_j \in
                                                                                        \mathbb{R}_{\geq
                                                                                        0}\right\}_{j=1}^D$ are the
                                                                                        singular eigenvalues of the
                                                                                        matrix in descending order and
                                                                                        $\left\{\left|\mathbf{u}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^N\right\}_{j=1}^D$
                                                                                        and
                                                                                        $\left\{\left|\mathbf{v}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^D\right\}_{j=1}^D$
                                                                                        are, respectively, the left and
                                                                                        right singular eigenvectors or
                                                                                        principal components. For the
                                                                                        rest of this section, we will
                                                                                        look at why we need qPCA, how we
                                                                                        can achieve it, and what is the
                                                                                        procedure for the underlying
                                                                                        algorithm. High-Dimensional
                                                                                        Quantum Data Reduction Algorithm
                                                                                        Here, we will analyze matrix
                                                                                        dimensionality reduction using
                                                                                        qPCA. Let $\xi$ be the precision
                                                                                        parameter and $p$ denote
                                                                                        variance retained after
                                                                                        dimensionality reduction. Given
                                                                                        efficient quantum access to the
                                                                                        matrix $A=U \Sigma V^T$ $=$
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$, let
                                                                                        the top $k$ right singular
                                                                                        vectors $\bar{V}^{(k)} \in
                                                                                        \mathbb{R}^{m \times k}$, where
                                                                                        $\left\|V^{(k)}-\bar{V}^{(k)}\right\|$
                                                                                        $\leq$ $\frac{\xi
                                                                                        \sqrt{p}}{\sqrt{2}}$. There
                                                                                        exists a quantum algorithm that
                                                                                        creates $|\bar{Y}\rangle$ $=$
                                                                                        $\frac{1}{\|Y\|_F}
                                                                                        \sum_i^n\left\|y_{i,
                                                                                        \cdot}\right\||i\rangle\left|y_{i,
                                                                                        \cdot}\right\rangle$
                                                                                        proportional to the projection
                                                                                        of $A$ in the PCA subspace.
                                                                                        Where the algorithmic
                                                                                        lower-bound probability $1-1 /
                                                                                        \operatorname{poly}(m)$ and with
                                                                                        error
                                                                                        $\||Y\rangle-|\bar{Y}\rangle \|$
                                                                                        $\leq$ $\xi$ with the time
                                                                                        complexity $\widetilde{O}(1 /
                                                                                        \sqrt{p})$. If, instead, we
                                                                                        estimate the value of
                                                                                        $\|\bar{Y}\|_F$ to a realtive
                                                                                        error of $\eta$ then the time
                                                                                        complexity becomes
                                                                                        $\widetilde{O}\left(\frac{1}{\sqrt{p}
                                                                                        \eta}\right)$ It is important to
                                                                                        note what data is
                                                                                        PCA-representable and
                                                                                        qPCA-representable. First, we
                                                                                        begin by defining
                                                                                        PCA-representable data. Let a
                                                                                        set of $n$ data points be
                                                                                        defined $m$ coordinates and
                                                                                        represented by the matrix $A$,
                                                                                        which was given earlier as
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$. Matrix
                                                                                        or data $A$ is PCA-representable
                                                                                        if there exists $p
                                                                                        \in\left[\frac{1}{2}, 1\right]$,
                                                                                        $\varepsilon \in[0,1 / 2]$,
                                                                                        $\beta \in[p-\varepsilon,
                                                                                        p+\varepsilon]$, and $\alpha
                                                                                        \in[0,1]$ such that: $\exists k
                                                                                        \in O(1)$ where the variance
                                                                                        retained after dimensionality
                                                                                        reduction $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$. For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.
                                                                                        Note, this allows for
                                                                                        algorithmic lower-bound
                                                                                        probability of $1-1 /
                                                                                        \operatorname{poly}(m)$ we saw
                                                                                        before. Let's consider
                                                                                        qPCA-representable case. Given
                                                                                        $a_{i}$ is a row of
                                                                                        $A\in\mathbb{R}^{n\times d}$,
                                                                                        the time complexity is
                                                                                        $\frac{\left\|a_i\right\|}{\left\|\bar{y}_i\right\|}$
                                                                                        $=$ $\frac{1}{\beta}$ $=$ $O(1)$
                                                                                        with the a probability greater
                                                                                        than $\alpha$. Now that we have
                                                                                        defined the parameters, data
                                                                                        assumptions, and goal for our
                                                                                        qPCA algorithm, let's walk
                                                                                        through the algorithm itself.
                                                                                        The abstract concept is that the
                                                                                        algorithm transforms the quantum
                                                                                        state $|\psi_{s}\rangle$ that
                                                                                        holds the original data in
                                                                                        quantum parallel to another
                                                                                        quantum state
                                                                                        $|\psi_{e}\rangle$, that will
                                                                                        store the new low-dimensional
                                                                                        data points: ${\displaystyle
                                                                                        |\psi_s\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes
                                                                                        \mathbf{x}_{i}}{||X||_{F}} }$
                                                                                        $\mapsto$ ${\displaystyle
                                                                                        |\psi_{e}\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes \mathbf{y}_{i}}{\|Y\|_F}
                                                                                        }$ such that, $\|X\|_F$ and
                                                                                        $\|Y\|_F$ are the Frobenius
                                                                                        norms of $X$ and $Y$. Let:
                                                                                        $\mathbf{x}_i=\sum_{j=1}^D x_{i
                                                                                        j}|j\rangle=||\mathbf{x}_i||\;|\mathbf{x}_{i}\rangle$
                                                                                        $\mathbf{y}_i=\sum_{j=1}^d y_{i
                                                                                        j}|j\rangle=||\mathbf{y}_i||\;|\mathbf{y}_i\rangle$
                                                                                        For the implementation of the
                                                                                        quantum algorithm, first
                                                                                        consider the data set
                                                                                        $\{\mathbf{x}_i\}_{i=1}^N$, or
                                                                                        matrix $X$, is stored in a
                                                                                        quantum random access memory
                                                                                        qRAM, such that qRAM takes
                                                                                        $|i\rangle|0\rangle|0\rangle
                                                                                        \rightarrow|i\rangle\left|a_i\right\rangle||
                                                                                        \vec{a}_i|\rangle$. Next, using
                                                                                        quantum access to the vectors
                                                                                        and norms, we constuct the state
                                                                                        $\sum_i\left|\vec{a}_i\right|\left|e_i\right\rangle\left|a_i\right\rangle$,
                                                                                        where the density matrix for the
                                                                                        first register is exactly $X$.
                                                                                        Now, we must create
                                                                                        $n=O\left(t^2
                                                                                        \epsilon^{-1}\right)$ copies of
                                                                                        $X$, in order to have an
                                                                                        accuracy $\epsilon$ in time
                                                                                        $O(n\log d)$ by completing
                                                                                        $e^{-i X t}$ implementations of
                                                                                        this process. An important
                                                                                        aspect to keep in mind about
                                                                                        this algorithm is that the
                                                                                        density matrix exponentiation is
                                                                                        most effective when some of the
                                                                                        eigenvalues are large. This is
                                                                                        because we are utilizing quantum
                                                                                        parallelism to amplify
                                                                                        deviations in the probability
                                                                                        amplitudes in order to reveal
                                                                                        eigenvectors corresponding to
                                                                                        the large eigenvalues of the
                                                                                        unknown state. If all
                                                                                        eigenvalues are of size
                                                                                        $O(1/d)$, then the time
                                                                                        complexity increases to $t=(d)$
                                                                                        to generate a transformation
                                                                                        such that it rotates the input
                                                                                        state $\sigma$ to an orthogonal
                                                                                        state. Extracting Quantum
                                                                                        Principal Components Let's
                                                                                        obtain the first $d$ principal
                                                                                        components in the quantum state
                                                                                        form $|v_{1}\rangle
                                                                                        ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue.
                                                                                        Efficient Discrete Feature
                                                                                        Encoding To continue.
                                                                                        High-Dimensional Quantum Feature
                                                                                        MappingQuantum Principal
                                                                                        Component Analysis Quantum
                                                                                        Principal Component Analysis
                                                                                        identifies large eigenvalues of
                                                                                        unknown density matrices
                                                                                        utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for state
                                                                                        discrimination and cluster
                                                                                        assignment for variational and
                                                                                        kernel classification machine
                                                                                        learning algorithms. Classical
                                                                                        Principal Component Analysis is
                                                                                        greatly used in signal
                                                                                        processing and machine learning
                                                                                        for dimension reduction with a
                                                                                        time complexity of $O(N^{3})$,
                                                                                        where $N$ is the dimension of
                                                                                        the data. The current issue with
                                                                                        this form of dimension reduction
                                                                                        is that when the data is large,
                                                                                        the classical PCA ends up
                                                                                        becoming non-tractable. A
                                                                                        possible solution for
                                                                                        dimensional reduction of large
                                                                                        data is utilizing Quantum
                                                                                        computing parallelism, where the
                                                                                        qPCA algorithm can then run with
                                                                                        a time complexity of $O(N
                                                                                        \operatorname{ploy}(\log N))$.
                                                                                        The outputs of qPCA are the
                                                                                        quantum states containing all
                                                                                        eigenvalues and eigenvectors of
                                                                                        the principal components of the
                                                                                        data for sampling. Consider a
                                                                                        dataset
                                                                                        $\left\{\mathbf{x}_i\right\}_{i=1}^N$
                                                                                        with $N$ data points where each
                                                                                        data point is a $D$-dimensional
                                                                                        column vector $\mathbf{x}_i=$
                                                                                        $\left(x_{i 1}, x_{i 2}, \cdots,
                                                                                        x_{i D}\right)^T \in
                                                                                        \mathbb{R}^D$. Through principal
                                                                                        component analysis we can lower
                                                                                        the dimensional space to a
                                                                                        $N\times D$ matrix
                                                                                        $X=\left(\mathbf{x}_1, \cdots,
                                                                                        \mathbf{x}_N\right)^T$ while
                                                                                        maximally preserving data
                                                                                        variance. Where an
                                                                                        eigendecomposition for a single
                                                                                        matrix $X$ is: $X=\sum_{j=1}^D
                                                                                        \sigma_j\left|\mathbf{u}_j\right\rangle\left\langle\mathbf{v}_j\right|$
                                                                                        such that $\left\{\sigma_j \in
                                                                                        \mathbb{R}_{\geq
                                                                                        0}\right\}_{j=1}^D$ are the
                                                                                        singular eigenvalues of the
                                                                                        matrix in descending order and
                                                                                        $\left\{\left|\mathbf{u}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^N\right\}_{j=1}^D$
                                                                                        and
                                                                                        $\left\{\left|\mathbf{v}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^D\right\}_{j=1}^D$
                                                                                        are, respectively, the left and
                                                                                        right singular eigenvectors or
                                                                                        principal components. For the
                                                                                        rest of this section, we will
                                                                                        look at why we need qPCA, how we
                                                                                        can achieve it, and what is the
                                                                                        procedure for the underlying
                                                                                        algorithm. High-Dimensional
                                                                                        Quantum Data Reduction Algorithm
                                                                                        Here, we will analyze matrix
                                                                                        dimensionality reduction using
                                                                                        qPCA. Let $\xi$ be the precision
                                                                                        parameter and $p$ denote
                                                                                        variance retained after
                                                                                        dimensionality reduction. Given
                                                                                        efficient quantum access to the
                                                                                        matrix $A=U \Sigma V^T$ $=$
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$, let
                                                                                        the top $k$ right singular
                                                                                        vectors $\bar{V}^{(k)} \in
                                                                                        \mathbb{R}^{m \times k}$, where
                                                                                        $\left\|V^{(k)}-\bar{V}^{(k)}\right\|$
                                                                                        $\leq$ $\frac{\xi
                                                                                        \sqrt{p}}{\sqrt{2}}$. There
                                                                                        exists a quantum algorithm that
                                                                                        creates $|\bar{Y}\rangle$ $=$
                                                                                        $\frac{1}{\|Y\|_F}
                                                                                        \sum_i^n\left\|y_{i,
                                                                                        \cdot}\right\||i\rangle\left|y_{i,
                                                                                        \cdot}\right\rangle$
                                                                                        proportional to the projection
                                                                                        of $A$ in the PCA subspace.
                                                                                        Where the algorithmic
                                                                                        lower-bound probability $1-1 /
                                                                                        \operatorname{poly}(m)$ and with
                                                                                        error
                                                                                        $\||Y\rangle-|\bar{Y}\rangle \|$
                                                                                        $\leq$ $\xi$ with the time
                                                                                        complexity $\widetilde{O}(1 /
                                                                                        \sqrt{p})$. If, instead, we
                                                                                        estimate the value of
                                                                                        $\|\bar{Y}\|_F$ to a realtive
                                                                                        error of $\eta$ then the time
                                                                                        complexity becomes
                                                                                        $\widetilde{O}\left(\frac{1}{\sqrt{p}
                                                                                        \eta}\right)$ It is important to
                                                                                        note what data is
                                                                                        PCA-representable and
                                                                                        qPCA-representable. First, we
                                                                                        begin by defining
                                                                                        PCA-representable data. Let a
                                                                                        set of $n$ data points be
                                                                                        defined $m$ coordinates and
                                                                                        represented by the matrix $A$,
                                                                                        which was given earlier as
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$. Matrix
                                                                                        or data $A$ is PCA-representable
                                                                                        if there exists $p
                                                                                        \in\left[\frac{1}{2}, 1\right]$,
                                                                                        $\varepsilon \in[0,1 / 2]$,
                                                                                        $\beta \in[p-\varepsilon,
                                                                                        p+\varepsilon]$, and $\alpha
                                                                                        \in[0,1]$ such that: $\exists k
                                                                                        \in O(1)$ where the variance
                                                                                        retained after dimensionality
                                                                                        reduction $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$. For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.
                                                                                        Note, this allows for
                                                                                        algorithmic lower-bound
                                                                                        probability of $1-1 /
                                                                                        \operatorname{poly}(m)$ we saw
                                                                                        before. Let's consider
                                                                                        qPCA-representable case. Given
                                                                                        $a_{i}$ is a row of
                                                                                        $A\in\mathbb{R}^{n\times d}$,
                                                                                        the time complexity is
                                                                                        $\frac{\left\|a_i\right\|}{\left\|\bar{y}_i\right\|}$
                                                                                        $=$ $\frac{1}{\beta}$ $=$ $O(1)$
                                                                                        with the a probability greater
                                                                                        than $\alpha$. Now that we have
                                                                                        defined the parameters, data
                                                                                        assumptions, and goal for our
                                                                                        qPCA algorithm, let's walk
                                                                                        through the algorithm itself.
                                                                                        The abstract concept is that the
                                                                                        algorithm transforms the quantum
                                                                                        state $|\psi_{s}\rangle$ that
                                                                                        holds the original data in
                                                                                        quantum parallel to another
                                                                                        quantum state
                                                                                        $|\psi_{e}\rangle$, that will
                                                                                        store the new low-dimensional
                                                                                        data points: ${\displaystyle
                                                                                        |\psi_s\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes
                                                                                        \mathbf{x}_{i}}{||X||_{F}} }$
                                                                                        $\mapsto$ ${\displaystyle
                                                                                        |\psi_{e}\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes \mathbf{y}_{i}}{\|Y\|_F}
                                                                                        }$ such that, $\|X\|_F$ and
                                                                                        $\|Y\|_F$ are the Frobenius
                                                                                        norms of $X$ and $Y$. Let:
                                                                                        $\mathbf{x}_i=\sum_{j=1}^D x_{i
                                                                                        j}|j\rangle=||\mathbf{x}_i||\;|\mathbf{x}_{i}\rangle$
                                                                                        $\mathbf{y}_i=\sum_{j=1}^d y_{i
                                                                                        j}|j\rangle=||\mathbf{y}_i||\;|\mathbf{y}_i\rangle$
                                                                                        For the implementation of the
                                                                                        quantum algorithm, first
                                                                                        consider the data set
                                                                                        $\{\mathbf{x}_i\}_{i=1}^N$, or
                                                                                        matrix $X$, is stored in a
                                                                                        quantum random access memory
                                                                                        qRAM, such that qRAM takes
                                                                                        $|i\rangle|0\rangle|0\rangle
                                                                                        \rightarrow|i\rangle\left|a_i\right\rangle||
                                                                                        \vec{a}_i|\rangle$. Next, using
                                                                                        quantum access to the vectors
                                                                                        and norms, we constuct the state
                                                                                        $\sum_i\left|\vec{a}_i\right|\left|e_i\right\rangle\left|a_i\right\rangle$,
                                                                                        where the density matrix for the
                                                                                        first register is exactly $X$.
                                                                                        Now, we must create
                                                                                        $n=O\left(t^2
                                                                                        \epsilon^{-1}\right)$ copies of
                                                                                        $X$, in order to have an
                                                                                        accuracy $\epsilon$ in time
                                                                                        $O(n\log d)$ by completing
                                                                                        $e^{-i X t}$ implementations of
                                                                                        this process. An important
                                                                                        aspect to keep in mind about
                                                                                        this algorithm is that the
                                                                                        density matrix exponentiation is
                                                                                        most effective when some of the
                                                                                        eigenvalues are large. This is
                                                                                        because we are utilizing quantum
                                                                                        parallelism to amplify
                                                                                        deviations in the probability
                                                                                        amplitudes in order to reveal
                                                                                        eigenvectors corresponding to
                                                                                        the large eigenvalues of the
                                                                                        unknown state. If all
                                                                                        eigenvalues are of size
                                                                                        $O(1/d)$, then the time
                                                                                        complexity increases to $t=(d)$
                                                                                        to generate a transformation
                                                                                        such that it rotates the input
                                                                                        state $\sigma$ to an orthogonal
                                                                                        state. Extracting Quantum
                                                                                        Principal Components Let's
                                                                                        obtain the first $d$ principal
                                                                                        components in the quantum state
                                                                                        form $|v_{1}\rangle
                                                                                        ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue. Quantum
                                                                                        Principal Component
                                                                                        AnalysisQuantum Principal
                                                                                        Component Analysis identifies
                                                                                        large eigenvalues of unknown
                                                                                        density matrices utilizing
                                                                                        corresponding eigenvectors in
                                                                                        $O(\log d)$. Where principal
                                                                                        component analysis analyzes
                                                                                        positive semi-definite Hermitian
                                                                                        matrices by decomposing
                                                                                        eigenvectors in relation to the
                                                                                        largest eigenvalues in the
                                                                                        matrix for dimensionality
                                                                                        reduction. Improved
                                                                                        computational complexity will
                                                                                        hopefully allow new methods for
                                                                                        state discrimination and cluster
                                                                                        assignment for variational and
                                                                                        kernel classification machine
                                                                                        learning algorithms. Classical
                                                                                        Principal Component Analysis is
                                                                                        greatly used in signal
                                                                                        processing and machine learning
                                                                                        for dimension reduction with a
                                                                                        time complexity of $O(N^{3})$,
                                                                                        where $N$ is the dimension of
                                                                                        the data. The current issue with
                                                                                        this form of dimension reduction
                                                                                        is that when the data is large,
                                                                                        the classical PCA ends up
                                                                                        becoming non-tractable. A
                                                                                        possible solution for
                                                                                        dimensional reduction of large
                                                                                        data is utilizing Quantum
                                                                                        computing parallelism, where the
                                                                                        qPCA algorithm can then run with
                                                                                        a time complexity of $O(N
                                                                                        \operatorname{ploy}(\log N))$.
                                                                                        The outputs of qPCA are the
                                                                                        quantum states containing all
                                                                                        eigenvalues and eigenvectors of
                                                                                        the principal components of the
                                                                                        data for sampling. Consider a
                                                                                        dataset
                                                                                        $\left\{\mathbf{x}_i\right\}_{i=1}^N$
                                                                                        with $N$ data points where each
                                                                                        data point is a $D$-dimensional
                                                                                        column vector $\mathbf{x}_i=$
                                                                                        $\left(x_{i 1}, x_{i 2}, \cdots,
                                                                                        x_{i D}\right)^T \in
                                                                                        \mathbb{R}^D$. Through principal
                                                                                        component analysis we can lower
                                                                                        the dimensional space to a
                                                                                        $N\times D$ matrix
                                                                                        $X=\left(\mathbf{x}_1, \cdots,
                                                                                        \mathbf{x}_N\right)^T$ while
                                                                                        maximally preserving data
                                                                                        variance. Where an
                                                                                        eigendecomposition for a single
                                                                                        matrix $X$ is: $X=\sum_{j=1}^D
                                                                                        \sigma_j\left|\mathbf{u}_j\right\rangle\left\langle\mathbf{v}_j\right|$
                                                                                        such that $\left\{\sigma_j \in
                                                                                        \mathbb{R}_{\geq
                                                                                        0}\right\}_{j=1}^D$ are the
                                                                                        singular eigenvalues of the
                                                                                        matrix in descending order and
                                                                                        $\left\{\left|\mathbf{u}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^N\right\}_{j=1}^D$
                                                                                        and
                                                                                        $\left\{\left|\mathbf{v}_j\right\rangle
                                                                                        \in
                                                                                        \mathbb{R}^D\right\}_{j=1}^D$
                                                                                        are, respectively, the left and
                                                                                        right singular eigenvectors or
                                                                                        principal components. For the
                                                                                        rest of this section, we will
                                                                                        look at why we need qPCA, how we
                                                                                        can achieve it, and what is the
                                                                                        procedure for the underlying
                                                                                        algorithm. High-Dimensional
                                                                                        Quantum Data Reduction Algorithm
                                                                                        Here, we will analyze matrix
                                                                                        dimensionality reduction using
                                                                                        qPCA. Let $\xi$ be the precision
                                                                                        parameter and $p$ denote
                                                                                        variance retained after
                                                                                        dimensionality reduction. Given
                                                                                        efficient quantum access to the
                                                                                        matrix $A=U \Sigma V^T$ $=$
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$, let
                                                                                        the top $k$ right singular
                                                                                        vectors $\bar{V}^{(k)} \in
                                                                                        \mathbb{R}^{m \times k}$, where
                                                                                        $\left\|V^{(k)}-\bar{V}^{(k)}\right\|$
                                                                                        $\leq$ $\frac{\xi
                                                                                        \sqrt{p}}{\sqrt{2}}$. There
                                                                                        exists a quantum algorithm that
                                                                                        creates $|\bar{Y}\rangle$ $=$
                                                                                        $\frac{1}{\|Y\|_F}
                                                                                        \sum_i^n\left\|y_{i,
                                                                                        \cdot}\right\||i\rangle\left|y_{i,
                                                                                        \cdot}\right\rangle$
                                                                                        proportional to the projection
                                                                                        of $A$ in the PCA subspace.
                                                                                        Where the algorithmic
                                                                                        lower-bound probability $1-1 /
                                                                                        \operatorname{poly}(m)$ and with
                                                                                        error
                                                                                        $\||Y\rangle-|\bar{Y}\rangle \|$
                                                                                        $\leq$ $\xi$ with the time
                                                                                        complexity $\widetilde{O}(1 /
                                                                                        \sqrt{p})$. If, instead, we
                                                                                        estimate the value of
                                                                                        $\|\bar{Y}\|_F$ to a realtive
                                                                                        error of $\eta$ then the time
                                                                                        complexity becomes
                                                                                        $\widetilde{O}\left(\frac{1}{\sqrt{p}
                                                                                        \eta}\right)$ It is important to
                                                                                        note what data is
                                                                                        PCA-representable and
                                                                                        qPCA-representable. First, we
                                                                                        begin by defining
                                                                                        PCA-representable data. Let a
                                                                                        set of $n$ data points be
                                                                                        defined $m$ coordinates and
                                                                                        represented by the matrix $A$,
                                                                                        which was given earlier as
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$. Matrix
                                                                                        or data $A$ is PCA-representable
                                                                                        if there exists $p
                                                                                        \in\left[\frac{1}{2}, 1\right]$,
                                                                                        $\varepsilon \in[0,1 / 2]$,
                                                                                        $\beta \in[p-\varepsilon,
                                                                                        p+\varepsilon]$, and $\alpha
                                                                                        \in[0,1]$ such that: $\exists k
                                                                                        \in O(1)$ where the variance
                                                                                        retained after dimensionality
                                                                                        reduction $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$. For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.
                                                                                        Note, this allows for
                                                                                        algorithmic lower-bound
                                                                                        probability of $1-1 /
                                                                                        \operatorname{poly}(m)$ we saw
                                                                                        before. Let's consider
                                                                                        qPCA-representable case. Given
                                                                                        $a_{i}$ is a row of
                                                                                        $A\in\mathbb{R}^{n\times d}$,
                                                                                        the time complexity is
                                                                                        $\frac{\left\|a_i\right\|}{\left\|\bar{y}_i\right\|}$
                                                                                        $=$ $\frac{1}{\beta}$ $=$ $O(1)$
                                                                                        with the a probability greater
                                                                                        than $\alpha$. Now that we have
                                                                                        defined the parameters, data
                                                                                        assumptions, and goal for our
                                                                                        qPCA algorithm, let's walk
                                                                                        through the algorithm itself.
                                                                                        The abstract concept is that the
                                                                                        algorithm transforms the quantum
                                                                                        state $|\psi_{s}\rangle$ that
                                                                                        holds the original data in
                                                                                        quantum parallel to another
                                                                                        quantum state
                                                                                        $|\psi_{e}\rangle$, that will
                                                                                        store the new low-dimensional
                                                                                        data points: ${\displaystyle
                                                                                        |\psi_s\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes
                                                                                        \mathbf{x}_{i}}{||X||_{F}} }$
                                                                                        $\mapsto$ ${\displaystyle
                                                                                        |\psi_{e}\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes \mathbf{y}_{i}}{\|Y\|_F}
                                                                                        }$ such that, $\|X\|_F$ and
                                                                                        $\|Y\|_F$ are the Frobenius
                                                                                        norms of $X$ and $Y$. Let:
                                                                                        $\mathbf{x}_i=\sum_{j=1}^D x_{i
                                                                                        j}|j\rangle=||\mathbf{x}_i||\;|\mathbf{x}_{i}\rangle$
                                                                                        $\mathbf{y}_i=\sum_{j=1}^d y_{i
                                                                                        j}|j\rangle=||\mathbf{y}_i||\;|\mathbf{y}_i\rangle$
                                                                                        For the implementation of the
                                                                                        quantum algorithm, first
                                                                                        consider the data set
                                                                                        $\{\mathbf{x}_i\}_{i=1}^N$, or
                                                                                        matrix $X$, is stored in a
                                                                                        quantum random access memory
                                                                                        qRAM, such that qRAM takes
                                                                                        $|i\rangle|0\rangle|0\rangle
                                                                                        \rightarrow|i\rangle\left|a_i\right\rangle||
                                                                                        \vec{a}_i|\rangle$. Next, using
                                                                                        quantum access to the vectors
                                                                                        and norms, we constuct the state
                                                                                        $\sum_i\left|\vec{a}_i\right|\left|e_i\right\rangle\left|a_i\right\rangle$,
                                                                                        where the density matrix for the
                                                                                        first register is exactly $X$.
                                                                                        Now, we must create
                                                                                        $n=O\left(t^2
                                                                                        \epsilon^{-1}\right)$ copies of
                                                                                        $X$, in order to have an
                                                                                        accuracy $\epsilon$ in time
                                                                                        $O(n\log d)$ by completing
                                                                                        $e^{-i X t}$ implementations of
                                                                                        this process. An important
                                                                                        aspect to keep in mind about
                                                                                        this algorithm is that the
                                                                                        density matrix exponentiation is
                                                                                        most effective when some of the
                                                                                        eigenvalues are large. This is
                                                                                        because we are utilizing quantum
                                                                                        parallelism to amplify
                                                                                        deviations in the probability
                                                                                        amplitudes in order to reveal
                                                                                        eigenvectors corresponding to
                                                                                        the large eigenvalues of the
                                                                                        unknown state. If all
                                                                                        eigenvalues are of size
                                                                                        $O(1/d)$, then the time
                                                                                        complexity increases to $t=(d)$
                                                                                        to generate a transformation
                                                                                        such that it rotates the input
                                                                                        state $\sigma$ to an orthogonal
                                                                                        state. Extracting Quantum
                                                                                        Principal Components Let's
                                                                                        obtain the first $d$ principal
                                                                                        components in the quantum state
                                                                                        form $|v_{1}\rangle
                                                                                        ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue.
                                                                                        High-Dimensional Quantum Data
                                                                                        Reduction AlgorithmHere, we will
                                                                                        analyze matrix dimensionality
                                                                                        reduction using qPCA. Let $\xi$
                                                                                        be the precision parameter and
                                                                                        $p$ denote variance retained
                                                                                        after dimensionality reduction.
                                                                                        Given efficient quantum access
                                                                                        to the matrix $A=U \Sigma V^T$
                                                                                        $=$ $\sum_i^r \sigma_i u_i
                                                                                        v_i^T\in \mathbb{R}^{n \times
                                                                                        m}$, let the top $k$ right
                                                                                        singular vectors $\bar{V}^{(k)}
                                                                                        \in \mathbb{R}^{m \times k}$,
                                                                                        where
                                                                                        $\left\|V^{(k)}-\bar{V}^{(k)}\right\|$
                                                                                        $\leq$ $\frac{\xi
                                                                                        \sqrt{p}}{\sqrt{2}}$. There
                                                                                        exists a quantum algorithm that
                                                                                        creates $|\bar{Y}\rangle$ $=$
                                                                                        $\frac{1}{\|Y\|_F}
                                                                                        \sum_i^n\left\|y_{i,
                                                                                        \cdot}\right\||i\rangle\left|y_{i,
                                                                                        \cdot}\right\rangle$
                                                                                        proportional to the projection
                                                                                        of $A$ in the PCA subspace.
                                                                                        Where the algorithmic
                                                                                        lower-bound probability $1-1 /
                                                                                        \operatorname{poly}(m)$ and with
                                                                                        error
                                                                                        $\||Y\rangle-|\bar{Y}\rangle \|$
                                                                                        $\leq$ $\xi$ with the time
                                                                                        complexity $\widetilde{O}(1 /
                                                                                        \sqrt{p})$. If, instead, we
                                                                                        estimate the value of
                                                                                        $\|\bar{Y}\|_F$ to a realtive
                                                                                        error of $\eta$ then the time
                                                                                        complexity becomes
                                                                                        $\widetilde{O}\left(\frac{1}{\sqrt{p}
                                                                                        \eta}\right)$ It is important to
                                                                                        note what data is
                                                                                        PCA-representable and
                                                                                        qPCA-representable. First, we
                                                                                        begin by defining
                                                                                        PCA-representable data. Let a
                                                                                        set of $n$ data points be
                                                                                        defined $m$ coordinates and
                                                                                        represented by the matrix $A$,
                                                                                        which was given earlier as
                                                                                        $\sum_i^r \sigma_i u_i v_i^T\in
                                                                                        \mathbb{R}^{n \times m}$. Matrix
                                                                                        or data $A$ is PCA-representable
                                                                                        if there exists $p
                                                                                        \in\left[\frac{1}{2}, 1\right]$,
                                                                                        $\varepsilon \in[0,1 / 2]$,
                                                                                        $\beta \in[p-\varepsilon,
                                                                                        p+\varepsilon]$, and $\alpha
                                                                                        \in[0,1]$ such that: $\exists k
                                                                                        \in O(1)$ where the variance
                                                                                        retained after dimensionality
                                                                                        reduction $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$. For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.
                                                                                        $\exists k \in O(1)$ where the
                                                                                        variance retained after
                                                                                        dimensionality reduction
                                                                                        $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$. For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.
                                                                                        $\exists k \in O(1)$ where the
                                                                                        variance retained after
                                                                                        dimensionality reduction
                                                                                        $p=\frac{\sum_i^k
                                                                                        \sigma_i^2}{\sum_i^m
                                                                                        \sigma_i^2}$.For at least
                                                                                        $\alpha n$ points $a_{i}$, it is
                                                                                        maintained that
                                                                                        $\frac{\left\|y_i\right\|}{\left\|a_i\right\|}
                                                                                        \geq \beta$, where
                                                                                        $\left\|y_i\right\|$ $=$
                                                                                        $\sqrt{\sum_i^k\left|\left\langle
                                                                                        a_i \mid
                                                                                        v_j\right\rangle\right|^2}\left\|a_i\right\|$.Note,
                                                                                        this allows for algorithmic
                                                                                        lower-bound probability of $1-1
                                                                                        / \operatorname{poly}(m)$ we saw
                                                                                        before. Let's consider
                                                                                        qPCA-representable case. Given
                                                                                        $a_{i}$ is a row of
                                                                                        $A\in\mathbb{R}^{n\times d}$,
                                                                                        the time complexity is
                                                                                        $\frac{\left\|a_i\right\|}{\left\|\bar{y}_i\right\|}$
                                                                                        $=$ $\frac{1}{\beta}$ $=$ $O(1)$
                                                                                        with the a probability greater
                                                                                        than $\alpha$. Now that we have
                                                                                        defined the parameters, data
                                                                                        assumptions, and goal for our
                                                                                        qPCA algorithm, let's walk
                                                                                        through the algorithm itself.
                                                                                        The abstract concept is that the
                                                                                        algorithm transforms the quantum
                                                                                        state $|\psi_{s}\rangle$ that
                                                                                        holds the original data in
                                                                                        quantum parallel to another
                                                                                        quantum state
                                                                                        $|\psi_{e}\rangle$, that will
                                                                                        store the new low-dimensional
                                                                                        data points: ${\displaystyle
                                                                                        |\psi_s\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes
                                                                                        \mathbf{x}_{i}}{||X||_{F}} }$
                                                                                        $\mapsto$ ${\displaystyle
                                                                                        |\psi_{e}\rangle:=\frac{\sum_{i=1}^{N}|i\rangle
                                                                                        \otimes \mathbf{y}_{i}}{\|Y\|_F}
                                                                                        }$ such that, $\|X\|_F$ and
                                                                                        $\|Y\|_F$ are the Frobenius
                                                                                        norms of $X$ and $Y$. Let:
                                                                                        $\mathbf{x}_i=\sum_{j=1}^D x_{i
                                                                                        j}|j\rangle=||\mathbf{x}_i||\;|\mathbf{x}_{i}\rangle$
                                                                                        $\mathbf{y}_i=\sum_{j=1}^d y_{i
                                                                                        j}|j\rangle=||\mathbf{y}_i||\;|\mathbf{y}_i\rangle$
                                                                                        For the implementation of the
                                                                                        quantum algorithm, first
                                                                                        consider the data set
                                                                                        $\{\mathbf{x}_i\}_{i=1}^N$, or
                                                                                        matrix $X$, is stored in a
                                                                                        quantum random access memory
                                                                                        qRAM, such that qRAM takes
                                                                                        $|i\rangle|0\rangle|0\rangle
                                                                                        \rightarrow|i\rangle\left|a_i\right\rangle||
                                                                                        \vec{a}_i|\rangle$. Next, using
                                                                                        quantum access to the vectors
                                                                                        and norms, we constuct the state
                                                                                        $\sum_i\left|\vec{a}_i\right|\left|e_i\right\rangle\left|a_i\right\rangle$,
                                                                                        where the density matrix for the
                                                                                        first register is exactly $X$.
                                                                                        Now, we must create
                                                                                        $n=O\left(t^2
                                                                                        \epsilon^{-1}\right)$ copies of
                                                                                        $X$, in order to have an
                                                                                        accuracy $\epsilon$ in time
                                                                                        $O(n\log d)$ by completing
                                                                                        $e^{-i X t}$ implementations of
                                                                                        this process. An important
                                                                                        aspect to keep in mind about
                                                                                        this algorithm is that the
                                                                                        density matrix exponentiation is
                                                                                        most effective when some of the
                                                                                        eigenvalues are large. This is
                                                                                        because we are utilizing quantum
                                                                                        parallelism to amplify
                                                                                        deviations in the probability
                                                                                        amplitudes in order to reveal
                                                                                        eigenvectors corresponding to
                                                                                        the large eigenvalues of the
                                                                                        unknown state. If all
                                                                                        eigenvalues are of size
                                                                                        $O(1/d)$, then the time
                                                                                        complexity increases to $t=(d)$
                                                                                        to generate a transformation
                                                                                        such that it rotates the input
                                                                                        state $\sigma$ to an orthogonal
                                                                                        state. Extracting Quantum
                                                                                        Principal Components Let's
                                                                                        obtain the first $d$ principal
                                                                                        components in the quantum state
                                                                                        form $|v_{1}\rangle
                                                                                        ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue.
                                                                                        $\mathbf{y}_i=\sum_{j=1}^d y_{i
                                                                                        j}|j\rangle=||\mathbf{y}_i||\;|\mathbf{y}_i\rangle$
                                                                                        For the implementation of the
                                                                                        quantum algorithm, first
                                                                                        consider the data set
                                                                                        $\{\mathbf{x}_i\}_{i=1}^N$, or
                                                                                        matrix $X$, is stored in a
                                                                                        quantum random access memory
                                                                                        qRAM, such that qRAM takes
                                                                                        $|i\rangle|0\rangle|0\rangle
                                                                                        \rightarrow|i\rangle\left|a_i\right\rangle||
                                                                                        \vec{a}_i|\rangle$. Next, using
                                                                                        quantum access to the vectors
                                                                                        and norms, we constuct the state
                                                                                        $\sum_i\left|\vec{a}_i\right|\left|e_i\right\rangle\left|a_i\right\rangle$,
                                                                                        where the density matrix for the
                                                                                        first register is exactly $X$.
                                                                                        Now, we must create
                                                                                        $n=O\left(t^2
                                                                                        \epsilon^{-1}\right)$ copies of
                                                                                        $X$, in order to have an
                                                                                        accuracy $\epsilon$ in time
                                                                                        $O(n\log d)$ by completing
                                                                                        $e^{-i X t}$ implementations of
                                                                                        this process. An important
                                                                                        aspect to keep in mind about
                                                                                        this algorithm is that the
                                                                                        density matrix exponentiation is
                                                                                        most effective when some of the
                                                                                        eigenvalues are large. This is
                                                                                        because we are utilizing quantum
                                                                                        parallelism to amplify
                                                                                        deviations in the probability
                                                                                        amplitudes in order to reveal
                                                                                        eigenvectors corresponding to
                                                                                        the large eigenvalues of the
                                                                                        unknown state. If all
                                                                                        eigenvalues are of size
                                                                                        $O(1/d)$, then the time
                                                                                        complexity increases to $t=(d)$
                                                                                        to generate a transformation
                                                                                        such that it rotates the input
                                                                                        state $\sigma$ to an orthogonal
                                                                                        state. Extracting Quantum
                                                                                        Principal Components Let's
                                                                                        obtain the first $d$ principal
                                                                                        components in the quantum state
                                                                                        form $|v_{1}\rangle
                                                                                        ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue.
                                                                                        Extracting Quantum Principal
                                                                                        ComponentsLet's obtain the first
                                                                                        $d$ principal components in the
                                                                                        quantum state form
                                                                                        $|v_{1}\rangle ,|v_{2}\rangle
                                                                                        $$...|v_{d}\rangle $, where the
                                                                                        variance proportion is:
                                                                                        ${\displaystyle
                                                                                        \lambda_j:=\frac{\sigma_{j}^{2}}{\sum_{j=1}^{D}
                                                                                        \sigma_{j}^{2}},}$ $j=1,2,
                                                                                        \cdots, D$ From the singular
                                                                                        value decomposition form of
                                                                                        matrix $X$, we can rewrite
                                                                                        $|\psi_{s}\rangle$ as:
                                                                                        $|\psi_{s}\rangle
                                                                                        =\sum_{j=1}^{D}
                                                                                        \sqrt{\lambda_{j}}
                                                                                        |\mathbf{u}_{j}\rangle
                                                                                        |\mathbf{v}_{j}\rangle $ The
                                                                                        state of the second quantum
                                                                                        register in QRAM is
                                                                                        $\rho=\operatorname{Tr}_{1}
                                                                                        \left(|\psi_{s}\rangle \right)$$
                                                                                        =\sum_{j=1}^{D}
                                                                                        \lambda_{j}|\mathbf{v}_{j}\rangle
                                                                                        \langle\mathbf{v}_{j}|$,
                                                                                        equivalent to $X^T X /
                                                                                        \operatorname{Tr}\left (X^T
                                                                                        X\right )$. To continue.
                                                                                        Efficient Discrete Feature
                                                                                        Encoding To continue. Efficient
                                                                                        Discrete Feature EncodingTo
                                                                                        continue.
                                                                                        [https://www.contextswitching.org/tcs]
                                                                                        Computer Science - Context
                                                                                        Switching Computer Science
                                                                                        Quantum Convolutional Neural
                                                                                        Network The input for the
                                                                                        quantum convolution layer is a
                                                                                        $3$D tensor input given as
                                                                                        $X^{\ell} \in
                                                                                        \mathbb{R}^{H^{\ell} \times
                                                                                        W^{\ell} \times D^{\ell}}$. The
                                                                                        weights layer, filter layer, or
                                                                                        $4$D tensor kernel layer is
                                                                                        denoted as $K^{\ell} \in
                                                                                        \mathbb{R}^{H \times W \times
                                                                                        D^{\ell} \times D^{\ell+1}}$.
                                                                                        The input and kernel layer are
                                                                                        both stored in QRAM. There are
                                                                                        precision parameters $\epsilon$
                                                                                        and $\Delta>0$... read more
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping Quantum Principal
                                                                                        Component Analysis identifies
                                                                                        large eigenvalues of unknown
                                                                                        density matrices utilizing
                                                                                        corresponding eigenvectors in
                                                                                        $O(\log d)$. Where principal
                                                                                        component analysis analyzes
                                                                                        positive semi-definite Hermitian
                                                                                        matrices by decomposing
                                                                                        eigenvectors in relation to the
                                                                                        largest eigenvalues in the
                                                                                        matrix for dimensionality
                                                                                        reduction. Improved
                                                                                        computational complexity will
                                                                                        hopefully allow new methods
                                                                                        for... read more Quantum Support
                                                                                        Vector Machine Using quantum
                                                                                        computing, the authors exploit
                                                                                        quantum mechanics for the
                                                                                        algorithmic complexity
                                                                                        optimization of a Support Vector
                                                                                        Machine with high-dimensional
                                                                                        feature space. Where the
                                                                                        high-dimensional classical data
                                                                                        is mapped non-linearly to
                                                                                        Hilbert Space and a hyperplane
                                                                                        in quantum space is used to
                                                                                        separate and label the data. By
                                                                                        using the... read more
                                                                                        Multilevel Development of
                                                                                        Cognitive Abilities for
                                                                                        Artificial Intelligence In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more Information
                                                                                        Theory Three properties were
                                                                                        required by Shannon: $I(p) \geq
                                                                                        0$, i.e. information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more Quantum
                                                                                        Computing Theory Quantum
                                                                                        Computing Theory is a field of
                                                                                        computer science that uses the
                                                                                        principles of quantum mechanics,
                                                                                        mathematics, and computer
                                                                                        science. By borrowing concepts
                                                                                        from each field scientists can
                                                                                        rigorously define both a broad
                                                                                        and narrow theoretical model of
                                                                                        a quantum computer and later
                                                                                        apply it to the real world.
                                                                                        These... read more Graph Theory
                                                                                        $G = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        Theory of Computation A
                                                                                        deterministic finite automaton
                                                                                        (DFA) is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more Algorithmic
                                                                                        Anaylsis Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more ... And More Soon! Computer
                                                                                        Science - Context Switching
                                                                                        Computer Science - Context
                                                                                        SwitchingComputer Science
                                                                                        Quantum Convolutional Neural
                                                                                        Network The input for the
                                                                                        quantum convolution layer is a
                                                                                        $3$D tensor input given as
                                                                                        $X^{\ell} \in
                                                                                        \mathbb{R}^{H^{\ell} \times
                                                                                        W^{\ell} \times D^{\ell}}$. The
                                                                                        weights layer, filter layer, or
                                                                                        $4$D tensor kernel layer is
                                                                                        denoted as $K^{\ell} \in
                                                                                        \mathbb{R}^{H \times W \times
                                                                                        D^{\ell} \times D^{\ell+1}}$.
                                                                                        The input and kernel layer are
                                                                                        both stored in QRAM. There are
                                                                                        precision parameters $\epsilon$
                                                                                        and $\Delta>0$... read more
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping Quantum Principal
                                                                                        Component Analysis identifies
                                                                                        large eigenvalues of unknown
                                                                                        density matrices utilizing
                                                                                        corresponding eigenvectors in
                                                                                        $O(\log d)$. Where principal
                                                                                        component analysis analyzes
                                                                                        positive semi-definite Hermitian
                                                                                        matrices by decomposing
                                                                                        eigenvectors in relation to the
                                                                                        largest eigenvalues in the
                                                                                        matrix for dimensionality
                                                                                        reduction. Improved
                                                                                        computational complexity will
                                                                                        hopefully allow new methods
                                                                                        for... read more Quantum Support
                                                                                        Vector Machine Using quantum
                                                                                        computing, the authors exploit
                                                                                        quantum mechanics for the
                                                                                        algorithmic complexity
                                                                                        optimization of a Support Vector
                                                                                        Machine with high-dimensional
                                                                                        feature space. Where the
                                                                                        high-dimensional classical data
                                                                                        is mapped non-linearly to
                                                                                        Hilbert Space and a hyperplane
                                                                                        in quantum space is used to
                                                                                        separate and label the data. By
                                                                                        using the... read more
                                                                                        Multilevel Development of
                                                                                        Cognitive Abilities for
                                                                                        Artificial Intelligence In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more Information
                                                                                        Theory Three properties were
                                                                                        required by Shannon: $I(p) \geq
                                                                                        0$, i.e. information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more Quantum
                                                                                        Computing Theory Quantum
                                                                                        Computing Theory is a field of
                                                                                        computer science that uses the
                                                                                        principles of quantum mechanics,
                                                                                        mathematics, and computer
                                                                                        science. By borrowing concepts
                                                                                        from each field scientists can
                                                                                        rigorously define both a broad
                                                                                        and narrow theoretical model of
                                                                                        a quantum computer and later
                                                                                        apply it to the real world.
                                                                                        These... read more Graph Theory
                                                                                        $G = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        Theory of Computation A
                                                                                        deterministic finite automaton
                                                                                        (DFA) is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more Algorithmic
                                                                                        Anaylsis Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more ... And More Soon! Computer
                                                                                        Science Computer Science
                                                                                        Computer Science Quantum
                                                                                        Convolutional Neural Network
                                                                                        Quantum Convolutional Neural
                                                                                        Network Quantum Convolutional
                                                                                        Neural Network The input for the
                                                                                        quantum convolution layer is a
                                                                                        $3$D tensor input given as
                                                                                        $X^{\ell} \in
                                                                                        \mathbb{R}^{H^{\ell} \times
                                                                                        W^{\ell} \times D^{\ell}}$. The
                                                                                        weights layer, filter layer, or
                                                                                        $4$D tensor kernel layer is
                                                                                        denoted as $K^{\ell} \in
                                                                                        \mathbb{R}^{H \times W \times
                                                                                        D^{\ell} \times D^{\ell+1}}$.
                                                                                        The input and kernel layer are
                                                                                        both stored in QRAM. There are
                                                                                        precision parameters $\epsilon$
                                                                                        and $\Delta>0$... read more The
                                                                                        input for the quantum
                                                                                        convolution layer is a $3$D
                                                                                        tensor input given as $X^{\ell}
                                                                                        \in \mathbb{R}^{H^{\ell} \times
                                                                                        W^{\ell} \times D^{\ell}}$. The
                                                                                        weights layer, filter layer, or
                                                                                        $4$D tensor kernel layer is
                                                                                        denoted as $K^{\ell} \in
                                                                                        \mathbb{R}^{H \times W \times
                                                                                        D^{\ell} \times D^{\ell+1}}$.
                                                                                        The input and kernel layer are
                                                                                        both stored in QRAM. There are
                                                                                        precision parameters $\epsilon$
                                                                                        and $\Delta>0$... read more read
                                                                                        more High-Dimensional Quantum
                                                                                        Feature Mapping High-Dimensional
                                                                                        Quantum Feature Mapping
                                                                                        High-Dimensional Quantum Feature
                                                                                        Mapping Quantum Principal
                                                                                        Component Analysis identifies
                                                                                        large eigenvalues of unknown
                                                                                        density matrices utilizing
                                                                                        corresponding eigenvectors in
                                                                                        $O(\log d)$. Where principal
                                                                                        component analysis analyzes
                                                                                        positive semi-definite Hermitian
                                                                                        matrices by decomposing
                                                                                        eigenvectors in relation to the
                                                                                        largest eigenvalues in the
                                                                                        matrix for dimensionality
                                                                                        reduction. Improved
                                                                                        computational complexity will
                                                                                        hopefully allow new methods
                                                                                        for... read more Quantum
                                                                                        Principal Component Analysis
                                                                                        identifies large eigenvalues of
                                                                                        unknown density matrices
                                                                                        utilizing corresponding
                                                                                        eigenvectors in $O(\log d)$.
                                                                                        Where principal component
                                                                                        analysis analyzes positive
                                                                                        semi-definite Hermitian matrices
                                                                                        by decomposing eigenvectors in
                                                                                        relation to the largest
                                                                                        eigenvalues in the matrix for
                                                                                        dimensionality reduction.
                                                                                        Improved computational
                                                                                        complexity will hopefully allow
                                                                                        new methods for... read more
                                                                                        read more Quantum Support Vector
                                                                                        Machine Quantum Support Vector
                                                                                        Machine Quantum Support Vector
                                                                                        Machine Using quantum computing,
                                                                                        the authors exploit quantum
                                                                                        mechanics for the algorithmic
                                                                                        complexity optimization of a
                                                                                        Support Vector Machine with
                                                                                        high-dimensional feature space.
                                                                                        Where the high-dimensional
                                                                                        classical data is mapped
                                                                                        non-linearly to Hilbert Space
                                                                                        and a hyperplane in quantum
                                                                                        space is used to separate and
                                                                                        label the data. By using the...
                                                                                        read more Using quantum
                                                                                        computing, the authors exploit
                                                                                        quantum mechanics for the
                                                                                        algorithmic complexity
                                                                                        optimization of a Support Vector
                                                                                        Machine with high-dimensional
                                                                                        feature space. Where the
                                                                                        high-dimensional classical data
                                                                                        is mapped non-linearly to
                                                                                        Hilbert Space and a hyperplane
                                                                                        in quantum space is used to
                                                                                        separate and label the data. By
                                                                                        using the... read more read more
                                                                                        Multilevel Development of
                                                                                        Cognitive Abilities for
                                                                                        Artificial Intelligence
                                                                                        Multilevel Development of
                                                                                        Cognitive Abilities for
                                                                                        Artificial Intelligence
                                                                                        Multilevel Development of
                                                                                        Cognitive Abilities for
                                                                                        Artificial Intelligence In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more In
                                                                                        biological intelliegent systems
                                                                                        there are multiple mechanisms
                                                                                        working in congruence on
                                                                                        multiple levels, both at the
                                                                                        structural and neurobiological
                                                                                        level to develop complex
                                                                                        cognitive abilities. What
                                                                                        remains unknown is which
                                                                                        mechanisms are necessary and
                                                                                        sufficent to synthetically
                                                                                        replicate these cognitive
                                                                                        abilities for artificial
                                                                                        intelligence. A
                                                                                        neurocomputational model is
                                                                                        offered... read more read more
                                                                                        Information Theory Information
                                                                                        Theory Information Theory Three
                                                                                        properties were required by
                                                                                        Shannon: $I(p) \geq 0$, i.e.
                                                                                        information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more Three
                                                                                        properties were required by
                                                                                        Shannon: $I(p) \geq 0$, i.e.
                                                                                        information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more Three
                                                                                        properties were required by
                                                                                        Shannon: $I(p) \geq 0$, i.e.
                                                                                        information is a real
                                                                                        non-negative measure.
                                                                                        $I(p_{1},p_{2})=I(p_{1})+I(p_{2})$
                                                                                        for independent events. $I(p)$
                                                                                        is a continous function of $p$.
                                                                                        The mathematical function that
                                                                                        satisfies these requirements is:
                                                                                        $I(p)=k\;log(p)$ In the
                                                                                        equation, the value of $k$ is
                                                                                        arbitrary... read more read more
                                                                                        Quantum Computing Theory Quantum
                                                                                        Computing Theory Quantum
                                                                                        Computing Theory Quantum
                                                                                        Computing Theory is a field of
                                                                                        computer science that uses the
                                                                                        principles of quantum mechanics,
                                                                                        mathematics, and computer
                                                                                        science. By borrowing concepts
                                                                                        from each field scientists can
                                                                                        rigorously define both a broad
                                                                                        and narrow theoretical model of
                                                                                        a quantum computer and later
                                                                                        apply it to the real world.
                                                                                        These... read more Graph Theory
                                                                                        $G = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        Theory of Computation A
                                                                                        deterministic finite automaton
                                                                                        (DFA) is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more Algorithmic
                                                                                        Anaylsis Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more ... And More Soon! Quantum
                                                                                        Computing Theory is a field of
                                                                                        computer science that uses the
                                                                                        principles of quantum mechanics,
                                                                                        mathematics, and computer
                                                                                        science. By borrowing concepts
                                                                                        from each field scientists can
                                                                                        rigorously define both a broad
                                                                                        and narrow theoretical model of
                                                                                        a quantum computer and later
                                                                                        apply it to the real world.
                                                                                        These... read more Quantum
                                                                                        Computing Theory is a field of
                                                                                        computer science that uses the
                                                                                        principles of quantum mechanics,
                                                                                        mathematics, and computer
                                                                                        science. By borrowing concepts
                                                                                        from each field scientists can
                                                                                        rigorously define both a broad
                                                                                        and narrow theoretical model of
                                                                                        a quantum computer and later
                                                                                        apply it to the real world.
                                                                                        These... read more read more
                                                                                        Graph Theory Graph Theory Graph
                                                                                        Theory $G = (V, E)$ $V$ is a set
                                                                                        of vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more $G
                                                                                        = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more $G
                                                                                        = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        read more Theory of Computation
                                                                                        Theory of Computation Theory of
                                                                                        Computation A deterministic
                                                                                        finite automaton (DFA) is a
                                                                                        5-tuple: $(Q, \Sigma, \delta,
                                                                                        q_{0}, F)$ where: $Q$ is a
                                                                                        finite set of states $\Sigma$ is
                                                                                        an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more A
                                                                                        deterministic finite automaton
                                                                                        (DFA) is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more A
                                                                                        deterministic finite automaton
                                                                                        (DFA) is a 5-tuple: $(Q, \Sigma,
                                                                                        \delta, q_{0}, F)$ where: $Q$ is
                                                                                        a finite set of states $\Sigma$
                                                                                        is an alphabet $\delta$ is a
                                                                                        transition function described as
                                                                                        $\delta : Q \times \Sigma
                                                                                        \rightarrow Q$ $q_{0} \in Q$ is
                                                                                        the initial state $F \subseteq
                                                                                        Q$ is a... read more read more
                                                                                        Algorithmic Anaylsis Algorithmic
                                                                                        Anaylsis Algorithmic Anaylsis
                                                                                        Algorithmic analysis is used to
                                                                                        help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more Algorithmic analysis is
                                                                                        used to help computer scientists
                                                                                        understand the resources
                                                                                        required by an algorithm for
                                                                                        time, storage, and other uses.
                                                                                        Algorithmic anlysis must analyze
                                                                                        algorithms in a methodical,
                                                                                        universal, and fair way. To do
                                                                                        this computer scientist
                                                                                        implement mathematical models
                                                                                        that describe the resources used
                                                                                        by algorithms. This work... read
                                                                                        more read more ... And More
                                                                                        Soon! ... And More Soon! ... And
                                                                                        More Soon!
                                                                                        [https://www.contextswitching.org/neuro/anatomyhistologyhippocampus]
                                                                                        Hippocampus: An Overview -
                                                                                        Context Switching Anatomy and
                                                                                        Histology of the Hippocampus
                                                                                        Section Reference: Cliques and
                                                                                        cavities in the human connectome
                                                                                        by Ann E. Sizemore, Chad Giusti,
                                                                                        Ari Kahn, Jean M. Vettel,
                                                                                        Richard F. Betzel, and Danielle
                                                                                        S. Bassett $^{[1]}$ Introduction
                                                                                        The hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that separates the hippocampus
                                                                                        from the neocortex. The
                                                                                        hippocampal set is the entire
                                                                                        set of hippocampal formation.
                                                                                        This formation is comprised by
                                                                                        some anatomists into four main
                                                                                        sections: Hippocampus proper,
                                                                                        dentate gyrus DG, subiculum, and
                                                                                        the entorhinal area EC. This
                                                                                        entire set of hippocampus zones
                                                                                        is called the hippocampal
                                                                                        formation. The hippocampus is
                                                                                        supplied blood by the posterior
                                                                                        Cerebral artery and is covered
                                                                                        by the choroid plexus.
                                                                                        Furthermore, the hippocampus
                                                                                        lies posterior to the limbic
                                                                                        system, while the anterior is
                                                                                        the amygdala. In adult humans,
                                                                                        the volume of the hippocampus in
                                                                                        each hemisphere is $\sim 3-3.5
                                                                                        \mathrm{~cm}^3$, where, relative
                                                                                        to the volume of the neocortex
                                                                                        $\sim 320-420 \mathrm{~cm}^3$ is
                                                                                        $100$ times smaller compared to
                                                                                        the cerebral cortex. The general
                                                                                        function of the hippocampus is
                                                                                        organizing emotional responses,
                                                                                        which are expressed in the
                                                                                        cingulate gyrus via mammillary
                                                                                        bodies. Further functions
                                                                                        included recollecting experience
                                                                                        and imagining the future,
                                                                                        playing a role in learning,
                                                                                        memory, and spatial navigation.
                                                                                        Cornu Ammonis Image Source:
                                                                                        Neurosurgery written board crash
                                                                                        course - Hippocampus $^{[2]}$
                                                                                        Anatomy and Histology Histology
                                                                                        tells us that the hippocampus
                                                                                        proper or Cornus Ammonis is
                                                                                        divided into four histological
                                                                                        domains: CA1, CA2, CA3, and CA4.
                                                                                        Across from the CA1 is the
                                                                                        subiculum, which is a formation
                                                                                        connecting the hippocampus to
                                                                                        the entorhinal cortex in the
                                                                                        ventricle. Where the entorhinal
                                                                                        cortex inputs to the dentata
                                                                                        gyrus and plays an important
                                                                                        role in pattern recognition and
                                                                                        encoding of memories. CA1
                                                                                        contains pyrimidal cells which
                                                                                        play a role in matching and
                                                                                        mismatching incoming information
                                                                                        from CA3. Image Source: Ammon's
                                                                                        Horn 2 (CA2) of the Hippocampus:
                                                                                        A Long-Known Region with a New
                                                                                        Potential Role in
                                                                                        Neurodegeneration $^{[3]}$
                                                                                        Dentate Gyrus To continue.
                                                                                        Glossary Limbic System The
                                                                                        limbic system is a part of the
                                                                                        primitive brain and its primary
                                                                                        functions concern hunger,
                                                                                        motivation, sex drive, mood,
                                                                                        pain, pleasure, appetite, and
                                                                                        memory. Pyramidal Neurons The
                                                                                        pyramidal neuron, named after
                                                                                        the conic-shaped soma, is a
                                                                                        multipolar neuron and is the
                                                                                        primary excitation unit of the
                                                                                        mammalian prefrontal cortex and
                                                                                        corticospinal tract. These
                                                                                        neurons are also found in the
                                                                                        hippocampus and amygdala.
                                                                                        Studies on pyramidal neurons
                                                                                        mainly include neuroplasticity
                                                                                        and cognition studies.
                                                                                        Hippocampus: An Overview -
                                                                                        Context Switching Hippocampus:
                                                                                        An Overview - Context
                                                                                        SwitchingAnatomy and Histology
                                                                                        of the Hippocampus Section
                                                                                        Reference: Cliques and cavities
                                                                                        in the human connectome by Ann
                                                                                        E. Sizemore, Chad Giusti, Ari
                                                                                        Kahn, Jean M. Vettel, Richard F.
                                                                                        Betzel, and Danielle S. Bassett
                                                                                        $^{[1]}$ Introduction The
                                                                                        hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that separates the hippocampus
                                                                                        from the neocortex. The
                                                                                        hippocampal set is the entire
                                                                                        set of hippocampal formation.
                                                                                        This formation is comprised by
                                                                                        some anatomists into four main
                                                                                        sections: Hippocampus proper,
                                                                                        dentate gyrus DG, subiculum, and
                                                                                        the entorhinal area EC. This
                                                                                        entire set of hippocampus zones
                                                                                        is called the hippocampal
                                                                                        formation. The hippocampus is
                                                                                        supplied blood by the posterior
                                                                                        Cerebral artery and is covered
                                                                                        by the choroid plexus.
                                                                                        Furthermore, the hippocampus
                                                                                        lies posterior to the limbic
                                                                                        system, while the anterior is
                                                                                        the amygdala. In adult humans,
                                                                                        the volume of the hippocampus in
                                                                                        each hemisphere is $\sim 3-3.5
                                                                                        \mathrm{~cm}^3$, where, relative
                                                                                        to the volume of the neocortex
                                                                                        $\sim 320-420 \mathrm{~cm}^3$ is
                                                                                        $100$ times smaller compared to
                                                                                        the cerebral cortex. The general
                                                                                        function of the hippocampus is
                                                                                        organizing emotional responses,
                                                                                        which are expressed in the
                                                                                        cingulate gyrus via mammillary
                                                                                        bodies. Further functions
                                                                                        included recollecting experience
                                                                                        and imagining the future,
                                                                                        playing a role in learning,
                                                                                        memory, and spatial navigation.
                                                                                        Cornu Ammonis Image Source:
                                                                                        Neurosurgery written board crash
                                                                                        course - Hippocampus $^{[2]}$
                                                                                        Anatomy and Histology Histology
                                                                                        tells us that the hippocampus
                                                                                        proper or Cornus Ammonis is
                                                                                        divided into four histological
                                                                                        domains: CA1, CA2, CA3, and CA4.
                                                                                        Across from the CA1 is the
                                                                                        subiculum, which is a formation
                                                                                        connecting the hippocampus to
                                                                                        the entorhinal cortex in the
                                                                                        ventricle. Where the entorhinal
                                                                                        cortex inputs to the dentata
                                                                                        gyrus and plays an important
                                                                                        role in pattern recognition and
                                                                                        encoding of memories. CA1
                                                                                        contains pyrimidal cells which
                                                                                        play a role in matching and
                                                                                        mismatching incoming information
                                                                                        from CA3. Image Source: Ammon's
                                                                                        Horn 2 (CA2) of the Hippocampus:
                                                                                        A Long-Known Region with a New
                                                                                        Potential Role in
                                                                                        Neurodegeneration $^{[3]}$
                                                                                        Dentate Gyrus To continue.
                                                                                        Glossary Limbic System The
                                                                                        limbic system is a part of the
                                                                                        primitive brain and its primary
                                                                                        functions concern hunger,
                                                                                        motivation, sex drive, mood,
                                                                                        pain, pleasure, appetite, and
                                                                                        memory. Pyramidal Neurons The
                                                                                        pyramidal neuron, named after
                                                                                        the conic-shaped soma, is a
                                                                                        multipolar neuron and is the
                                                                                        primary excitation unit of the
                                                                                        mammalian prefrontal cortex and
                                                                                        corticospinal tract. These
                                                                                        neurons are also found in the
                                                                                        hippocampus and amygdala.
                                                                                        Studies on pyramidal neurons
                                                                                        mainly include neuroplasticity
                                                                                        and cognition studies. Anatomy
                                                                                        and Histology of the Hippocampus
                                                                                        Section Reference: Cliques and
                                                                                        cavities in the human connectome
                                                                                        by Ann E. Sizemore, Chad Giusti,
                                                                                        Ari Kahn, Jean M. Vettel,
                                                                                        Richard F. Betzel, and Danielle
                                                                                        S. Bassett $^{[1]}$ Introduction
                                                                                        The hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that separates the hippocampus
                                                                                        from the neocortex. The
                                                                                        hippocampal set is the entire
                                                                                        set of hippocampal formation.
                                                                                        This formation is comprised by
                                                                                        some anatomists into four main
                                                                                        sections: Hippocampus proper,
                                                                                        dentate gyrus DG, subiculum, and
                                                                                        the entorhinal area EC. This
                                                                                        entire set of hippocampus zones
                                                                                        is called the hippocampal
                                                                                        formation. The hippocampus is
                                                                                        supplied blood by the posterior
                                                                                        Cerebral artery and is covered
                                                                                        by the choroid plexus.
                                                                                        Furthermore, the hippocampus
                                                                                        lies posterior to the limbic
                                                                                        system, while the anterior is
                                                                                        the amygdala. In adult humans,
                                                                                        the volume of the hippocampus in
                                                                                        each hemisphere is $\sim 3-3.5
                                                                                        \mathrm{~cm}^3$, where, relative
                                                                                        to the volume of the neocortex
                                                                                        $\sim 320-420 \mathrm{~cm}^3$ is
                                                                                        $100$ times smaller compared to
                                                                                        the cerebral cortex. The general
                                                                                        function of the hippocampus is
                                                                                        organizing emotional responses,
                                                                                        which are expressed in the
                                                                                        cingulate gyrus via mammillary
                                                                                        bodies. Further functions
                                                                                        included recollecting experience
                                                                                        and imagining the future,
                                                                                        playing a role in learning,
                                                                                        memory, and spatial navigation.
                                                                                        Cornu Ammonis Image Source:
                                                                                        Neurosurgery written board crash
                                                                                        course - Hippocampus $^{[2]}$
                                                                                        Anatomy and Histology Histology
                                                                                        tells us that the hippocampus
                                                                                        proper or Cornus Ammonis is
                                                                                        divided into four histological
                                                                                        domains: CA1, CA2, CA3, and CA4.
                                                                                        Across from the CA1 is the
                                                                                        subiculum, which is a formation
                                                                                        connecting the hippocampus to
                                                                                        the entorhinal cortex in the
                                                                                        ventricle. Where the entorhinal
                                                                                        cortex inputs to the dentata
                                                                                        gyrus and plays an important
                                                                                        role in pattern recognition and
                                                                                        encoding of memories. CA1
                                                                                        contains pyrimidal cells which
                                                                                        play a role in matching and
                                                                                        mismatching incoming information
                                                                                        from CA3. Image Source: Ammon's
                                                                                        Horn 2 (CA2) of the Hippocampus:
                                                                                        A Long-Known Region with a New
                                                                                        Potential Role in
                                                                                        Neurodegeneration $^{[3]}$
                                                                                        Dentate Gyrus To continue.
                                                                                        Glossary Limbic System The
                                                                                        limbic system is a part of the
                                                                                        primitive brain and its primary
                                                                                        functions concern hunger,
                                                                                        motivation, sex drive, mood,
                                                                                        pain, pleasure, appetite, and
                                                                                        memory. Pyramidal Neurons The
                                                                                        pyramidal neuron, named after
                                                                                        the conic-shaped soma, is a
                                                                                        multipolar neuron and is the
                                                                                        primary excitation unit of the
                                                                                        mammalian prefrontal cortex and
                                                                                        corticospinal tract. These
                                                                                        neurons are also found in the
                                                                                        hippocampus and amygdala.
                                                                                        Studies on pyramidal neurons
                                                                                        mainly include neuroplasticity
                                                                                        and cognition studies. Anatomy
                                                                                        and Histology of the Hippocampus
                                                                                        Section Reference: Cliques and
                                                                                        cavities in the human connectome
                                                                                        by Ann E. Sizemore, Chad Giusti,
                                                                                        Ari Kahn, Jean M. Vettel,
                                                                                        Richard F. Betzel, and Danielle
                                                                                        S. Bassett $^{[1]}$ Introduction
                                                                                        The hippocampus is distinguished
                                                                                        externally as a layer of densely
                                                                                        packed neurons that form a
                                                                                        S-shaped structure and extends
                                                                                        to the temporal lobe of the
                                                                                        cerebral cortex. It is also a is
                                                                                        a sub-cortical structure in the
                                                                                        limbic lobe, and contains two
                                                                                        parts: cornu ammonis and dentate
                                                                                        gyrus, where the hippocampal
                                                                                        sulcus separates both parts. The
                                                                                        parts curve into each other and
                                                                                        below the sulcus lies the
                                                                                        subiculum. Since the hippocampus
                                                                                        is a part of the allocortex or
                                                                                        archicortex, there exists a zone
                                                                                        that separates the hippocampus
                                                                                        from the neocortex. The
                                                                                        hippocampal set is the entire
                                                                                        set of hippocampal formation.
                                                                                        This formation is comprised by
                                                                                        some anatomists into four main
                                                                                        sections: Hippocampus proper,
                                                                                        dentate gyrus DG, subiculum, and
                                                                                        the entorhinal area EC. This
                                                                                        entire set of hippocampus zones
                                                                                        is called the hippocampal
                                                                                        formation. The hippocampus is
                                                                                        supplied blood by the posterior
                                                                                        Cerebral artery and is covered
                                                                                        by the choroid plexus.
                                                                                        Furthermore, the hippocampus
                                                                                        lies posterior to the limbic
                                                                                        system, while the anterior is
                                                                                        the amygdala. In adult humans,
                                                                                        the volume of the hippocampus in
                                                                                        each hemisphere is $\sim 3-3.5
                                                                                        \mathrm{~cm}^3$, where, relative
                                                                                        to the volume of the neocortex
                                                                                        $\sim 320-420 \mathrm{~cm}^3$ is
                                                                                        $100$ times smaller compared to
                                                                                        the cerebral cortex. The general
                                                                                        function of the hippocampus is
                                                                                        organizing emotional responses,
                                                                                        which are expressed in the
                                                                                        cingulate gyrus via mammillary
                                                                                        bodies. Further functions
                                                                                        included recollecting experience
                                                                                        and imagining the future,
                                                                                        playing a role in learning,
                                                                                        memory, and spatial navigation.
                                                                                        Cornu Ammonis Image Source:
                                                                                        Neurosurgery written board crash
                                                                                        course - Hippocampus $^{[2]}$
                                                                                        Anatomy and Histology Histology
                                                                                        tells us that the hippocampus
                                                                                        proper or Cornus Ammonis is
                                                                                        divided into four histological
                                                                                        domains: CA1, CA2, CA3, and CA4.
                                                                                        Across from the CA1 is the
                                                                                        subiculum, which is a formation
                                                                                        connecting the hippocampus to
                                                                                        the entorhinal cortex in the
                                                                                        ventricle. Where the entorhinal
                                                                                        cortex inputs to the dentata
                                                                                        gyrus and plays an important
                                                                                        role in pattern recognition and
                                                                                        encoding of memories. CA1
                                                                                        contains pyrimidal cells which
                                                                                        play a role in matching and
                                                                                        mismatching incoming information
                                                                                        from CA3. Image Source: Ammon's
                                                                                        Horn 2 (CA2) of the Hippocampus:
                                                                                        A Long-Known Region with a New
                                                                                        Potential Role in
                                                                                        Neurodegeneration $^{[3]}$
                                                                                        Dentate Gyrus To continue.
                                                                                        Glossary Limbic System The
                                                                                        limbic system is a part of the
                                                                                        primitive brain and its primary
                                                                                        functions concern hunger,
                                                                                        motivation, sex drive, mood,
                                                                                        pain, pleasure, appetite, and
                                                                                        memory. Pyramidal Neurons The
                                                                                        pyramidal neuron, named after
                                                                                        the conic-shaped soma, is a
                                                                                        multipolar neuron and is the
                                                                                        primary excitation unit of the
                                                                                        mammalian prefrontal cortex and
                                                                                        corticospinal tract. These
                                                                                        neurons are also found in the
                                                                                        hippocampus and amygdala.
                                                                                        Studies on pyramidal neurons
                                                                                        mainly include neuroplasticity
                                                                                        and cognition studies. Anatomy
                                                                                        and Histology of the
                                                                                        HippocampusSection Reference:
                                                                                        Cliques and cavities in the
                                                                                        human connectome by Ann E.
                                                                                        Sizemore, Chad Giusti, Ari Kahn,
                                                                                        Jean M. Vettel, Richard F.
                                                                                        Betzel, and Danielle S. Bassett
                                                                                        $^{[1]}$ Cliques and cavities in
                                                                                        the human connectome by Ann E.
                                                                                        Sizemore, Chad Giusti, Ari Kahn,
                                                                                        Jean M. Vettel, Richard F.
                                                                                        Betzel, and Danielle S. Bassett
                                                                                        $^{[1]}$ Cliques and cavities in
                                                                                        the human connectome by Ann E.
                                                                                        Sizemore, Chad Giusti, Ari Kahn,
                                                                                        Jean M. Vettel, Richard F.
                                                                                        Betzel, and Danielle S. Bassett
                                                                                        Introduction The hippocampus is
                                                                                        distinguished externally as a
                                                                                        layer of densely packed neurons
                                                                                        that form a S-shaped structure
                                                                                        and extends to the temporal lobe
                                                                                        of the cerebral cortex. It is
                                                                                        also a is a sub-cortical
                                                                                        structure in the limbic lobe,
                                                                                        and contains two parts: cornu
                                                                                        ammonis and dentate gyrus, where
                                                                                        the hippocampal sulcus separates
                                                                                        both parts. The parts curve into
                                                                                        each other and below the sulcus
                                                                                        lies the subiculum. Since the
                                                                                        hippocampus is a part of the
                                                                                        allocortex or archicortex, there
                                                                                        exists a zone that separates the
                                                                                        hippocampus from the neocortex.
                                                                                        The hippocampal set is the
                                                                                        entire set of hippocampal
                                                                                        formation. This formation is
                                                                                        comprised by some anatomists
                                                                                        into four main sections:
                                                                                        Hippocampus proper, dentate
                                                                                        gyrus DG, subiculum, and the
                                                                                        entorhinal area EC. This entire
                                                                                        set of hippocampus zones is
                                                                                        called the hippocampal
                                                                                        formation. The hippocampus is
                                                                                        supplied blood by the posterior
                                                                                        Cerebral artery and is covered
                                                                                        by the choroid plexus.
                                                                                        Furthermore, the hippocampus
                                                                                        lies posterior to the limbic
                                                                                        system, while the anterior is
                                                                                        the amygdala. In adult humans,
                                                                                        the volume of the hippocampus in
                                                                                        each hemisphere is $\sim 3-3.5
                                                                                        \mathrm{~cm}^3$, where, relative
                                                                                        to the volume of the neocortex
                                                                                        $\sim 320-420 \mathrm{~cm}^3$ is
                                                                                        $100$ times smaller compared to
                                                                                        the cerebral cortex. The general
                                                                                        function of the hippocampus is
                                                                                        organizing emotional responses,
                                                                                        which are expressed in the
                                                                                        cingulate gyrus via mammillary
                                                                                        bodies. Further functions
                                                                                        included recollecting experience
                                                                                        and imagining the future,
                                                                                        playing a role in learning,
                                                                                        memory, and spatial navigation.
                                                                                        IntroductionThe hippocampus is
                                                                                        distinguished externally as a
                                                                                        layer of densely packed neurons
                                                                                        that form a S-shaped structure
                                                                                        and extends to the temporal lobe
                                                                                        of the cerebral cortex. It is
                                                                                        also a is a sub-cortical
                                                                                        structure in the limbic lobe,
                                                                                        and contains two parts: cornu
                                                                                        ammonis and dentate gyrus, where
                                                                                        the hippocampal sulcus separates
                                                                                        both parts. The parts curve into
                                                                                        each other and below the sulcus
                                                                                        lies the subiculum. Since the
                                                                                        hippocampus is a part of the
                                                                                        allocortex or archicortex, there
                                                                                        exists a zone that separates the
                                                                                        hippocampus from the neocortex.
                                                                                        The hippocampal set is the
                                                                                        entire set of hippocampal
                                                                                        formation. This formation is
                                                                                        comprised by some anatomists
                                                                                        into four main sections:
                                                                                        Hippocampus proper, dentate
                                                                                        gyrus DG, subiculum, and the
                                                                                        entorhinal area EC. This entire
                                                                                        set of hippocampus zones is
                                                                                        called the hippocampal
                                                                                        formation. The hippocampus is
                                                                                        supplied blood by the posterior
                                                                                        Cerebral artery and is covered
                                                                                        by the choroid plexus.
                                                                                        limbiccornu ammonis dentate
                                                                                        gyrusFurthermore, the
                                                                                        hippocampus lies posterior to
                                                                                        the limbic system, while the
                                                                                        anterior is the amygdala. In
                                                                                        adult humans, the volume of the
                                                                                        hippocampus in each hemisphere
                                                                                        is $\sim 3-3.5 \mathrm{~cm}^3$,
                                                                                        where, relative to the volume of
                                                                                        the neocortex $\sim 320-420
                                                                                        \mathrm{~cm}^3$ is $100$ times
                                                                                        smaller compared to the cerebral
                                                                                        cortex. The general function of
                                                                                        the hippocampus is organizing
                                                                                        emotional responses, which are
                                                                                        expressed in the cingulate gyrus
                                                                                        via mammillary bodies. Further
                                                                                        functions included recollecting
                                                                                        experience and imagining the
                                                                                        future, playing a role in
                                                                                        learning, memory, and spatial
                                                                                        navigation. Cornu Ammonis Image
                                                                                        Source: Neurosurgery written
                                                                                        board crash course - Hippocampus
                                                                                        $^{[2]}$ Anatomy and Histology
                                                                                        Histology tells us that the
                                                                                        hippocampus proper or Cornus
                                                                                        Ammonis is divided into four
                                                                                        histological domains: CA1, CA2,
                                                                                        CA3, and CA4. Across from the
                                                                                        CA1 is the subiculum, which is a
                                                                                        formation connecting the
                                                                                        hippocampus to the entorhinal
                                                                                        cortex in the ventricle. Where
                                                                                        the entorhinal cortex inputs to
                                                                                        the dentata gyrus and plays an
                                                                                        important role in pattern
                                                                                        recognition and encoding of
                                                                                        memories. CA1 contains pyrimidal
                                                                                        cells which play a role in
                                                                                        matching and mismatching
                                                                                        incoming information from CA3.
                                                                                        Image Source: Ammon's Horn 2
                                                                                        (CA2) of the Hippocampus: A
                                                                                        Long-Known Region with a New
                                                                                        Potential Role in
                                                                                        Neurodegeneration $^{[3]}$ Cornu
                                                                                        AmmonisImage Source:
                                                                                        Neurosurgery written board crash
                                                                                        course - Hippocampus $^{[2]}$
                                                                                        Neurosurgery written board crash
                                                                                        course - Hippocampus $^{[2]}$
                                                                                        Neurosurgery written board crash
                                                                                        course - Hippocampus Anatomy and
                                                                                        Histology Histology tells us
                                                                                        that the hippocampus proper or
                                                                                        Cornus Ammonis is divided into
                                                                                        four histological domains: CA1,
                                                                                        CA2, CA3, and CA4. Across from
                                                                                        the CA1 is the subiculum, which
                                                                                        is a formation connecting the
                                                                                        hippocampus to the entorhinal
                                                                                        cortex in the ventricle. Where
                                                                                        the entorhinal cortex inputs to
                                                                                        the dentata gyrus and plays an
                                                                                        important role in pattern
                                                                                        recognition and encoding of
                                                                                        memories. CA1 contains pyrimidal
                                                                                        cells which play a role in
                                                                                        matching and mismatching
                                                                                        incoming information from CA3.
                                                                                        Image Source: Ammon's Horn 2
                                                                                        (CA2) of the Hippocampus: A
                                                                                        Long-Known Region with a New
                                                                                        Potential Role in
                                                                                        Neurodegeneration $^{[3]}$
                                                                                        Anatomy and HistologyHistology
                                                                                        tells us that the hippocampus
                                                                                        proper or Cornus Ammonis is
                                                                                        divided into four histological
                                                                                        domains: CA1, CA2, CA3, and CA4.
                                                                                        Across from the CA1 is the
                                                                                        subiculum, which is a formation
                                                                                        connecting the hippocampus to
                                                                                        the entorhinal cortex in the
                                                                                        ventricle. Where the entorhinal
                                                                                        cortex inputs to the dentata
                                                                                        gyrus and plays an important
                                                                                        role in pattern recognition and
                                                                                        encoding of memories. CA1
                                                                                        contains pyrimidal cells which
                                                                                        play a role in matching and
                                                                                        mismatching incoming information
                                                                                        from CA3. pyrimidalImage Source:
                                                                                        Ammon's Horn 2 (CA2) of the
                                                                                        Hippocampus: A Long-Known Region
                                                                                        with a New Potential Role in
                                                                                        Neurodegeneration $^{[3]}$
                                                                                        Ammon's Horn 2 (CA2) of the
                                                                                        Hippocampus: A Long-Known Region
                                                                                        with a New Potential Role in
                                                                                        Neurodegeneration $^{[3]}$
                                                                                        Ammon's Horn 2 (CA2) of the
                                                                                        Hippocampus: A Long-Known Region
                                                                                        with a New Potential Role in
                                                                                        Neurodegeneration Dentate Gyrus
                                                                                        To continue. Dentate GyrusTo
                                                                                        continue. Glossary Limbic System
                                                                                        The limbic system is a part of
                                                                                        the primitive brain and its
                                                                                        primary functions concern
                                                                                        hunger, motivation, sex drive,
                                                                                        mood, pain, pleasure, appetite,
                                                                                        and memory. Pyramidal Neurons
                                                                                        The pyramidal neuron, named
                                                                                        after the conic-shaped soma, is
                                                                                        a multipolar neuron and is the
                                                                                        primary excitation unit of the
                                                                                        mammalian prefrontal cortex and
                                                                                        corticospinal tract. These
                                                                                        neurons are also found in the
                                                                                        hippocampus and amygdala.
                                                                                        Studies on pyramidal neurons
                                                                                        mainly include neuroplasticity
                                                                                        and cognition studies.
                                                                                        GlossaryLimbic System The limbic
                                                                                        system is a part of the
                                                                                        primitive brain and its primary
                                                                                        functions concern hunger,
                                                                                        motivation, sex drive, mood,
                                                                                        pain, pleasure, appetite, and
                                                                                        memory. Limbic SystemThe limbic
                                                                                        system is a part of the
                                                                                        primitive brain and its primary
                                                                                        functions concern hunger,
                                                                                        motivation, sex drive, mood,
                                                                                        pain, pleasure, appetite, and
                                                                                        memory. Pyramidal Neurons The
                                                                                        pyramidal neuron, named after
                                                                                        the conic-shaped soma, is a
                                                                                        multipolar neuron and is the
                                                                                        primary excitation unit of the
                                                                                        mammalian prefrontal cortex and
                                                                                        corticospinal tract. These
                                                                                        neurons are also found in the
                                                                                        hippocampus and amygdala.
                                                                                        Studies on pyramidal neurons
                                                                                        mainly include neuroplasticity
                                                                                        and cognition studies. Pyramidal
                                                                                        NeuronsThe pyramidal neuron,
                                                                                        named after the conic-shaped
                                                                                        soma, is a multipolar neuron and
                                                                                        is the primary excitation unit
                                                                                        of the mammalian prefrontal
                                                                                        cortex and corticospinal tract.
                                                                                        These neurons are also found in
                                                                                        the hippocampus and amygdala.
                                                                                        Studies on pyramidal neurons
                                                                                        mainly include neuroplasticity
                                                                                        and cognition studies.
                                                                                        [https://www.contextswitching.org/math]
                                                                                        Mathematics - Context Switching
                                                                                        Mathematics Chern Classes Chern
                                                                                        classes are a part of algebraic
                                                                                        topology, as well as other math
                                                                                        groups, and are characteristic
                                                                                        classes related to complex
                                                                                        vector bundles. Let $X$ be a
                                                                                        topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more Abstract Algebra Abstract
                                                                                        Algebra or modern algebra can be
                                                                                        defined as the theory of
                                                                                        algebraic structures. For the
                                                                                        most part, abstract algebra
                                                                                        deals with four algebraic
                                                                                        structures: groups, rings,
                                                                                        fields, and vector spaces. We
                                                                                        will look at and examine these
                                                                                        four algebraic strucutres in
                                                                                        this page. The three most
                                                                                        commonly studied algebraic...
                                                                                        read more Hidden Markov
                                                                                        Processes Let us first start
                                                                                        with a formal definition of a 2
                                                                                        vector convex combination. Then
                                                                                        we will break down the
                                                                                        definition into parts and
                                                                                        analyze the definition. Then we
                                                                                        will formally define and analyze
                                                                                        a convex combination with a
                                                                                        finite number of vectors in the
                                                                                        same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more Differential Manifolds
                                                                                        An $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more Graph Theory $G =
                                                                                        (V, E)$ $V$ is a set of vertices
                                                                                        $E \subseteq \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more ...
                                                                                        And More Soon! Mathematics -
                                                                                        Context Switching Mathematics -
                                                                                        Context SwitchingMathematics
                                                                                        Chern Classes Chern classes are
                                                                                        a part of algebraic topology, as
                                                                                        well as other math groups, and
                                                                                        are characteristic classes
                                                                                        related to complex vector
                                                                                        bundles. Let $X$ be a
                                                                                        topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more Abstract Algebra Abstract
                                                                                        Algebra or modern algebra can be
                                                                                        defined as the theory of
                                                                                        algebraic structures. For the
                                                                                        most part, abstract algebra
                                                                                        deals with four algebraic
                                                                                        structures: groups, rings,
                                                                                        fields, and vector spaces. We
                                                                                        will look at and examine these
                                                                                        four algebraic strucutres in
                                                                                        this page. The three most
                                                                                        commonly studied algebraic...
                                                                                        read more Hidden Markov
                                                                                        Processes Let us first start
                                                                                        with a formal definition of a 2
                                                                                        vector convex combination. Then
                                                                                        we will break down the
                                                                                        definition into parts and
                                                                                        analyze the definition. Then we
                                                                                        will formally define and analyze
                                                                                        a convex combination with a
                                                                                        finite number of vectors in the
                                                                                        same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more Differential Manifolds
                                                                                        An $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more Graph Theory $G =
                                                                                        (V, E)$ $V$ is a set of vertices
                                                                                        $E \subseteq \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more ...
                                                                                        And More Soon! Mathematics
                                                                                        Mathematics Mathematics Chern
                                                                                        Classes Chern Classes Chern
                                                                                        Classes Chern classes are a part
                                                                                        of algebraic topology, as well
                                                                                        as other math groups, and are
                                                                                        characteristic classes related
                                                                                        to complex vector bundles. Let
                                                                                        $X$ be a topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more Chern classes are a part of
                                                                                        algebraic topology, as well as
                                                                                        other math groups, and are
                                                                                        characteristic classes related
                                                                                        to complex vector bundles. Let
                                                                                        $X$ be a topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more Chern classes are a part of
                                                                                        algebraic topology, as well as
                                                                                        other math groups, and are
                                                                                        characteristic classes related
                                                                                        to complex vector bundles. Let
                                                                                        $X$ be a topological space of
                                                                                        closure-finite weak CW complex
                                                                                        and let $V$ be a line bundle.
                                                                                        The first chern class is the
                                                                                        only nontrivial Chern class and
                                                                                        is an element of the second
                                                                                        cohomology group of $X$.... read
                                                                                        more read more Abstract Algebra
                                                                                        Abstract Algebra Abstract
                                                                                        Algebra Abstract Algebra or
                                                                                        modern algebra can be defined as
                                                                                        the theory of algebraic
                                                                                        structures. For the most part,
                                                                                        abstract algebra deals with four
                                                                                        algebraic structures: groups,
                                                                                        rings, fields, and vector
                                                                                        spaces. We will look at and
                                                                                        examine these four algebraic
                                                                                        strucutres in this page. The
                                                                                        three most commonly studied
                                                                                        algebraic... read more Abstract
                                                                                        Algebra or modern algebra can be
                                                                                        defined as the theory of
                                                                                        algebraic structures. For the
                                                                                        most part, abstract algebra
                                                                                        deals with four algebraic
                                                                                        structures: groups, rings,
                                                                                        fields, and vector spaces. We
                                                                                        will look at and examine these
                                                                                        four algebraic strucutres in
                                                                                        this page. The three most
                                                                                        commonly studied algebraic...
                                                                                        read more Abstract Algebra or
                                                                                        modern algebra can be defined as
                                                                                        the theory of algebraic
                                                                                        structures. For the most part,
                                                                                        abstract algebra deals with four
                                                                                        algebraic structures: groups,
                                                                                        rings, fields, and vector
                                                                                        spaces. We will look at and
                                                                                        examine these four algebraic
                                                                                        strucutres in this page. The
                                                                                        three most commonly studied
                                                                                        algebraic... read more read more
                                                                                        Hidden Markov Processes Hidden
                                                                                        Markov Processes Hidden Markov
                                                                                        Processes Let us first start
                                                                                        with a formal definition of a 2
                                                                                        vector convex combination. Then
                                                                                        we will break down the
                                                                                        definition into parts and
                                                                                        analyze the definition. Then we
                                                                                        will formally define and analyze
                                                                                        a convex combination with a
                                                                                        finite number of vectors in the
                                                                                        same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more Let us first start
                                                                                        with a formal definition of a 2
                                                                                        vector convex combination. Then
                                                                                        we will break down the
                                                                                        definition into parts and
                                                                                        analyze the definition. Then we
                                                                                        will formally define and analyze
                                                                                        a convex combination with a
                                                                                        finite number of vectors in the
                                                                                        same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more Let us first start
                                                                                        with a formal definition of a 2
                                                                                        vector convex combination. Then
                                                                                        we will break down the
                                                                                        definition into parts and
                                                                                        analyze the definition. Then we
                                                                                        will formally define and analyze
                                                                                        a convex combination with a
                                                                                        finite number of vectors in the
                                                                                        same manner. A subset $S
                                                                                        \subseteq \mathbb{R}^{n}$...
                                                                                        read more read more Differential
                                                                                        Manifolds Differential Manifolds
                                                                                        Differential Manifolds An
                                                                                        $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more An
                                                                                        $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more An
                                                                                        $n$-dimensional manifold is a
                                                                                        topological space where each
                                                                                        point has a neighborhood that is
                                                                                        homeomorphic to an open subset
                                                                                        on $n$-dimensional Euclidean
                                                                                        space. Let $M$ be a topological
                                                                                        space. A chart in $M$ consists
                                                                                        of an open subset $U \subset M$
                                                                                        and a homomorphism $h$ of $U$
                                                                                        onto an open subset of $R^{m}$.
                                                                                        A... read more read more Graph
                                                                                        Theory Graph Theory Graph Theory
                                                                                        $G = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more $G
                                                                                        = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more $G
                                                                                        = (V, E)$ $V$ is a set of
                                                                                        vertices $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$ A simple
                                                                                        undirected graph $G$ is an
                                                                                        ordered pair or tuple $(V, E)$
                                                                                        where $V$ and $E$ are finite
                                                                                        sets. $E \subseteq
                                                                                        \left\{\left\{x,
                                                                                        y\right\}\;|\;x, y \in V\;and\;x
                                                                                        \neq y\right\}$... read more
                                                                                        read more ... And More Soon! ...
                                                                                        And More Soon! ... And More
                                                                                        Soon!
                                                                                    </body>

                                                                                    </html>